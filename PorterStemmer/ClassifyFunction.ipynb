{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e719da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d8661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStemPost(post):\n",
    "    urls = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n",
    "    numbers = re.compile(r'\\d+(\\.\\d+)?')\n",
    "    posessivePronouns = re.compile(r\"’s\")\n",
    "    apostrophe=re.compile(r\"’\")\n",
    "    someSigns =re.compile(r\"\\\\n|\\\\r\")\n",
    "    punctuation = re.compile(r\"[^\\w\\s]\")\n",
    "    whitespaces = re.compile(r'\\s+')\n",
    "    leadTrailWhitespace = re.compile(r'^\\s+|\\s+?$')\n",
    "    \n",
    "    cleanPost = post.lower()\n",
    "    cleanPost = urls.sub('url',cleanPost)\n",
    "    cleanPost = numbers.sub('nmbr',cleanPost)\n",
    "    cleanPost = posessivePronouns.sub('',cleanPost)\n",
    "    cleanPost = apostrophe.sub('',cleanPost)\n",
    "    cleanPost = someSigns.sub('',cleanPost)\n",
    "    cleanPost = punctuation.sub(' ',cleanPost)\n",
    "    cleanPost = whitespaces.sub(' ',cleanPost)\n",
    "    cleanPost = leadTrailWhitespace.sub('',cleanPost)\n",
    "    \n",
    "    ps = nltk.PorterStemmer()\n",
    "    \n",
    "    # Create an empty list containing Stemmed words\n",
    "    stemmed_list = []\n",
    "    \n",
    "    post_words = cleanPost.split(\" \")\n",
    "    \n",
    "    # Iterate through every word to Stem\n",
    "    for word in post_words:\n",
    "        stemmed_list.append(ps.stem(word))\n",
    "        \n",
    "    # Join the list\n",
    "    stemmed_post = \" \".join(stemmed_list)\n",
    "    \n",
    "    cleanPost = stemmed_post\n",
    "    \n",
    "    # stop words\n",
    "    with open('./../stop_words_no_punct.data', 'rb') as filehandle:\n",
    "        # read the data as binary data stream\n",
    "        stop_words_no_punct = pickle.load(filehandle)\n",
    "        \n",
    "    post_without_stop_words = []\n",
    "\n",
    "    text_tokens = word_tokenize(cleanPost)\n",
    "    tokens_without_stop_words = [word for word in text_tokens if not word in stop_words_no_punct]\n",
    "    post_without_stop_words = (\" \").join(tokens_without_stop_words)\n",
    "\n",
    "    cleanPost = post_without_stop_words\n",
    "    \n",
    "    return cleanPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ive alway mutual time grow never proper close friend let alon one person feel mutual first choic start last year saw almost everi singl day knew everyth best friend even anxieti fill obsess overthink brain could know sure know happen know wrong start hang thi group pretti popular kid realli racist big bulli slowli start chang could tell want hang anymor pain wa let happen ask come anymor text unnecessarili becaus last thing want annoy hard know wrong miss close want like\n"
     ]
    }
   ],
   "source": [
    "testPost = \"Ive always had a few mutuals at a time growing up, but never a proper close friend. Let alone that one person who feels that mutual “you are my first choice” for. Until the start of last year :) I saw her almost every single day and we knew everything about each other. We were both each others best friend and even my anxiety filled, obsessive, overthinking brain could know that for sure. I dont know what happened. I dont know what i did wrong. She started hanging out with this group of “pretty popular kids” who are really racist and big bullies. Slowly she started changing, and i could tell she just didnt want to hang out with me anymore. As painful as it was i let it happen and didnt ask to come over anymore or text her unnecessarily because the last thing i want to do is annoy her. Its hard not knowing what i did wrong and i miss being close to her. I just want her to like me again.\"\n",
    "parsedPost = cleanStemPost(testPost)\n",
    "print(parsedPost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759bfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(post):\n",
    "    words = word_tokenize(post)\n",
    "    features = {}\n",
    "    \n",
    "    with open('word_features.data', 'rb') as filehandle:\n",
    "        # read the data as binary data stream\n",
    "        word_features = pickle.load(filehandle)\n",
    "    \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1308a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformSubreddit(x):\n",
    "    switcher={\n",
    "            0:'depression',\n",
    "            1:'unpopularopinion',\n",
    "            2:'lonely',\n",
    "            3:'MachineLearning',\n",
    "         }\n",
    "    return switcher.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139e7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDepression(x):\n",
    "    switcher={\n",
    "            0:'r/depression',\n",
    "            1:'not r/depression'\n",
    "         }\n",
    "    return switcher.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430e9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySubreddit(post):\n",
    "    modelFile = './Models/finalized_model_subreddits.sav'\n",
    "    post = cleanStemPost(post)\n",
    "    \n",
    "    loaded_model = pickle.load(open(modelFile, 'rb'))\n",
    "    \n",
    "    featureset = find_features(post)\n",
    "    \n",
    "    result = loaded_model.classify(featureset)\n",
    "        \n",
    "    return transformSubreddit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8cf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyDepression(post):\n",
    "    modelFile = './Models/finalized_model_is_depression.sav'\n",
    "    post = cleanStemPost(post)\n",
    "    \n",
    "    loaded_model = pickle.load(open(modelFile, 'rb'))\n",
    "    \n",
    "    featureset = find_features(post)\n",
    "    \n",
    "    result = loaded_model.classify(featureset)\n",
    "        \n",
    "    return transformDepression(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757d1ad",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8df8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lonely\n"
     ]
    }
   ],
   "source": [
    "result = classifySubreddit(testPost)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40679b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not r/depression\n"
     ]
    }
   ],
   "source": [
    "result = classifyDepression(testPost)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f799c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
