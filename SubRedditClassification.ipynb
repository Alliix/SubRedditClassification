{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba2efbb",
   "metadata": {},
   "source": [
    "###  1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab7b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "ps = nltk.PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852d54d",
   "metadata": {},
   "source": [
    "### 2. Define functions to get subreddit posts data from Reddit and write it to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c75d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Scraper 1.0 by /u/Alliix\"\n",
    "reddit = praw.Reddit(\n",
    "  client_id=os.environ['CLIENT_ID'],\n",
    "  client_secret=os.environ['CLIENT_SECRET'],\n",
    "  user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c5bdd239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPostsData(postsFromCsv):\n",
    "#     clean posts data\n",
    "    postsProcessed = []\n",
    "\n",
    "#     RegEx\n",
    "    zeroSpaceWidth = re.compile(r'&#x200B')\n",
    "    urls = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', re.IGNORECASE)\n",
    "    numbers = re.compile(r' \\d+(\\.\\d+)? ')\n",
    "    punctuation = re.compile(r'[^\\w\\d\\s]')\n",
    "    whitespaces = re.compile(r'\\s+')\n",
    "    leadTrailWhitespace = re.compile(r'^\\s+|\\s+?$')\n",
    "\n",
    "    for d in postsFromCsv.Post:\n",
    "        if(type(d)==str):\n",
    "            x = d\n",
    "            # Replace zero width space with ' '\n",
    "            x = zeroSpaceWidth.sub(' ',x)\n",
    "            # Replace URLs with 'url'\n",
    "            x = urls.sub('url',x)\n",
    "            # Replace numbers with 'nmbr'\n",
    "            x = numbers.sub('nmbr',x)\n",
    "            # Remove punctuation\n",
    "            x = punctuation.sub(' ',x)\n",
    "            # Replace whitespace between terms with ' '\n",
    "            x = whitespaces.sub(' ',x)\n",
    "            # Remove leading and trailing whitespace\n",
    "            x = leadTrailWhitespace.sub(' ',x)\n",
    "\n",
    "            text_tokens = word_tokenize(x)\n",
    "            # Remove word stems using a Porter stemmer\n",
    "            tokens_without_ws = [ps.stem(word) for word in text_tokens]\n",
    "            x = (\" \").join(tokens_without_ws)\n",
    "            \n",
    "            # remove stop words from posts\n",
    "            text_tokens = word_tokenize(x)\n",
    "            tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "            x = (\" \").join(tokens_without_sw)\n",
    "            postsProcessed.append(x)\n",
    "        else: \n",
    "            postsProcessed.append(d)\n",
    "            \n",
    "    return postsProcessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4dd29471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAndCleanPostsData(subreddit, postOutputFile, processedPostOutputFile):\n",
    "#     get\n",
    "    posts = set()\n",
    "    for submission in reddit.subreddit(subreddit).new(limit=None):\n",
    "        posts.add(submission)\n",
    "    postsLength = len(posts)\n",
    "    \n",
    "#     write posts to file\n",
    "    data = []\n",
    "    for post in posts:\n",
    "        data.append({'Author': post.author, 'Subreddit': post.subreddit, 'Date': post.created_utc, 'Title': post.title, 'Post': post.selftext})    \n",
    "    postsData = pd.DataFrame(data, columns=['Author', 'Subreddit', 'Date', 'Title', 'Post'])\n",
    "    postsData.to_csv(postOutputFile, index=False)\n",
    "    \n",
    "    postsFromCsv = pd.read_csv(postOutputFile)\n",
    "    postsProcessed = cleanPostsData(postsFromCsv)\n",
    "    \n",
    "#     write processed posts data to file\n",
    "    processedData = []\n",
    "    for i in range(0, postsLength):\n",
    "        processedData.append({'Author': postsFromCsv.Author[i], 'Subreddit': postsFromCsv.Subreddit[i], 'Date': postsFromCsv.Date[i], 'Title': postsFromCsv.Title[i], 'Post': postsProcessed[i]})\n",
    "\n",
    "    postsData = pd.DataFrame(processedData, columns=['Author', 'Subreddit', 'Date', 'Title', 'Post'])\n",
    "    postsData.to_csv(processedPostOutputFile, index=False)\n",
    "    \n",
    "    return postsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e14481",
   "metadata": {},
   "source": [
    "### 3. Run the functions to get data from subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5132adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "# r/depression\n",
    "depressedPostsLength = getAndCleanPostsData('depression', 'depressed_posts.csv', 'depressed_posts_processed.csv')\n",
    "print(depressedPostsLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "099245ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n"
     ]
    }
   ],
   "source": [
    "# r/unpopularopinion\n",
    "unpopularopinionPostsLength = getAndCleanPostsData('unpopularopinion', 'unpopularopinion_posts.csv', 'unpopularopinion_posts_processed.csv')\n",
    "print(unpopularopinionPostsLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3298b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "# r/lonely\n",
    "lonelyPostsLength = getAndCleanPostsData('lonely', 'lonely_posts.csv', 'lonely_posts_processed.csv')\n",
    "print(lonelyPostsLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d97b7abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n"
     ]
    }
   ],
   "source": [
    "# r/MachineLearning\n",
    "machinelearningPostsLength = getAndCleanPostsData('machinelearning', 'machinelearning_posts.csv', 'machinelearning_posts_processed.csv')\n",
    "print(machinelearningPostsLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6c7cb63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>username123456111111</td>\n",
       "      <td>depression</td>\n",
       "      <td>1.620596e+09</td>\n",
       "      <td>i just want to time travel back and fix my mis...</td>\n",
       "      <td>english second languag sorri made mistak watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iSlurpees</td>\n",
       "      <td>depression</td>\n",
       "      <td>1.620602e+09</td>\n",
       "      <td>Is this depression</td>\n",
       "      <td>felt top world friday night everyth wa go feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awkward-Cat98</td>\n",
       "      <td>depression</td>\n",
       "      <td>1.620643e+09</td>\n",
       "      <td>White hair due to stress and depression</td>\n",
       "      <td>hi everyon realli want hear anyon ha experi wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitkatpaddywat</td>\n",
       "      <td>depression</td>\n",
       "      <td>1.620593e+09</td>\n",
       "      <td>Everything is “getting back to normal”</td>\n",
       "      <td>make super depress noth go back job gone scare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sugarpopp</td>\n",
       "      <td>depression</td>\n",
       "      <td>1.620508e+09</td>\n",
       "      <td>Am I happy or am I delusional in my bubble?</td>\n",
       "      <td>25f gon na turnnmbron 15th may suffer sever de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Author   Subreddit          Date  \\\n",
       "0  username123456111111  depression  1.620596e+09   \n",
       "1             iSlurpees  depression  1.620602e+09   \n",
       "2         Awkward-Cat98  depression  1.620643e+09   \n",
       "3        kitkatpaddywat  depression  1.620593e+09   \n",
       "4             sugarpopp  depression  1.620508e+09   \n",
       "\n",
       "                                               Title  \\\n",
       "0  i just want to time travel back and fix my mis...   \n",
       "1                                 Is this depression   \n",
       "2            White hair due to stress and depression   \n",
       "3             Everything is “getting back to normal”   \n",
       "4        Am I happy or am I delusional in my bubble?   \n",
       "\n",
       "                                                Post  \n",
       "0  english second languag sorri made mistak watch...  \n",
       "1  felt top world friday night everyth wa go feel...  \n",
       "2  hi everyon realli want hear anyon ha experi wh...  \n",
       "3  make super depress noth go back job gone scare...  \n",
       "4  25f gon na turnnmbron 15th may suffer sever de...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv('depressed_posts_processed.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d0166",
   "metadata": {},
   "source": [
    "### 4. Generating Features function\n",
    "Features will be the most common words in posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c5cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFrequency(processedPostOutputFile):\n",
    "    posts = pd.read_csv(processedPostOutputFile)\n",
    "\n",
    "    allPostsConcat = ''\n",
    "    for post in posts.Post:\n",
    "        if(type(post)==str):\n",
    "            allPostsConcat+=post\n",
    "\n",
    "    # create bag-of-words\n",
    "    all_words = []\n",
    "\n",
    "    words = word_tokenize(allPostsConcat)\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a77bf4",
   "metadata": {},
   "source": [
    "### 5. Generate Word Frequency for Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "826b6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6766\n",
      "Most common words: [('feel', 1531), ('wa', 1403), ('thi', 1302), ('like', 1274), ('want', 918), ('get', 877), ('know', 824), ('becaus', 786), ('life', 740), ('depress', 673), ('even', 666), ('time', 639), ('go', 592), ('thing', 526), ('think', 507)]\n"
     ]
    }
   ],
   "source": [
    "# r/depression\n",
    "depressionFreqWords = getWordFrequency('depressed_posts_processed.csv')\n",
    "\n",
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(depressionFreqWords)))\n",
    "print('Most common words: {}'.format(depressionFreqWords.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9bc340fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 8630\n",
      "Most common words: [('peopl', 919), ('like', 889), ('thi', 795), ('get', 583), ('becaus', 495), ('make', 451), ('wa', 427), ('think', 410), ('one', 398), ('want', 355), ('say', 349), ('time', 347), ('thing', 340), ('even', 335), ('go', 309)]\n"
     ]
    }
   ],
   "source": [
    "# r/unpopularopinion\n",
    "unpopularopinionFreqWords = getWordFrequency('unpopularopinion_posts_processed.csv')\n",
    "\n",
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(unpopularopinionFreqWords)))\n",
    "print('Most common words: {}'.format(unpopularopinionFreqWords.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "76057e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6037\n",
      "Most common words: [('feel', 1133), ('like', 1084), ('wa', 1043), ('thi', 1002), ('friend', 987), ('want', 791), ('peopl', 751), ('get', 645), ('talk', 602), ('one', 592), ('know', 584), ('go', 572), ('time', 571), ('life', 546), ('becaus', 535)]\n"
     ]
    }
   ],
   "source": [
    "# r/lonely\n",
    "lonelyFreqWords = getWordFrequency('lonely_posts_processed.csv')\n",
    "\n",
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(lonelyFreqWords)))\n",
    "print('Most common words: {}'.format(lonelyFreqWords.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4db0e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 8454\n",
      "Most common words: [('url', 1820), ('thi', 1131), ('model', 942), ('use', 881), ('learn', 671), ('data', 573), ('train', 495), ('paper', 428), ('like', 403), ('work', 365), ('would', 362), ('ani', 350), ('imag', 302), ('one', 301), ('make', 300)]\n"
     ]
    }
   ],
   "source": [
    "# r/machinelearning\n",
    "machinelearningFreqWords = getWordFrequency('machinelearning_posts_processed.csv')\n",
    "\n",
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(machinelearningFreqWords)))\n",
    "print('Most common words: {}'.format(machinelearningFreqWords.most_common(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288aeefb",
   "metadata": {},
   "source": [
    "### 6. Generate features for all posts\n",
    "We will tokenize each word and will use the 2000 most common words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ad87d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all posts to one file\n",
    "\n",
    "depressedData = pd.read_csv('depressed_posts_processed.csv')\n",
    "unpopularopinionData = pd.read_csv('unpopularopinion_posts_processed.csv')\n",
    "lonelyData = pd.read_csv('lonely_posts_processed.csv')\n",
    "machinelearningData = pd.read_csv('machinelearning_posts_processed.csv')\n",
    "\n",
    "allPosts = depressedData.append(unpopularopinionData).append(lonelyData).append(machinelearningData)\n",
    "\n",
    "allPosts.to_csv('all_posts_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e666c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression          998\n",
      "unpopularopinion    988\n",
      "lonely              980\n",
      "MachineLearning     972\n",
      "Name: Subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check subreddit distribution\n",
    "\n",
    "subreddits = allPosts.Subreddit\n",
    "print(subreddits.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2dc2af4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 18661\n",
      "Most common words: [('thi', 4230), ('like', 3650), ('wa', 3139), ('feel', 3013), ('peopl', 2293), ('get', 2282), ('want', 2248), ('becaus', 1920), ('url', 1865), ('know', 1859), ('time', 1831), ('one', 1734), ('make', 1727), ('even', 1650), ('go', 1573), ('friend', 1541), ('would', 1500), ('life', 1497), ('use', 1475), ('think', 1450), ('thing', 1350), ('realli', 1297), ('tri', 1230), ('ha', 1154), ('work', 1136), ('day', 1109), ('ani', 1074), ('talk', 1072), ('onli', 1012), ('much', 993), ('someon', 965), ('say', 962), ('model', 958), ('never', 955), ('way', 922), ('good', 892), ('see', 889), ('need', 871), ('year', 863), ('depress', 840), ('also', 829), ('whi', 824), ('help', 819), ('learn', 819), ('someth', 805), ('person', 802), ('look', 789), ('love', 787), ('start', 785), ('live', 762), ('could', 760), ('still', 737), ('alway', 726), ('take', 717), ('anyon', 696), ('better', 687), ('find', 651), ('veri', 650), ('got', 638), ('thought', 626), ('anyth', 619), ('back', 618), ('point', 614), ('mani', 611), ('bad', 591), ('data', 590), ('lot', 588), ('end', 577), ('care', 560), ('everi', 545), ('come', 544), ('well', 544), ('train', 541), ('tell', 539), ('everyth', 537), ('fuck', 536), ('everyon', 534), ('sinc', 530), ('alon', 527), ('problem', 514), ('new', 513), ('noth', 512), ('long', 510), ('school', 506), ('first', 505), ('post', 498), ('happi', 494), ('seem', 483), ('hate', 479), ('right', 476), ('keep', 475), ('job', 473), ('ask', 466), ('hi', 461), ('hard', 452), ('around', 451), ('paper', 448), ('actual', 445), ('famili', 443), ('chang', 439)]\n"
     ]
    }
   ],
   "source": [
    "all_words_combined = getWordFrequency('all_posts_processed.csv')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(all_words_combined)))\n",
    "print('Most common words: {}'.format(all_words_combined.most_common(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c945a2",
   "metadata": {},
   "source": [
    "### 7. FindFeatures fuction\n",
    "function will determine which of the 1500 word features are contained in the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e016fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the 1500 most common words as features\n",
    "\n",
    "word_features = list(all_words_combined.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4448cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(post):\n",
    "    words = word_tokenize(post)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9fc6f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "go\n",
      "back\n",
      "keep\n",
      "life\n",
      "top\n",
      "world\n",
      "friday\n",
      "night\n",
      "everyth\n",
      "wa\n",
      "feel\n",
      "low\n",
      "put\n",
      "act\n",
      "whenev\n",
      "peopl\n",
      "let\n",
      "get\n",
      "walk\n",
      "care\n",
      "ask\n",
      "would\n",
      "come\n",
      "home\n",
      "drink\n",
      "stop\n",
      "becaus\n",
      "make\n",
      "parti\n",
      "hour\n",
      "small\n",
      "alon\n",
      "thought\n",
      "start\n",
      "felt\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "posts = pd.read_csv('all_posts_processed.csv')\n",
    "\n",
    "features = find_features(posts.Post[1])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13afe9df",
   "metadata": {},
   "source": [
    "### 6. Save training, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "postsArr = []\n",
    "i = 0 \n",
    "for post in posts.Post:\n",
    "    if(type(post)==str):\n",
    "        postsArr.append(post)\n",
    "        Y.append(posts.Subreddit[i])\n",
    "    i+=1\n",
    "    \n",
    "# Now find features for all posts\n",
    "posts_all = list(zip(postsArr, Y))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(posts_all)\n",
    "\n",
    "# call find_features function for each post\n",
    "featuresets = [(find_features(text), label) for (text, label) in posts_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "895e7c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938\n",
      "2849\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)\n",
    "\n",
    "print(len(posts))\n",
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d059661",
   "metadata": {},
   "source": [
    "### 7. Scikit-Learn Classifiers with NLTK\n",
    "Now that we have our dataset, we can start building algorithms! Let's start with a simple linear support vector classifier, then expand to other algorithms. We'll need to import each algorithm we plan on using from sklearn. We also need to import some performance metrics, such as accuracy_score and classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbac4b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 73.57894736842105\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f266215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 39.26315789473684\n",
      "Decision Tree Accuracy: 60.94736842105263\n",
      "Random Forest Accuracy: 75.68421052631578\n",
      "Logistic Regression Accuracy: 76.52631578947368\n",
      "SGD Classifier Accuracy: 74.94736842105263\n",
      "Naive Bayes Accuracy: 79.78947368421052\n",
      "SVM Linear Accuracy: 73.57894736842105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dccb80a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 73.57894736842105\n"
     ]
    }
   ],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea70d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08977b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " MachineLearning       0.86      0.92      0.89       224\n",
      "      depression       0.72      0.72      0.72       236\n",
      "          lonely       0.71      0.75      0.73       249\n",
      "unpopularopinion       0.88      0.76      0.82       241\n",
      "\n",
      "        accuracy                           0.79       950\n",
      "       macro avg       0.79      0.79      0.79       950\n",
      "    weighted avg       0.79      0.79      0.79       950\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>depression</th>\n",
       "      <th>unpopularopinion</th>\n",
       "      <th>lonely</th>\n",
       "      <th>MachineLearning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">actual</th>\n",
       "      <th>depression</th>\n",
       "      <td>207</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpopularopinion</th>\n",
       "      <td>7</td>\n",
       "      <td>170</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lonely</th>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>186</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MachineLearning</th>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         predicted                                        \n",
       "                        depression unpopularopinion lonely MachineLearning\n",
       "actual depression              207                4      8               5\n",
       "       unpopularopinion          7              170     47              12\n",
       "       lonely                   15               40    186               8\n",
       "       MachineLearning          13               23     22             183"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual','actual', 'actual'], ['depression', 'unpopularopinion', 'lonely', 'MachineLearning']],\n",
    "    columns = [['predicted', 'predicted','predicted', 'predicted'], ['depression', 'unpopularopinion', 'lonely', 'MachineLearning']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8648c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_subreddits.sav'\n",
    "pickle.dump(nltk_ensemble, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a09ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " MachineLearning       0.86      0.92      0.89       224\n",
      "      depression       0.72      0.72      0.72       236\n",
      "          lonely       0.71      0.75      0.73       249\n",
      "unpopularopinion       0.88      0.76      0.82       241\n",
      "\n",
      "        accuracy                           0.79       950\n",
      "       macro avg       0.79      0.79      0.79       950\n",
      "    weighted avg       0.79      0.79      0.79       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "result = loaded_model.classify_many(txt_features)\n",
    "print(classification_report(labels, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b022b9",
   "metadata": {},
   "source": [
    "### 8. Classifier for r/depression posts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a16a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_combined = getWordFrequency('all_posts_processed.csv')\n",
    "word_features = list(all_words_combined.keys())[:1500]\n",
    "\n",
    "# Training, testing data\n",
    "posts = pd.read_csv('all_posts_processed.csv')\n",
    "\n",
    "Y = []\n",
    "postsArr = []\n",
    "i = 0 \n",
    "for post in posts.Post:\n",
    "    if(type(post)==str):\n",
    "        postsArr.append(post)\n",
    "        if(posts.Subreddit[i]=='depression'):\n",
    "            Y.append(posts.Subreddit[i])\n",
    "        else:\n",
    "            Y.append('not r/depression')\n",
    "    i+=1\n",
    "    \n",
    "# Now find features for all posts\n",
    "posts_all = list(zip(postsArr, Y))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(posts_all)\n",
    "\n",
    "# call find_features function for each post\n",
    "featuresets = [(find_features(text), label) for (text, label) in posts_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e4cd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938\n",
      "2849\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)\n",
    "\n",
    "print(len(posts))\n",
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ecc0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 76.21052631578948\n",
      "Decision Tree Accuracy: 77.26315789473685\n",
      "Random Forest Accuracy: 83.26315789473684\n",
      "Logistic Regression Accuracy: 83.89473684210526\n",
      "SGD Classifier Accuracy: 81.78947368421052\n",
      "Naive Bayes Accuracy: 81.05263157894737\n",
      "SVM Linear Accuracy: 80.94736842105263\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "315f3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 80.94736842105263\n"
     ]
    }
   ],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a0eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f9c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      depression       0.71      0.60      0.65       224\n",
      "not r/depression       0.88      0.92      0.90       726\n",
      "\n",
      "        accuracy                           0.85       950\n",
      "       macro avg       0.79      0.76      0.77       950\n",
      "    weighted avg       0.84      0.85      0.84       950\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>depression</th>\n",
       "      <th>not r/depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>depression</th>\n",
       "      <td>134</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not r/depression</th>\n",
       "      <td>56</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         predicted                 \n",
       "                        depression not r/depression\n",
       "actual depression              134               90\n",
       "       not r/depression         56              670"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['depression', 'not r/depression']],\n",
    "    columns = [['predicted', 'predicted'], ['depression', 'not r/depression']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8afec9",
   "metadata": {},
   "source": [
    "### 9. Save trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c31e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_depression_or_not.sav'\n",
    "pickle.dump(nltk_ensemble, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2966c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      depression       0.71      0.60      0.65       224\n",
      "not r/depression       0.88      0.92      0.90       726\n",
      "\n",
      "        accuracy                           0.85       950\n",
      "       macro avg       0.79      0.76      0.77       950\n",
      "    weighted avg       0.84      0.85      0.84       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "result = loaded_model.classify_many(txt_features)\n",
    "print(classification_report(labels, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548e3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
