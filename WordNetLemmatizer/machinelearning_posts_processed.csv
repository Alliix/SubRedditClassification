Author,Subreddit,Date,Title,Post,Post_Parsed
SisyphusGuy,MachineLearning,1617898792.0,[D] Are there any reliable open-source out-of-the-box Face Anti-Spoofing detectors?,"I am looking through the [Face Anti-Spoofing section](https://paperswithcode.com/task/face-anti-spoofing/latest) of Papers With Code and I can't find any reliable implementation of an open-source Face Anti-Spoofing detector for RGB images. 

Some implementations that I found at Papers With Code require depth images, others don't make the pre-trained weights available and others even make the wrong pre-trained weights available. The ones that I can actually use as an out-of-the-box model don't present very good generalization.

For my project, ideally, I would like to have a reliable passive Face Anti-Spoofing detector (i.e., a detector that doesn't actively challenges the user).",look face anti spoof section url paper code find reliable implementation open source face anti spoof detector rgb image implementations find paper code require depth image others make pre train weight available others even make wrong pre train weight available ones actually use box model present good generalization project ideally would like reliable passive face anti spoof detector e detector actively challenge user
dadadidi,MachineLearning,1616942192.0,"[P] Guide: Finetune GPT2-XL (1.5 Billion Parameters, the biggest model) on a single 16 GB VRAM V100 Google Cloud instance with Huggingface Transformers using DeepSpeed","I needed to finetune the GPT2 1.5 Billion parameter model for a project, but the model didn't fit on my gpu. So i figured out how to run it with deepspeed and gradient checkpointing, which reduces the required GPU memory. Now it can fit on just one GPU.

Here i explain the setup and commands to get it running: [https://github.com/Xirider/finetune-gpt2xl](https://github.com/Xirider/finetune-gpt2xl)

I was also able to fit the currently largest GPT-NEO model (2.7 B parameters) on one 16 GB VRAM gpu for finetuning, but i think there might be some issues with Huggingface's implementation.

I hope this helps some people, who also want to finetune GPT2, but don't want to set up distributed training.",need finetune gpt2 nmbr billion parameter model project model fit gpu figure run deepspeed gradient checkpointing reduce require gpu memory fit one gpu explain setup command get run url also able fit currently largest gpt neo model 2 7 b parameters one nmbr gb vram gpu finetuning think might issue huggingface implementation hope help people also want finetune gpt2 want set distribute train
mroc_lak,MachineLearning,1616579417.0,[D] How does one test for shortcut learning of deep neural networks in computer vision?,"Hi all, 

  
I’ve come across reviews of Covid-19 diagnosis based on Chest-X-rays and many studies were flawed. Some learned shortcuts (e.g. using hospital specific tokens in an image). How does one systematically test that the neural net isn’t using any shortcuts?",hi ive come across review covid 19 diagnosis base chest x ray many study flaw learn shortcuts e g use hospital specific tokens image one systematically test neural net use shortcuts
jafioti,MachineLearning,1617299891.0,[D] Adaptive Computation Time Uses?,"I read the paper for Adaptive Computation Time ([https://arxiv.org/abs/1603.08983](https://arxiv.org/abs/1603.08983)) by Alex Graves a while ago, and I've only played around with it, but my basic question is: why has this not been used more? The only mainstream project I've seen it in was ALBERT, where one version used adaptive computation time to determine the number of copies of layers to run.

It seems like this would be hugely important as it allows for iterative refinement of a speculative output, like many networks, but for a dynamic number of times, decided by the network. Originally it was implemented for RNNs, but I think its pretty trivial to implement it for most other architectures. I would surely think this could be implemented with a Mixture of Experts model where the model can rerun the same layers and choose a different expert each time, which could allow for big parameter space without having to be constrained to using one or two experts each forward pass. Maybe I'm overhyping ACT and its really not very useful, but is there any reason that it hasn't seen more widespread adoption?",read paper adaptive computation time url alex grave ago play around basic question use mainstream project see albert one version use adaptive computation time determine number copy layer run seem like would hugely important allow iterative refinement speculative output like many network dynamic number time decide network originally implement rnns think pretty trivial implement architectures would surely think could implement mixture experts model model rerun layer choose different expert time could allow big parameter space without constrain use one two experts forward pass maybe overhyping act really useful reason see widespread adoption
goktugkt,MachineLearning,1618608106.0,[P] Minimal PyTorch Library for Natural Evolution Strategies,"https://github.com/goktug97/nes-torch

My main goal with this project was to test my new configuration system library `pipcs`
https://github.com/goktug97/pipcs and see how practical to write a library with it. Also, it can act as an example for pipcs. 

It also supports mpi4py. Without changing anything you can run the same script with `mpirun` to train in parallel.",url main goal project test new configuration system library pipcs url see practical write library also act example pipcs also support mpi4py without change anything run script mpirun train parallel
Yuqing7,MachineLearning,1618587259.0,[N] ETH Zurich & UC Berkeley Method Automates Deep Reward-Learning by Simulating the Past,"A research team from ETH and UC Berkeley proposes a Deep Reward Learning by Simulating the Past (Deep RLSP) algorithm that represents rewards directly as a linear combination of features learned through self-supervised representation learning and enables agents to simulate human actions backwards in time to infer what they must have done.

Here is a quick read: [ETH Zurich & UC Berkeley Method Automates Deep Reward-Learning by Simulating the Past](https://syncedreview.com/2021/04/14/eth-zurich-uc-berkeley-method-automates-deep-reward-learning-by-simulating-the-past/).

The paper Learning What To Do by Simulating the Past is on [arXiv](https://arxiv.org/pdf/2104.03946.pdf).",research team eth uc berkeley propose deep reward learn simulate past deep rlsp algorithm represent reward directly linear combination feature learn self supervise representation learn enable agents simulate human action backwards time infer must quick read eth zurich uc berkeley method automate deep reward learn simulate past url paper learn simulate past arxiv url
BRadoslaw,MachineLearning,1620071706.0,[D] Transformer Positional Embeddings for the nth time - are the representations unique?,"Hi all,

I'm trying to get the intuition behind positional embeddings described in the famous paper [*""Attention is all you need""*](https://arxiv.org/abs/1706.03762). It makes total sense for me but I notice one caveat.

I make a case where a particular tokenized word at position x might result in the same vector as another word at another position.

Let me make up a case for it:

    representation_1 = tokenize(""learning"") + positional_embedding(position=1)
    representation_2 = tokenize(""other"") + positional_embedding(position=20)
    representation_1 == representation_2

It would means that word representations are not unique. If so, why does the model learn the sequential nature of input? Is my case valid?

&#x200B;

[https:\/\/kazemnejad.com\/blog\/transformer\_architecture\_positional\_encoding\/](https://preview.redd.it/tjr3ziednyw61.png?width=1528&format=png&auto=webp&s=17740eb65a883a74d9565bad0122d707f26612d0)",hi try get intuition behind positional embeddings describe famous paper attention need url make total sense notice one caveat make case particular tokenized word position x might result vector another word another position let make case representation_1 tokenize learn positional_embedding position 1 representation_2 tokenize positional_embedding position 20 representation_1 representation_2it would mean word representations unique model learn sequential nature input case valid x200b https kazemnejad com blog transformer _architecture _positional _encoding url
limarg,MachineLearning,1618568522.0,[D] Marginal Likelihood Estimation based on VAE,"Why is the Marginal Likelihood Estimator proposed in VAE not based on importance sampling?

I would expect the straightforward way to estimate the marginal likelihood to be based on importance sampling:

*𝑝*(*𝑥*)=∫*𝑧𝑝*(*𝑧*)*𝑝*(*𝑥*|*𝑧*)*𝑞*(*𝑧*|*𝑥*)*𝑞*(*𝑧*|*𝑥*)*𝑑𝑧*=𝔼*𝑞*(*𝑧*|*𝑥*)\[*𝑝*(*𝑥*|*𝑧*)*𝑝*(*𝑧*)*𝑞*(*𝑧*|*𝑥*)\]

However,

1. in the original VAE paper, the authors suggest a different method in appendix D that is *not* based on importance sampling. Is there a good reason for this?
2. The authors furthermore suggest to estimate *𝑞*(*𝑧*|*𝑥*) with a density estimation after drawing samples from it. Why is this necessary if we know *𝑞*(*𝑧*|*𝑥*) explicitly?",marginal likelihood estimator propose vae base importance sample would expect straightforward way estimate marginal likelihood base importance sample 𝑝 𝑥 𝑧𝑝 𝑧 𝑝 𝑥 𝑧 𝑞 𝑧 𝑥 𝑞 𝑧 𝑥 𝑑𝑧 𝔼 𝑞 𝑧 𝑥 𝑝 𝑥 𝑧 𝑝 𝑧 𝑞 𝑧 𝑥 however 1 original vae paper author suggest different method appendix base importance sample good reason 2 author furthermore suggest estimate 𝑞 𝑧 𝑥 density estimation draw sample necessary know 𝑞 𝑧 𝑥 explicitly
yaxu,MachineLearning,1617265002.0,[D] Non-automated machine learning?,"Machine  learning can be a bit unfathomable for most people. What machine  learning/AI algorithms are there that can be reasonably worked out on  paper? Is there a way to understand an ANN by getting 10 people together  exchanging numbers and adjusting state? Is there a way to do machine  learning without a machine?

I'm  guessing that a lot of systems are too complex/only work at scale.. But  it would be interesting at least to de-automate at least part of a  machine learning system. Kids learn sorting algorithms like bubblesort  at school by standing in a line and following an algorithm. Is anything  like this possible with ML algorithms?",machine learn bite unfathomable people machine learn ai algorithms reasonably work paper way understand ann get nmbr people together exchange number adjust state way machine learn without machine guess lot systems complex work scale would interest least de automate least part machine learn system kid learn sort algorithms like bubblesort school stand line follow algorithm anything like possible ml algorithms
Tuba202,MachineLearning,1617219025.0,My fork of RameenAbdal's StyleFlow! [P],"For those of you who don't know, StyleFlow is a [SUPER COOL AI](https://youtu.be/Lt4Z5oOAeEY) that edits facial parameters like age and gender.

The only issue is that it wouldn't run! After a week of on-and-off work, I finally got it working, and decided to share my work in the form of a fork of the project.

[My StyleFlow Fork](https://github.com/Tuba202/StyleFlow-Made-Easy)

As long as you have a windows computer with a Cuda compatible GPU, please take some time and check it out!

[An example of what it can do!](https://preview.redd.it/3lhtf2eq0fq61.png?width=1919&format=png&auto=webp&s=b5d3ecdc844a5dfad2796fe9b91db4a2e648fe4b)",know styleflow super cool ai url edit facial parameters like age gender issue run week work finally get work decide share work form fork project styleflow fork url long windows computer cuda compatible gpu please take time check example url
Vegetable_Ganache_37,MachineLearning,1616679356.0,[R] New Pre-Print: Bio-Inspired Robustness: A Review,"Hello everyone,

We recently added a new pre-print on how human visual system-inspired components can help with adversarial robustness. We study recent attempts in the area and analyze their properties and evaluation criteria for robustness. Please let us know what you think of the paper and any feedback is highly appreciated!!! :)

P.S Please forgive the word format TT TT, first and last time I do this in my life. Else it's Latex all the way.

Title: 'Bio-Inspired Robustness: A Review '

Arxiv link: [https://arxiv.org/abs/2103.09265](https://t.co/m1lZbQqhEW?amp=1)

Abstract: Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision-inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.",hello everyone recently add new pre print human visual system inspire components help adversarial robustness study recent attempt area analyze properties evaluation criteria robustness please let us know think paper feedback highly appreciate p please forgive word format tt tt first last time life else latex way title bio inspire robustness review arxiv link url deep convolutional neural network dcnns revolutionize computer vision often advocate good model human visual system however currently many shortcomings dcnns preclude model human vision example case adversarial attack add small amount noise image include object lead strong misclassification object humans noise often invisible vulnerability adversarial noise fix dcnns take serious model human vision many study try add feature human visual system dcnns make robust adversarial attack however fully clear whether human vision inspire components increase robustness performance evaluations novel components dcnns often inconclusive propose set criteria proper evaluation analyze different model accord criteria finally sketch future efforts make dccns one step closer model human vision
Seankala,MachineLearning,1618615742.0,[D] If I can't reproduce the exact reported scores of a paper (1-2% difference) then is it okay to report the scores I obtained or should I copy the originally reported scores?,"Sorry if the title is a little confusing. I'm sure many people have experienced this, but what I'm referring to is that situation where you take the code released by the authors of a particular model, set up your virtual environment to match theirs, and still can't reproduce the exact reported scores. By ""exact"" I'm referring to a difference of maybe 0.1-0.2%. A particular SoTA baseline I'm using shows this behavior and I'm getting a 1-2% difference.

Is it fine to use those scores or should I copy and paste the originally reported scores? Thanks.",sorry title little confuse sure many people experience refer situation take code release author particular model set virtual environment match still reproduce exact report score exact refer difference maybe 0 1 0 2 particular sota baseline use show behavior get 1 2 difference fine use score copy paste originally report score thank
Shoulder_Feeling,MachineLearning,1619417114.0,[Project] DataTap provides droplets ( containers for datasets) to make working on popular deep learning datasets easy.,"Excited to share [DataTap](https://www.datatap.dev), An open-source dataset management tool that makes it easy to ""containerize"" datasets to let you  focus on machine learning not data ops.  DataTap lets you build data set droplets ( think of a droplet as a docker container for data ).  A droplet encapsulates a dataset that can then easily be used , imported, shared across different teams and projects.

Each Data Droplet consists for 2 items

* **Droplet Template**, similar to a docker file this specifies the dataset schema
* **Dataset annotations, metadata and media** (this is typically images / videos / rich media )

Learn more about how you can start using this here [https://github.com/zensors/datatap-python](https://github.com/zensors/datatap-python)

Many machine learning projects use proprietary data formats that require tools and utilities to be re-written from scratch to accommodate them. Not only does this slow down development substantially, but it also increases the probability that developers introduce bugs in the very code that validates models’ performance.

As part of dataTap’s efforts to allow machine learning engineers to focus only on the machine learning itself, we introduced an open-source data interchange format called Droplet. The data container format, called “annotation,” provides a standardized way to describe what is in an image. DataTap is designed to be the data platform for Software 2.0. Machine learning on reach media like images , audio or video needs a special data pipeline to version and manage data much like there are MLOps tools to version and manage models

Currently the project has common datasets available that you can download or stream with 3 lines of code.

* coco
* Open-Imagees
* AI food Dataset
* Large Person Dataset
* Combined Vehicles Dataset

See the full list 

[https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e](https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e)

Request Your own to be added in or use the open source tools to import data into the droplet format using this example

[https://zensors.typeform.com/to/WXo3ZlSN](https://zensors.typeform.com/to/WXo3ZlSN)",excite share datatap url open source dataset management tool make easy containerize datasets let focus machine learn data ops datatap let build data set droplets think droplet docker container data droplet encapsulate dataset easily use import share across different team project data droplet consist nmbr items droplet template similar docker file specify dataset schema dataset annotations metadata media typically image videos rich media learn start use url machine learn project use proprietary data format require tool utilities write scratch accommodate slow development substantially also increase probability developers introduce bug code validate model performance part datataps efforts allow machine learn engineer focus machine learn introduce open source data interchange format call droplet data container format call annotation provide standardize way describe image datatap design data platform software 2 0 machine learn reach media like image audio video need special data pipeline version manage data much like mlops tool version manage modelscurrently project common datasets available download stream nmbr line code coco open image ai food dataset large person dataset combine vehicles datasetsee full list url add use open source tool import data droplet format use example url
s-lilo,MachineLearning,1620223692.0,[N] Call for Participation in a Shared Task about occupations detection in clinical texts,"Hi, everyone!

I'm a researcher from the Text Mining Unit at the Barcelona Supercomputing Center, and I wanted to share with you some information about MEDDOPROF, a Shared Task that we are currently organizing focused on the detection and normalization of professions and employment status in clinical texts in Spanish.

Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone's occupation can have a radical impact in their physical and mental health, habits, lifestyle choices, ... There is even an entire medical specialty, occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations have been specially affected (for instance, health professionals and other essential workers). The detection of these terms will help researchers to better characterize health risks of specific occupations.

Outside medicine, we foresee that the systems resulting from MEDDOPROF may be used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the task is the inclusion of employment status in a broad sense. We have annotated unemployed and retired people, family caregivers, people who are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.

Additionally, each mention in the corpus (which includes 2000 documents from over 20 different medical specialties) has been normalized to either the European Skills, Competences, Qualifications and Occupations classification (ESCO) or SNOMED-CT. These are multilingual vocabularies, which we hope might inspire similar tasks in other languages (to the best of our knowledge, there haven't been any similar tasks yet).

We released the training set some weeks ago, and on June 1st we will release the test set. If you are interested in the task, want to see some annotated examples, the data or the annotation guidelines, ... please check out the task's website:[ https://temu.bsc.es/meddoprof/](https://temu.bsc.es/meddoprof/)

Thank you if you have read up to here, I hope to see at least some of you at the task!",hi everyone researcher text mine unit barcelona supercomputing center want share information meddoprof share task currently organize focus detection normalization professions employment status clinical texts spanish even type entities might seem really niche every day learn importance think someone occupation radical impact physical mental health habit lifestyle choices even entire medical specialty occupational medicine center around topic context current pandemic many people specific occupations specially affect instance health professionals essential workers detection term help researchers better characterize health risk specific occupations outside medicine foresee systems result meddoprof may use field social care human resources legal nlp even gender study personally think one main contributions task inclusion employment status broad sense annotate unemployed retire people family caregivers people homeless people depend government subsidies etc strengthen social side project additionally mention corpus include nmbr document nmbr different medical specialties normalize either european skills competences qualifications occupations classification esco snomed ct multilingual vocabularies hope might inspire similar task languages best knowledge similar task yet release train set weeks ago june 1st release test set interest task want see annotate examples data annotation guidelines please check task website url read hope see least task
jj4646,MachineLearning,1619072260.0,"[D] is this the ""unanswered question"" of machine learning?",""" Generalization performance of classifiers in deep learning has recently become a subject of intense study . Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this “overfitting”, they perform well on test data, a phenomenon not yet fully understood.  ""

Source: [http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf](http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf) 

What is the consensus about this question in the statistics community? Is this the equivalent of ""the big bang"" or ""how did the dinosaurs go extinct"" questions for the statistics/machine learning community? Is it fundamentally impossible to answer this question - or just extremely difficult?

My very naïve answer to this question (i.e. why do deep learning models generalize well to unseen data?) is that :

A) ""unseen"" data can apparently (on some level) be well represented by complex (e.g. non-linear) combinations of ""seen"" data : deep learning models are very good at recognizing and figuring out these complex combinations

B) ""show me your 5 closest friends and I will tell you who you are"" : on some level, data probably displays the ""nearest neighbor principle""  - in big and complex data sets, there are small ""pockets"" of homogenous data  that the model uses as stepping stones to generalize.

This of course does not explain the mathematics of why deep learning models are able to generalize to unseen data - but is this basically the essence of the matter? Deep learning models are able to generalize to unseen data because on some level, unseen data is similar to seen data. A silly and extreme example, it is unlikely that even the ""strongest"" recurrent neural networks when provided data about a certain stock from 1980-1995 would be able to predict today's weather - this is because the data too fundamentally different to allow even the best model to generalize. Or, suppose you want to use a regular statistical model to predict information about a normally distributed variable with a certain mean and standard deviation - if you drastically change the mean and standard deviation of this variable and ask the previously trained model to continue making predictions (assume the model doesn't know that anything has changed), it's natural to expect that the predictions will be less accurate. Statistical models (no matter how ""great"" they are) are able to generalize to unseen data, so long as this unseen data comes from the same ballpark as the seen data.

Am I understanding this correctly?",generalization performance classifiers deep learn recently become subject intense study deep model typically heavily parametrized tend fit train data exactly despite overfitting perform well test data phenomenon yet fully understand source url consensus question statistics community equivalent big bang dinosaurs go extinct question statistics machine learn community fundamentally impossible answer question extremely difficult naïve answer question e deep learn model generalize well unseen data unseen data apparently level well represent complex e g non linear combinations see data deep learn model good recognize figure complex combinationsb show nmbr closest friends tell level data probably display nearest neighbor principle big complex data set small pocket homogenous data model use step stone generalize course explain mathematics deep learn model able generalize unseen data basically essence matter deep learn model able generalize unseen data level unseen data similar see data silly extreme example unlikely even strongest recurrent neural network provide data certain stock 1980 1995 would able predict today weather data fundamentally different allow even best model generalize suppose want use regular statistical model predict information normally distribute variable certain mean standard deviation drastically change mean standard deviation variable ask previously train model continue make predictions assume model know anything change natural expect predictions less accurate statistical model matter great able generalize unseen data long unseen data come ballpark see data understand correctly
RandomTensor,MachineLearning,1618938639.0,[N] Workshop on the Theory of Overparameterized Machine Learning **Going on right now!!**,"Just FYI the Workshop on the Theory of Overparameterized Machine Learning (TOPML) is happening right now! Lots of big names with hard hitting talks on this fascinating phenomenon. Registration is free! Although I'm not sure if its still open.  


[http://topml.rice.edu/](http://topml.rice.edu/)",fyi workshop theory overparameterized machine learn topml happen right lot big name hard hit talk fascinate phenomenon registration free although sure still open url
universome,MachineLearning,1618499655.0,[P] Aligning Latent and Image Spaces to Connect the Unconnectable,"Hi! Wanted to share our latest project on infinite image generation: [http://universome.github.io/alis](http://universome.github.io/alis)

[The method works without any conditioning and learns from a dataset of unrelated square images](https://reddit.com/link/mrgrdn/video/53apkx1asct61/player)

Basically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.

Our generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), that have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.

&#x200B;

[𝛿 is the value of the coordinates shift. Pixel values inside circles are equal for different generations \(up to numerical precision\)](https://preview.redd.it/4wbquwfgsct61.jpg?width=1389&format=pjpg&auto=webp&s=b8e75ffec468a79a12cdaba1b97f2280a9ebf71d)

A surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very ""difficult"" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.

&#x200B;

[Results on LSUN bedroom](https://preview.redd.it/ugyrknposct61.jpg?width=2000&format=pjpg&auto=webp&s=398e0c2b9083027c2f89645a3d5762afc6a3ac1c)

Besides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.

**Drawbacks of the approach:**

* Generating patches completely independently limits the generation quality (by \\\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).
* Not all the datasets are ""connectable"". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the ""study"" on this in tables 3 & 4 in Appendix C)

Project page: [https://universome.github.io/alis](https://universome.github.io/alis)

Code: [https://github.com/universome/alis](https://github.com/universome/alis)

Paper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)",hi want share latest project infinite image generation url method work without condition learn dataset unrelated square image url work follow way put latent code position coordinate grid image pixels locate compute pixel interpolation nearby latent cod train generate frame random position coordinate grid fee discriminator test time allow us produce image position infinite plane stitch seamlessly one another generator compute image independent patch like cocogan url less extreme version inr gin url independence pixel level technical tweak coordinate embeddings make periodically spatially equivariant shift coordinate output image shift accordingly pixel value common coordinate equal numerical precision illustrate x200b 𝛿 value coordinate shift pixel value inside circle equal different generations numerical precision url surprise thing approach work dataset nature landscape spatially invariant image statistics also extent lsun bedroom difficult dataset infinite image generation wall close object make hard impossible extrapolate image leave right since dataset picture wall close object middle model nowhere learn knowledge x200b result lsun bedroom url also collect preprocessed high quality dataset 90k landscape image landscape hq unsplash flickr release soon drawbacks approach generate patch completely independently limit generation quality 30 experiment moreover due patchwise generation model learn ignore noise injection powerful tool image edit also happen inr gin cips datasets connectable lsun bedroom work filter away image contain spatially invariant statistics see sec nmbr appendix c ffhq imagenet imagine work see study table nmbr nmbr appendix c project page url url url
pmp-dash1,MachineLearning,1619732343.0,[R] Improving ETA Prediction Accuracy for Long-tail Events - Doordash ML Blogpost,"If you are interested in improving the accuracy of long-tail events for your ML models, check out this blog article I wrote about making DoorDash's Delivery ETA 10% more accurate. Some of our orders were taking longer than we expected to arrive, so we added some key historical and real-time features into the mix and iterated on custom loss functions to improve the accuracy of our predictions and overall customer experience. Check out the technical details or just learn all about long-tail prediction problems here ([link](https://doordash.engineering/2021/04/28/improving-eta-prediction-accuracy-for-long-tail-events/))

[#MachineLearning](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#ETA](https://www.linkedin.com/feed/hashtag/?keywords=eta&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#mapping](https://www.linkedin.com/feed/hashtag/?keywords=mapping&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#logistics](https://www.linkedin.com/feed/hashtag/?keywords=logistics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432)",interest improve accuracy long tail events ml model check blog article write make doordash delivery eta 10 accurate order take longer expect arrive add key historical real time feature mix iterate custom loss function improve accuracy predictions overall customer experience check technical detail learn long tail prediction problems link url datascience url eta url map url logistics url
amasterblaster,MachineLearning,1619128441.0,[Discussion] Any new / good hyperparam tuning approaches?," 

I'm looking to tune a very slow function (one call takes 20 mins \~ 1hr). How do you guys handle optimizing something this slow? Have there been any good advancements / new libraries for this problem?

I feel like the techniques I use suck, and am looking to see what everyone is up to here.",look tune slow function one call take nmbr mins 1hr guy handle optimize something slow good advancements new libraries problem feel like techniques use suck look see everyone
grid_world,MachineLearning,1617182279.0,[R] Dataset for research paper,"I  am in process for publishing a paper in ""Deep Learning compression"" by  comparing a model's original size and performance vs. compressed size  and performance on some dataset. Majority of the research papers either  focus on CIFAR-10 and/or ImageNet.

ImageNet  becomes an infrastructure challenge since the dataset size is upward of  150 GB. The problem with CIFAR-10 is that you have a smaller dataset  (60K images) which doesn't scale well if your model size grows ->  think ResNet-50 and bigger.

Therefore,  can you all suggest some other dataset which sits somewhere in between  and whose results will be accepted by journals, conferences, etc. (from  the academic point of view)?",process publish paper deep learn compression compare model original size performance vs compress size performance dataset majority research paper either focus cifar 10 imagenet imagenet become infrastructure challenge since dataset size upward nmbr gb problem cifar 10 smaller dataset 60k image scale well model size grow think resnet 50 bigger therefore suggest dataset sit somewhere whose result accept journals conferences etc academic point view
regalalgorithm,MachineLearning,1619187929.0,[D] Your Favorite AI Podcasts / Blogs / Newsletters / YouTube Channels?,"Hi there, I want to write a little blog post summarizing different ways of keeping up with AI by way of Podcasts / Blogs / Newsletters / YouTube Channels. Yeah there are a million of these, but most are not so well curated, miss a lot of stuff, and are not up to date. Criteria: still active, focused primarily on AI, high quality.

Here's what I have so far, would appreciate if you can suggest any additions!

* **Podcasts**
   * [**Machine Learning Street Talk**](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ)
   * **Lex Fridman (mainly first \~150 eps)**
   * **Gigaom Voices in AI**
   * **Data Skeptic**
   * **Eye on AI**
   * **Gradient Dissent**
   * **Robot Brains**
   * **RE Work podcast**
   * **AI Today Podcast**
   * **Chat Time Data Science**
   * **Let’s Talk AI**
   * **In Machines We Trust**
* **Publications**
   * **The Gradient**
   * **Towards Data Science**
   * **Analytics Vidhya**
   * **Distill**
* **Personal Blogs**
   * [**Lil’Log**](https://lilianweng.github.io/lil-log/)
   * **Gwern**
   * **Sebastian Ruder**
   * **Alex Irpan**
   * **Chris Olah**
   * **Democratizing Automation**
   * **Approximately Correct**
   * **Off the Convex Path**
   * **Arg min blog**
   * **I’m a bandit**
* **Academic Blogs**
   * **SAIL Blog**
   * **Berkeley AI Blog**
   * **Machine Learning at Berkeley Blog**
   * **CMU ML Blog**
   * **ML MIT**
   * **ML Georgia Tech**
   * **Google / Facebook / Salesforce / Microsoft / Baidu / OpenAI /  DeepMind** 
* **Journalists**
   * **Karen Hao** 
   * **Cade Metz**
   * **Will Knight**
   * **Khari Johnson**
* **Newsletters**
   * **Last Week in AI**
   * **Batch.AI**
   * **Sebasting Ruder**
   * **Artificial Intelligence Weekly News**
   * **Wired AI newsletter**
   * **Papers with Code**
   * **The Algorithm**
   * **AI Weekly**
   * **Weekly Robotics**
   * **Import AI**
   * **Deep Learning Weekly**
   * **H+ Weekly**
   * **ChinAI Newsletter**
   * **THe EuropeanAI Newsletter**

**Youtube Channels**

* **Talks**
   * [**Amii Intelligence**](https://www.youtube.com/channel/UCxxisInVr7upxv1yUhSgdBA)
   * [**CMU AI Seminar**](https://www.youtube.com/channel/UCLh3OUmBGe4wPyVZiI771ng)
   * [**Robotics Institute Seminar Series**](https://www.youtube.com/playlist?list=PLCFD85BC79FE703DF)
   * [**Machine Learning Center at Georgia Tech**](https://www.youtube.com/channel/UCugI4c0S6-yVi9KfdkDU0aw/videos)
   * [**Robotics Today**](https://www.youtube.com/channel/UCtfiXX2nJ5Qz-ZxGEwDCy5A)
   * [**Stanford MLSys Seminars**](https://www.youtube.com/channel/UCzz6ructab1U44QPI3HpZEQ)
   * [**MIT Embodied Intelligence**](https://www.youtube.com/channel/UCnXGbvgu9071i3koFooncAw)
* **Interviews**
   * **See podcasts**
* **Paper Summaries** 
   * [**AI Coffee Break with Letitia**](https://www.youtube.com/c/AICoffeeBreak/featured)
   * [**Henry AI Labs**](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw)
   * [**Yannic Kilcher**](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)
   * **Arxiv Insights**
* **Lessons**
   * [**3Blue1Brown**](https://www.youtube.com/c/3blue1brown/featured)
   * [**Jordan Harrod**](https://www.youtube.com/channel/UC1H1NWNTG2Xi3pt85ykVSHA)
   * [**vcubingx**](https://www.youtube.com/channel/UCv0nF8zWevEsSVcmz6mlw6A)
   * [**Leo Isikdogan**](https://www.youtube.com/channel/UC-YAxUbpa1hvRyfJBKFNcJA)
* **Demos**
   * [**bycloud**](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng)
   * [**Two Minute Papers**](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
   * [**Code Bullet**](https://www.youtube.com/channel/UC0e3QhIYukixgh5VVpKHH9Q)
   * [**What's AI**](https://www.youtube.com/c/WhatsAI/videos)",hi want write little blog post summarize different ways keep ai way podcast blog newsletters youtube channel yeah million well curated miss lot stuff date criteria still active focus primarily ai high quality far would appreciate suggest additions podcast machine learn street talk url lex fridman mainly first 150 eps gigaom voice ai data skeptic eye ai gradient dissent robot brain work podcast ai today podcast chat time data science let talk ai machine trust publications gradient towards data science analytics vidhya distill personal blog lillog url gwern sebastian ruder alex irpan chris olah democratize automation approximately correct convex path arg min blog im bandit academic blog sail blog berkeley ai blog machine learn berkeley blog cmu ml blog ml mit ml georgia tech google facebook salesforce microsoft baidu openai deepmind journalists karen hao cade metz knight khari johnson newsletters last week ai batch ai sebasting ruder artificial intelligence weekly news wire ai newsletter paper code algorithm ai weekly weekly robotics import ai deep learn weekly h weekly chinai newsletter europeanai newsletter youtube channel talk amii intelligence url cmu ai seminar url robotics institute seminar series url machine learn center georgia tech url robotics today url stanford mlsys seminars url mit embody intelligence url interview see podcast paper summaries ai coffee break letitia url henry ai labs url yannic kilcher url arxiv insights lessons 3blue1brown url jordan harrod url vcubingx url leo isikdogan url demo bycloud url two minute paper url code bullet url ai url
New_Date5540,MachineLearning,1618921283.0,[P] I have created a script to convert video into slides (ppt) for StatQuest,"I was wondering if I can get those slides as the video contents are awesome so I have decided to create a script to extract the slides from the video using OpenCV and generated ppt from the slides.
You can check the code. Any feedback would be highly appreciable.
Repo: https://github.com/ninjakx/youtube-video2ppt",wonder get slide video content awesome decide create script extract slide video use opencv generate ppt slide check code feedback would highly appreciable repo url
VerySecretCactus,MachineLearning,1618782264.0,"[D] Is the ""Super Harsh Guide to ML"" reddit post out of date yet? (2021 version)","Last time this was posted was over a year ago. Wondering if anything's changed.

Original post: https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/

Last year's update: https://www.reddit.com/r/MachineLearning/comments/emmxp6/d_is_the_super_harsh_guide_to_ml_reddit_post_out/",last time post year ago wonder anything change original post url year update url
L-MK,MachineLearning,1620290611.0,[R] Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet,"TL;DR: Got scooped by MLP-Mixer, so I'm releasing my writeup/code/models. I hope someone finds them interesting/useful.

Lately I've been trying a couple variants of simple vision transformers to better understand what makes them perform well. About a month ago, I found that you could replace the attention layers with feed-forward layers and get quite good results. Last week I started a short writeup of the experiment (just a few pages, as I didn't see it as a full paper).

Today Google put out a paper (MLP-Mixer) that proposes exactly the same architecture.

When I saw the paper earlier today I considered scrapping what I had done, but now I figure that I might as well just put it out there.

For those who are interested, here's a [GitHub repo](https://github.com/lukemelas/do-you-even-need-attention) with pretrained models, a [W&B log](https://wandb.ai/lukemelas2/deit-experiments/reports/Do-You-Even-Need-Attention---Vmlldzo2NjUxMzI?accessToken=8kebvweue0gd1s6qiav2orco97v85glogsi8i83576j42bb1g39e59px56lkk4zu) of the experiments, and a 3-page [writeup](https://github.com/lukemelas/do-you-even-need-attention/blob/main/Do-You-Even-Need-Attention.pdf).

Also, if anyone has stories about getting scooped, feel free to share -- I'd imagine people have some crazy stories.

Edit: Wow, thank you all for the support! I really didn't expect this. Based on your suggestions, I've also uploaded a version of the report to arXiv: [https://arxiv.org/abs/2105.02723](https://arxiv.org/abs/2105.02723) ",tl dr get scoop mlp mixer release writeup code model hope someone find interest useful lately try couple variants simple vision transformers better understand make perform well month ago find could replace attention layer fee forward layer get quite good result last week start short writeup experiment page see full paper today google put paper mlp mixer propose exactly architecture saw paper earlier today consider scrap figure might well put interest github repo url pretrained model w b log url experiment 3 page writeup url anyone stories get scoop feel free share imagine people crazy stories edit wow thank support really expect base suggestions also upload version report arxiv url
patrickkidger,MachineLearning,1617795134.0,"[P] torchtyping -- documentation + runtime type checking of tensor shapes (and dtypes, ...)","Hello everyone. I'm excited to announce [torchtyping](https://github.com/patrick-kidger/torchtyping), as a way to document -- and check -- that PyTorch tensors have the correct shape (dtype, names, layout, ...).

Turn this:

    def batch_outer_product(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        # x has shape (batch, x_channels)
        # y has shape (batch, y_channels)
        # return has shape (batch, x_channels, y_channels)
    
        return x.unsqueeze(-1) * y.unsqueeze(-2)

into this:

    def batch_outer_product(x:   TensorType[""batch"", ""x_channels""],
                            y:   TensorType[""batch"", ""y_channels""]
                            ) -> TensorType[""batch"", ""x_channels"", ""y_channels""]:

        return x.unsqueeze(-1) * y.unsqueeze(-2)

**with runtime checking that the sizes of these channels line up and are consistent.**

Bye-bye bugs! Say hello to enforced, clear documentation of your code.

---

Personally I find that I leave comments on the shape of tensors all over my code, just to keep track of what each function expects. `torchtyping` is designed to fix this.

Check out the documentation [on GitHub](https://github.com/patrick-kidger/torchtyping) for usage, examples, and ways to extend `torchtyping`.

Finally if you're curious, then have a look at the `torchtyping` FAQ: there's a few other libraries out there doing similar things (e.g. for JAX) so if `torchtyping` isn't quite what you're after, then you have a few other options too. :)",hello everyone excite announce torchtyping url way document check pytorch tensors correct shape dtype name layout turn def batch_outer_product x torch tensor torch tensor torch tensor x shape batch x_channels shape batch y_channels return shape batch x_channels y_channels return x unsqueeze 1 unsqueeze 2 def batch_outer_product x tensortype batch x_channels tensortype batch y_channels tensortype batch x_channels y_channels return x unsqueeze 1 unsqueeze 2 runtime check size channel line consistent bye bye bug say hello enforce clear documentation code personally find leave comment shape tensors code keep track function expect torchtyping design fix check documentation github url usage examples ways extend torchtyping finally curious look torchtyping faq libraries similar things e g jax torchtyping quite options
Transit-Strike,MachineLearning,1616942574.0,[D] Does Dataset balance matter for a Style GAN?," When we look at classifiers, if Class 1 is dominant over Class 0. (A lot more samples) that really hurts the accuracy of our model since there is a strong bias towards one of the classes. Our model can now blindly assume that all samples are from Class 1 and it would be right very often.

With a Style GAN I am looking at, I fear I have a similar issue. I need to convert photos into fake paintings and I would say it is easy, but I have a lot more photos than painting.

Would something like that have an affect on Binary Cross Entropy? I think it would since one class is better represented. But at the same time, since the fake images are generated based on real images, we could just ensure that we generate the same number of fake images as the number of real images in the same class.

(If we have 100 paintings, generate 100 fake paintings). The number of Photos then may or may not matter. I am not sure which.

But in such a case, would a Wasserstein Loss over BCE help mitigate any issues? Since it cares about distributions and not class labels.",look classifiers class nmbr dominant class 0 lot sample really hurt accuracy model since strong bias towards one class model blindly assume sample class nmbr would right often style gin look fear similar issue need convert photos fake paint would say easy lot photos paint would something like affect binary cross entropy think would since one class better represent time since fake image generate base real image could ensure generate number fake image number real image class nmbr paint generate nmbr fake paint number photos may may matter sure case would wasserstein loss bce help mitigate issue since care distributions class label
jj4646,MachineLearning,1619072295.0,[D] why did kernel methods become less popular than neural networks?,"I was reading today that in earlier neural networks, a popular choice of the the activation function in neural networks was the ""radial basis function"" (RBF). This is apparently why earlier neural networks were called ""kernel approximators"". This was a bit surprising to me, seeing as now most people right away assume that the popular choice of activation functions in neural networks is ReLu. It seems to me that the transition away from RBF activation functions happened around the same time that neural networks overtook SVM's in terms of popularity.

Does anyone know why this happened? I read that neural networks with the RBF function also (along with standard neural networks) have the ""universal approximation property"" - i.e., theoretically, they are able to approximate any target function to any level of accuracy (how efficiently, this is another question).  

Is there a reason that RBF's lost their popularity? Does anyone know why ReLu is the go-to activation function for neural networks these days? 

Just a guess: perhaps a neural network with ReLu activations is able to ""better"" (consistency, convergence) approximate the same target function compared to RBF, given the same ""resources"" (e.g. number of neurons and layers)?

Thanks",read today earlier neural network popular choice activation function neural network radial basis function rbf apparently earlier neural network call kernel approximators bite surprise see people right away assume popular choice activation function neural network relu seem transition away rbf activation function happen around time neural network overtake svm term popularity anyone know happen read neural network rbf function also along standard neural network universal approximation property e theoretically able approximate target function level accuracy efficiently another question reason rbf lose popularity anyone know relu go activation function neural network days guess perhaps neural network relu activations able better consistency convergence approximate target function compare rbf give resources e g number neurons layer thank
zhangboknight,MachineLearning,1617946769.0,[P] Colorizing the legacy videos with attention mechanism,"We recently released the code for our paper ""Deep Exemplar-based Video Colorization"". The code along with the **Colab demo** is available at: [https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization](https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization). Welcome to have a try.

https://preview.redd.it/qsx2amq253s61.png?width=2466&format=png&auto=webp&s=96eee6ce9db190c57f2701cf0b517f4def866d6f

Youtube demo is available at:

[https://youtube.com/watch?v=HXWR5h5vVYI&feature=share](https://youtube.com/watch?v=HXWR5h5vVYI&feature=share)",recently release code paper deep exemplar base video colorization code along colab demo available url welcome try url demo available url
rafcy,MachineLearning,1617542304.0,[P] Object Detection and Tracking,"Sharing here this project on my github since this version is being published on CodeOcean as well. Noted that you can change the Darknet's YOLO trained weights with your own YOLO detector and use this repository as a detector-tracker for the objects of your choice.

Language: Python

Project refers to this paper : [https://arxiv.org/abs/2007.03227](https://arxiv.org/abs/2007.03227) (also an IEEE publication can be found in the repo description)

Github Repository : [https://github.com/rafcy/HarpyTM](https://github.com/rafcy/HarpyTM)

Brief description :

>The main purpose of the application is to extract traffic data from vehicles on roads using aerial footage taken from static UAVs. To process the footage, deep neural network detector is used (YOLO) alongside with the OpenCV library in ordered to be executed in python. Furthermore, multiple algorithms are used, such as Kalman, Hungarian, in order to match the detections between sequential frames and extract the vehicles and their trajectories. Hence, the velocities and the moving direction of the vehicles are also calculated for each vehicle for every frame.

You are free to discuss about it or suggest any improvments/features.",share project github since version publish codeocean well note change darknet yolo train weight yolo detector use repository detector tracker object choice language pythonproject refer paper url also ieee publication find repo description github repository url description main purpose application extract traffic data vehicles roads use aerial footage take static uavs process footage deep neural network detector use yolo alongside opencv library order execute python furthermore multiple algorithms use kalman hungarian order match detections sequential frame extract vehicles trajectories hence velocities move direction vehicles also calculate vehicle every frame free discuss suggest improvments feature
Yungpastorphillswift,MachineLearning,1616744476.0,[D] Is it possible to combine the weights of multiple CNN Models?,"First of all, I want to thank all of you guys for the help you've given me since I first took the dive into Data Science and Machine Learning. I don't post often at all, but I have gone through almost all of the material and resources in the Data Science Wiki, and I'm lurking around in here almost everyday, picking up little jewels of information. I have now clocked in about 800 or so dedicated hours to learning, and I am super excited about all the wild and interesting things I still don't know about. The deeper I go, the more I realize that this rabbit hole never ends. So for real and honestly, thank you guys!

Now on to my question; I am currently working on a semi large-scale Facial Recognition project with a small group. We have scraped, compiled and labeled a huge dataset of image files (aprox. 2mil images). We have them stored in a cloud database, and will train our model in batches by downloading and unzipping directly from the cloud to our notebook. I had the idea that it would be extremely time efficient if everyone in our group pulled a different batch of images, trained models on our own respective machines, and then combined our saved weights into a final model. In theory, the idea sounded good, however, after searching around google for a bit, it's not clear if this is possible, or if it's even a beneficial idea.

Looking for any input, direction towards resources, or helpful advice in general. Thanks in advance!",first want thank guy help give since first take dive data science machine learn post often go almost material resources data science wiki lurk around almost everyday pick little jewel information clock nmbr dedicate hours learn super excite wild interest things still know deeper go realize rabbit hole never end real honestly thank guy question currently work semi large scale facial recognition project small group scrap compile label huge dataset image file aprox 2mil image store cloud database train model batch download unzip directly cloud notebook idea would extremely time efficient everyone group pull different batch image train model respective machine combine save weight final model theory idea sound good however search around google bite clear possible even beneficial idea look input direction towards resources helpful advice general thank advance
optimized-adam,MachineLearning,1620252116.0,[D] Sub-pixel convolutions vs. transposed convolutions,"I am trying to understand the different types of convolutions used for upsampling. In particular, the difference between sub-pixel convolutions and transposed convolutions (or lack thereof). My current understanding is that they are equivalent operations (and from my understanding the authors of the sub-pixel convolution have shown this equivalency in the original paper [https://arxiv.org/abs/1609.05158](https://arxiv.org/abs/1609.05158)). However the difference is that the sub-pixel convolution can be implemented more efficiently.

Is this understanding correct? If so, why are some people (e.g. [https://github.com/atriumlts/subpixel](https://github.com/atriumlts/subpixel)) strongly recommending sub-pixel convolutions over transposed convolutions for what seem to be reasons other than just performance?",try understand different type convolutions use upsampling particular difference sub pixel convolutions transpose convolutions lack thereof current understand equivalent operations understand author sub pixel convolution show equivalency original paper url however difference sub pixel convolution implement efficiently understand correct people e g url strongly recommend sub pixel convolutions transpose convolutions seem reason performance
natalieberlin,MachineLearning,1618937360.0,[P] Applied our research (ML on dynamic knowledge graphs) to files and documents,"We have built our first ""smart"" feature into our app. Now, we can show when ""similar"" content likely needs changes. Say, you have 20 files (invoices/contracts/job postings) that all include an address, a company description/name, an IBAN/routing number, legal clauses. You open one of them and edit that clause/change the address/IBAN/company description. Our engine will say ""there are 4 files that probably require changes, too"". 

We use LSH & MinHashing for the file similarities and run our ML for dynamic knowledge graphs to determine which of those 20 ""similar files"" are still active (papers below)

A quick graphic: 

https://preview.redd.it/imrsumx8xcu61.png?width=2856&format=png&auto=webp&s=926e1d90dbba93457917d6cbf09720e415069a86

Papers here:  

[https://dl.acm.org/doi/abs/10.1145/3038912.3052672](https://dl.acm.org/doi/abs/10.1145/3038912.3052672)

[https://arxiv.org/abs/1905.05305](https://arxiv.org/abs/1905.05305)

[http://proceedings.mlr.press/v124/tabibian20a](http://proceedings.mlr.press/v124/tabibian20a)

[https://www.pnas.org/content/116/10/3988.short](https://www.pnas.org/content/116/10/3988.short)

[https://dl.acm.org/doi/abs/10.1145/3018661.3018685](https://dl.acm.org/doi/abs/10.1145/3018661.3018685)

[https://dl.acm.org/doi/abs/10.1145/2939672.2939875](https://dl.acm.org/doi/abs/10.1145/2939672.2939875)

[https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)

&#x200B;

(for good measure: we're just building a little community here [r/reasonal](https://www.reddit.com/r/reasonal/), or you can get to the survey to get access to our closed beta here [reason.al](https://reason.al/?utm_source=reddit.com&utm_medium=referral&utm_campaign=comm-rd-21-010-mling))",build first smart feature app show similar content likely need change say nmbr file invoice contract job post include address company description name iban rout number legal clauses open one edit clause change address iban company description engine say nmbr file probably require change use lsh minhashing file similarities run ml dynamic knowledge graph determine nmbr similar file still active paper quick graphic url url good measure build little community r reasonal url get survey get access close beta reason al url
svij137,MachineLearning,1620442366.0,[D] Number of businesses that actually spend money on training their own AI models?,"I have been looking for this data but seems most reports are heavily skewed towards larger companies like amazon and google.

Or i get a generic answer that companies spent over $100B last year

But that’s not the answer i am looking for as 90% of that spend could be just from the top 100 companies

What’s the best way to get around this?",look data seem report heavily skew towards larger company like amazon google get generic answer company spend 100b last yearbut thats answer look 90 spend could top nmbr companieswhats best way get around
designer1one,MachineLearning,1618667070.0,[P] *Semantic* Video Search with OpenAI’s CLIP Neural Network,"I made a simple tool that lets you search a video \*semantically\* with AI. 🎞️🔍

✨ Live web app: [http://whichframe.com](http://whichframe.com/) ✨

Example: Which video frame has a person with sunglasses and earphones?

The querying is powered by OpenAI’s CLIP neural network for performing ""zero-shot"" image classification and the interface was built with Streamlit.

Try searching with text, image, or text + image and please share your discoveries!

👇 More examples  
[https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)",make simple tool let search video semantically ai live web app url example video frame person sunglasses earphones query power openais clip neural network perform zero shoot image classification interface build streamlit try search text image text image please share discoveries examples url
ykilcher,MachineLearning,1618150557.0,"[D] Paper Explained - DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning (Full Video Analysis)","[https://youtu.be/qtu0aSTDE2I](https://youtu.be/qtu0aSTDE2I)

Classic Machine Learning struggles with few-shot generalization for tasks where humans can easily generalize from just a handful of examples, for example sorting a list of numbers. Humans do this by coming up with a short program, or algorithm, that explains the few data points in a compact way. DreamCoder emulates this by using neural guided search over a language of primitives, a library, that it builds up over time. By doing this, it can iteratively construct more and more complex programs by building on its own abstractions and therefore solve more and more difficult tasks in a few-shot manner by generating very short programs that solve the few given datapoints. The resulting system can not only generalize quickly but also delivers an explainable solution to its problems in form of a modular and hierarchical learned library. Combining this with classic Deep Learning for low-level perception is a very promising future direction.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:55 - DreamCoder System Architecture

9:00 - Wake Phase: Neural Guided Search

19:15 - Abstraction Phase: Extending the Internal Library

24:30 - Dreaming Phase: Training Neural Search on Fictional Programs and Replays

30:55 - Abstraction by Compressing Program Refactorings

32:40 - Experimental Results on LOGO Drawings

39:00 - Ablation Studies

39:50 - Re-Discovering Physical Laws

42:25 - Discovering Recursive Programming Algorithms

44:20 - Conclusions & Discussion

&#x200B;

Paper: [https://arxiv.org/abs/2006.08381](https://arxiv.org/abs/2006.08381)

Code: [https://github.com/ellisk42/ec](https://github.com/ellisk42/ec)",url machine learn struggle shoot generalization task humans easily generalize handful examples example sort list number humans come short program algorithm explain data point compact way dreamcoder emulate use neural guide search language primitives library build time iteratively construct complex program build abstractions therefore solve difficult task shoot manner generate short program solve give datapoints result system generalize quickly also deliver explainable solution problems form modular hierarchical learn library combine classic deep learn low level perception promise future direction x200b outline 0 00 intro overview4 55 dreamcoder system architecture9 00 wake phase neural guide search19 15 abstraction phase extend internal library24 30 dream phase train neural search fictional program replays30 55 abstraction compress program refactorings32 40 experimental result logo drawings39 00 ablation studies39 50 discover physical laws42 25 discover recursive program algorithms44 20 conclusions discussion x200b paper url url
eatpasta_runfastah,MachineLearning,1619618374.0,[D] Masking gradients before the update,"Hello  


I was reading this paper [Learning explanations that are hard to vary](https://arxiv.org/abs/2009.00329) and found the relative [github repo](https://github.com/gibipara92/learning-explanations-hard-to-vary/blob/main/notebooks/linear_regression_ilc.ipynb). To keep it short, before updating the parameters  `theta = theta - lr * final_grads` pytorch (cuda) computes by default the arithmetic mean of the gradients, whereas I want to compute the geometric mean or to apply a mask as shown in the code. 

 Is there a way to do this leveraging pytorch autograd + cuda without the need to write a custom training loop?",hello read paper learn explanations hard vary url find relative github repo url keep short update parameters theta theta lr final_grads pytorch cuda compute default arithmetic mean gradients whereas want compute geometric mean apply mask show code way leverage pytorch autograd cuda without need write custom train loop
prestodigitarium,MachineLearning,1619288479.0,"[P] Gourdian Free Dataset Download: OpenStreetMap Points of Interest (Restaurants, Bars, Grocery Stores, Transit, Shops, Swingers Clubs, Hospitals, etc)","Hi there!

A friend and I are working on something to help people search for, filter, and download subsets of datasets.

We're excited to share that we've just incorporated all(?) of the points of interest from OpenStreetMap, broken down by group (from their ontologies here https://wiki.openstreetmap.org/wiki/Key:amenity and here https://wiki.openstreetmap.org/wiki/Key:shop )

The groups are below, with the tags that went into each. 

What do people think these might be useful for? Maybe making your own version of WalkScore? Perhaps cross referencing with real estate listings to find a house that's within walking distance of a bakery, library, cafe, and pyrotechnics shop? LoveHotelMapper.com? The possibilities are endless!

**Restaurants and Bars**: https://gourdian.net/g/eric/osm_points_of_interest.restaurants_and_bars

Amenities points of interest labeled with bar, biergarten, cafe, fast_food, food_court, ice_cream, pub, or restaurant.

**Education Services**: https://gourdian.net/g/eric/osm_points_of_interest.education_services

Amenities points of interest labeled with college, driving_school, kindergarten, language_school, library, toy_library, music_school, school, or university.

**Transportation Related**: https://gourdian.net/g/eric/osm_points_of_interest.transportation_related

Amenities points of interest labeled with bicycle_parking, bicycle_repair_station, bicycle_rental, boat_rental, boat_sharing, bus_station, car_rental, car_sharing, car_wash, vehicle_inspection, charging_station, ferry_terminal, fuel, grit_bin, motorcycle_parking, parking, parking_entrance, parking_space, or taxi.

**Financial**: https://gourdian.net/g/eric/osm_points_of_interest.financial

Amenities points of interest labeled with atm, bank, or bureau_de_change.

**Healthcare Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.healthcare_facilities

Amenities points of interest labeled with baby_hatch, clinic, dentist, doctors, hospital, nursing_home, pharmacy, social_facility, or veterinary.

**Entertainment**: https://gourdian.net/g/eric/osm_points_of_interest.entertainment

Amenities points of interest labeled with arts_centre, brothel, casino, cinema, community_centre, conference_centre, events_venue, fountain, gambling, love_hotel, nightclub, planetarium, public_bookcase, social_centre, stripclub, studio, swingerclub, or theatre.

**Public Services**: https://gourdian.net/g/eric/osm_points_of_interest.public_services

Amenities points of interest labeled with courthouse, embassy, fire_station, police, post_box, post_depot, post_office, prison, ranger_station, or townhall.

**Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.facilities

Amenities points of interest labeled with bbq, bench, dog_toilet, drinking_water, give_box, shelter, shower, telephone, toilets, water_point, or watering_place.

**Waste Management**: https://gourdian.net/g/eric/osm_points_of_interest.waste_management

Amenities points of interest labeled with sanitary_dump_station, recycling, waste_basket, waste_disposal, or waste_transfer_station.

**Other Amenities**: https://gourdian.net/g/eric/osm_points_of_interest.other_amenities

Amenities points of interest labeled with animal_boarding, animal_breeding, animal_shelter, baking_oven, childcare, clock, crematorium, dive_centre, funeral_hall, grave_yard, gym, hunting_stand, internet_cafe, kitchen, kneipp_water_cure, lounger, marketplace, monastery, photo_booth, place_of_mourning, place_of_worship, public_bath, public_building, refugee_site, or vending_machine.

**Food Shops**: https://gourdian.net/g/eric/osm_points_of_interest.food_shops

Shops points of interest labeled with alcohol, bakery, beverages, brewing_supplies, butcher, cheese, chocolate, coffee, confectionery, convenience, deli, dairy, farm, frozen_food, greengrocer, health_food, ice_cream, organic, pasta, pastry, seafood, spices, tea, wine, or water.

**General Shops**: https://gourdian.net/g/eric/osm_points_of_interest.general_shops

Shops points of interest labeled with department_store, general, kiosk, mall, supermarket, or wholesale.

**Clothing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.clothing_shops

Shops points of interest labeled with baby_goods, bag, boutique, clothes, fabric, fashion, fashion_accessories, jewelry, leather, sewing, shoes, tailor, watches, or wool.

**Second Hand Shops**: https://gourdian.net/g/eric/osm_points_of_interest.second_hand_shops

Shops points of interest labeled with charity, second_hand, or variety_store.

**Health and Beauty Shops**: https://gourdian.net/g/eric/osm_points_of_interest.health_and_beauty_shops

Shops points of interest labeled with beauty, chemist, cosmetics, drugstore, erotic, hairdresser, hairdresser_supply, hearing_aids, herbalist, massage, medical_supply, nutrition_supplements, optician, perfumery, or tattoo.

**Hardware Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hardware_shops

Shops points of interest labeled with agrarian, appliance, bathroom_furnishing, doityourself, electrical, energy, fireplace, florist, garden_centre, garden_furniture, gas, glaziery, groundskeeping, hardware, houseware, locksmith, paint, security, trade, or windows.

**Furnishing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.furnishing_shops

Shops points of interest labeled with antiques, bed, candles, carpet, curtain, doors, flooring, furniture, household_linen, interior_decoration, kitchen, lamps, lighting, tiles, or window_blind.

**Electronics Shops**: https://gourdian.net/g/eric/osm_points_of_interest.electronics_shops

Shops points of interest labeled with computer, electronics, hifi, mobile_phone, radiotechnics, or vacuum_cleaner.

**Vehicle and Outdoor Shops**: https://gourdian.net/g/eric/osm_points_of_interest.vehicle_and_outdoor_shops

Shops points of interest labeled with atv, bicycle, boat, car, car_repair, car_parts, caravan, fuel, fishing, golf, hunting, jetski, military_surplus, motorcycle, outdoor, scuba_diving, ski, snowmobile, sports, swimming_pool, trailer, or tyres.

**Hobby Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hobby_shops

Shops points of interest labeled with art, collector, craft, frame, games, model, music, musical_instrument, photo, camera, trophy, video, or video_games.

**Stationary and Gift Shops**: https://gourdian.net/g/eric/osm_points_of_interest.stationary_and_gift_shops

Shops points of interest labeled with anime, books, gift, lottery, newsagent, 
stationery, or ticket.

**Other Shops**: https://gourdian.net/g/eric/osm_points_of_interest.other_shops

Shops points of interest labeled with bookmaker, cannabis, copyshop, dry_cleaning, e-cigarette, funeral_directors, laundry, money_lender, party, pawnbroker, pet, pet_grooming, pest_control, pyrotechnics, religion, storage_rental, tobacco, toys, travel_agency, vacant, weapons, outpost, or user defined.


A bit about what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Feedback welcome! If there are any datasets that you'd like to see added, let us know!

Also, if you'd like updates as we put up more datasets, we just got on the Twitters as [@GourdianData](https://twitter.com/GourdianData) .",hi friend work something help people search filter download subsets datasets excite share incorporate point interest openstreetmap break group ontologies url url group tag go people think might useful maybe make version walkscore perhaps cross reference real estate list find house within walk distance bakery library cafe pyrotechnics shop lovehotelmapper com possibilities endless restaurants bar url point interest label bar biergarten cafe fast_food food_court ice_cream pub restaurant education service url point interest label college driving_school kindergarten language_school library toy_library music_school school university transportation relate url point interest label bicycle_parking bicycle_repair_station bicycle_rental boat_rental boat_sharing bus_station car_rental car_sharing car_wash vehicle_inspection charging_station ferry_terminal fuel grit_bin motorcycle_parking park parking_entrance parking_space taxi financial url point interest label atm bank bureau_de_change healthcare facilities url point interest label baby_hatch clinic dentist doctor hospital nursing_home pharmacy social_facility veterinary entertainment url point interest label arts_centre brothel casino cinema community_centre conference_centre events_venue fountain gamble love_hotel nightclub planetarium public_bookcase social_centre stripclub studio swingerclub theatre public service url point interest label courthouse embassy fire_station police post_box post_depot post_office prison ranger_station townhall facilities url point interest label bbq bench dog_toilet drinking_water give_box shelter shower telephone toilets water_point watering_place waste management url point interest label sanitary_dump_station recycle waste_basket waste_disposal waste_transfer_station amenities url point interest label animal_boarding animal_breeding animal_shelter baking_oven childcare clock crematorium dive_centre funeral_hall grave_yard gym hunting_stand internet_cafe kitchen kneipp_water_cure lounger marketplace monastery photo_booth place_of_mourning place_of_worship public_bath public_building refugee_site vending_machine food shop url point interest label alcohol bakery beverages brewing_supplies butcher cheese chocolate coffee confectionery convenience deli dairy farm frozen_food greengrocer health_food ice_cream organic pasta pastry seafood spice tea wine water general shop url point interest label department_store general kiosk mall supermarket wholesale clothe shop url point interest label baby_goods bag boutique clothe fabric fashion fashion_accessories jewelry leather sew shoe tailor watch wool second hand shop url point interest label charity second_hand variety_store health beauty shop url point interest label beauty chemist cosmetics drugstore erotic hairdresser hairdresser_supply hearing_aids herbalist massage medical_supply nutrition_supplements optician perfumery tattoo hardware shop url point interest label agrarian appliance bathroom_furnishing doityourself electrical energy fireplace florist garden_centre garden_furniture gas glaziery groundskeeping hardware houseware locksmith paint security trade windows furnish shop url point interest label antique bed candle carpet curtain doors floor furniture household_linen interior_decoration kitchen lamps light tile window_blind electronics shop url point interest label computer electronics hifi mobile_phone radiotechnics vacuum_cleaner vehicle outdoor shop url point interest label atv bicycle boat car car_repair car_parts caravan fuel fish golf hunt jetski military_surplus motorcycle outdoor scuba_diving ski snowmobile sport swimming_pool trailer tyres hobby shop url point interest label art collector craft frame game model music musical_instrument photo camera trophy video video_games stationary gift shop url point interest label anime book gift lottery newsagent stationery ticket shop url point interest label bookmaker cannabis copyshop dry_clean e cigarette funeral_directors laundry money_lender party pawnbroker pet pet_grooming pest_control pyrotechnics religion storage_rental tobacco toy travel_agency vacant weapons outpost user define bite try build filter optional click button csv arrive hard drive download always single csv bundle weird directory structure format csvs index filterable column type lat long date time moment download part want open license datasets free download signup require download open datasets search within across datasetsfeedback welcome datasets like see add let us know also like update put datasets get twitter gourdiandata url
spot4992,MachineLearning,1619196341.0,[D] Going From 4 Core/8 Thread CPU To 32 Core/64 Thread CPU," What kind of speed up could be expected going from a 4c/8t CPU to a 32c/64t CPU for a training that runs in parallel on all threads? The specific method being used is the Catboost CPU training. I'm considering upgrading to a CPU with significantly more threads than my current CPU, but want to make sure it's worth it and I can't find anything on Google.",kind speed could expect go 4c 8t cpu 32c 64t cpu train run parallel thread specific method use catboost cpu train consider upgrade cpu significantly thread current cpu want make sure worth find anything google
everybody_wants_some,MachineLearning,1618599546.0,[P] Is such a project/task doable using machine learning?,"Hello everyone. I hope that this is the right subreddit for this kind of question. If not, then I want to apologise in advance. 

So my problem is that I was offered to solve a problem with machine learning which I am not sure that it actually is possible to do so. I could not find any literature on it. 

**So here is the background story:**

I was offered a research internship topic at my university. It would be 9 weeks full-time. From the expected level it is between a bachelor’s thesis and a master’s thesis. My supervisors do not have much knowledge about machine learning, since this is not the main focus of their work and can therefore not help me much. I have undergrad knowledge of machine learning. My main knowledge is in classical ML algorithms (SVM, kNN etc.) and DL (mainly CNNs).

However, I am not sure if this topic can be solved with machine learning and if yes then how. 

I am going to describe the goal of the internship and what they expect of me as good as possible. 

The goal is to get accurate wind velocity values for small areas/regions using machine learning. 

**Those are the things I have:**

So I have the MERA dataset which has hourly weather data, like wind velocity, pressure etc. for the entire world.The resolution of the data is 50 km times 50 km. I am going to refer to such 2500 km\^2 squares as big squares. So now one can divide Europe into such big squares. However, since the squares are very big, the accuracy of the wind velocity is not good. Therefore, I have a second dataset the Wind Atlas dataset which has a higher resolution. The squares are 3 km times 3 km. I am going to name this squares as small squares. 

So each big square can be divided into many small squares. Approximately 280 small squares fit into one big square. However, this squares do not have hourly data. They only have one wind velocity value per year. 

Additionally, I will have topological data, e.g. heights, where forests, mountains etc. lie.

**This is what I am supposed to do:**

I am somehow supposed to predict (with the help of ML) for each of this small squares which wind speed it had during some time in the past. For example, in some small square what was the wind velocity of it in May 10 2010 (optionally between 3 - 4 PM).

As a “test set“ I will only have some values from a few weather stations. Those are of course very accurate but only for a very small region. But the number of weather stations will be very small. I will have like maybe 10 weather stations for the entire Germany. Meanwhile, Germany consist of approximately 140 big squares and 40 000 small squares.  

**So my question is:**

I do not have much knowledge regarding this kind of problem. Can this actually be done with machine learning? My gut instinct is no, since it seems too utopian for me. The task seems too difficult and there is hardly any useful test data. But I would like to verify that by some of you who are way more experienced than me. 

Is this doable in general with only machine learning without any other mathematical methods? 

Can someone with my knowledge do something like that on my own in just 9 weeks? 

And if it doable, what methods, algorithms are out there? 

&#x200B;

P.S. I am sorry if the formatting is not good. I am not someone who usually posts on Reddit.

But I really need some help with that. Also I am sorry for any grammar errors since I am not a native speaker.",hello everyone hope right subreddit kind question want apologise advance problem offer solve problem machine learn sure actually possible could find literature background story offer research internship topic university would nmbr weeks full time expect level bachelor thesis master thesis supervisors much knowledge machine learn since main focus work therefore help much undergrad knowledge machine learn main knowledge classical ml algorithms svm knn etc dl mainly cnns however sure topic solve machine learn yes go describe goal internship expect good possible goal get accurate wind velocity value small areas regions use machine learn things mera dataset hourly weather data like wind velocity pressure etc entire world resolution data nmbr km time nmbr km go refer nmbr km 2 square big square one divide europe big square however since square big accuracy wind velocity good therefore second dataset wind atlas dataset higher resolution square nmbr km time nmbr km go name square small square big square divide many small square approximately nmbr small square fit one big square however square hourly data one wind velocity value per year additionally topological data e g heights forest mountains etc lie suppose somehow suppose predict help ml small square wind speed time past example small square wind velocity may nmbr 2010 optionally nmbr nmbr pm test set value weather station course accurate small region number weather station small like maybe nmbr weather station entire germany meanwhile germany consist approximately nmbr big square nmbr 000 small square question much knowledge regard kind problem actually machine learn gut instinct since seem utopian task seem difficult hardly useful test data would like verify way experience doable general machine learn without mathematical methods someone knowledge something like nmbr weeks doable methods algorithms x200b p sorry format good someone usually post reddit really need help also sorry grammar errors since native speaker
SQL_beginner,MachineLearning,1619582066.0,"[D] understanding the ""bottleneck"" principle in machine learning","https://openreview.net/forum?id=ry_WPG-A-

Can someone please try to explain in simple terms, what is the bottleneck principle in machine learning and why its important?

Thanks",url someone please try explain simple term bottleneck principle machine learn important thank
Massive-Marzipan,MachineLearning,1617927599.0,[P] Feedback requested on nlp project related to news story chains changing over time,"This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event. Does this approach sound reasonable? Is there anything else I should be trying instead? Thanks everyone!",sub helpful past hop get feedback project essentially try find way detect change time news narrative stage apply algorithm successfully group together stories follow development event need find way track analyze maybe quantify events coverage change current approach use topic model find important keywords article use key word map stories change overtime basic term identify key word first article narrative chain compare keywords change different key word identify subsequent article event approach sound reasonable anything else try instead thank everyone
kanxx030,MachineLearning,1616756076.0,[D] How to get into prestigious research labs,"Hi all. I have completed a Master's and would like to explore PhD opportunities. However, it's been hard to get in touch with anyone from the research groups that I'm interested in. Not to mention I've recently moved countries (currently in the UK). Would love to get advice from the community on how to get myself noticed by top research labs/ ways to start a conversation about researching with them besides cold emailing. Thank you in advance :)",hi complete master would like explore phd opportunities however hard get touch anyone research group interest mention recently move countries currently uk would love get advice community get notice top research labs ways start conversation research besides cold email thank advance
Arioxel_,MachineLearning,1620586861.0,[P] How do you cope with very little data ?,"I am currently working on a project of machine learning interpolation (more or less, but what's to know is it's not classification and my output is a vector of floats) and my issue is I have very little data.

I have a unique set of data of around 50 items to train and test my model with. That's all and nothing more. Fortunately, those are quite simple (not pictures whatsoever).

How do you think I could cope with this issue and especially how to divide the set between training and testing ? I thought that maybe I could build new data out of thin air by, for example, averaging two data.",currently work project machine learn interpolation less know classification output vector float issue little data unique set data around nmbr items train test model nothing fortunately quite simple picture whatsoever think could cope issue especially divide set train test think maybe could build new data thin air example average two data
trackerFF,MachineLearning,1616484384.0,[D] What's the most comprehensive book on mathematical theory behind Deep Learning?,"Hi, 

I'm looking for a book on the math behind current Deep Learning topics - and a lot of the papers I read simply reference the Ian Goodfellow et.al book, when it comes to mathematical proofs. 

Has there been released any comprehensive book that focuses on the mathematical rigor behind Deep Learning - anything the past 2-4 years that's worth checking out?",hi look book math behind current deep learn topics lot paper read simply reference ian goodfellow et al book come mathematical proof release comprehensive book focus mathematical rigor behind deep learn anything past 2 4 years worth check
QueasyArm8328,MachineLearning,1619987846.0,[Discussion] Graphics in Python,"Hello!  I've been wondering what libraries are used for graphics in Python by professionals in ML; I've just been using Pygame for my pet projects but something tells me this isn't the industry standard.

Thanks!",hello wonder libraries use graphics python professionals ml use pygame pet project something tell industry standard thank
Yuqing7,MachineLearning,1617841746.0,"[N] DeepMind, Microsoft, Allen AI & UW Researchers Convert Pretrained Transformers into RNNs, Lowering Memory Cost While Retaining High Accuracy","A research team from University of Washington, Microsoft, DeepMind and Allen Institute for AI develop a method to convert pretrained transformers into efficient RNNs. The Transformer-to-RNN (T2R) approach speeds up generation and reduces memory cost.

Here is a quick read: [DeepMind, Microsoft, Allen AI & UW Researchers Convert Pretrained Transformers into RNNs, Lowering Memory Cost While Retaining High Accuracy](https://syncedreview.com/2021/04/07/deepmind-microsoft-allen-ai-uw-researchers-convert-pretrained-transformers-into-rnns-lowering-memory-cost-while-retaining-high-accuracy/)

The paper *Finetuning Pretrained Transformers into RNNs* is on [arXiv](https://arxiv.org/pdf/2103.13076.pdf).",research team university washington microsoft deepmind allen institute ai develop method convert pretrained transformers efficient rnns transformer rnn t2r approach speed generation reduce memory cost quick read deepmind microsoft allen ai uw researchers convert pretrained transformers rnns lower memory cost retain high accuracy url paper finetuning pretrained transformers rnns arxiv url
study_ai,MachineLearning,1616922070.0,[D][R] Best way to pick up functional analysis for kernel methods' research,"I am looking into applying for PhD in ML. It is quite possible that my research area would be kernel methods.

The problem is - I never had a course in functional analysis. My background: CS Master's in ML, self-learned real analysis at the level of baby Rudin chapters 1-5, abstract algebra, statistics/probability from stats department, and currently reading Casella & Berger.

I am looking a concise book in functional analysis. 2 options I found: Kreiszig's ""Introductory functional analysis with applications"" and Axler's ""Measure, Integration and Real Analysis"".

The problem is; I am not sure how much efforts these books are. I would like some rigorous material BUT with simple exercises. Something way simpler than baby Rudin in terms of exercises.

Would you recommend some book?

Or maybe i approach the problem incorrectly? How did YOU learn kernel methods' prerequisites?",look apply phd ml quite possible research area would kernel methods problem never course functional analysis background cs master ml self learn real analysis level baby rudin chapters 1 5 abstract algebra statistics probability stats department currently read casella berger look concise book functional analysis nmbr options find kreiszig introductory functional analysis applications axler measure integration real analysis problem sure much efforts book would like rigorous material simple exercise something way simpler baby rudin term exercise would recommend book maybe approach problem incorrectly learn kernel methods prerequisites
kaiser_17,MachineLearning,1617537298.0,[D] How is the current research in Long tailed classification?,"I have been going through a lot of literature on long tailed distribution based classification recently. It seems if you were cluster those papers ,they will most likely belong to one of these 3 types:

1) Sampling Based

2) Class weighted losses

3) Meta learning based(which is the new trend)

My question is ,are there any papers which go beyond these 3 categories or has the research been limited to only these 3?",go lot literature long tail distribution base classification recently seem cluster paper likely belong one nmbr type 1 sample based2 class weight losses3 meta learn base new trend question paper go beyond nmbr categories research limit 3
yourpaljon,MachineLearning,1619119971.0,[D] Why isn't quantile regression used more in neural networks?,"Isn't quantile regression a good solution for estimating uncertainty in neural networks? I haven't seen much use of it, any reason why?",quantile regression good solution estimate uncertainty neural network see much use reason
craffel,MachineLearning,1620323975.0,"[R] ICLR Workshop on Enormous Language Models - May 7th, 2021 (livestream)","The  ICLR workshop on Enormous Language Models will take place on May 7th,  2021, virtually. The workshop will include talks and panels by experts  on training large LMs. The goal is to answer questions like: Will  scaling lead to models that outperform humans on all text-based tasks,  or are there limits to the scalability of these models? Should we focus  on simply scaling these models, or should we design more sophisticated  architectures and training schemes? Do our current benchmark effectively  test capabilities that humans can master but large language models  lack? How can we address the legal and ethical issues that arise from  using unstructured web crawls for training language models? What can we  learn from the fields of cognition, linguistics, and philosophy as we  attempt to measure the ""intelligence"" of machines?

Full information is available here: [https://welmworkshop.github.io/](https://welmworkshop.github.io/) and a livestream will appear at [https://welmworkshop.github.io/livestream/](https://welmworkshop.github.io/livestream/).",iclr workshop enormous language model take place may 7th 2021 virtually workshop include talk panel experts train large lms goal answer question like scale lead model outperform humans text base task limit scalability model focus simply scale model design sophisticate architectures train scheme current benchmark effectively test capabilities humans master large language model lack address legal ethical issue arise use unstructured web crawl train language model learn field cognition linguistics philosophy attempt measure intelligence machine full information available url livestream appear url
GiuPaolo,MachineLearning,1617648036.0,[R] Call for Papers: Evolutionary Reinforcement Learning workshop @ GECCO 2021,"Time is passing fast! Only 1 week to go before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)

In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.

Recent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.

Nevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.

The goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.

The topics at the heart of the workshop include:

* Evolutionary reinforcement learning
* Evolution strategies
* Population-based methods for policy search
* Neuroevolution
* Hard exploration and sparse reward problems
* Deceptive reward
* Novelty and diversity search methods
* Divergent search
* Sample-efficient direct policy search
* Intrinsic motivation, curiosity
* Building or designing behaviour characterizations
* Meta-learning, hierarchical learning
* Evolutionary AutoML
* Open-ended learning

Autors are invited to submit **new original work**, or **new perspectives on recently published work**  on those topics. Top submissions will be selected for oral presentation and be presented alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). 

&#x200B;

Important dates

* Submission deadline: **April 12, 2021**
* Notification: **April 26, 2021**
* Camera-ready: **May 3, 2021**

**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**",time pass fast nmbr week go deadline 1st evolutionary reinforcement learn workshop gecco 2021 premiere conference evolutionary compute year hold virtually lille france july 10 14 2021 recent years reinforcement learn rl receive lot attention thank performance ability address complex task time evolutionary algorithms ea prove competitive standard rl algorithms certain problems simpler scalable recent advance ea lead development algorithms like novelty search quality diversity capable efficiently address complex exploration problems find wealth different policies result developments spark strong renew interest population base computational approach nevertheless even eas perform well hard exploration problems still suffer low sample efficiency limitation less present rl methods notably sample reuse contrary struggle hard exploration settings complementary characteristics rl algorithms eas push researchers explore new approach merge two order harness respective strengths avoid shortcomings goal workshop foster collaboration share perspectives spread best practice within grow community intersection rl ea topics heart workshop include evolutionary reinforcement learn evolution strategies population base methods policy search neuroevolution hard exploration sparse reward problems deceptive reward novelty diversity search methods divergent search sample efficient direct policy search intrinsic motivation curiosity build design behaviour characterizations meta learn hierarchical learn evolutionary automl open end learningautors invite submit new original work new perspectives recently publish work topics top submissions select oral presentation present alongside keynote speaker jeff clune ex team leader uberai labs current research team leader openai x200b important date submission deadline april 12 2021 notification april 26 2021 camera ready may 3 2021 find info workshop website url
Hydra1721,MachineLearning,1620342579.0,"Has There Been Any Follow Up Research Papers for the Anti-FRS AI Called ""Fawkes"" ""[Discussion]""","Last year a research paper was published that described a AI that could alter a imagine in a certain manner that prevented FRS from correctly identifying a individual's face without changing the appearance of the photo to the viewer:

[https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes](https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes)

It was stated at the time that Fawkes was VERY computationally expensive and therefore was unable to perform these modifications in real time. Since the publication, I have not read about any other research papers derived from their original work. Has there been any developments in this field of research since then and if so has anyone managed to develop a network that runs in real-time?",last year research paper publish describe ai could alter imagine certain manner prevent frs correctly identify individual face without change appearance photo viewer url state time fawkes computationally expensive therefore unable perform modifications real time since publication read research paper derive original work developments field research since anyone manage develop network run real time
kaia_1527,MachineLearning,1617626377.0,[P] Basic Floor Plan Image Recognition,"Hi everyone, first-time poster. This one should be easy: is there a model that, given an image, recognizes whether the image is floor plan (typically of a residential property)? Other than a boolean, don't need anything else. Should be quick to train one, but wanted to check whether there's a generally accepted model out there. Thanks!",hi everyone first time poster one easy model give image recognize whether image floor plan typically residential property boolean need anything else quick train one want check whether generally accept model thank
ottawalanguages,MachineLearning,1620432416.0,[D] Reinforcement Learning with R software,"Reinforcement Learning seems to be becoming a popular topic these days. Does anyone know if people are working on Reinforcement Learning problems using R? Does anyone have any links/tutorials that show Reinforcement Learning and Game Theory material being done using R software?

Thanks",reinforcement learn seem become popular topic days anyone know people work reinforcement learn problems use r anyone link tutorials show reinforcement learn game theory material use r software thank
JosiahWGibbs,MachineLearning,1619932666.0,"[D] How do you try out architecture changes, etc. when a model takes days to train?","I am currently training a model that takes an extremely long time to converge. The accuracty is good but not excellent, so I've been trying to introduce some modifications to the architecture, tune its hyperparameters, etc.

Since the model takes so long to train (couple of days, close to a week) it is very hard to get immediate feedback on wether the changes I'm making mean anything at all or if they are just keeping things the same, or worsening them.

I notice that many SOTA models also take an extremely long time to train, so this is probably an issue many people are used to dealing with.

How do researchers typically deal with this problem? Do you have any recomendations? I first thought of extracting a subset of my dataset and training there, and hoping that the results would extrapolate to the whole thing, but it seems to me that the results don't extrapolate very well at all. Sometimes an idea that seems to work very well on the subset just doesn't introduce any  changes on the full dataset, and the reverse also happens (usually for the worse).",currently train model take extremely long time converge accuracty good excellent try introduce modifications architecture tune hyperparameters etc since model take long train couple days close week hard get immediate feedback wether change make mean anything keep things worsen notice many sota model also take extremely long time train probably issue many people use deal researchers typically deal problem recomendations first think extract subset dataset train hop result would extrapolate whole thing seem result extrapolate well sometimes idea seem work well subset introduce change full dataset reverse also happen usually worse
DeMorrr,MachineLearning,1619215359.0,[P] TorchPQ: Efficient Nearest Neighbor Search and Clustering on GPUs,"Hi everyone!

I’m happy to introduce an open source project that I have been working for a while:

[TorchPQ](https://github.com/DeMoriarty/TorchPQ) is a python library for approximate nearest neighbor search on GPUs. It has efficient implementations of [IVFPQ](https://hal.inria.fr/inria-00514462v2/document) algorithm as well as some of its variants (e.g IVFPQ+R). The project is written mostly in python using pytorch library, with some custom CUDA kernels to accelerate clustering, searching and indexing.

TorchPQ allows you to search with tens of thousands of queries in millions of vectors within a second. In some settings where high recall rate is prioritized, TorchPQ outperforms the implementation of the same algorithm in [faiss](https://github.com/facebookresearch/faiss). For benchmark results see the bottom part of the README page.

I’ve also spent a lot of time optimizing the k-means clustering algorithm for gpu, as a result it’s ultra fast and memory efficient. I recommend you to give it a try even if you have no interest in nearest neighbor search.

The project is still in an early stage, there could be bugs or performance issues, feel free to create an issue if you encounter any of those.

Contributions are welcomed!",hi everyone im happy introduce open source project work torchpq url python library approximate nearest neighbor search gpus efficient implementations ivfpq url algorithm well variants e g ivfpq r project write mostly python use pytorch library custom cuda kernels accelerate cluster search index torchpq allow search tens thousands query millions vectors within second settings high recall rate prioritize torchpq outperform implementation algorithm faiss url benchmark result see bottom part readme page ive also spend lot time optimize k mean cluster algorithm gpu result ultra fast memory efficient recommend give try even interest nearest neighbor search project still early stage could bug performance issue feel free create issue encounter contributions welcome
Kal217,MachineLearning,1617403979.0,[P] Intuitive StarGAN Implemented in Tensorflow 2.3,"Lately, a lot of my work has been involving StarGAN, so I wanted to create an easy to read implementation of the architecture that functions, can be freely used, and most importantly that explains what's going on!  \[StarGAN\]([https://arxiv.org/abs/1711.09020](https://arxiv.org/abs/1711.09020)) is a class translation model that uses a single generator to translate freely between some number of classes.  It is an advanced GAN, and although outdated, shares some important ideas.  My implementation documentation is aimed at those who have a familiarity with machine learning.  Please let me know any feedback you might have, this is my first time sharing a project!  


[https://github.com/Kal213/StarGAN-Tutorial-Tensorflow-2.3](https://github.com/Kal213/StarGAN-Tutorial-Tensorflow-2.3)",lately lot work involve stargan want create easy read implementation architecture function freely use importantly explain go stargan url class translation model use single generator translate freely number class advance gin although outdated share important ideas implementation documentation aim familiarity machine learn please let know feedback might first time share project url
putinwhat,MachineLearning,1618758422.0,[D] MLOps Stack,I’ve been researching the different libraries and tools available to use for experiment reproducibility and I recently implemented a few open-source tools like MLFlow for tracking and model storage as well as DVC for data versioning and pipeline generation. I was curious to know what tools have been tried by others and how they’ve worked out?,ive research different libraries tool available use experiment reproducibility recently implement open source tool like mlflow track model storage well dvc data versioning pipeline generation curious know tool try others theyve work
born_in_cyberspace,MachineLearning,1618471311.0,[R][D] On the Impossibility of Supersized Machines,"Found an excellent satire about the typical arguments against artificial general intelligence, by Garfinkel et al (2017):

 [On the Impossibility of Supersized Machines](https://arxiv.org/abs/1703.10987)

**Abstract**

In recent years, a number of prominent computer scientists, along with academics in fields such as philosophy and physics, have lent credence to the notion that machines may one day become as large as humans. Many have further argued that machines could even come to exceed human size by a significant margin. However, there are at least seven distinct arguments that preclude this outcome. We show that it is not only implausible that machines will ever exceed human size, but in fact impossible.",find excellent satire typical arguments artificial general intelligence garfinkel et al 2017 impossibility supersized machine url recent years number prominent computer scientists along academics field philosophy physics lend credence notion machine may one day become large humans many argue machine could even come exceed human size significant margin however least seven distinct arguments preclude outcome show implausible machine ever exceed human size fact impossible
divergentdata,MachineLearning,1620228956.0,[D] Leveraging Dropout for Uncertainty Quantification / Adversary Rejection,"Hey all, I use uncertainty quantification a lot at work for some of our production models and risk mitigation. I made a write-up of some of the underlying ideas [here](https://www.rossidata.com/DropoutTensorFlowUncertaintyErrorMNIST) for some of the background and put a demo notebook [here](https://github.com/NicholasARossi/UQ_methods/blob/master/notebooks/05_Neural_Network_Uncertainty_Quantification_with_Dropout.ipynb). Would love some feedback as i'm the only ML engineer at my company.  Thanks!",hey use uncertainty quantification lot work production model risk mitigation make write underlie ideas url background put demo notebook url would love feedback ml engineer company thank
bendee983,MachineLearning,1617629350.0,[R] Data poisoning circumvents certified adversarial training methods,"A paper accepted at CVPR introduces a new data poisoning method that undermines ""randomized smoothing"" training techniques that make machine learning models against adversarial attacks.

Called “Poisoning Against Certified Defenses” (PACD), the method generates poisoned data that have been optimized for the target robustness techniques. The result is a dataset that reduces the average certified radius (ACR), the distance within which a trained machine learning model remains robust against adversarial perturbations. The technique was tested on GA, MACER, and SmoothAdv, three popular randomized smoothing techniques.

This is a grey-box attack: The attacker needs some knowledge of the target ML model architecture and the training method used. But they don't need access to model weights. PACD data generated for one randomized smoothing technique also transfers to other methods, though not to optimal state.

The main takeaway of the paper is that data security is an underrated aspect of adversarial attacks. While many defense techniques are focused on making model weights robust against adversarial perturbation, there's need for more efforts on detecting adversarial perturbations in training data. We also need more measures to certify the provenance of training data and protect machine learning development and deployment pipelines to prevent the compromise of training data.

Read the coverage of the paper and interview with lead author here:

[https://bdtechtalks.com/2021/04/05/machine-learning-data-poisoning-2/](https://bdtechtalks.com/2021/04/05/machine-learning-data-poisoning-2/)

Full paper here:

[https://arxiv.org/abs/2012.01274](https://arxiv.org/abs/2012.01274)

Implementation here:

[https://github.com/akshaymehra24/poisoning\_certified\_defenses](https://github.com/akshaymehra24/poisoning_certified_defenses)",paper accept cvpr introduce new data poison method undermine randomize smooth train techniques make machine learn model adversarial attack call poison certify defenses pacd method generate poison data optimize target robustness techniques result dataset reduce average certify radius acr distance within train machine learn model remain robust adversarial perturbations technique test ga macer smoothadv three popular randomize smooth techniques grey box attack attacker need knowledge target ml model architecture train method use need access model weight pacd data generate one randomize smooth technique also transfer methods though optimal state main takeaway paper data security underrate aspect adversarial attack many defense techniques focus make model weight robust adversarial perturbation need efforts detect adversarial perturbations train data also need measure certify provenance train data protect machine learn development deployment pipelines prevent compromise train data read coverage paper interview lead author url paper url url
TheCockatoo,MachineLearning,1618916138.0,[D] Arguments for supervised approach when an unsupervised one already exists?,"Assume a niche topic that has received little attention in terms of machine / deep learning papers. There's only one, but it does *unsupervised* deep learning. I'd like to propose a supervised approach, but how can I argue for it in the presence of an unsupervised one? Doesn't ""works without labels"" trump anything supervised?",assume niche topic receive little attention term machine deep learn paper one unsupervised deep learn like propose supervise approach argue presence unsupervised one work without label trump anything supervise
mcbal31,MachineLearning,1620119944.0,[P] Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms,"In this project, we model attention in terms of the collective response of a statistical-mechanical system. We consider a vector generalization of an Ising-like spin system and treat incoming data as applied magnetic fields and outputs of attention modules as spin expectation values in order to rephrase attention as an (inner-loop) fixed-point optimization.

We introduce a slow, explicit attention module which implements adaptive Thouless–Anderson–Palmer mean-field theory and a fast, neural one which parametrizes the so-called Onsager self-correction term. The latter module looks a lot like a transformer module. By approximating/constraining the mean-field equations, we show how a simplified update step appears which mirrors the vanilla transformer architecture, explaining the origin of feed-forward layers and the importance of residual connections.

**Blog:** https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/

**Code:** https://github.com/mcbal/deep-implicit-attention

**TL;DR:**A project on modeling attention as the collective response of a statistical-mechanical system. We use deep equilibrium models to solve a set of self-consistent equations and provide a mean-field theory perspective on transformers. The gist of the post is a combination of a physics-y mean-field interpretation of the DEQ Transformer introduced in [Deep Equilibrium Models](https://arxiv.org/abs/1909.01377) and a bunch of papers on Boltzmann machines from the 90s. Comments welcome.",project model attention term collective response statistical mechanical system consider vector generalization ising like spin system treat incoming data apply magnetic field output attention modules spin expectation value order rephrase attention inner loop fix point optimization introduce slow explicit attention module implement adaptive thouless anderson palmer mean field theory fast neural one parametrizes call onsager self correction term latter module look lot like transformer module approximate constrain mean field equations show simplify update step appear mirror vanilla transformer architecture explain origin fee forward layer importance residual connections blog url url project model attention collective response statistical mechanical system use deep equilibrium model solve set self consistent equations provide mean field theory perspective transformers gist post combination physics mean field interpretation deq transformer introduce deep equilibrium model url bunch paper boltzmann machine 90s comment welcome
juliensalinas,MachineLearning,1616595392.0,[D] Production-Ready Machine Learning NLP API with FastAPI and spaCy,"Hey,

FastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.

The [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&utm_campaign=l80a8332-aaaf-11eb-bcbc-0242ac130002) API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:

[https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/](https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/)

I'd love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I'm not aware of? Or can you think of better tools for machine learning APIs?

Thanks!",hey fastapi nice addition python ecosystem opinion make api creation easier less error prone also come great performances make perfectly suit machine learn apis nlpcloud io url api develop use fastapi think would interest write concrete article set nlp api fastapi serve spacy model ner url love feedback guy also fastapi users notice caveats aware think better tool machine learn apis thank
yusuf-bengio,MachineLearning,1616429243.0,[D] Jürgen Schmidhuber - But what has he published recently (Ep. 2),"Because so many people requested it last time, here it is:

**Jürgen Schmidhuber** has worked on some truly revolutionary ideas and algorithms back in the early 90s. Among other things, he proposed the artificial curiosity (AC) framework, an early prototype of what is now known as generative adversarial networks (GANs).

In 1997 Together with his student Hochreiter he proposed the infamous LSTM architecture, which is still considered the de-facto standard RNN architecture today. 

But since then, he only sporadically published a few interesting papers.

When comparing his high status and generous funding with other machine learning groups led by famous researchers, his research output is relatively low.

So my questions is, **what has he published recently?**",many people request last time jürgen schmidhuber work truly revolutionary ideas algorithms back early 90s among things propose artificial curiosity ac framework early prototype know generative adversarial network gans nmbr together student hochreiter propose infamous lstm architecture still consider de facto standard rnn architecture today since sporadically publish interest paper compare high status generous fund machine learn group lead famous researchers research output relatively low question publish recently
CvikliHaMar,MachineLearning,1618568912.0,[D] Pushforward vs Pullback algorithms,"Hello guys,

For a long time I skipped pushforward but I just read it here: [https://juliadiff.org/ChainRulesCore.jl/dev/](https://juliadiff.org/ChainRulesCore.jl/dev/) and it was very clear description. The terms here are frule, rrule, I tried to google in the internet but I didn't get why don't we use pushforward for gradient computation.

Why are we using pullback, is it faster? What are the downside of using pushforward to compute gradients does anyone know?",hello guy long time skip pushforward read url clear description term frule rrule try google internet get use pushforward gradient computation use pullback faster downside use pushforward compute gradients anyone know
flippy98026,MachineLearning,1616699703.0,[N] Common Application Framework (CAF) for Synthetic Data Generation by Rendered.ai at GTC21,"**CAF supports containerized simulation applications** with all the tools needed to produce, analyze, and integrate synthetic data into a computer vision project. This includes tools for scenario generation, compute management, collaboration, analysis, data management, GUI and API interfaces, chain-able agent factories, and standard and custom scene modifiers. The Framework (CAF) hosts both Rendered.ai and **3rd party simulation and synthetic data engines** (NIR/WMIR/Thermal), RADA,SAR, Satellite-EO and Xray sensors. #Nathan Kundtz , #[rendered.ai](https://rendered.ai) \#GTC21

[Synthetic Data](https://preview.redd.it/bnbc7hvd48p61.png?width=155&format=png&auto=webp&s=1e20ccd5711e16f6766aaa1cd0e9a9aa27dbcab0)",caf support containerize simulation applications tool need produce analyze integrate synthetic data computer vision project include tool scenario generation compute management collaboration analysis data management gui api interfaces chain able agent factories standard custom scene modifiers framework caf host render ai 3rd party simulation synthetic data engines nir wmir thermal rada sar satellite eo xray sensors nathan kundtz render ai url gtc21 synthetic data url
mgalarny,MachineLearning,1619030679.0,[P] Attention Nets and More with RLlib’s Trajectory View API,"Hey Everyone,

I wanted to share two new features now stable in [RLlib](https://docs.ray.io/en/master/rllib.html): Support for Attention networks as custom models, and the “trajectory view API” ([RLlib](https://docs.ray.io/en/master/rllib.html) is a popular reinforcement learning library that is part of the open-source [Ray project](https://github.com/ray-project/ray)).

There’s [a blog post](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65) with code snippets about what this is, how it works, and all that, but I want to share the motivation/importance of this here as well.

# Motivation

The goal of any RL algorithm is to train the neural network, such that its action choices become optimal with respect to a reward signal, which is also provided by the environment. We often refer to our neural network function as the *policy* or π: `action = π(observation) (Eq. 1)`

In the common case above (Eq. 1), observation is the current “frame” seen by the agent, but more and more often we’re seeing RLlib users try out models where this isn’t enough. For example:

* In “frame stacking”, the model sees the last n observations to account for the fact that a single time frame does not capture the entire state of the environment (think of a ball seen in a screenshot of a game and we wouldn’t know whether it’s flying to the left or right).

https://preview.redd.it/p3kr4huknku61.png?width=770&format=png&auto=webp&s=913e0486c74bf903117a81f6043310ddc51e6627

`action = π(observations[t, t-1, t-2, ..]) (Eq. 2)`

* In recurrent neural networks (RNN), the model sees the last observation, but also a tracked hidden state or memory vector that has previously been produced by that model itself and is altered over time:  
`action, memory[t] = π(observation[t], memory[t-1]). (Eq. 3)`
* Furthermore, in attention nets (e.g. transformer models), the model sees the last observation and also the last N tracked memory vectors:  
`action, memory[t] = π(observation[t], memory[t-n:t]) (Eq. 4)`

RLlib’s **new trajectory view API** that makes these complex policy models possible (and fast). Building on that functionality, we’ll show how this enables [efficient attention net support in RLlib.](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)

Let me know if you have any thoughts or questions!",hey everyone want share two new feature stable rllib url support attention network custom model trajectory view api rllib url popular reinforcement learn library part open source ray project url blog post url code snippets work want share motivation importance well motivationthe goal rl algorithm train neural network action choices become optimal respect reward signal also provide environment often refer neural network function policy π action π observation eq 1 common case eq 1 observation current frame see agent often see rllib users try model enough example frame stack model see last n observations account fact single time frame capture entire state environment think ball see screenshot game know whether fly leave right url π observations 1 2 eq 2 recurrent neural network rnn model see last observation also track hide state memory vector previously produce model alter time action memory π observation memory 1 eq 3 furthermore attention net e g transformer model model see last observation also last n track memory vectors action memory π observation memory n eq 4 rllibs new trajectory view api make complex policy model possible fast build functionality well show enable efficient attention net support rllib url know thoughts question
mediaml,MachineLearning,1619099948.0,[D] What are good places to advertise PhD and post-doc positions in ML?,"I  am principal investigator in a European research group focused on applied machine learning. I am interested in experiences of what are good places to advertise PhD and post-doc positions. Specifically,

* PhD students, where do you go to look for machine learning PhD and post-doc positions?
* Faculty, what have you found are good venues to advertise PhD and post-doc positions?

In your experience, is it worth investing money for ads in a portal like [findaphd.com](https://findaphd.com/), or is advertising on group webpage + social media promotion the way to go?

I  would in particular like to reach female candidates and applicants from other underrepresented groups in AI with adequate qualifications.  Any insights on how to advertise to reach those groups are also very  welcome!

EDIT: Some commenters seem to have misconstrued that I am looking to exclusively hire female candidates. This is of course not the case. I want to increase the pool of female applicants and people from other underrepresented groups to increase the probability of finding excellent candidates from those groups. This is partially because I believe that building a diverse research group leads to a more interesting and healthier lab life, and partially because I strongly believe the research concerning [gender bias](http://curt-rice.com/2017/09/23/wheres-the-evidence-a-little-science-about-bias-and-gender-equality/) and [affirmative action](https://www.gse.harvard.edu/news/uk/18/07/case-affirmative-action).",principal investigator european research group focus apply machine learn interest experience good place advertise phd post doc position specifically phd students go look machine learn phd post doc position faculty find good venues advertise phd post doc position experience worth invest money ads portal like findaphd com url advertise group webpage social media promotion way go would particular like reach female candidates applicants underrepresented group ai adequate qualifications insights advertise reach group also welcome edit commenters seem misconstrue look exclusively hire female candidates course case want increase pool female applicants people underrepresented group increase probability find excellent candidates group partially believe build diverse research group lead interest healthier lab life partially strongly believe research concern gender bias url affirmative action url
OnlyProggingForFun,MachineLearning,1618588775.0,"[News] Create 3D Models from Images! AI and Game Development, Design... GANverse3D & NVIDIA Omniverse","Omniverse, NVIDIA, (2021): [https://www.nvidia.com/en-us/omniverse/](https://www.nvidia.com/en-us/omniverse/)

Zhang et al., (2020), ""IMAGE GANS MEET DIFFERENTIABLE RENDERING FOR INVERSE GRAPHICS AND INTERPRETABLE 3D NEURAL RENDERING"": [https://arxiv.org/pdf/2010.09125.pdf](https://arxiv.org/pdf/2010.09125.pdf)

GANverse3D official NVIDIA video: [https://youtu.be/0PQnrnUIBlU](https://youtu.be/0PQnrnUIBlU)

NVIDIA'S GANverse 3D blog article: [https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/](https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/)

Watch the video demo [https://youtu.be/dvjwRBZ3Hnw](https://youtu.be/dvjwRBZ3Hnw)",omniverse nvidia 2021 url et al 2020 image gans meet differentiable render inverse graphics interpretable 3d neural render url official nvidia video url ganverse 3d blog article url video demo url
davidbun,MachineLearning,1620139610.0,[N] Access Google Objectron (~1.92 TBs) in less than 5 seconds with Activeloop Hub,"&#x200B;

https://i.redd.it/y52s5u8594x61.gif

Hi r/machinelearning,

My team at [Activeloop](https://activeloop.ai/?utm_source=social&utm_medium=reddit&utm_campaign=objectron) partnered with Google to make Google Objectron available in under ±5 seconds (per dataset category). Google Objectron is one of Google’s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4M annotated images).

All you need to do to get started is:

Install [Hub, the open-source package](https://github.com/activeloopai/Hub) that converts computer vision datasets into cloud-native NumPy-like arrays and enables a few nifty features like streaming to PyTorch and TensorFlow, dataset version-control, collaboration, etc.

`pip install hub`

And then Load the data for the bike category.

`import hub`

`bikes = hub.Dataset(""google/bike"")`

In ±5 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). The whole dataset ( \~1.92 TBs +metadata) would take about 33 seconds to access.

Thanks to Hub, you can visualize Google Objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2Fshoe&utm_source=social&utm_medium=reddit&utm_campaign=objectron)).

More details on using [Objectron with Hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&utm_medium=reddit&utm_campaign=objectron).

\*Please make sure that you are using latest update for hub.

https://i.redd.it/e1179xa394x61.gif

We’re working to get more datasets on the platform and improve [github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) as a tool. Let us know if you have any feedback - we’d like to deliver maximum value to the community.

Thanks,

DavitBun",x200b url r machinelearning team activeloop url partner google make google objectron available 5 second per dataset category google objectron one google popular datasets contain short object centric video clip pose annotations 15000 annotate videos 4m annotate image need get start install hub open source package url convert computer vision datasets cloud native numpy like array enable nifty feature like stream pytorch tensorflow dataset version control collaboration etc pip install hub load data bike category import hub bike hub dataset google bike 5 second dataset available work e g filter apply transformations etc whole dataset 1 92 tbs metadata would take nmbr second access thank hub visualize google objectron computer vision dataset web app app activeloop ai url detail use objectron hub available release blogpost url make sure use latest update hub url work get datasets platform improve github com activeloopai hub url tool let us know feedback wed like deliver maximum value community thank davitbun
Tea_Pearce,MachineLearning,1618352493.0,[R][P] Counter-Strike from Pixels with Behavioural Cloning,"https://reddit.com/link/mqd1ho/video/l2o09485n0t61/player

A deep neural network that plays CSGO deathmatch from pixels. It's trained on a dataset of 70 hours (4 million frames) of human play, using behavioural cloning.

ArXiv paper: [https://arxiv.org/abs/2104.04258](https://arxiv.org/abs/2104.04258)

Gameplay examples: [https://youtu.be/p01vWk7uMvM](https://youtu.be/p01vWk7uMvM)

""Counter-strike Deatmatch with Large-Scale Behavioural Cloning""

Tim Pearce (twitter [https://twitter.com/Tea\_Pearce](https://twitter.com/Tea_Pearce)), Jun Zhu

Tsinghua Unviersity | University of Cambridge",url deep neural network play csgo deathmatch pixels train dataset nmbr hours 4 million frame human play use behavioural clone arxiv paper url examples url deatmatch large scale behavioural clone tim pearce twitter url jun zhutsinghua unviersity university cambridge
windbreaker14,MachineLearning,1616513372.0,[D] Disappointed with the reviews in ICML21.,"I submitted a paper in ICML21. I received reviews 3 days ago.

I was supposed to receive 4 reviews from reviewers 5,6,7,8.

However, I only received only 2 reviews from reviewers 5,8 and two are missing. 

&#x200B;

Even though, I sent a message for this to the contact e-mail [""icml2021chairs@gmail.com](mailto:""icml2021chairs@gmail.com)"", I have not received any responses.  

&#x200B;

&#x200B;

Also, the quality of 1 review is very poor. The reviewer weakly rejected my paper with the reason that the paper is not well organized (only three sentences). There were no other critical issues. I already submitted in ICLR2021 and my paper was praised as being well-written and well-organized.

&#x200B;

To sum up, I was expected to receive 4 reviews but two of them are missing and one of them is very poor. Therefore, I was quite disappointed.",submit paper icml21 receive review nmbr days ago suppose receive nmbr review reviewers 5 6 7 8 however receive nmbr review reviewers 5 8 two miss x200b even though send message contact e mail icml2021chairs gmail com mailto icml2021chairs gmail com receive responses x200b x200b also quality nmbr review poor reviewer weakly reject paper reason paper well organize three sentence critical issue already submit iclr2021 paper praise well write well organize x200b sum expect receive nmbr review two miss one poor therefore quite disappoint
proof_required,MachineLearning,1616753332.0,[D] How Facebook got addicted to spreading misinformation,"Behind paywall:

With new machine-learning models coming online daily, the company created a new system to track their impact and maximize user engagement. The process is still the same today. Teams train up a new machine-learning model on FBLearner, whether to change the ranking order of posts or to better catch content that violates Facebook’s community standards (its rules on what is and isn’t allowed on the platform). Then they test the new model on a small subset of Facebook’s users to measure how it changes engagement metrics, such as the number of likes, comments, and shares, says Krishna Gade, who served as the engineering manager for news feed from 2016 to 2018.

If a model reduces engagement too much, it’s discarded. Otherwise, it’s deployed and continually monitored. On Twitter, Gade explained that his engineers would get notifications every few days when metrics such as likes or comments were down. Then they’d decipher what had caused the problem and whether any models needed retraining.

But this approach soon caused issues. The models that maximize engagement also favor controversy, misinformation, and extremism: put simply, people just like outrageous stuff. Sometimes this inflames existing political tensions. The most devastating example to date is the case of Myanmar, where viral fake news and hate speech about the Rohingya Muslim minority escalated the country’s religious conflict into a full-blown genocide. Facebook admitted in 2018, after years of downplaying its role, that it had not done enough “to help prevent our platform from being used to foment division and incite offline violence.”

While Facebook may have been oblivious to these consequences in the beginning, it was studying them by 2016. In an internal presentation from that year, reviewed by the Wall Street Journal, a company researcher, Monica Lee, found that Facebook was not only hosting a large number of extremist groups but also promoting them to its users: “64% of all extremist group joins are due to our recommendation tools,” the presentation said, predominantly thanks to the models behind the “Groups You Should Join” and “Discover” features.

https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/",behind paywall new machine learn model come online daily company create new system track impact maximize user engagement process still today team train new machine learn model fblearner whether change rank order post better catch content violate facebooks community standards rule allow platform test new model small subset facebooks users measure change engagement metrics number like comment share say krishna gade serve engineer manager news fee nmbr 2018 model reduce engagement much discard otherwise deploy continually monitor twitter gade explain engineer would get notifications every days metrics like comment theyd decipher cause problem whether model need retrain approach soon cause issue model maximize engagement also favor controversy misinformation extremism put simply people like outrageous stuff sometimes inflame exist political tensions devastate example date case myanmar viral fake news hate speech rohingya muslim minority escalate countrys religious conflict full blow genocide facebook admit 2018 years downplay role enough help prevent platform use foment division incite offline violence facebook may oblivious consequences begin study 2016 internal presentation year review wall street journal company researcher monica lee find facebook host large number extremist group also promote users 64 extremist group join due recommendation tool presentation say predominantly thank model behind group join discover feature url
LSTMeow,MachineLearning,1620131082.0,[N] ClearML release: v1.0 (open source MLOPs solution),"Hi r/ml,

It has been almost two years since I first posted here about our open-source solution, and here we are again (thanks for the amazing Github star bump btw, you guys rocked!).

Today, after what seems like over 9000 person-years of working, and tinkering, long into the night, I am pleased to announce ClearML has hit version 1.0.

Following quickly after the release of ClearML 0.17.5, we added the last remaining features we felt 1.0 needed. Namely multi-model support, as well as improved batch operations.

It has been a long and sometimes bumpy path to get us to version 1.0. There have been good times and some mis-steps. Every journey is different, and we have been helped along the way by having one of the nicest slack communities you could ever wish for. 

This feels truly like a milestone. A birthday. I hope you will help us in celebrating it.

More details about the 1.0 release here:

[https://t.clear.ml/horrayv1](https://t.clear.ml/horrayv1)

Our  community roadmap is here:

[https://t.clear.ml/roadmap1](https://t.clear.ml/roadmap1)

Formal changelog:

[https://github.com/allegroai/clearml/releases/tag/1.0.0](https://github.com/allegroai/clearml/releases/tag/1.0.0)

Special Episode of my show (featuring a tiger):

[https://youtu.be/r2BMMDzfyA0](https://youtu.be/r2BMMDzfyA0)

Celebratory meme:

[https://twitter.com/clearmlapp/status/1389201845957054466](https://twitter.com/clearmlapp/status/1389201845957054466)

&#x200B;

PS: I still owe this community the so-called ""comparison matrix"" that most like to glance at. This was a hard thing to do, especially since our MLOps tooling ecosystem is constantly growing and changing. I'm happy to say that some form of comparison will be here this week, and I will edit this post when it arrives ;)",hi r ml almost two years since first post open source solution thank amaze github star bump btw guy rock today seem like nmbr person years work tinker long night please announce clearml hit version 1 0 follow quickly release clearml 0 17 5 add last remain feature felt nmbr need namely multi model support well improve batch operations long sometimes bumpy path get us version 1 0 good time mis step every journey different help along way one nicest slack communities could ever wish feel truly like milestone birthday hope help us celebrate detail nmbr release url community roadmap url changelog url episode show feature tiger url meme url still owe community call comparison matrix like glance hard thing especially since mlops tool ecosystem constantly grow change happy say form comparison week edit post arrive
RaivoK,MachineLearning,1616590712.0,Fair Model Comparison [R],"Have not published before and need some advice. I am comparing the accuracy of different video classification models, by training each one myself in the same training environment with the same training tricks. I would like to make the comparison as fair as possible. There are a few differences between the models:

1. Each model uses a certain number of input frames. Some might use 16, some 32, and some 64.
2. Each model uses a certain spatial size. Some 114x114, others 224x224.

Now, I am not completely sure how to standardize this in my experiment. Do I train each model with the exact choices they made in their paper, or do I use the same spatial size and frame number for all models?

A reviewer might say that forcing all models to use 224x224 and 16 input frames is not fair because these values were not what some of the models were designed for.

On the other hand, if I train every model with the values that they use in their paper, some might get an advantage just from using **way** more compute.

I would like to use the same spatial size and number of frames for all of them and then compare how fast vs how accurate they are on this. Will a reviewer criticize this if I do it like this?",publish need advice compare accuracy different video classification model train one train environment train trick would like make comparison fair possible differences model 1 model use certain number input frame might use 16 32 64 2 model use certain spatial size 114x114 others 224x224 completely sure standardize experiment train model exact choices make paper use spatial size frame number model reviewer might say force model use 224x224 nmbr input frame fair value model design hand train every model value use paper might get advantage use way compute would like use spatial size number frame compare fast vs accurate reviewer criticize like
ElCobo,MachineLearning,1618694910.0,[P] Wiggle-GAN: Stereoscopic camera simulation using generative adversarial neural networks,"Hello everyone,

I wanted to post my grad thesis here, in which I tried to simulate the use of a stereoscopic camera utilizing a monocular image as input. The main goal was to give a new alternative when someone wanted to create a [wigglegram](https://www.reddit.com/r/wigglegrams/), because all the ways you could achieve the effect have limitations. For example, you could use a normal camera and take many pictures with it but if the scene is moving, the effect would mimic a stop motion more than a wigglegram. On the other hand, you could use an array of cameras but that's expensive or you could use a stereoscopic camera with 3 of 4 lenses but these kinds of cameras are analogic so it takes more time to do the effect. That's the reason why we took an approach by using Neural Networks where you don't have any of these limitations.  

In this project I used a WGAN with Improved Consistency Regularization and L1 loss comparing the output and real image, you can see some the result of adjusting this values in the [github](https://github.com/RoCoBo44/Wiggle-GAN) (and the code). Also, you can see how I trained multiple some multiple solutions:

* An autoencoder (AE) which is only one net
* The Wiggle-GAN solution (two nets fighting)
* The Wiggle-GAN solution but extracting some metrics to see what it does (Wiggle-GAN no CR)

The input of the Wiggle-GAN are the images you want to move, their depth maps and the direction (left or right) and the outputs are the new images moved and their depth map estimations, so you could iterate multiple times.  If you want to try it out I made a [google colab](https://colab.research.google.com/drive/1N5HJ1geVM1ymLoE5C2jkLs3F_tQn-s-r?usp=sharing), but you have to download the checkpoints. 

The limitation of the net is that all are trained with images of 128x128 pixels except the last one in the colab that is 256x256. The code automatically changes the scale of your image to the size it needs so you don't have to change it manually.

Sometimes it doesn't generate the wiggle expected and that may be due to the depth map estimation. If that happens you could download the depth map image in the folder (/Image/Input - Test/) with the name ""{number}\_d.png"", then with an editing software you could change it and uploaded it with the same name in the same place.

If you want to try it and you have some problems with that, feel free to ask me for anything.

Lots of love",hello everyone want post grad thesis try simulate use stereoscopic camera utilize monocular image input main goal give new alternative someone want create wigglegram url ways could achieve effect limitations example could use normal camera take many picture scene move effect would mimic stop motion wigglegram hand could use array cameras expensive could use stereoscopic camera nmbr nmbr lenses kinds cameras analogic take time effect reason take approach use neural network limitations project use wgan improve consistency regularization l1 loss compare output real image see result adjust value github url code also see train multiple multiple solutions autoencoder ae one net wiggle gin solution two net fight wiggle gin solution extract metrics see wiggle gin cr input wiggle gin image want move depth map direction leave right output new image move depth map estimations could iterate multiple time want try make google colab url download checkpoints limitation net train image 128x128 pixels except last one colab 256x256 code automatically change scale image size need change manually sometimes generate wiggle expect may due depth map estimation happen could download depth map image folder image input test name number _d png edit software could change upload name place want try problems feel free ask anything lot love
TuSharma,MachineLearning,1616862277.0,[R] Suggestions welcome: Code smell detection using deep learning,"Recently, our paper ""Code smell detection by deep direct-learning and transfer-learning"" was accepted in [JSS](https://www.sciencedirect.com/science/article/abs/pii/S0164121221000339). You may find preprint [here](https://tusharma.in/preprints/JSS21_Code-Smell-Detection-Using-Deep-Learning_Preprint.pdf).

The key contribution of the paper is that 1) we can detect smells without explicit feature engg (e.g., metrics) esp. impl smells, 2) autoencoder works best among other options (CNN, RNN), and 3) transfer learning works in this context.

[GitHub reposiotory](https://github.com/tushartushar/DeepLearningSmells) and [autoencoder implementation article on Medium](https://tusharma.medium.com/autoencoders-for-source-code-analysis-a-use-case-4600da86b718) provide implementation details.

Would you like to provide any opinion/suggestion/feedback on this work?",recently paper code smell detection deep direct learn transfer learn accept jss url may find preprint url key contribution paper 1 detect smell without explicit feature engg e g metrics esp impl smell 2 autoencoder work best among options cnn rnn 3 transfer learn work context github reposiotory url autoencoder implementation article medium url provide implementation detail would like provide opinion suggestion feedback work
chasep255,MachineLearning,1617966989.0,[D] Loss function for audio auto encoder.,"Trying to build an auto encoder for raw audio data (time domain.)  I did this once before a while back and never got a good result.  I tried using mean squared error loss and the output really only captured a muted version of the low frequency signals.  I also tried computing the mean squared error of the fft of the output but that did not do much better.  

Now I am trying to combine WGAN-GP with some sort of other loss function.  I did this same approach last week with images where I combined a WGAN and VGG19 perceptual loss.  This produced really sharp images.  However I don't have any sort of pretrained models for raw audio (I supposed I could make one.)  So I am trying to combine the FFT loss with WGAN.  I just kicked off training and have not seen the result yet, however, I am worried that the FFT loss will prevent the model from learning the higher frequency signals since there will be a larger penalty for guessing something that sounds realistic but is say 180 degrees out of phase.  Does anyone know of a better loss function I could try for sound data?  Right now my fft loss is defined as ...

fft\_loss = tf.reduce\_mean(tf.square(tf.abs(tf.signal.stft(g, 64, 32) - tf.signal.stft(r, 64, 32))))",try build auto encoder raw audio data time domain back never get good result try use mean square error loss output really capture mute version low frequency signal also try compute mean square error fft output much better try combine wgan gp sort loss function approach last week image combine wgan vgg19 perceptual loss produce really sharp image however sort pretrained model raw audio suppose could make one try combine fft loss wgan kick train see result yet however worry fft loss prevent model learn higher frequency signal since larger penalty guess something sound realistic say nmbr degrees phase anyone know better loss function could try sound data right fft loss define fft _loss tf reduce _mean tf square tf abs tf signal stft g 64 32 tf signal stft r 64 32
wasabi_toast,MachineLearning,1616635709.0,[Discussion] PlaidML and Intel's Iris Xe GPU support...?,"This is probably just wishful thinking, but since [PlaidML](https://github.com/plaidml/plaidml) is part of the Intel AI project and with the upcoming release of both the discrete and integrated Iris Xe units, will we soon be able to use these chips as GPUs for machine learning?

Yes, I know that these new Iris Xe units are not all that powerful at all compared to Nvidia and AMD’s offerings, but it does not look like the end of this cryptomining craze or GPU shortage is coming soon and I would really like to be able to test out some DL codes on something for work, so any sort of GPU would help tremendously.",probably wishful think since plaidml url part intel ai project upcoming release discrete integrate iris xe units soon able use chip gpus machine learn yes know new iris xe units powerful compare nvidia amds offer look like end cryptomining craze gpu shortage come soon would really like able test dl cod something work sort gpu would help tremendously
__Julia,MachineLearning,1616598885.0,"[D] Research in the Global South: Academic conference should build bridges, not barriers","Hello,

In the last few months, I have seen ""the academic community on twitter and on r/MachineLearning "" striving to promote diversity, [defend researchers](https://www.wired.com/story/second-ai-researcher-says-fired-google/), and question some companies' approaches to silence controversy over its Ethical AI. However, I stand in awe and think of how the same community is gate-keeping research  for other groups of the same community. The same community (who organizes **International** academic conferences) asks ""high"" fees based on a **national life standards** to researchers from the Global South. ""high""  is relative to the life standards of some countries.

It is hard to forge a path without being included. It is hard for students and professors to learn, and share because they need to pay ""tax of being included"". it is hard to see the same people defending a group of this community and ignore another group. [It is hard to see most International academic conferences in  places (that bans several nationalities).](https://twitter.com/andrewyng/status/825745002831585280?lang=eu)

Today, in order for a student from the global south to make it to a good university in US/Canada/EU for PhD program, the students need at least a paper in top tier conferences. Obviously, this is not doable for most people.

There are some initiatives such as [this one](http://www.datascienceafrica.org/dsa2019addis/), but I think we -- as a community -- can do better.

If this message sounds like a rant, it's not, or maybe it is .. but I have been thinking about this after stumbling upon this tweet.

&#x200B;

>A student in Africa who wishes to start a career in Research does their best, self-learn, submit to a workshop, get accepted.     Now online conferences ask them to pay +1 month of their house rent to present their work in a zoom call.   Why are we gatekeeping research?

[https://twitter.com/hadyelsahar/status/1374699909451030542](https://twitter.com/hadyelsahar/status/1374699909451030542) 

&#x200B;

If you **review papers** for a conference, or if you organize a conference, ... you can make a difference by defending democratizing science. 

We can do better, you can do better to lower the bar for those who live in the other part of the world.",hello last months see academic community twitter r machinelearning strive promote diversity defend researchers url question company approach silence controversy ethical ai however stand awe think community gate keep research group community community organize international academic conferences ask high fee base national life standards researchers global south high relative life standards countries hard forge path without include hard students professors learn share need pay tax include hard see people defend group community ignore another group hard see international academic conferences place ban several nationalities url order student global south make good university us canada eu phd program students need least paper top tier conferences obviously doable people initiatives one url think community better message sound like rant maybe think stumble upon tweet x200b student africa wish start career research best self learn submit workshop get accept online conferences ask pay 1 month house rent present work zoom call gatekeeping research url x200b review paper conference organize conference make difference defend democratize science better better lower bar live part world
KingHultan,MachineLearning,1617799367.0,[D] Metric for computational efficiency?," I'm trying to find a metric for measuring the amount of predictive power of a model in relation to the complexity of the model. Similar to the fuel efficiency of a car.

I have my own metric, which is penalty = loss \* flops  
. The objective is to minimize the penalty of a model to find the most predictive efficient model configuration (quite useful in scenarios where computational power is severely limited, like a phone or an autonomous vehicle).

I have been trying to find a similar metric in the literature but to no avail. Do any of you know of a metric to measure the computational efficiency of a model?",try find metric measure amount predictive power model relation complexity model similar fuel efficiency car metric penalty loss flop objective minimize penalty model find predictive efficient model configuration quite useful scenarios computational power severely limit like phone autonomous vehicle try find similar metric literature avail know metric measure computational efficiency model
dummy-gummy,MachineLearning,1619449303.0,"[P] Pytorch reimplementation of Encode-Attend-Navigate, a RL-based TSP solver"," [https://github.com/astariul/encode-attend-navigate-pytorch](https://github.com/astariul/encode-attend-navigate-pytorch)

I recently re-implemented [encode-attend-navigate](https://github.com/MichelDeudon/encode-attend-navigate), a TSP solver based on RL. The official repo was using tensorflow 1.x, so I decided to re-implement it in Pytorch. I wanted to share it here to get some opinion :)

*You can train the model using a free GPU from Google Colab, a Colab notebook is provided in the README !*",url recently implement encode attend navigate url tsp solver base rl official repo use tensorflow 1 x decide implement pytorch want share get opinion train model use free gpu google colab colab notebook provide readme
nicolas-gervais,MachineLearning,1620053920.0,"[D] Half of my team knows Tensorflow, the other half PyTorch. How can we decide on which to use?","Sadly, the title says it all. We don't know what to use as we need to start collaborating together. All I can find online is ""Pytorch is more often used in research"" which is probably not even true anymore, since everyone says that because that's the only thing people are saying.",sadly title say know use need start collaborate together find online pytorch often use research probably even true anymore since everyone say thing people say
SmittyMcSmitherson,MachineLearning,1617948856.0,[D] MLCommons & inference benchmarking,Why isn’t MLCommons (MLPerf) constantly adding new results? Is there a better resource for benchmark results of various inferencing ASICs?,mlcommons mlperf constantly add new result better resource benchmark result various inferencing asics
PetarVelickovic,MachineLearning,1619684755.0,"[R] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics and Gauges (""proto-book"" + blog + talk)","Hi everyone,

I am proud to share with you the first version of a project on a geometric unification of deep learning that has kept us busy throughout COVID times (having started in February 2020).

We release our 150-page ""proto-book"" on geometric deep learning (with Michael Bronstein, Joan Bruna and Taco Cohen)! We have currently released the arXiv preprint and a companion blog post at:

[https://geometricdeeplearning.com/](https://geometricdeeplearning.com/)

Through the lens of symmetries, invariances and group theory, we attempt to distill ""all you need to build the neural architectures that are all you need"". All the 'usual suspects' such as CNNs, GNNs, Transformers and LSTMs are covered, while also including recent exciting developments such as Spherical CNNs, SO(3)-Transformers and Gauge Equivariant Mesh CNNs.

Hence, we believe that our work can be a useful way to navigate the increasingly challenging landscape of deep learning architectures. We hope you will find it a worthwhile perspective!

I also recently gave a virtual talk at FAU Erlangen-Nuremberg (the birthplace of Felix Klein's ""Erlangen Program"", which was one of our key guiding principles!) where I attempt to distill the key concepts of the text within a \~1 hour slot:

[https://www.youtube.com/watch?v=9cxhvQK9ALQ](https://www.youtube.com/watch?v=9cxhvQK9ALQ)

More goodies, blogs and talks coming soon! If you are attending ICLR'21, keep an eye out for Michael's keynote talk :)

Our work is very much a work-in-progress, and we welcome any and all feedback!",hi everyone proud share first version project geometric unification deep learn keep us busy throughout covid time start february 2020 release 150 page proto book geometric deep learn michael bronstein joan bruna taco cohen currently release arxiv preprint companion blog post url lens symmetries invariances group theory attempt distill need build neural architectures need usual suspect cnns gnns transformers lstms cover also include recent excite developments spherical cnns 3 transformers gauge equivariant mesh cnns hence believe work useful way navigate increasingly challenge landscape deep learn architectures hope find worthwhile perspective also recently give virtual talk fau erlangen nuremberg birthplace felix klein erlangen program one key guide principles attempt distill key concepts text within 1 hour slot url goodies blog talk come soon attend iclr 21 keep eye michael keynote talk work much work progress welcome feedback
prestodigitarium,MachineLearning,1617931758.0,"[P] Gourdian Free Dataset Download: EPA Air Quality System Daily CO, NO2, O3, SO2 Concentrations since 1980","Howdy!

We've just added another few free dataset downloads to our project (Gourdian), this time from the EPA Air Quality System, daily levels of the following pollutants from measuring stations throughout the US, going back to 1980:

* Carbon Monoxide (CO): https://gourdian.net/g/eric/epa_aqs.co_daily_summary
* Nitrogen Dioxide (NO2): https://gourdian.net/g/eric/epa_aqs.no2_daily_summary
* Ozone (O3): https://gourdian.net/g/eric/epa_aqs.ozone_daily_summary
* Sulfur Dioxide (SO2): https://gourdian.net/g/eric/epa_aqs.so2_daily_summary

This dataset is only updated twice per year.

Why should you care about these things? Well, they're pretty nasty pollutants, which frequently affect human health, especially over long exposure periods. Perhaps concentrations should affect where we build homes more than they do, for example? They're also useful proxies for various types of human activity.

**Carbon monoxide** can be deadly in high concentrations, as it binds to hemoglobin, and makes it so that one's blood can't carry as much oxygen. Created via combustion of fossil fuels.

**Nitrogen dioxide** is one of the results of road traffic and other fossil fuel combustion, is one of the precursors for other harmful pollutants such as ozone and particulates, and plays a role in the formation of acid rain (it becomes nitric acid). It's also generally correlated with exposure to other byproducts of road traffic, many of which have been linked to respiratory illnesses and systemic inflammation, which can lead to a whole host of other health issues.

**Ozone** is a powerful oxidant, which makes it so that it can cause damage to respiratory tissues. It also attacks various polymers like rubber, eventually causing it to crack. It's formed primarily from photochemical reactions between nitrogen oxides (like NO2) and volatile organic compounds (VOCs). High concentrations aren't limited to urban areas, however, and it can travel hundreds of miles downwind from the source.

Finally, **sulfur dioxide**. Formed largely by the combustion of fossil fuels with high levels of sulfur, as well as volcanic activity. It's a precursor to acid rain (it becomes sulfuric acid). Sulfur dioxide is known to be at least mildly toxic, and can be hazardous in high concentrations. Long term exposure to low concentrations can also be problematic. The amount of sulfur in fossil fuels vary widely by type - automotive gasoline has much less than bunker fuel commonly used by container ships, for example.

If you'd like to download any part of these, you can filter down to the parts you care part via the lat/long filter (button on the map to select area) or by year, via sliders. As you do this, the download size noted on the download button in the upper right should change. When you've got the part you want, click download, and you should get a CSV.

A bit about our goals and what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Feedback welcome! If there are any datasets that you'd like to see added, let us know!",howdy add another free dataset download project gourdian time epa air quality system daily level follow pollutants measure station throughout us go back 1980 carbon monoxide co url nitrogen dioxide no2 url ozone o3 url sulfur dioxide so2 url dataset update twice per year care things well pretty nasty pollutants frequently affect human health especially long exposure periods perhaps concentrations affect build home example also useful proxies various type human activity carbon monoxide deadly high concentrations bind hemoglobin make one blood carry much oxygen create via combustion fossil fuel nitrogen dioxide one result road traffic fossil fuel combustion one precursors harmful pollutants ozone particulates play role formation acid rain become nitric acid also generally correlate exposure byproducts road traffic many link respiratory illnesses systemic inflammation lead whole host health issue ozone powerful oxidant make cause damage respiratory tissue also attack various polymers like rubber eventually cause crack form primarily photochemical reactions nitrogen oxides like no2 volatile organic compound vocs high concentrations limit urban areas however travel hundreds miles downwind source finally sulfur dioxide form largely combustion fossil fuel high level sulfur well volcanic activity precursor acid rain become sulfuric acid sulfur dioxide know least mildly toxic hazardous high concentrations long term exposure low concentrations also problematic amount sulfur fossil fuel vary widely type automotive gasoline much less bunker fuel commonly use container ship example like download part filter part care part via lat long filter button map select area year via sliders download size note download button upper right change get part want click download get csv bite goals try build filter optional click button csv arrive hard drive download always single csv bundle weird directory structure format csvs index filterable column type lat long date time moment download part want open license datasets free download signup require download open datasets search within across datasetsfeedback welcome datasets like see add let us know
weifz,MachineLearning,1620356445.0,[D]A question about causal discovery and causal inference,"Hi there,

I know causality consists of causal discovery and causal inference, and I wonder if there exists some order between these two components, e.g., should we konw the causal structure(causal discovery) before causal inference?",hi know causality consist causal discovery causal inference wonder exist order two components e g konw causal structure causal discovery causal inference
Electronic-Lie4077,MachineLearning,1617117959.0,[P] OpenAI CLIP: Connecting Text and Images Gradio web demo,"&#x200B;

https://reddit.com/link/mgiimg/video/qstepjl4o6q61/player

web demo for open ai clip for visual classification, try it out here: [https://gradio.app/hub/AK391/CLIP](https://gradio.app/hub/AK391/CLIP)",x200b url demo open ai clip visual classification try url
prakhar21,MachineLearning,1620325388.0,[D] graph2vec: Learning Distributed Representations of Graphs | ML with Graphs (Paper Walkthrough),"Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. graph2vec proposes a technique to embed entire graph in high dimension vector space. It is inspired from doc2vec learning approach over graphs and rooted subgraphs.

Paper Walkthrough: https://youtu.be/h400_OMWNLo",recent work representation learn graph structure data predominantly focus learn distribute representations graph substructures nod subgraphs graph2vec propose technique embed entire graph high dimension vector space inspire doc2vec learn approach graph root subgraphs paper walkthrough url
TartarQ,MachineLearning,1620576418.0,[Discussion] Defining optimal false-positives and false-negatives balance with a cost function,"\#roc\_curve  

https://preview.redd.it/80m5ee9yb4y61.png?width=1382&format=png&auto=webp&s=79aa05882369f6e1e2449679e062ae3c26634aed

&#x200B;

https://preview.redd.it/sheqqo2zb4y61.png?width=1548&format=png&auto=webp&s=e9904a82673b9d340b9db6a176c42501ea831279

&#x200B;

https://preview.redd.it/qifab1rzb4y61.png?width=932&format=png&auto=webp&s=4c2ae6f5f80a014ac60d0def347e6d333618de80",roc _curve url
False-Grape7566,MachineLearning,1620578000.0,[P] Keytotext Convert Keywords to Large texts,"Hello, Presenting Keytotext: Keytotext is an NLP model that can convert keywords to sentences and larger texts. It is built using the T5 model. Keytotext has a PyPI installation and on-demand inference API too. It also features a UI built using streamlit and a GPU-enabled colab notebook for easy usage! Please do check it out on GitHub: [https://github.com/gagan3012/keytotext](https://github.com/gagan3012/keytotext) Please to star 📷 if you liked the work!",hello present keytotext keytotext nlp model convert keywords sentence larger texts build use t5 model keytotext pypi installation demand inference api also feature ui build use streamlit gpu enable colab notebook easy usage please check github url please star like work
CC_sciguy,MachineLearning,1618532149.0,[D] Anyone have experience running pytorch with AMD GPUs,"In March pytorch mainstreamed ROCm support meaning that AMD GPUs could be viable for DL workflows. I have seen some compelling benchmarks for AMD mi100 GPUs, but nothing that does end-to-end tests on real deep learning workflows.

Does anyone have experience running pytorch with AMD gpus? Are the speeds really better than top of the line nvidia gpus? And more importantly, is the support as good as nvidia at this point, or is the SW side still maturing?",march pytorch mainstreamed rocm support mean amd gpus could viable dl workflows see compel benchmarks amd mi100 gpus nothing end end test real deep learn workflows anyone experience run pytorch amd gpus speed really better top line nvidia gpus importantly support good nvidia point sw side still mature
Philipp,MachineLearning,1618835247.0,[P] [D] Using GPT-3 to write short stories,"Hopfully this is of interest to some, I created an ongoing [series](https://aiwrotethis.substack.com/) of short stories on Substack which were co-written wih the GPT-3 AI. Here's [a video](https://www.youtube.com/watch?v=8fWK0k7aSRs) showing some approaches on how to move a story in a certain direction.

Would also love to discuss this topic of AI fiction writing. I'm utterly fascinated by the use of GTP-3 as a creative tool.",hopfully interest create ongoing series url short stories substack co write wih gpt 3 ai video url show approach move story certain direction would also love discuss topic ai fiction write utterly fascinate use gtp 3 creative tool
setzRFD,MachineLearning,1617646105.0,[Discussion] What metaheuristics are interesting as an exercise to expand my knowledge of AI/ML?,"I'm interested in metaheuristic searches, specifically how they can be interwoven with neural networks with some interesting results. I've worked with particle swarms and genetic algorithms to train a neural network and I want to try some others since it's a fun exercise.

I've looked into ant colony and bee colony optimization metaheuristics and they seem to only work on problems that can be reduced to graph traversal. Are there other metaheuristics you can think of that are more suited to optimizing error functions?",interest metaheuristic search specifically interweave neural network interest result work particle swarm genetic algorithms train neural network want try others since fun exercise look ant colony bee colony optimization metaheuristics seem work problems reduce graph traversal metaheuristics think suit optimize error function
xdtolm,MachineLearning,1617563498.0,[P] Nvidia A100 and AMD MI100 benchmarks - join VkFFT panel on Nvidia GTC 2021,"Hello! I am the creator of [VkFFT](https://github.com/dtolm/VkFFT) \- Vulkan/CUDA/HIP Fast Fourier Transform library. I would like to invite you to the [GTC 2021](https://gtc21.event.nvidia.com/) panel of VkFFT, which will happen on April 13th at 4 PM CEST in the Higher Education and Research category. It will be focused on implemented optimizations and how to create cross-platform code that can scale from Raspberry Pi 4 to HPC GPUs like Nvidia A100. The session will also compare Vulkan, CUDA and HIP compute platforms on Nvidia A100 and AMD MI100 GPUs.

In this post, I would like to give you a sneak peek at a part of the talk regarding VkFFT/cuFFT/rocFFT performance comparison in single precision in 1D batched FFT test of all systems from 2 to 4096, representable as an arbitrary multiplication of 2s, 3s, 5s, 7s, 11s and 13s. The bandwidth is calculated as total memory transferred (2x system size) divided by the time taken, so the higher - the better.

[Nvidia A100 results](https://preview.redd.it/jook8yvtf7r61.png?width=10000&format=png&auto=webp&s=2ead6c986ae71e506fc0c717e8ec005e4768577b)

[AMD MI100 results](https://preview.redd.it/8okf9jevf7r61.png?width=10000&format=png&auto=webp&s=e70ca0b6abd5d39a70f760c483c8192d56bd1fab)

The talk will also cover double-precision, multidimensional tests and their analysis, so I would really appreciate it if you can check it out!",hello creator vkfft url vulkan cuda hip fast fourier transform library would like invite gtc 2021 url panel vkfft happen april 13th nmbr pm cest higher education research category focus implement optimizations create cross platform code scale raspberry pi nmbr hpc gpus like nvidia a100 session also compare vulkan cuda hip compute platforms nvidia a100 amd mi100 gpus post would like give sneak peek part talk regard vkfft cufft rocfft performance comparison single precision 1d batch fft test systems nmbr 4096 representable arbitrary multiplication 2s 3s 5s 7s 11s 13s bandwidth calculate total memory transfer 2x system size divide time take higher better nvidia a100 result url mi100 result url talk also cover double precision multidimensional test analysis would really appreciate check
lfolle,MachineLearning,1620324878.0,[R] Deep learning methods allow fully automated segmentation of metacarpal bones to quantify volumetric bone mineral density,Check out our latest research on applying deep learning-based segmentation networks in the field of rheumatology: https://www.nature.com/articles/s41598-021-89111-9 [Open access],check latest research apply deep learn base segmentation network field rheumatology url open access
seagullonthetop,MachineLearning,1619320683.0,Machine learning for a theoretical physicist [D],"Hi,

My background is in theoretical (high energy) physics and cosmology. I'm comfortable with Mathematica, and have used MATLAB and Python in the past, though not for large-scale projects. I'm interested in learning machine learning, in a way that is suited for someone with my background experience. Has anyone trod a similar path? What are some useful resources, books, online courses and already-done projects/examples (preferably by someone in physics) that I can run through myself to get a hands-on, practical experience with ML?

Thanks",hi background theoretical high energy physics cosmology comfortable mathematica use matlab python past though large scale project interest learn machine learn way suit someone background experience anyone tread similar path useful resources book online course already project examples preferably someone physics run get hand practical experience ml thank
chimp73,MachineLearning,1620077927.0,[D] An RL agent based on a large NN that one-shot learns in single SGD updates. Recall and planning through generalization from one-shot learned predictions and policy updates,"[Yesterday](https://www.reddit.com/r/MachineLearning/comments/n2ts0l/d_how_far_can_we_get_with_oneshot_learning/), I explored the consequences if the scaling hypothesis were true, in particular what if upscaling gives us one-shot learning in single SDG updates as well as (super)human-level inference, prediction and generalization abilities.

Today, I've been thinking how a concrete implementation of this approach could look like. Ideas are cheap, but here it goes:

The model I've come up with is simply a fully-connected, wide VAE that at each step performs one inference and then produces two Gaussian samples and two predictions based on the samples. The first sample is used to predict the future, and the second sample is used to predict its own prediction (based on the first sample). As a consequence, the model would one-shot learn both the thought and sensory experience to have occurred.

Let x\_t be an N x T tensor containing T time steps (say 2 seconds sampled at about 10 Hz) of N = S + P + 1 features, where S is the length of the sensor vector s, P is the number of motor neurons p (muscle contractions between 0 and 1, i.e. sigmoidal) and one extra dimension for the experienced reward r. Let the first prediction be x'\_t = VAE(concat(x\_{t-1}, x''\_{t-1})) and the second prediction x''_t corresponding to the second sample from the VAE produced in the same way. Then, minimize the loss by SGD: (x_t - x'_t)^2 + (x'_t - x''_t)^2 + KLD(z') + KLD(z'') + p'_t(p'_t - α∙r_t)^2 + p''_t(p''_t - α∙r''_t)^2 + λ||p'_t||_1, where KLD is the KL regularizer for each Gaussian sample, α is a scaling constant for the reward such that strong absolute reward is > 1 and ||p'_t||_1 is a sparsity prior on the policy to encourage competition between actions. The two RL losses simply punish/reinforce actions that coincide with reward (though a slight temporal delay would likely help). The second loss acts on the imagined policy and imagined reward.

I'm unsure the thoughts can actually become goal-directed this way and I'd be extremely surprised if this actually works, but it is fun to think about.",yesterday url explore consequences scale hypothesis true particular upscaling give us one shoot learn single sdg update well super human level inference prediction generalization abilities today think concrete implementation approach could look like ideas cheap go model come simply fully connect wide vae step perform one inference produce two gaussian sample two predictions base sample first sample use predict future second sample use predict prediction base first sample consequence model would one shoot learn think sensory experience occur let x _t n x tensor contain time step say nmbr second sample nmbr hz n p nmbr feature length sensor vector p number motor neurons p muscle contractions nmbr 1 e sigmoidal one extra dimension experience reward r let first prediction x _t vae concat x _ 1 x _ 1 second prediction x _t correspond second sample vae produce way minimize loss sgd x_t x _t 2 x _t x _t 2 kld z kld z p _t p _t α r_t 2 p _t p _t α r _t 2 λ p _t _1 kld kl regularizer gaussian sample α scale constant reward strong absolute reward nmbr p _t _1 sparsity prior policy encourage competition action two rl losses simply punish reinforce action coincide reward though slight temporal delay would likely help second loss act imagine policy imagine reward unsure thoughts actually become goal direct way extremely surprise actually work fun think
Superb-Drawer5214,MachineLearning,1618760402.0,[D]What universities are considered top10 and top20 for ML/CV/NLP?,"It seems top4 are Berkeley, MIT, Stanford, CMU, but I am not sure what universities are considered top 10 and top 20 for ML/CV/NLP... Whenever some person mentions that he or she has applied to the top 20 schools for ML, what schools is he or she referring to?",seem top4 berkeley mit stanford cmu sure universities consider top nmbr top nmbr ml cv nlp whenever person mention apply top nmbr school ml school refer
grid_world,MachineLearning,1617303683.0,[D] Quantization in Deep Learning,"For Deep Learning model compression, the standard steps appear to be: pruning, clustering and quantization. I have experimented and implemented the first two steps in detail. Now, I am interested in learning about quantization techniques applied to deep learning for their compression. Can you point me to a nice resource (research paper, blog, tutorial, video, etc.) as a starting point?

Thanks!",deep learn model compression standard step appear prune cluster quantization experiment implement first two step detail interest learn quantization techniques apply deep learn compression point nice resource research paper blog tutorial video etc start point thank
m1900kang2,MachineLearning,1617071083.0,[R] Predicting Multiple Sclerosis from Gait Dynamics Using an Instrumented Treadmill – A Machine Learning Approach,"This paper by researchers from University of Illinois at Urbana-Champaign looks into ML being able to spot gait problems in individuals with multiple sclerosis.

\[[1-Min Presentation Video](https://crossminds.ai/video/predicting-multiple-sclerosis-from-gait-dynamics-using-an-instrumented-treadmill-a-machine-learning-approach-60626a18eb4e66fab2491414/)\] \[[Paper Link](https://ieeexplore.ieee.org/document/9311191)\] 

**Abstract:** Multiple Sclerosis (MS) is one of the most common neurological conditions worldwide whose prevalence is now greatest among people 50-60 years of age. While clinical presentations of MS are highly heterogeneous, mobility limitations is one of the most frequent symptoms. The aims of this study were to examine MS and disability related changes in spatiotemporal and kinetic gait features after normalization; and evaluate the effectiveness of a gait data-based machine learning (ML) framework for MS prediction (GML4MS). Methods: In this study, gait data during self-paced walking on an instrumented treadmill from 20 persons with MS and 20 age, weight, height and gender-matched healthy older adults (HOA) were obtained. We explored two normalization strategies, namely size-N (standard body size-based normalization) and regress-N (regression-based normalization using scaling factors derived by regressing gait features on multiple subject demographics) to minimize the dependency of derived gait features on the subject demographics; and proposed GML4MS, a ML based methodology to classify individual strides of older persons with MS (PwMS) from healthy controls, so as to generalize across different walking tasks and subjects after gait normalization. Results: We observed that regress-N improved the accuracy of identifying pathological gait using ML when compared to size-N. When generalizing from comfortable walking to walking while talking, gradient boosting machine achieved the optimal subject classification accuracy and AUC of 94.3% and 1.0, respectively and for subject generalization, a multilayer perceptron resulted in the best accuracy and AUC of 80% and 0.86, respectively, both with regress-N normalized data. Conclusion: The integration of gait data and ML to predict MS may provide a viable patient-centric approach to aid clinicians in disease monitoring and relapse treatment. This work is the first attempt to employ and demonstrate the potential of ML for this domain. Significance: The results of this study have future implications for the way regression normalized gait features may be clinically used to design ML-based disease prediction strategies and monitor disease progression in PwMS.

&#x200B;

[Butterfly Diagram of the Center of Pressure trajectory during a subject's walk](https://preview.redd.it/9vqlbv5xs2q61.png?width=620&format=png&auto=webp&s=9678ea41aa31370bcf4be7088b4c77fcae234704)

**Authors:** Rachneet Kaur, Zizhang Chen, Robert Motl, Manuel Enrique Hernandez, Richard Sowers (University of Illinois at Urbana-Champaign)",paper researchers university illinois urbana champaign look ml able spot gait problems individuals multiple sclerosis 1 min presentation video url paper link url abstract multiple sclerosis ms one common neurological condition worldwide whose prevalence greatest among people 50 60 years age clinical presentations ms highly heterogeneous mobility limitations one frequent symptoms aim study examine ms disability relate change spatiotemporal kinetic gait feature normalization evaluate effectiveness gait data base machine learn ml framework ms prediction gml4ms methods study gait data self pace walk instrument treadmill nmbr persons ms nmbr age weight height gender match healthy older adults hoa obtain explore two normalization strategies namely size n standard body size base normalization regress n regression base normalization use scale factor derive regress gait feature multiple subject demographics minimize dependency derive gait feature subject demographics propose gml4ms ml base methodology classify individual stride older persons ms pwms healthy control generalize across different walk task subject gait normalization result observe regress n improve accuracy identify pathological gait use ml compare size n generalize comfortable walk walk talk gradient boost machine achieve optimal subject classification accuracy auc 94 3 1 0 respectively subject generalization multilayer perceptron result best accuracy auc 80 0 86 respectively regress n normalize data conclusion integration gait data ml predict ms may provide viable patient centric approach aid clinicians disease monitor relapse treatment work first attempt employ demonstrate potential ml domain significance result study future implications way regression normalize gait feature may clinically use design ml base disease prediction strategies monitor disease progression pwms x200b butterfly diagram center pressure trajectory subject walk url rachneet kaur zizhang chen robert motl manuel enrique hernandez richard sowers university illinois urbana champaign
fripperML,MachineLearning,1617772778.0,[D] Is it always recommended to scale features to be predicted the same way as the training data was scaled?,"I don't know if this has been asked before... And the question might seem somewhat silly, so let me start briefly with the standard approach:

* The general advice I have always seen is that, if you decide to scale your data (which, in general, is recommended), you should fit\_transform a scaler during the training process and then apply the same scaler to the data to be predicted.
* This makes a lot of sense, and allows the model to predict from one only sample to whatever number of samples.

I am not questioning that. But I have the feeling that sometimes it is not the best approach. Let me give you an example:

* Let's suppose that we are dealing with a model that predicts probability of default of a person using economic features, like the annual income, amount of debts, etc.
* Let's suppose that the only preprocessing we make is scaling of the data.
* Let's suppose that we trained the model with data from 2018 and we are still using this model in production.

In this setting, it is very likely that there is some kind of drift, worsening the performance, and so one could retrain the model using more recent data. But my intuition is that, at least partially, one could reduce the problem by scaling the features of the instances to be predicted using the information of the year in course. I think so because those kind of economic features are all affected by inflation, so, for example, an annual income of 80K in 2018 is comparable to an annual income of, say, 85K in the present year, and scaling with a ""moving ruler"" can be better than scaling with a ""fixed ruler"".

This approach could be useful whenever the feature has a mean that changes in time. What do you think? Do you think it does make sense?

Of course, one limitation is that it is not easy to implement (the only way I can think of is when you use your model to make batch predictions for big enough chunks, so that scaling the features according to the distribution of those chunks is meaningful).",know ask question might seem somewhat silly let start briefly standard approach general advice always see decide scale data general recommend fit _transform scaler train process apply scaler data predict make lot sense allow model predict one sample whatever number sample question feel sometimes best approach let give example let suppose deal model predict probability default person use economic feature like annual income amount debts etc let suppose preprocessing make scale data let suppose train model data nmbr still use model production set likely kind drift worsen performance one could retrain model use recent data intuition least partially one could reduce problem scale feature instance predict use information year course think kind economic feature affect inflation example annual income 80k nmbr comparable annual income say 85k present year scale move ruler better scale fix ruler approach could useful whenever feature mean change time think think make sense course one limitation easy implement way think use model make batch predictions big enough chunk scale feature accord distribution chunk meaningful
jnbrrn,MachineLearning,1616769447.0,[R] Baking Neural Radiance Fields for Real-Time View Synthesis,"Real-time photorealistic neural rendering in your browser. [https://nerf.live](https://nerf.live)

Live demos: [https://nerf.live/#demos](https://nerf.live/#demos)

Paper PDF: [https://nerf.live/#baking\_neural\_radiance\_fields\_for\_real\_time\_view\_synthesis.pdf](https://nerf.live/#baking_neural_radiance_fields_for_real_time_view_synthesis.pdf)

Explainer video: [https://www.youtube.com/watch?v=5jKry8n5YO8](https://www.youtube.com/watch?v=5jKry8n5YO8)",real time photorealistic neural render browser url demo url pdf url video url
SanjivGautamOfficial,MachineLearning,1619603499.0,[P] MLOverflow - A Webapp for ML/DL,"Hello ML/AI enthusiasts,

I created a small (minimalistic design ) webapp for Machine Learning feeds, papers and events and this webapp literally has **Feeds**, **Events** and **Papers** section.

Here is the link: [https://mloverflow.com](https://mloverflow.com/) (I am currently adding contents, but having more people engaged on it would be delightful)

Let's check the features of our webapp if you could spare a moment:

**Feeds** is where you share your medium articles or anything that you find interesting over internet related to ML/AI. May it be linkedin posts about ML/DL. Even reddit link about your project you did related to AI/ML. You made an impression detection app and uploaded in youtube? How about sharing it on this website? Any medium blogs you wrote after reading a paper? Share the **link**.

**Events** is where you share **link** of events related to ML/AI.

**Papers** is where we discuss research papers. People can comment different blogs/videos **links** which will be useful for other users to come.

**COMMENT SECTION:**

We do have comment section for Feeds and Papers, so it would be easier for people to post related ML/DL posts.

There'll be 4 types of comments. **Audio, Video, Text, Explain**.

Audio, Video,Text will contain links of websites that will have audio, video or blog post respectively. The **links** not the actual video/audio. **Explain** is where you clarify/write about things there in comment section where you don't need help with links.

**MOTIVATION:**

What's the point of creating a site when we already have [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) subreddit, Medium for article sharing and of course when we can google search?

I believe this will make navigation for things easier and having a central place for ML/DL related posts would make it convenient for people interested in ML/DL to further consolidate their understanding.

**Miscellaneous**:

I am not really a great developer, but I think it might help few enthusiasts out there as I am one of them.

Note: *There is no option for uploading audio/video/gifs/image as of now as I don't see a point of it. But if we need it in future, I would be happy to add them as well.*

P.S: There is a suggestion/report section. So any bugs or any features you think I need to resolve/add, kindly tell me. You can log in and go to [https://mloverflow.com/report](https://mloverflow.com/report) and write it down.

Thank you for reading till here. Cheers!",hello ml ai enthusiasts create small minimalistic design webapp machine learn feed paper events webapp literally feed events paper section link url currently add content people engage would delightful let check feature webapp could spare moment feed share medium article anything find interest internet relate ml ai may linkedin post ml dl even reddit link project relate ai ml make impression detection app upload youtube share website medium blog write read paper share link events share link events relate ml ai paper discuss research paper people comment different blog videos link useful users come comment section comment section feed paper would easier people post relate ml dl post nmbr type comment audio video text explain audio video text contain link websites audio video blog post respectively link actual video audio explain clarify write things comment section need help link motivation point create site already r machinelearning url subreddit medium article share course google search believe make navigation things easier central place ml dl relate post would make convenient people interest ml dl consolidate understand miscellaneous really great developer think might help enthusiasts one note option upload audio video gifs image see point need future would happy add well p suggestion report section bug feature think need resolve add kindly tell log go url write thank read till cheer
mikegartrell,MachineLearning,1617917905.0,[N] Call for papers: KDD 2021 Workshop on Bayesian Causal Inference for Real-World Interactive Systems,"[https://bcirwis2021.github.io](https://bcirwis2021.github.io)

August 14 - 18, 2021 (final workshop date TBD)

&#x200B;

\---

Submission deadline: May 10, 2021, anywhere on Earth

Format: 3 page extended abstract + references + appendices, ACM Proceeding Template

Submission website: [https://cmt3.research.microsoft.com/BCIRWIS2021](https://cmt3.research.microsoft.com/BCIRWIS2021)

\---

&#x200B;

Increasingly we use machine learning to build interactive systems that learn from past actions and the reward obtained. Theory suggests several possible approaches, such as contextual bandits, reinforcement learning, the do-calculus, or plain old Bayesian decision theory. What are the most theoretically appropriate and practical approaches to doing causal inference for interactive systems?

We are particularly interested in case studies of applying machine learning methods to interactive systems that *did* or *did* *not* use Bayesian or *likelihood* based methods, with a discussion about why this choice was made in terms of practical or theoretical arguments. We also welcome submissions in the following areas:

* Offline evaluation of recommender and interactive systems.
* Comparison of Bayesian, off-policy and other heuristic approaches for offline metrics.
* Probabilistic approaches applied to contextual bandits and reinforcement learning approaches.
* Probabilistic approaches to incrementality and attribution.
* Non-Bayesian approaches and trade-offs with Bayesian/Likelihood approaches.
* Bayesian methods in a production environment.

&#x200B;

Organizers

* Nicholas Chopin (ENSAE)
* Mike Gartrell (Criteo AI Lab)
* Dawen Liang (Netflix)
* Alberto Lumbreras (Criteo AI Lab)
* David Rohde (Criteo AI Lab)
* Yixin Wang (UC Berkeley)",url nmbr 18 nmbr final workshop date tbd x200b submission deadline may 10 2021 anywhere earthformat nmbr page extend abstract reference appendices acm proceed templatesubmission website url use machine learn build interactive systems learn past action reward obtain theory suggest several possible approach contextual bandits reinforcement learn calculus plain old bayesian decision theory theoretically appropriate practical approach causal inference interactive systems particularly interest case study apply machine learn methods interactive systems use bayesian likelihood base methods discussion choice make term practical theoretical arguments also welcome submissions follow areas offline evaluation recommender interactive systems comparison bayesian policy heuristic approach offline metrics probabilistic approach apply contextual bandits reinforcement learn approach probabilistic approach incrementality attribution non bayesian approach trade bayesian likelihood approach bayesian methods production environment x200b organizers nicholas chopin ensae mike gartrell criteo ai lab dawen liang netflix alberto lumbreras criteo ai lab david rohde criteo ai lab yixin wang uc berkeley
TrainYourMonkeyBrain,MachineLearning,1616507632.0,[D] Polyline prediction literature,"I've encountered multiple occasions where I had the need to predict a line or curve, often represented as a polyline, in a 2D or 3D image. So far I've always obtained these by postprocessing standard CNN predictions like segmentations and the like. 

I was wondering, is there any existing research in CNN-based models that directly predict polylines? I have been unable to find any. I can imagine that a dedicated loss function based on somehow minimizing the area between a predicted and a ground truth curve would work best, but I'm running into the issue that it's difficult to determine the points that define each of the line segments, aka how many points should one predict, and where should they be placed best? 

Does anyone have literature or experience for this?",encounter multiple occasion need predict line curve often represent polyline 2d 3d image far always obtain postprocessing standard cnn predictions like segmentations like wonder exist research cnn base model directly predict polylines unable find imagine dedicate loss function base somehow minimize area predict grind truth curve would work best run issue difficult determine point define line segment aka many point one predict place best anyone literature experience
CarlJohnson2222,MachineLearning,1618958279.0,"[D] How long does it take to publish a research paper? How long is the the time from the moment you think of an idea, to the moment you submit the research paper to a journal (not including the time it takes for the journal to approve your paper)?","As the question states above, for those of you who know more about research, how long does it take to publish a paper? What is a normal amount of time to be able to conduct the research and then fully finish writing the paper and submit it?",question state know research long take publish paper normal amount time able conduct research fully finish write paper submit
DaredevilMeetsL,MachineLearning,1617157406.0,[D] What would you advice to make the most of a virtual conference?,"I am (presenting at and) attending a conference (International Symposium on Biomedical Imaging, a venue dedicated for medical and biological imaging and analysis) next month virtually. It is not a huge conference ([technical program](https://embs.papercept.net/conferences/conferences/ISBI21/program/ISBI21_ProgramAtAGlanceWeb.html)) like CVPR, NeurIPS, etc. (MICCAI is much larger for medical imaging) because the focus of the venue is quite niche.

&#x200B;

As a PhD student, such conferences could be quite useful for forging professional contacts and finding potential collaborators. What advice/suggestions would you have to make the most of my experience from a virtual conference? Ideally, I would like this to be the opportunity to networks and build connections. Any advice/suggestions would be appreciated. Thank you.

&#x200B;

* A very common advice is to look at the conference program beforehand to select some of the most relevant works. But this is limiting in that I do not know much about the work merely from the title and the abstract.
* Another (minor) inconvenience is the time zone: the event is in CET which makes the conference program from \~4 AM to \~9 AM for me.",present attend conference international symposium biomedical image venue dedicate medical biological image analysis next month virtually huge conference technical program url like cvpr neurips etc miccai much larger medical image focus venue quite niche x200b phd student conferences could quite useful forge professional contact find potential collaborators advice suggestions would make experience virtual conference ideally would like opportunity network build connections advice suggestions would appreciate thank x200b common advice look conference program beforehand select relevant work limit know much work merely title abstract another minor inconvenience time zone event cet make conference program 4 9
jj4646,MachineLearning,1618637941.0,[D] Is there such a theorem in machine learning?,"Is there an official theorem in statistics and machine learning that states: ""for a machine learning algorithm to ""work"" (generalize well), the test data must be ""similar"" to the training data?""

I know this is common sense, but I have been searching all over the internet to see an official reference of this principle. Is this formally codified somewhere? Is this referenced in the literature of PAC theory, which in turn, provides the basis for all machine learning algorithms?",official theorem statistics machine learn state machine learn algorithm work generalize well test data must similar train data know common sense search internet see official reference principle formally codify somewhere reference literature pac theory turn provide basis machine learn algorithms
ZenDragon,MachineLearning,1617152883.0,[Discussion] What do you think would happen if a GPT model was continuously fine-tuned on its own I/O?,"**Edit: The AI does not simply talk to itself. It talks to a human and the responses from *both* parties are used for further training. (In batches, paired together with symbols to indicate who is saying what)**

This would be quite computationally expensive obviously but suppose I've got the hardware and a lot of time and patience. Suppose I have daily conversations with the system, and the input/output of every interaction is immediately fed back in as training. (And we set the learning rate higher than normal because strong fitting would actually be good in this case)

Since natural conversation with a single person would take a long time to generate an appreciable volume of text, I was thinking maybe there'd also be ""introspective"" periods of training where it generates both sides of the conversation by itself, pretending there's still a human participant.

We could even add a simple ""judgement"" pass where the weight of fine-tuning for the AI's output is adjusted depending on how the human responds.

Worth a try or completely stupid idea?",edit ai simply talk talk human responses party use train batch pair together symbols indicate say would quite computationally expensive obviously suppose get hardware lot time patience suppose daily conversations system input output every interaction immediately feed back train set learn rate higher normal strong fit would actually good case since natural conversation single person would take long time generate appreciable volume text think maybe also introspective periods train generate side conversation pretend still human participant could even add simple judgement pass weight fine tune ai output adjust depend human respond worth try completely stupid idea
Ok_Reality2341,MachineLearning,1617487648.0,[D] How large can you go with CNN input images?,"How large can you reasonably go with CNNs in regards to the input image? I am training on a week worth of accelerometer signal data (at 100 data points per second) converted to a spectrogram. So, the higher the resolution of the spectrogram the better.

Is something like 6000x100 feasible?

I tried on a 600x100 image and it wasn't enough resolution to pick up the details of the full week. Any papers that relate to using CNN on large images is also a plus.",large reasonably go cnns regard input image train week worth accelerometer signal data nmbr data point per second convert spectrogram higher resolution spectrogram better something like 6000x100 feasible try 600x100 image enough resolution pick detail full week paper relate use cnn large image also plus
ubcengineer123,MachineLearning,1618605741.0,[R] Graphs for training loss per epoch in publications,"Hello all,

I'm researching and writing a paper on using ML (CNN architectures) in semantic segmentation for medical imaging. I'm hoping to create figures that look something like [This Graph](https://www.researchgate.net/profile/Mohammad-Pashaei-4/publication/339987654/figure/fig13/AS:870150437883905@1584471439655/Average-loss-per-epoch-for-training-and-validation-steps.ppm), but for me (70,000 training image patches), the epoch goes to a steady-state value after \~2-3 epochs? 

Has anyone encountered this before? Should I lower the batches per epoch? That won't be accurate because each epoch should consist of the full training dataset. 

Few more small details:

* I'm using weighted binary cross entropy to account for positive - negative label imbalance
* Default Adam optimizer

Can anyone suggest possible solutions or why it's happening? It's not a bad thing per se because the model works, but I won't be able to get nice looking figures if I reaches steady-state so quickly.

Thank you!",hello research write paper use ml cnn architectures semantic segmentation medical image hop create figure look something like graph url 70 000 train image patch epoch go steady state value 2 3 epochs anyone encounter lower batch per epoch win accurate epoch consist full train dataset small detail use weight binary cross entropy account positive negative label imbalance default adam optimizercan anyone suggest possible solutions happen bad thing per se model work win able get nice look figure reach steady state quickly thank
zawerf,MachineLearning,1617278796.0,[D] Cheating Detection (from a recent a Google Code Jam competition),"Problem statement can be found here: https://codingcompetitions.withgoogle.com/codejam/round/000000000043580a/00000000006d1155

Rough summary is: you have 100 players and 10000 questions, each player has a skill level `S_i` and each question has a difficulty `Q_j`. `S_i ~ Uniform(-3, 3)` and `Q_j ~ Uniform(-3, 3)`. A player with skill `S_i` will answer a question with difficulty `Q_j` correctly with probability `sigmoid(S_i - Q_j)`. Except for one cheater who will answer correctly with probability `0.5 * sigmoid(S_i - Q_j) + 0.5`. You have the results of what each player answered for each question. Identify the cheater.

To pass you only need to find the cheater 86% of the time, but it's possible to do much better.

Since this was a coding competition you were expected to come up with a simple heuristic (see the ""Analysis"" tab for the intended solution).

But I'm curious how stats/ML people would approach this instead.",problem statement find url summary nmbr players nmbr question player skill level s_i question difficulty q_j s_i uniform 3 3 q_j uniform 3 3 player skill s_i answer question difficulty q_j correctly probability sigmoid s_i q_j except one cheater answer correctly probability 0 5 sigmoid s_i q_j 0 5 result player answer question identify cheater pass need find cheater 86 time possible much better since cod competition expect come simple heuristic see analysis tab intend solution curious stats ml people would approach instead
allasamhita,MachineLearning,1617799679.0,[P] Accelerate Your Machine Learning and Data Workflows to Production using Flyte!,"Imagine the pain behind orchestrating your ML workflows. You'd have to take care of maintaining the model artifacts, caching the results, facilitating backtracking to an error source, sharing your results with your team members, containerizing the jobs, and ensuring scalability all the time. This isn't cheap. A lot has to happen in the backend to ensure that your ML workflows are running seamlessly. If you're a part of a more prominent company, it's very much vital to ensure that yours is a fool-proof ML service spanning multiple teams. Here, **Flyte** could be your solution. 

**Flyte** makes it easy to create concurrent, scalable, and maintainable workflows for **machine learning** and **data processing (you heard it right! It's ML + data processing).**

Flyte is used in production at **Lyft**, **Spotify**, **Freenome**, and others. At Lyft, Flyte has been serving production model training and data processing for over four years, becoming the de-facto platform for teams like Pricing, Locations, ETA, Mapping, Autonomous, and more. In fact, Flyte manages over 10,000 unique workflows at Lyft, totaling over **1,000,000 executions** every month, **20 million tasks**, and **40 million containers!**

You can check out Flyte here: [https://github.com/flyteorg/flyte](https://github.com/flyteorg/flyte), and here is the [website](https://flyte.org/).",imagine pain behind orchestrate ml workflows take care maintain model artifacts cache result facilitate backtrack error source share result team members containerize job ensure scalability time cheap lot happen backend ensure ml workflows run seamlessly part prominent company much vital ensure fool proof ml service span multiple team flyte could solution flyte make easy create concurrent scalable maintainable workflows machine learn data process hear right ml data process flyte use production lyft spotify freenome others lyft flyte serve production model train data process four years become de facto platform team like price locations eta map autonomous fact flyte manage 10 000 unique workflows lyft total 1 000 000 executions every month 20 million task 40 million containers check flyte url website url
the_travelo_,MachineLearning,1617795448.0,[D] what's the best approach to document a machine learning project?,"At my company we""re struggling to correctly document projects. We use confluence for governance but haven't found a proper rythm to document everything involved in the development of the project.

What tools/processes/code or anything do you use to solve this problem?",company struggle correctly document project use confluence governance find proper rythm document everything involve development project tool process code anything use solve problem
cereal_final,MachineLearning,1620446819.0,[P] How to get better performance with styleGAN2-ada for cartoons,"I'm trying to generate pokemon with styleGAN2-ada, and I'm not getting the [best results](https://imgur.com/a/h9u6nVh). I would say 50% look like legit pokemon, but the other 50% are kinda trash like the image I linked. I tried training it longer, but I believe the [model collapsed](https://imgur.com/a/5ZIvJ27). How can I improve results? The dataset is 5k images of pokemon headshots like [this.](https://imgur.com/a/bxZu3vU)",try generate pokemon stylegan2 ada get best result url would say 50 look like legit pokemon 50 kinda trash like image link try train longer believe model collapse url improve result dataset 5k image pokemon headshots like url
JFHermes,MachineLearning,1618835916.0,[D]Ethics in famous Machine Learning papers.,"Hi,

I've been asked to write an Ethics Review for my STAT class at University. Most of the papers you would think to do have already been done, and I know there is a lot of great research happening in AI so I thought I would ask for some advice here.

I just need a single paper. It doesn't have to be unethical or even gray, but it would be better if I had something to talk about. 

So yeah, just throwing it out there in case anyone in here knows of one off the top of their head. Thanks for your time!",hi ask write ethics review stat class university paper would think already know lot great research happen ai think would ask advice need single paper unethical even gray would better something talk yeah throw case anyone know one top head thank time
LosinCash,MachineLearning,1617995159.0,[D] Looking for some clarification on Big Sleep variables.,"Hi all, hoping a few may be able to produce some insight into the variables inside of the Big Sleep ( hoping u/Whiskey has a moment to respond). 

I've installed and am running the Big Sleep on a Nvidia Jetson Xavier (both in Python and as a Jupyter notebook) and am using up about 16gb of it's ram, so plenty left to abuse. When drilling down through the variables there are a couple that have me stumped and I'd like to know more about them while I'm experimenting with them. 

Specifically: 

seed
gradient accumulate
torch_deterministic
class_temperature 

Thanks for any help or resources you can point me towards.",hi hop may able produce insight variables inside big sleep hop u whiskey moment respond instal run big sleep nvidia jetson xavier python jupyter notebook use 16gb ram plenty leave abuse drill variables couple stump like know experiment specifically seedgradient accumulatetorch_deterministicclass_temperature thank help resources point towards
timscarfe,MachineLearning,1617499307.0,"[D] Christian Szegedy - Formal Reasoning, Program Synthesis [Video Show]","[https://youtu.be/ehNGGYFO6ms](https://youtu.be/ehNGGYFO6ms) 

Dr. Christian Szegedy from Google Research is a deep learning heavyweight. He invented adversarial examples, one of the first object detection algorithms, the inceptionnet architecture, and co-invented batchnorm. He thinks that if you bet on computers and software in 1990 you would have been as right as if you bet on AI now. But he thinks that we have been programming computers the same way since the 1950s and there has been a huge stagnation ever since. Mathematics is the process of taking a fuzzy thought and formalising it. But could we automate that? Could we create a system which will act like a super human mathematician but you can talk to it in natural language? This is what Christian calls autoformalisation. Christian thinks that automating many of the things we do in mathematics is the first step towards software synthesis and building human-level AGI. Mathematics ability is the litmus test for general reasoning ability. Christian has a fascinating take on transformers too.  

Touching on;

A Promising Path Towards Autoformalization and General Artificial Intelligence \[Szegedy\]

[https://link.springer.com/chapter/10.1007/978-3-030-53518-6\_1](https://link.springer.com/chapter/10.1007/978-3-030-53518-6_1)

Learning to Reason in Large Theories without Imitation \[Bansal/Szegedy\]

[https://arxiv.org/pdf/1905.10501.pdf](https://arxiv.org/pdf/1905.10501.pdf)

MATHEMATICAL REASONING VIA SELF-SUPERVISED SKIP-TREE TRAINING \[Rabe .. Szegedy\]

[https://openreview.net/pdf?id=YmqAnY0CMEy](https://openreview.net/pdf?id=YmqAnY0CMEy)

LIME: LEARNING INDUCTIVE BIAS FOR PRIMITIVES OF MATHEMATICAL REASONING \[Wu..Szegedy\]

[https://arxiv.org/abs/2101.06223v1](https://arxiv.org/abs/2101.06223v1)

DEEP LEARNING FOR SYMBOLIC MATHEMATICS \[Lample\]

[https://arxiv.org/pdf/1912.01412.pdf](https://arxiv.org/pdf/1912.01412.pdf)

It’s Not What Machines Can Learn, It’s What We Cannot Teach \[Yehuda\]

[https://arxiv.org/pdf/2002.09398.pdf](https://arxiv.org/pdf/2002.09398.pdf)

Investigating the Limitations of Transformers with Simple Arithmetic Tasks \[Nogueira\]

[https://arxiv.org/pdf/2102.13019.pdf](https://arxiv.org/pdf/2102.13019.pdf)

Provable Bounds for Learning Some Deep Representations \[Arora\]

[https://arxiv.org/pdf/1310.6343.pdf](https://arxiv.org/pdf/1310.6343.pdf)

Neural nets learn to program neural nets with fast weights \[Schmidhuber\]

[https://people.idsia.ch/\~juergen/fast-weight-programmer-1991-transformer.html](https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html)

How does Batch Normalization Help Optimization? \[Ilyas\]

[https://gradientscience.org/batchnorm/](https://gradientscience.org/batchnorm/)

How to Train Your ResNet 7: Batch Norm

[https://myrtle.ai/learn/how-to-train-your-resnet-7-batch-norm/](https://myrtle.ai/learn/how-to-train-your-resnet-7-batch-norm/)

Training a ResNet to 94% Accuracy on CIFAR-10 in 26 Seconds on a Single GPU – \[KUHN\]

[https://efficientdl.com/how-to-train-a-resnet-efficiently/#7-batch-norm-does-reducs-internal-covariate-shift](https://efficientdl.com/how-to-train-a-resnet-efficiently/#7-batch-norm-does-reducs-internal-covariate-shift)",url dr christian szegedy google research deep learn heavyweight invent adversarial examples one first object detection algorithms inceptionnet architecture co invent batchnorm think bet computers software nmbr would right bet ai think program computers way since 1950s huge stagnation ever since mathematics process take fuzzy think formalise could automate could create system act like super human mathematician talk natural language christian call autoformalisation christian think automate many things mathematics first step towards software synthesis build human level agi mathematics ability litmus test general reason ability christian fascinate take transformers touch promise path towards autoformalization general artificial intelligence szegedy url reason large theories without imitation bansal szegedy url reason via self supervise skip tree train rabe szegedy url learn inductive bias primitives mathematical reason wu szegedy url learn symbolic mathematics lample url machine learn teach yehuda url limitations transformers simple arithmetic task nogueira url bound learn deep representations arora url net learn program neural net fast weight schmidhuber url batch normalization help optimization ilyas url train resnet 7 batch norm url resnet 94 accuracy cifar 10 nmbr second single gpu kuhn url
Mjjjokes,MachineLearning,1617939562.0,[R] CPU algorithm trains deep neural nets up to 15 times faster than top GPU trainers,"Link: https://techxplore.com/news/2021-04-rice-intel-optimize-ai-commodity.html?fbclid=IwAR3uvvw6fOHDMliJxSi3AVoW1JNwtYkDIUcf0Tmuc9dWwdAH8irtTMABYjs

""The whole industry is fixated on one kind of improvement—faster matrix multiplications,"" Shrivastava said. ""Everyone is looking at specialized hardware and architectures to push matrix multiplication. People are now even talking about having specialized hardware-software stacks for specific kinds of deep learning. Instead of taking an expensive algorithm and throwing the whole world of system optimization at it, I'm saying, 'Let's revisit the algorithm.'""

From the article",link url whole industry fixate one kind improvement faster matrix multiplications shrivastava say everyone look specialize hardware architectures push matrix multiplication people even talk specialize hardware software stack specific kinds deep learn instead take expensive algorithm throw whole world system optimization say let revisit algorithm article
meldiwin,MachineLearning,1617392472.0,"[N] ""DeepDream"" Questions for Alexander Mordvintsev, a research scientist at Google","&#x200B;

Hello,

For those who maybe famiilar with DeepDreams  a [computer vision](https://en.wikipedia.org/wiki/Computer_vision)  program created by  Alexander Mordvintsev, we (IEEE Soft Robotics  podcast) are going to have him on the podcast, if you have any questions you can send  them here: [https://docs.google.com/forms/d/e/1FAIpQLSdThTdL5BKFBRRpoA0scZWFDDq4HIJwqGt2SLSugh9NdA0LNA/viewform?vc=0&c=0&w=1&flr=0&gxids=7628](https://docs.google.com/forms/d/e/1FAIpQLSdThTdL5BKFBRRpoA0scZWFDDq4HIJwqGt2SLSugh9NdA0LNA/viewform?vc=0&c=0&w=1&flr=0&gxids=7628)

&#x200B;

&#x200B;

https://preview.redd.it/kqdkz2aoctq61.png?width=922&format=png&auto=webp&s=e693219b6b5b893492e4aab6c9b81bc8fc52e08c",x200b hello maybe famiilar deepdreams computer vision url program create alexander mordvintsev ieee soft robotics podcast go podcast question send url
SieunPark,MachineLearning,1617422929.0,[R] How to download the DIV8K dataset for Super-Resolution?,"[DIV8K](https://people.ee.ethz.ch/~timofter/publications/Gu-ICCVW-2019b.pdf) is a dataset used for Super-Resolution. This was used in the 2019 AIM challenge and 2020 NTIRE challenge.

The link to the challenge is [https://competitions.codalab.org/competitions/22217#learn\_the\_details-evaluation](https://competitions.codalab.org/competitions/22217#learn_the_details-evaluation).

But unfortunately, I couldn't find a link to download the dataset anywhere on the internet. How can I download the DIV8K dataset?

Thank you.",div8k url dataset use super resolution use nmbr aim challenge nmbr ntire challenge link challenge url unfortunately find link download dataset anywhere internet download div8k dataset thank
OShackathon,MachineLearning,1618620959.0,[P] Darknet on AMD Hardware,[https://github.com/AlexeyAB/darknet/compare/master...os-hackathon:master](https://github.com/AlexeyAB/darknet/compare/master...os-hackathon:master),url
iarai-weather4cast,MachineLearning,1617294382.0,[N] Join our new IARAI Multi-sensor Weather Forecast Competition!,"Join our new Multi-sensor Weather Forecast Competition!

* Study multi-channel weather movies.
* Predict weather products in various earth regions.
* Apply transfer learning to new earth regions.

**Goal**  
The goal of the competition is a short-term prediction of selected weather products in three regions  (**core challenge**) and apply the transfer learning to predict weather products in the three additional regions (**transfer learning challenge**).

Following our recent success of our Traffic4cast competitions at NeurIPS in 2019 and 2020, this challenge similarly presents weather forecast as a video frame prediction task. The competition goal is to predict the next 32 images (8 hours in 15 minute intervals) of the weather movies. The images contain four channels encoding the following weather products: temperature (on accessible surface: top cloud or earth), convective rainfall rate, probability of occurrence of tropopause folding, and cloud mask, based on meteorological satellites data obtained in collaboration with AEMET/ NWC SAF. Each pixel in the images represents the area of about 4 km x 4 km, and  each region contains 256 x 256 pixels.  The regions span varying  landscapes including mountains, deserts,  islands and seas, and others.

**The challenge offers real-world benchmark for few shot and transfer learning and allows testing multi-sensor data fusion!**

Join us and learn more about the data on our website: [weather4cast.ai](https://weather4cast.ai)

**Prizes**  
The winners of the core competition and the transfer learning competition will be awarded the following prizes:

* 1st place –  a voucher or cash prize worth 5,000€ to the participant/team;
* 2nd place – a voucher or cash prize worth 3,000€ to the participant/team;
* 3rd place – a voucher or cash prize worth 2,000€ to the participant/team.

**Deadlines**

* Competition starts: April 1, 2021
* Submission deadline: May 31, 2021, 23:59 AOE.
   * held-out dataset available: June 1, 2021.
* Abstract submission and held-out dataset prediction: June 6, 2021, 23:59 AOE.
* Announcement of the winners: June 21, 2021.",join new multi sensor weather forecast competition study multi channel weather movies predict weather products various earth regions apply transfer learn new earth regions goal goal competition short term prediction select weather products three regions core challenge apply transfer learn predict weather products three additional regions transfer learn challenge follow recent success traffic4cast competitions neurips nmbr 2020 challenge similarly present weather forecast video frame prediction task competition goal predict next nmbr image 8 hours nmbr minute intervals weather movies image contain four channel encode follow weather products temperature accessible surface top cloud earth convective rainfall rate probability occurrence tropopause fold cloud mask base meteorological satellite data obtain collaboration aemet nwc saf pixel image represent area nmbr km x nmbr km region contain nmbr x nmbr pixels regions span vary landscape include mountains desert islands seas others challenge offer real world benchmark shoot transfer learn allow test multi sensor data fusion join us learn data website weather4cast ai url winners core competition transfer learn competition award follow prize 1st place voucher cash prize worth 5 000 participant team 2nd place voucher cash prize worth 3 000 participant team 3rd place voucher cash prize worth 2 000 participant team deadlines competition start april 1 2021 submission deadline may 31 2021 23 59 aoe hold dataset available june 1 2021 abstract submission hold dataset prediction june 6 2021 23 59 aoe announcement winners june 21 2021
StrasJam,MachineLearning,1617867075.0,[D] Facebook's use of Softmax in multi-label classification,"I was reading this [paper](https://arxiv.org/pdf/1805.00932.pdf)  put out by a group of researchers at Facebook where they found that using a softmax and CE loss function during  training led to improved results over sigmoid + BCE. During training they change the one-hot label vector such that each '1' is divided by the  number of labels for    the given image (e.g. from \[0, 1, 1, 0\] to \[0, 0.5,  0.5, 0\]).

However, they do not mention how this could then be used in the  inference stage, because the required threshold for selecting the  correct labels is not clear and would theoretically need to be set based upon the expected number of labels for the image (which is information which wouldn't be available at inference).

Has anyone else read this paper or have an idea how this could work?",read paper url put group researchers facebook find use softmax ce loss function train lead improve result sigmoid bce train change one hot label vector 1 divide number label give image e g 0 1 1 0 0 0 5 0 5 0 however mention could use inference stage require threshold select correct label clear would theoretically need set base upon expect number label image information available inference anyone else read paper idea could work
jj4646,MachineLearning,1619296637.0,[D] relationship between svm and neural networks,"Is this correct?

Svm's project the data into higher dimensions to look for patterns and then project them back into lower dimensions to make the final decision; whereas neural networks directly project the data into a lower dimension to make the decision?",correct svm project data higher dimension look pattern project back lower dimension make final decision whereas neural network directly project data lower dimension make decision
zy415,MachineLearning,1619705753.0,[D] IJCAI 2021 Paper Acceptance Result,IJCAI 2021 paper acceptance results were released. Creating a discussion thread for this year's results.,ijcai nmbr paper acceptance result release create discussion thread year result
huseinzol05,MachineLearning,1618732918.0,"[P] Malaya-Speech, Speech-Toolkit library for Malay language, powered by Deep Learning Tensorflow","## Features

-   **Age Detection**, detect age in speech using Finetuned Speaker Vector.
-   **Speaker Diarization**, diarizing speakers using Pretrained Speaker Vector.
-   **Emotion Detection**, detect emotions in speech using Finetuned Speaker Vector.
-   **Gender Detection**, detect genders in speech using Finetuned Speaker Vector.
-   **Language Detection**, detect hyperlocal languages in speech using Finetuned Speaker Vector.
-   **Multispeaker Separation**, Multispeaker separation using FastSep on 8k Wav.
-   **Noise Reduction**, reduce multilevel noises using STFT UNET.
-   **Speaker Change**, detect changing speakers using Finetuned Speaker Vector.
-   **Speaker overlap**, detect overlap speakers using Finetuned Speaker Vector.
-   **Speaker Vector**, calculate similarity between speakers using Pretrained Speaker Vector.
-   **Speech Enhancement**, enhance voice activities using Waveform UNET.
-   **Speech-to-Text**, End-to-End Speech to Text for Malay and Mixed (Malay and Singlish) using RNN-Transducer.
-   **Super Resolution**, Super Resolution 4x for Waveform.
-   **Text-to-Speech**, Text to Speech for Malay and Singlish using Tacotron2 and FastSpeech2.
-   **Vocoder**, convert Mel to Waveform using MelGAN, Multiband MelGAN and Universal MelGAN Vocoder.
-   **Voice Activity Detection**, detect voice activities using Finetuned Speaker Vector.
-   **Voice Conversion**, Many-to-One, One-to-Many, Many-to-Many, and Zero-shot Voice Conversion.
-   **Hybrid 8-bit Quantization**, provide hybrid 8-bit quantization for all models to reduce inference time up to 2x and model size up to 4x.

## Pretrained Models

-   **Wave UNET**, Multi-Scale Neural Network for End-to-End Audio Source Separation, https://arxiv.org/abs/1806.03185
-   **Wave ResNet UNET**, added ResNet style into Wave UNET, no paper produced.
-   **Wave ResNext UNET**, added ResNext style into Wave UNET, no paper produced.
-   **Deep Speaker**, An End-to-End Neural Speaker Embedding System, https://arxiv.org/pdf/1705.02304.pdf
-   **SpeakerNet**, 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification, https://arxiv.org/abs/2010.12653
-   **VGGVox**, a large-scale speaker identification dataset, https://arxiv.org/pdf/1706.08612.pdf
-   **GhostVLAD**, Utterance-level Aggregation For Speaker Recognition In The Wild, https://arxiv.org/abs/1902.10107
-   **Conformer**, Convolution-augmented Transformer for Speech Recognition, https://arxiv.org/abs/2005.08100
-   **ALConformer**, A lite Conformer, no paper produced.
-   **Jasper**, An End-to-End Convolutional Neural Acoustic Model, https://arxiv.org/abs/1904.03288
-   **Tacotron2**, Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, https://arxiv.org/abs/1712.05884
-   **FastSpeech2**, Fast and High-Quality End-to-End Text to Speech, https://arxiv.org/abs/2006.04558
-   **MelGAN**, Generative Adversarial Networks for Conditional Waveform Synthesis, https://arxiv.org/abs/1910.06711
-   **Multi-band MelGAN**, Faster Waveform Generation for High-Quality Text-to-Speech, https://arxiv.org/abs/2005.05106
-   **SRGAN**, Modified version of SRGAN to do 1D Convolution, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, https://arxiv.org/abs/1609.04802
-   **Speech Enhancement UNET**, https://github.com/haoxiangsnr/Wave-U-Net-for-Speech-Enhancement
-   **Speech Enhancement ResNet UNET**, Added ResNet style into Speech Enhancement UNET, no paper produced.
-   **Speech Enhancement ResNext UNET**, Added ResNext style into Speech Enhancement UNET, no paper produced.
-   **Universal MelGAN**, Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform Generation in Multiple Domains, https://arxiv.org/abs/2011.09631
-   **FastVC**, Faster and Accurate Voice Conversion using Transformer, no paper produced.
-   **FastSep**, Faster and Accurate Speech Separation using Transformer, no paper produced.
-   **wav2vec 2.0**, A Framework for Self-Supervised Learning of Speech Representations, https://arxiv.org/abs/2006.11477

Check latest documentation at https://malaya-speech.readthedocs.io/

Check github repository at https://github.com/huseinzol05/malaya-speech",feature age detection detect age speech use finetuned speaker vector speaker diarization diarizing speakers use pretrained speaker vector emotion detection detect emotions speech use finetuned speaker vector gender detection detect genders speech use finetuned speaker vector language detection detect hyperlocal languages speech use finetuned speaker vector multispeaker separation multispeaker separation use fastsep 8k wav noise reduction reduce multilevel noise use stft unet speaker change detect change speakers use finetuned speaker vector speaker overlap detect overlap speakers use finetuned speaker vector speaker vector calculate similarity speakers use pretrained speaker vector speech enhancement enhance voice activities use waveform unet speech text end end speech text malay mix malay singlish use rnn transducer super resolution super resolution 4x waveform text speech text speech malay singlish use tacotron2 fastspeech2 vocoder convert mel waveform use melgan multiband melgan universal melgan vocoder voice activity detection detect voice activities use finetuned speaker vector voice conversion many one one many many many zero shoot voice conversion hybrid 8 bite quantization provide hybrid 8 bite quantization model reduce inference time 2x model size 4x pretrained model wave unet multi scale neural network end end audio source separation url wave resnet unet add resnet style wave unet paper produce wave resnext unet add resnext style wave unet paper produce deep speaker end end neural speaker embed system url speakernet 1d depth wise separable convolutional network text independent speaker recognition verification url vggvox large scale speaker identification dataset url ghostvlad utterance level aggregation speaker recognition wild url conformer convolution augment transformer speech recognition url alconformer lite conformer paper produce jasper end end convolutional neural acoustic model url tacotron2 natural tts synthesis condition wavenet mel spectrogram predictions url fastspeech2 fast high quality end end text speech url melgan generative adversarial network conditional waveform synthesis url multi band melgan faster waveform generation high quality text speech url srgan modify version srgan 1d convolution photo realistic single image super resolution use generative adversarial network url speech enhancement unet url speech enhancement resnet unet add resnet style speech enhancement unet paper produce speech enhancement resnext unet add resnext style speech enhancement unet paper produce universal melgan universal melgan robust neural vocoder high fidelity waveform generation multiple domains url fastvc faster accurate voice conversion use transformer paper produce fastsep faster accurate speech separation use transformer paper produce wav2vec 2 0 framework self supervise learn speech representations url latest documentation url github repository url
marksteve4,MachineLearning,1617752955.0,[D] state of art for Speaker Diarization?," I've tried [Resemblyzer's](https://github.com/resemble-ai/Resemblyzer) method, yet it always either cut out too much of his voice, or included too much of others. It also required that i have a clip of him talking, and the quality of that clip heavily impacted its performance.",try resemblyzer url method yet always either cut much voice include much others also require clip talk quality clip heavily impact performance
malia912,MachineLearning,1620140015.0,YOLOV5 with a second stage classifier [D],"[D]
Hello everyone. This is a question regarding YOLOV5 In detect.py module I saw a second stage classifier but it is set to false.

PROBLEM STATEMENT: I want to classify images with a secondary classifier. In first stage YOLOV5 just draws bounding box around images of Dogs (basically localising the dogs in the image) and with the help of SECONDARY CLASSIFIER want to classify dogs into respective breeds.

RESULT: bounding box around the dog with names of the

breed.

Has anyone tried this approach?

Thank you for your time.",hello everyone question regard yolov5 detect py module saw second stage classifier set false problem statement want classify image secondary classifier first stage yolov5 draw bound box around image dog basically localise dog image help secondary classifier want classify dog respective breed result bound box around dog name thebreed anyone try approach thank time
ztzyz615,MachineLearning,1619087386.0,[D] Any statistical theory for mixture density network?, I want to ask whether there are any paper or work in relation to the statistical theory of mixture density network ([https://publications.aston.ac.uk/id/eprint/373/1/NCRG\_94\_004.pdf](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf))? Like the convergence rate or estimation error bound. I did a lot of searching but failed to find. Many thanks!,want ask whether paper work relation statistical theory mixture density network url like convergence rate estimation error bind lot search fail find many thank
PaganPasta,MachineLearning,1618989276.0,[D] Adding under review/submitted papers on Resumé?,"Recently, I have been skimming through resumes of potential candidates for a research role and came across plenty of them which add ""submitted"" or ""under-review"" @CVPR, ICML etc. to their publication list. 

Is this a common practice ? It becomes annoying when the paper is not on arxiv and the candidate wishes to present it but also can't share it. Most of the times these papers are underwhelming which lack rigour on many fronts. I just see it as a poor effort at adding the reputed conference names to the resumé. Or perhaps, I am missing the utility of this information.",recently skim resume potential candidates research role come across plenty add submit review cvpr icml etc publication list common practice become annoy paper arxiv candidate wish present also share time paper underwhelming lack rigour many front see poor effort add repute conference name resumé perhaps miss utility information
hnipun,MachineLearning,1617549237.0,[P][D] Dynamic Hyper-parameters,"*Hyper-parameters control the learning process of models. They are set before training starts, either by intuition or a hyper-parameter search. They either stay static or change based on a pre-determined schedule. We are introducing dynamic hyper-parameters which can be manually adjusted during the training based on model training stats.*

**Github:** [**https://github.com/lab-ml/labml**](https://github.com/lab-ml/labml) **App:** [**https://app.labml.ai/**](https://app.labml.ai/)

## What are hyper-parameters?

Hyper-parameters are parameters that control the learning process of models, such as the learning rate, batch size, and weight decay. The model might not learn if the hyper-parameters are not set correctly. Setting hyper-parameters is a key part of deep learning research. Researchers find them based on intuition or by running a hyper-parameter search. In a hyper-parameter search, the model is trained repeatedly with different hyper-parameters to find the best set of hyper-parameters. Hyper-parameter search becomes costly as the number of hyper-parameters increases and the model training time increases.

Some hyper-parameters change during the training based on a pre-determined schedule. For example, you could slowly decrease the learning rate, or you could decrease the coefficient of an auxiliary loss as the model learns. Finding these schedules is nearly impossible with a hyper-parameter search, and are usually determined based on the intuition of the researchers.

## Why is it hard to determine hyper-parameters?

Setting hyper-parameters require quite a bit of experience with the kind of models and sizes you are training as well as the dataset. For instance, consider fine-tuning a pre-trained language model to classify tweets. You get a pre-trained language model as the backbone and attach a layer (or two) as the classification head. First, you freeze the parameters of the backbone and train the head for a certain number of updates, and then you unfreeze all parameters and train all the parameters. The number of steps to keep the backbone frozen is generally set to 1 epoch. This is a hyper-parameter. And the common practice of freezing for 1 epoch might be too small or too large depending on the size of the model as well as the dataset size. Someone who has worked with similar models and datasets will have a good intuition on this hyper-parameter. If you are new, you will have to try training the model to get a feel about it.

## ⚙️ Introducing Dynamic Hyper-parameters

Dynamic hyper-parameters are hyper-parameters that researchers can adjust while the model is being trained. This allows researchers to actively control how the model trains, instead of letting the model train with a pre-determined set of hyper-parameters. Dynamic hyper-parameters help train the model faster and better on a single training session. Also, they let researchers play around with the hyper-parameters during a single training run to gather insights.

Sometimes researchers save the model checkpoints and restart the training with changed hyper-parameter values. This has a similar effect to dynamic hyper-parameters but it's quite cumbersome to do.

## How does it work?

You need to create a dynamic hyper-parameter and register them along with other configurations.

    from labml import experiment
    from labml.configs import FloatDynamicHyperParam
    
    lr = FloatDynamicHyperParam(2.5e-4, range_=(0, 0.1))
    
    experiment.configs({
      'learning_rate': lr,
      ...,
    })

Then can call the dynamic hyper-parameter to get the current value. For example:

    def train(batch):
      optimizer.set_lr(lr())
      optimizer.step()

The call `lr()` will return the current learning rate set in [labml.ai](https://labml.ai) [app](https://github.com/lab-ml/app).

[*This*](https://github.com/lab-ml/labml/raw/master/guides/dynamic_hp.png) *is a screenshot of the mobile web interface for changing dynamic hyper-parameters. In this* [*Demo*](https://app.labml.ai/run/6eff28a0910e11eb9b008db315936e2f/hyper_params) *demo we adjusted the learning rate, clipping range, and the number of training epochs (per sample) to speed up the training of a* [*PPO agent*](https://nn.labml.ai/rl/ppo/experiment.html) *for Atari Breakout. A standard learning rate decay and other static hyper-parameter values would have taken a lot of training updates to get over the score of 1.*

## Example use-cases

**Freezing pre-trained layers**: When fine-tuning a language model, you can train with the backbone frozen until the rate of improvement of loss drops, and change the hyper-parameter affecting which layers are frozen. This is better and faster than going with the common practice of keeping the backbone frozen for 1 epoch for all models and datasets.

**Learning-rate warm-up and decay**: The learning rate can be manually increased during the initial training updates. You could decide how long to warm up for based on the loss curves. Similarly, you can decay the learning rate when the loss values stabilize. This allows you to use higher learning rates initially to speed up the training.

**Increase sequence length**: Recurrent models train faster when the BPTT length is shorter. But you need higher BPTT lengths to improve accuracy. Therefore, it is a common practice to start with a shorter BPTT length and increase it later. Again deciding when to do this beforehand is hard. Changing this dynamically is a lot easier.

**Adjusting regularization parameters**: You can start with lower weight decay and lower dropout probabilities initially. Especially if you are not sure about the representation capacity of the model. You can then increase these regularization parameters later when the validation loss stops improving (higher variance).

**Adjusting reinforcement learning hyper-parameters**: Reinforcement learning tends to have more hyper-parameters. Most of which need to change during training, such as discount factor, entropy-bonus coefficients, learning-rate, etc.  Pre-determining them is almost impossible without observing a few training runs, and those training runs go many hours or days even for simple gaming environments. Changing these during training based on the agent's performance and other stats is a lot easier.

## What's next

**Updating hyper-parameter schedules**: Our current implementation only allows users to update hyper-parameter values. This can take too much user time. For instance, let's say based on current loss curves the user figures out that he wants to drop the learning rate from `1e-3` to `2.5e-4` during the next 100,000 updates. With our current implementation, she would have to make several manual  changes. We want to let users set and update hyper-parameter schedules so that user has to manually intervene only when necessary.

**Rewind**: Often when training with dynamic hyper-parameters you feel like experimenting with them. Sort of like a small hyper-parameter search while the model is training. But when things go wrong you want to reset. To enable this we are working on a simple rewind (or undo) option, where the user could restart at  any checkpoint, with a couple of taps on the screen.",hyper parameters control learn process model set train start either intuition hyper parameter search either stay static change base pre determine schedule introduce dynamic hyper parameters manually adjust train base model train stats github url app url hyper parameters hyper parameters parameters control learn process model learn rate batch size weight decay model might learn hyper parameters set correctly set hyper parameters key part deep learn research researchers find base intuition run hyper parameter search hyper parameter search model train repeatedly different hyper parameters find best set hyper parameters hyper parameter search become costly number hyper parameters increase model train time increase hyper parameters change train base pre determine schedule example could slowly decrease learn rate could decrease coefficient auxiliary loss model learn find schedule nearly impossible hyper parameter search usually determine base intuition researchers hard determine hyper parameters set hyper parameters require quite bite experience kind model size train well dataset instance consider fine tune pre train language model classify tweet get pre train language model backbone attach layer two classification head first freeze parameters backbone train head certain number update unfreeze parameters train parameters number step keep backbone freeze generally set nmbr epoch hyper parameter common practice freeze nmbr epoch might small large depend size model well dataset size someone work similar model datasets good intuition hyper parameter new try train model get feel introduce dynamic hyper parametersdynamic hyper parameters hyper parameters researchers adjust model train allow researchers actively control model train instead let model train pre determine set hyper parameters dynamic hyper parameters help train model faster better single train session also let researchers play around hyper parameters single train run gather insights sometimes researchers save model checkpoints restart train change hyper parameter value similar effect dynamic hyper parameters quite cumbersome work need create dynamic hyper parameter register along configurations labml import experiment labml configs import floatdynamichyperparam lr floatdynamichyperparam 2 5e 4 range_ 0 0 1 experiment configs learning_rate lr call dynamic hyper parameter get current value example def train batch optimizer set_lr lr optimizer step call lr return current learn rate set labml ai url app url screenshot mobile web interface change dynamic hyper parameters demo url demo adjust learn rate clip range number train epochs per sample speed train ppo agent url atari breakout standard learn rate decay static hyper parameter value would take lot train update get score 1 example use case freeze pre train layer fine tune language model train backbone freeze rate improvement loss drop change hyper parameter affect layer freeze better faster go common practice keep backbone freeze nmbr epoch model datasets learn rate warm decay learn rate manually increase initial train update could decide long warm base loss curve similarly decay learn rate loss value stabilize allow use higher learn rat initially speed train increase sequence length recurrent model train faster bptt length shorter need higher bptt lengths improve accuracy therefore common practice start shorter bptt length increase later decide beforehand hard change dynamically lot easier adjust regularization parameters start lower weight decay lower dropout probabilities initially especially sure representation capacity model increase regularization parameters later validation loss stop improve higher variance adjust reinforcement learn hyper parameters reinforcement learn tend hyper parameters need change train discount factor entropy bonus coefficients learn rate etc pre determine almost impossible without observe train run train run go many hours days even simple game environments change train base agent performance stats lot easier next update hyper parameter schedule current implementation allow users update hyper parameter value take much user time instance let say base current loss curve user figure want drop learn rate 1e 3 2 5e 4 next 100 000 update current implementation would make several manual change want let users set update hyper parameter schedule user manually intervene necessary rewind often train dynamic hyper parameters feel like experiment sort like small hyper parameter search model train things go wrong want reset enable work simple rewind undo option user could restart checkpoint couple tap screen
init__27,MachineLearning,1620470429.0,[P] Video/Discussion on Building an Aircooled rig to accommodate 3x GPUs,"Video URL: [https://www.youtube.com/watch?v=S0-lM6mZJn0](https://www.youtube.com/watch?v=S0-lM6mZJn0&t=2s)

Hi everyone! 

This project started with me trying to find guides on effectively air-cooling >=2 3090 GPUs for a DL Box. I couldn't find any detailed discussions on picking the correct parts so after a lot of iterations, I was able to build one myself. 

I decided to publish a video discussing the parts and my reasoning behind selecting the same so that anyone could use this as a template to build their own rigs. 

PC Part list: [https://pcpartpicker.com/list/gFGtF8](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa05NYlpndUVnMzhldjV5MldNSUU5NUo2MVRSQXxBQ3Jtc0ttQ3ozU01ENFNpaWFDVW8zVFZGR3p6aktDeHdxb0JpV3otZEdpNTA1SnBNVXZkbUk1QjlSaTlCOTZEMWR4Uzhwd2dHMkFSbmtiLUZxcE1BVllYRGVCWUYtMFVKYTBZQVhOYlFNX3pOZWNBTTZDenU2UQ&q=https%3A%2F%2Fpcpartpicker.com%2Flist%2FgFGtF8)​ (Note it doesn't show the correct GPUs, I have an MSI Gaming X Trio 3090, a MSI Ventus 3x & an A6000)

Other resources: 

Apart from this, I would highly encourage referring the solid writeup by u/emilwallner: [https://www.emilwallner.com/p/ml-rig](https://www.emilwallner.com/p/ml-rig) as well as the legendary Tim Detter's blog: [https://timdettmers.com](https://timdettmers.com)

Happy to answer any Qs. TIA!",video url url everyone project start try find guide effectively air cool 2 nmbr gpus dl box find detail discussions pick correct part lot iterations able build one decide publish video discuss part reason behind select anyone could use template build rig pc part list url note show correct gpus msi game x trio 3090 msi ventus 3x a6000 resources apart would highly encourage refer solid writeup u emilwallner url well legendary tim detter blog url answer qs tia
phenomenonical,MachineLearning,1620462828.0,[D] Tech stack and resources for a ML project,"What would be the best tech stack / what resources would I need for this potential ML product?

* Assume that input data is already stored on Azure (but could possibly be stored on internal servers or other cloud provider)
* End product would be a web app that shows predictions on a map. The web app would ideally have registered users and would collect some light data on user info and activity
* Machine learning model would collect new data, be retrained, and make new predictions annually
* Model maintenance would be done by my company (unless this is not a good idea?)
* The clients are municipal administrations, so budget is small and they would only provide url hosting
* We are in the EU so any user data collected would need to be GDPR compliant

Background: I am a solo-ing data scientist (MSc in data science but no actual ML work experience so I haven't seen precedents) trying to get ML up and running at a large (15,000 person) firm. I came up with a good idea to weave in ML with a traditional service that the firm provides, but now I need to figure out how actually do the thing. I've done a proof of concept with some toy data and it gave promising results.

Could the end-to-end product be done within Azure? Is there a cheaper option since the ML model would only need to be updated annually? My company has difficulty funding innovations that are not tied directly to a client project, so I essentially have no budget until I get a client buy-in.

What other team members would it make sense to bring on? Should I outsource any part of this to an external company? I can handle the model building, but I have no idea what happens after I make an API to broadcast the predictions. There is a company lawyer that oversees the country's GDPR compliance (so not too sure how much free time he has). Also, I currently work closely with a developer-ish person who was involved in creating a web app, but not too sure how much technical knowledge he has. Whenever I press him for details he seems to have no clue (for example, he had never heard of Azure before I mentioned it. He said something about using GitHub for the web app.).",would best tech stack resources would need potential ml product assume input data already store azure could possibly store internal servers cloud provider end product would web app show predictions map web app would ideally register users would collect light data user info activity machine learn model would collect new data retrain make new predictions annually model maintenance would company unless good idea clients municipal administrations budget small would provide url host eu user data collect would need gdpr compliantbackground solo ing data scientist msc data science actual ml work experience see precedents try get ml run large 15 000 person firm come good idea weave ml traditional service firm provide need figure actually thing proof concept toy data give promise result could end end product within azure cheaper option since ml model would need update annually company difficulty fund innovations tie directly client project essentially budget get client buy team members would make sense bring outsource part external company handle model build idea happen make api broadcast predictions company lawyer oversee country gdpr compliance sure much free time also currently work closely developer ish person involve create web app sure much technical knowledge whenever press detail seem clue example never hear azure mention say something use github web app
Daddy_Long_Legs,MachineLearning,1619199043.0,[D] Hyperparameter tuning with a budget constraint on total number of model parameters,"Is this possible to do with any of the hyperparameter frameworks out there? I have a convolutional network with the hypterparameters being kernel size, number of layers, and number of channels, and would like to search through those parameters while placing a limit on the total number of parameters present in the model due to where I'm deploying it. I found this [paper](https://arxiv.org/pdf/1902.00532.pdf) but could not find any code implementing their strategy. Has anyone implemented something like this already?

Facebook's Ax allows something similar, but only with linear combinations of parameters which is tough since the number of parameters for a conv layer is superlinear (((m \* n \* d) + 1) \* k). Seems like this should be a common problem, so curious why there isn't more out there on this as well.",possible hyperparameter frameworks convolutional network hypterparameters kernel size number layer number channel would like search parameters place limit total number parameters present model due deploy find paper url could find code implement strategy anyone implement something like already facebook ax allow something similar linear combinations parameters tough since number parameters conv layer superlinear n 1 k seem like common problem curious well
MacaronFraise,MachineLearning,1619255019.0,[D] Improve KNN calculation time,"Hey guys,

I am currently working on a machine learning project using KNN and I have a performance issue. At the moment, my KNN is loaded on a Flask server hosted on Heroku. Whenever I want to make a prediction, I send to the flask server the parameters of the prediction.

The problem I encounter is that it takes too much time to make a single prediction (about 1 second). For my project, I actually need to make 5000 predictions in one go.

So I'm posting here in order to ask you for some solution about it. Is there a way to optimize KNN so that it can handle 5000 predictions quickly ? Or, should I host my KNN on another hosting service with more computing power, more adapted to Machine Learning like Azure ?

All that is quite new for me. Thanks a lot for your help

UPDATE : After a few tests, I realized that I am using all the 500MB RAM available on Heroku. I don't know if it is a normal figure or if I my code is not optimized and using too much memory",hey guy currently work machine learn project use knn performance issue moment knn load flask server host heroku whenever want make prediction send flask server parameters prediction problem encounter take much time make single prediction nmbr second project actually need make nmbr predictions one go post order ask solution way optimize knn handle nmbr predictions quickly host knn another host service compute power adapt machine learn like azure quite new thank lot helpupdate test realize use 500mb ram available heroku know normal figure code optimize use much memory
dptzippy,MachineLearning,1619208491.0,[D] What are some projects/tutorials that I could use to get started with ML?,"Hello, everybody! I have been trying to learn more about ML, and I have been improving, but I can't find any projects/tutorials that explain exactly what is going on. I am looking for any free, preferably in written form, tutorials that can help me, and others, start out with ML.

Side-question: I have been trying to find a way to create a program/model that can be ""fed"" text files, which add to its vocabulary, and can be used to generate text. I can't find something to do this, and I am sure that it's out there, so I wanted to see if anybody knew of such a project/framework/tutorial. 


I am new to Python/ML, but I have been programming for several years, so I might need some dumbing-down of specific stuff, but I can probably understand broader concepts.

Thanks!",hello everybody try learn ml improve find project tutorials explain exactly go look free preferably write form tutorials help others start ml side question try find way create program model feed text file add vocabulary use generate text find something sure want see anybody know project framework tutorial new python ml program several years might need dumbing specific stuff probably understand broader concepts thank
pircherth,MachineLearning,1619201704.0,[R] The structure dilemma in biological and artificial neural networks,"The role of internal structure in information processing networks in artificial and biological based models and how this structure changes during learning. We showed that the structure leads to the edge weights, but the structures are not problem exclusive.

https://www.nature.com/articles/s41598-021-84813-6",role internal structure information process network artificial biological base model structure change learn show structure lead edge weight structure problem exclusive url
RchGrav,MachineLearning,1618790215.0,[D] I made a script that does all the work to deploy GPT-NEO on Windows 10. (Please Test),"Envisioned Purpose: Windows 10 GPT-NEO Local Demo for the Average Joe.

[https://gist.github.com/RchGrav/8bb3769a825540a4587e73ce6ea26053/](https://gist.github.com/RchGrav/8bb3769a825540a4587e73ce6ea26053/)

So what this does is deploy miniconda3 and configures the base environment which then downloads and runs one of the GPT-NEO models on either a CPU or CUDA capable device (GPU w/ CUDA installed).  I chose miniconda3 because the concept behind this was to allow someone to run GPT-NEO locally for experimentation or a foundation as a base environment.   If this doesn't work for you please report back with some details.  This is a private GIST at the moment but I may put it on my Github page if people say this would be a benefit to anyone who only has a Windows system and is struggling to get a working system going.   I'm only starting off on my journey of actually working with machine learning hands on, but I have been doing deploy scripts for years in the IT field.

Some technical notes:  The script installs and uses the choco command (modeled after apt on linux) from [chocolately.org](https://chocolately.org) to bootstrap miniconda3, it uses the echo to generate a few other supporting scripts for requesting elevation and to reload the path and environment variables so that its a one step process..  run the script.  Finally the script also creates a example python file that executes to text generating text, but also a condition for the main script so if it is run again it won't try redeploying everything, it just drops to the GPT-NEO demo.

There is some notes at the bottom of the Gist which will be helpful for anyone looking to run on a GPU, or use the larger transformer model.

Minimum Recommended Specs: Tons of Cores, 16GB of or more..   Fast storage (NVMe / SSD) suggested, 12GB of HDD space for the smaller model, 2x that for the bigger model, and 3x that for both.)

To run on GPU: I don't know..  I have a GeForce RTX 3090, so that works for sure.. you will need to install CUDA from NVidia, this much I do know.

Side Note: Personally, I was hoping to get GPT-NEO up and working on a Jetson Nano board, but even with swap I think its going to be a really tight fit.. sounds like its gonna cost me more than $100 if I wanna pretend to be Michael Knight and converse with my car.  :-P

Please share feedback..  This may help others who are looking for a quick and easy bootstrap and way to play with GPT Text Generation.  It shouldn't mess with Anaconda at all since miniconda3 runs in its own environment, so as far as I can surmise I can't see it messing up anything on your system..",envision purpose windows nmbr gpt neo local demo average joe url deploy miniconda3 configure base environment download run one gpt neo model either cpu cuda capable device gpu w cuda instal choose miniconda3 concept behind allow someone run gpt neo locally experimentation foundation base environment work please report back detail private gist moment may put github page people say would benefit anyone windows system struggle get work system go start journey actually work machine learn hand deploy script years field technical note script install use choco command model apt linux chocolately org url bootstrap miniconda3 use echo generate support script request elevation reload path environment variables one step process run script finally script also create example python file execute text generate text also condition main script run win try redeploy everything drop gpt neo demo note bottom gist helpful anyone look run gpu use larger transformer model minimum recommend specs tons core 16gb fast storage nvme ssd suggest 12gb hdd space smaller model 2x bigger model 3x run gpu know geforce rtx 3090 work sure need install cuda nvidia much know side note personally hop get gpt neo work jetson nano board even swap think go really tight fit sound like gon na cost 100 wan na pretend michael knight converse car pplease share feedback may help others look quick easy bootstrap way play gpt text generation mess anaconda since miniconda3 run environment far surmise see mess anything system
TheCockatoo,MachineLearning,1618909454.0,[D] How to do stratified train/test split for semantic segmentation?,"In other words, how to ensure similar class distributions in the train and test sets when some samples have (arbitrarily) more pixels of class A than B, and others have more pixels of class B than A?",word ensure similar class distributions train test set sample arbitrarily pixels class b others pixels class b
techsucker,MachineLearning,1618502126.0,"[R] Brown University Researchers Introduce DeepONet, A Model Based On Deep Neural Network, To Approximate Both Linear and Nonlinear Operators (Paper and Github link included)","Researchers from Brown University have built DeepONet, a novel neural network-based model that can efficiently learn both linear and nonlinear operators. This novel model was inspired by earlier studies led by researchers at Fudan University. 

A **continuous function** does not have any abrupt changes in value. More precisely, small changes in continuous function’s output can be assured by restricting to sufficiently small changes in its input. Many studies show that artificial neural networks (ANN) are highly efficient approximators of continuous functions. However, not many studies have yet focused on their ability to approximate nonlinear operators.

Inspired by the papers published by Chen and Chen at Fudan University, which discusses the functional approximation using a single layer of neurons, the researchers decided to explore the possibility of building a neural network that could approximate both linear and nonlinear operators

Summary: [https://www.marktechpost.com/2021/04/15/brown-university-researchers-introduce-deeponet-a-model-based-on-deep-neural-network-to-approximate-both-linear-and-nonlinear-operators/](https://www.marktechpost.com/2021/04/15/brown-university-researchers-introduce-deeponet-a-model-based-on-deep-neural-network-to-approximate-both-linear-and-nonlinear-operators/) 

Paper: [https://www.nature.com/articles/s42256-021-00302-5](https://www.nature.com/articles/s42256-021-00302-5) 

Github: [https://github.com/lululxvi/deeponet](https://github.com/lululxvi/deeponet)",researchers brown university build deeponet novel neural network base model efficiently learn linear nonlinear operators novel model inspire earlier study lead researchers fudan university continuous function abrupt change value precisely small change continuous function output assure restrict sufficiently small change input many study show artificial neural network ann highly efficient approximators continuous function however many study yet focus ability approximate nonlinear operators inspire paper publish chen chen fudan university discuss functional approximation use single layer neurons researchers decide explore possibility build neural network could approximate linear nonlinear operatorssummary url paper url github url
maroxtn,MachineLearning,1617669407.0,[D] Is there better options than beam search in translation ?,"Are there better sampling strategy than beam search used by big companies in translations (such as google translation ...) ? 

When I train a transformer for translation, I get a lot of repeated ngrams when using beam search, and longer sentences.",better sample strategy beam search use big company translations google translation train transformer translation get lot repeat ngrams use beam search longer sentence
crack_pop_rocks,MachineLearning,1620581059.0,Any suggestions for deep learning textbooks that have a neuroscience perspective? [Discussion],"I was wondering if anybody experts here have any suggestions on textbooks that address deep learning from a neuroscience perspective? There are several good publications on the subject, but I am having difficulty finding something that is more comprehensive.

I've been studying ML for about 6 months now with pretty steady progress, and just started getting into neural networks. My background is in neurobiology/cognitive neurosciences, so the fundamental principles behind DLNN model architecture and behavior are already familiar. Deep learning is much more statistics oriented than neuroscience given the nature of modeling and the ease of access to quantitative data, but I'm actually surprised how little biological systems are discussed/referenced, especially given how much we know about the various mechanisms for modulating learning.

&#x200B;

[Synaptic Integration in Dendritic Trees](https://sci-hub.se/10.1002/neu.20144)

[Functional Significance of Passive and Active Dendritic Properties in the Synaptic Integration by an Identified Nonspiking Interneuron of Crayfish](https://journals.physiology.org/doi/full/10.1152/jn.00680.2006)

&#x200B;

Also--if there are any PhDs reading this, look into jellyfish. They have primordial neural nets composed of just ganglia, which anatomically seems the most similar to deep learning model architectures.

&#x200B;

Abstract

Jellyfish nerve nets provide insight into the origins of nervous systems, as both their taxonomic position and their evolutionary age imply that jellyfish resemble some of the earliest neuron-bearing, actively-swimming animals. Here, we develop the first neuronal network model for the nerve nets of jellyfish. Specifically, we focus on the moon jelly Aurelia aurita and the control of its energy-efficient swimming motion. The proposed single neuron model disentangles the contributions of different currents to a spike. The network model identifies factors ensuring non-pathological activity and suggests an optimization for the transmission of signals. After modeling the jellyfish’s muscle system and its bell in a hydrodynamic environment, we explore the swimming elicited by neural activity. We find that different delays between nerve net activations lead to well-controlled, differently directed movements. Our model bridges the scales from single neurons to behavior, allowing for a comprehensive understanding of jellyfish neural control of locomotion.

&#x200B;

[https://elifesciences.org/articles/50084](https://elifesciences.org/articles/50084)",wonder anybody experts suggestions textbooks address deep learn neuroscience perspective several good publications subject difficulty find something comprehensive study ml nmbr months pretty steady progress start get neural network background neurobiology cognitive neurosciences fundamental principles behind dlnn model architecture behavior already familiar deep learn much statistics orient neuroscience give nature model ease access quantitative data actually surprise little biological systems discuss reference especially give much know various mechanisms modulate learn x200b synaptic integration dendritic tree url significance passive active dendritic properties synaptic integration identify nonspiking interneuron crayfish url phds read look jellyfish primordial neural net compose ganglia anatomically seem similar deep learn model architectures x200b abstractjellyfish nerve net provide insight origins nervous systems taxonomic position evolutionary age imply jellyfish resemble earliest neuron bear actively swim animals develop first neuronal network model nerve net jellyfish specifically focus moon jelly aurelia aurita control energy efficient swim motion propose single neuron model disentangle contributions different currents spike network model identify factor ensure non pathological activity suggest optimization transmission signal model jellyfishs muscle system bell hydrodynamic environment explore swim elicit neural activity find different delay nerve net activations lead well control differently direct movements model bridge scale single neurons behavior allow comprehensive understand jellyfish neural control locomotion x200b url
bachier,MachineLearning,1617909974.0,"[D] CVPR 2021 paper ""The Affective Growth of Computer Vision""","[https://authentic.sice.indiana.edu/publications/Su\_Crandall-AffectiveGrowthCV-CVPR21.pdf](https://authentic.sice.indiana.edu/publications/Su_Crandall-AffectiveGrowthCV-CVPR21.pdf)

Authors: Norman Makoto Su, David J. Crandall

Abstract: The success of deep learning has led to intense growth and interest in computer vision, along with concerns about its potential impact on society. Yet we know little about how these changes have affected the people that research and practice computer vision: we as a community spend so much effort trying to replicate the abilities of humans, but so little time considering the impact of this work on ourselves. In this paper, we report on a study in which we asked computer vision researchers and practitioners to write stories about emotionally-salient events that happened to them. Our analysis of over 50 responses found tremendous affective (emotional) strain in the computer vision community. While many describe excitement and success, we found strikingly frequent feelings of isolation, cynicism, apathy, and exasperation over the state of the field. This is especially true among people who do not share the unbridled enthusiasm for normative standards for computer vision research and who do not see themselves as part of the “incrowd.” Our findings suggest that these feelings are closely tied to the kinds of research and professional practices now expected in computer vision. We argue that as a community with significant stature, we need to work towards an inclusive culture that makes transparent and addresses the real emotional toil of its members.",url norman makoto su david j crandallabstract success deep learn lead intense growth interest computer vision along concern potential impact society yet know little change affect people research practice computer vision community spend much effort try replicate abilities humans little time consider impact work paper report study ask computer vision researchers practitioners write stories emotionally salient events happen analysis nmbr responses find tremendous affective emotional strain computer vision community many describe excitement success find strikingly frequent feel isolation cynicism apathy exasperation state field especially true among people share unbridle enthusiasm normative standards computer vision research see part incrowd find suggest feel closely tie kinds research professional practice expect computer vision argue community significant stature need work towards inclusive culture make transparent address real emotional toil members
ilia10000,MachineLearning,1617749313.0,"[D] Is ""data"" plural in modern machine learning literature?","As the title suggests I'm trying to figure out whether modern machine learning literature considers ""data"" to be a plural noun (e.g. The data are sparse) or a singular/mass noun (e.g. The data is sparse). My PhD supervisor argues that the plural form is correct, but I feel like it just doesn't sound quite right (at least in CS/ML contexts) when it hits my ear.

Google searches suggest that while the plural form is historically considered to be the correct one, in general, usage of ""data is"" is several times higher than usage of ""data are"". Apparently, in scientific literature, their usage is about equal.

Does anyone have any statistics or comments on this topic specifically for contemporary machine learning literature?",title suggest try figure whether modern machine learn literature consider data plural noun e g data sparse singular mass noun e g data sparse phd supervisor argue plural form correct feel like sound quite right least cs ml contexts hit ear google search suggest plural form historically consider correct one general usage data several time higher usage data apparently scientific literature usage equal anyone statistics comment topic specifically contemporary machine learn literature
givdwiel,MachineLearning,1616408714.0,[P] pyRDF2Vec 0.2.0 is out!,"# pyRDF2Vec 0.2.0 is out!!

This  release is packed with many new features and optimizations under the  hood. An entire overview of what's new can be found in our CHANGELOG ([https://github.com/IBCNServices/pyRDF2Vec/releases/tag/0.2.0](https://github.com/IBCNServices/pyRDF2Vec/releases/tag/0.2.0)). An overview of some major updates:

&#x200B;

**0) What is RDF2Vec?**

RDF2Vec is a technique to generate embeddings for nodes or entities in a Knowledge Graph (often represented in RDF format, hence the name).

The technique is unsupervised and therefore task-agnostic. You can generate embeddings for nodes and use those for multiple downstream tasks.

A concrete example is a graph containing information on chemical compounds, an example snippet of this data:

`<Compound rdf:about=""#d193"">`

`<cytogen_sce rdf:datatype=""&xsd;boolean"">true</cytogen_sce>`

`<hasAtom rdf:resource=""#d193\_13""/>`

`<mouse_lymph rdf:datatype=""&xsd;boolean"">false</mouse_lymph>`

`<amesTestPositive rdf:datatype=""\&xsd;boolean"">false</amesTestPositive>`

`<hasAtom rdf:resource=""#d193\_10""/>`

`<hasBond rdf:resource=""#bond3301""/>`

`<hasBond rdf:resource=""#bond3299""/>`

`<hasBond rdf:resource=""#bond3308""/>`

`<hasAtom rdf:resource=""#d193\_3""/>`

`<hasBond rdf:resource=""#bond3305""/>`

`<hasAtom rdf:resource=""#d193\_12""/>`

`<hasAtom rdf:resource=""#d193\_8""/>`

`<hasBond rdf:resource=""#bond3311""/>`

`<cytogen_ca rdf:datatype=""&xsd;boolean"">false</cytogen_ca>`

`</Compound>`

Which can then be turned into numerical representations through RDF2Vec:

&#x200B;

https://preview.redd.it/xdqhk1zev4p61.png?width=882&format=png&auto=webp&s=61e0975d79a3dc7a2865968d51cb904ddeb24423

&#x200B;

&#x200B;

**1) Rudimentary Literal Support**In  addition to node embeddings, pyRDF2Vec can extract numerical  information from the neighbourhood around a node. Users can specify a  path of predicates that can be followed to obtain numerical information.

https://preview.redd.it/w1ueuoev2ko61.png?width=1691&format=png&auto=webp&s=cd4fc03c5123ed4e44569e339ee3ba789ad15ae1

**2) Online Learning**Originally,  the entire model had to be re-trained when the underlying knowledge  graph changed. This is no longer the case, pyRDF2Vec 0.2.0 now features  online learning to update your embedding model dynamically.

https://preview.redd.it/5wasofiy2ko61.png?width=692&format=png&auto=webp&s=5d97a0e84a1df78b37f8a3e9fb223a8eb92f4c7d

**3) Reverse Walking**The  original walking algorithm extracted children starting from a certain  root recursively. In pyRDF2Vec 0.2.0, parents can be extracted as well.  This enables better interaction with the underlying windowing used by  Word2Vec.

https://preview.redd.it/rijl8pdz2ko61.png?width=651&format=png&auto=webp&s=427c6d5b9bfed1b1552f822ec239935f3dee789c

**4) Blazing Fast Walking**Many optimizations have  been made under the hood to speed up the walk extraction. Optimizations  include: multiprocessing, caching, asynchronous operations, etc...  Speedups of an order of magnitude can easily be achieved!

&#x200B;

https://preview.redd.it/3thhykc03ko61.png?width=1603&format=png&auto=webp&s=8ecd858414d667f5c2fae3e166fb412f61b4871e

**EXTRA)**

A blog post on this will be published shortly.   Moreover, we have noticed that pyRDF2Vec is being increasingly used within different studies, which is great to see!!!

We have compiled an overview: [https://pyrdf2vec.readthedocs.io/en/latest/posts-papers.html](https://pyrdf2vec.readthedocs.io/en/latest/posts-papers.html)",pyrdf2vec 0 2 0 release pack many new feature optimizations hood entire overview new find changelog url overview major update x200b 0 rdf2vec rdf2vec technique generate embeddings nod entities knowledge graph often represent rdf format hence name technique unsupervised therefore task agnostic generate embeddings nod use multiple downstream task concrete example graph contain information chemical compound example snippet data compound rdf d193 cytogen_sce rdf datatype xsd boolean true cytogen_sce hasatom rdf resource d193 _13 mouse_lymph rdf datatype xsd boolean false mouse_lymph amestestpositive rdf datatype xsd boolean false amestestpositive hasatom rdf resource d193 _10 hasbond rdf resource bond3301 hasbond rdf resource bond3299 hasbond rdf resource bond3308 hasatom rdf resource d193 _3 hasbond rdf resource bond3305 hasatom rdf resource d193 _12 hasatom rdf resource d193 _8 hasbond rdf resource bond3311 cytogen_ca rdf datatype xsd boolean false cytogen_ca compound turn numerical representations rdf2vec x200b url rudimentary literal support addition node embeddings pyrdf2vec extract numerical information neighbourhood around node users specify path predicate follow obtain numerical information url online learn originally entire model train underlie knowledge graph change longer case pyrdf2vec 0 2 0 feature online learn update embed model dynamically url reverse walk original walk algorithm extract children start certain root recursively pyrdf2vec 0 2 0 parent extract well enable better interaction underlie windowing use word2vec url blaze fast walk many optimizations make hood speed walk extraction optimizations include multiprocessing cache asynchronous operations etc speedups order magnitude easily achieve x200b url blog post publish shortly moreover notice pyrdf2vec increasingly use within different study great see compile overview url
hyunwoongko,MachineLearning,1616353077.0,[P]Summarizers: Easy to use controllable summarization package,"&#x200B;

https://preview.redd.it/4tlzfppwhfo61.png?width=2130&format=png&auto=webp&s=dabb97a246c2e904f84baf200ecf01022916d79f

Hello, I'm studying natural language processing. I made a package this weekend, and I'm writing a post for sharing because I think some of you might be able to use it well.

&#x200B;

The package name is summarizers, and as the name suggests, it is an easy tool to summarize text. However, summarizers support a variety of controllable summarization options, like above image, as well as a variety of domains such as general article summaries, paper summaries, and patent summaries.

&#x200B;

For more information, please visit [https://github.com/hyunwoongko/summarizers](https://github.com/hyunwoongko/summarizers).",x200b url study natural language process make package weekend write post share think might able use well x200b package name summarizers name suggest easy tool summarize text however summarizers support variety controllable summarization options like image well variety domains general article summaries paper summaries patent summaries x200b information please visit url
davidbun,MachineLearning,1617030776.0,"[N] do well in a CVPR/Kaggle Plant Pathology Challenge, win $500","**tl;dr top 20 teams that use package activeloopai/hub in CVPR's Plant Pathology Kaggle challenge win $500**

Hey r/MachineLearning,

My team and I have created Hub ([github.com/activeloopai/Hub](http://github.com/activeloopai/Hub)). The package makes unstructured dataset of any size accessible from any machine at any scale, and helps seamlessly stream data to PyTorch and TF, as if it were local. As part of our ongoing effort to support the machine learning community, we will support teams who do well in this year's CVPR FGVC8 challenges, starting with the [Plant Pathology Challenge](https://sites.google.com/view/fgvc8/competitions/plantpathologychallenge2021?authuser=0). Teams that place in the top 20 for these challenges (on the Kaggle leaderboard) and use the package in their solution will receive $500.For additional information, see the [wiki](https://github.com/activeloopai/Hub/wiki/Hub's-Plant-Pathology-2021-Challenge) or let me know here! [Here's a short notebook](https://github.com/mynameisvinn/Hub-Tutorials/blob/master/Pushing%20Plant%20Pathology%20Dataset%20to%20Hub.ipynb) on how to use Hub with the CVPR  dataset.  


**Features of Hub you might find relevant during the challenge:**

* Create large datasets with huge (10\^5 x 10\^5) size arrays and store locally, on hub storage, or any cloud.
* Collaborate with your team on the same dataset.
* Version control the dataset from the API itself.
* Filter datasets to only get the samples you need.
* Create data pipelines and transform the data.
* Easily access and visualize any slice of the dataset without downloading the entire dataset.
* Directly plug Hub datasets into tensorflow and pytorch and start training.
* Transfer datasets across different locations easily.

Good luck,

davidbun",tl dr top nmbr team use package activeloopai hub cvpr plant pathology kaggle challenge win 500 hey r machinelearning team create hub github com activeloopai hub url package make unstructured dataset size accessible machine scale help seamlessly stream data pytorch tf local part ongoing effort support machine learn community support team well year cvpr fgvc8 challenge start plant pathology challenge url team place top nmbr challenge kaggle leaderboard use package solution receive 500 additional information see wiki url let know short notebook url use hub cvpr dataset feature hub might find relevant challenge create large datasets huge 10 5 x 10 5 size array store locally hub storage cloud collaborate team dataset version control dataset api filter datasets get sample need create data pipelines transform data easily access visualize slice dataset without download entire dataset directly plug hub datasets tensorflow pytorch start train transfer datasets across different locations easily good luck davidbun
user01052018,MachineLearning,1616920279.0,[D] Pointer sentinel mixture model - Why is $P_{ptr}$ is $V$ dimensional?,"I have been checking this paper [https://arxiv.org/abs/1609.07843](https://arxiv.org/abs/1609.07843) . In the research paper, they said the model is capable of predicting not only rare or less frequent words but also unseen words. Now I noticed $P\_{ptr}$ is of size V. That means if we have some way encoded the information of an unseen word in those V sized vector. I wonder how can it be possible for an unseen word. When you are training you are considering a |V| size vocabulary which didn’t appear in softmax vocabulary?

&#x200B;

To be clear, suppose my own RNN vocabulary contains 3 words.

\[“Reddit”,”Users”,”<UNK>”\].  Now suppose I have 4 unseen  words in the text like

\[“Quora”,”Stackoverflow”,”Lichess”,”Wikipedia”\]. How would a 3 size vocabulary handle it?",check paper url research paper say model capable predict rare less frequent word also unseen word notice p _ ptr size v mean way encode information unseen word v size vector wonder possible unseen word train consider v size vocabulary appear softmax vocabulary x200b clear suppose rnn vocabulary contain nmbr word reddit users unk suppose nmbr unseen word text like quora stackoverflow lichess wikipedia would nmbr size vocabulary handle
ykilcher,MachineLearning,1616435112.0,[D] Paper Explained - Perceiver: General Perception with Iterative Attention (Full Video Analysis),"[https://youtu.be/P\_xeshTnPZg](https://youtu.be/P_xeshTnPZg)

Inspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver's weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:20 - Built-In assumptions of Computer Vision Models

5:10 -  The Quadratic Bottleneck of Transformers

8:00 - Cross-Attention in Transformers

10:45 - The Perceiver Model Architecture & Learned Queries

20:05 - Positional Encodings via Fourier Features

23:25 - Experimental Results & Attention Maps

29:05 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2103.03206](https://arxiv.org/abs/2103.03206)",url fact biological creatures attend multiple modalities time deepmind release new perceiver model base transformer architecture perceiver make assumptions modality input data also solve long stand quadratic bottleneck problem achieve latent low dimensional transformer input data feed multiple time via cross attention perceiver weight also share across layer make similar rnn perceivers achieve competitive performance imagenet state art modalities make architectural adjustments input data x200b outline 0 00 intro overview2 20 build assumptions computer vision models5 10 quadratic bottleneck transformers8 00 cross attention transformers10 45 perceiver model architecture learn queries20 05 positional encode via fourier features23 25 experimental result attention maps29 05 comment conclusion x200b paper url
AerysSk,MachineLearning,1620222682.0,[D] A small dataset with high resolution?,"Hello everyone. I am testing my augmentation strategy. Currently I am using CIFAR, but it is too small (32x32) to visualize the results. Is there any dataset that is small in size (like < 100 images, or smaller), but has high resolution (above 100x100 maybe)?

I am using PyTorch. If there is the code to load it too, I am greatly appreciated.",hello everyone test augmentation strategy currently use cifar small 32x32 visualize result dataset small size like nmbr image smaller high resolution 100x100 maybe use pytorch code load greatly appreciate
MrAcurite,MachineLearning,1619615551.0,[D] Recommendations for increasing training stability with extremely small batches?,"Howdy folks,

I'm currently in a situation where my batch size basically cannot exceed 2. If I had a cluster, I'd be able to do much better than that, but I don't, so I can't. This, I think, is leading to stability issues, because if the phenomena that the model is taking as input are homogeneous in a given step, the model will simply try and optimize towards giving the appropriate output generally, rather than conditionally.

What should I be trying to do? Should I be bumping up the betas in the Adam optimizer? Holding onto gradients over multiple batches and then applying them all at once? Any ideas?",howdy folks currently situation batch size basically exceed 2 cluster able much better think lead stability issue phenomena model take input homogeneous give step model simply try optimize towards give appropriate output generally rather conditionally try bump betas adam optimizer hold onto gradients multiple batch apply ideas
yashchandak,MachineLearning,1619781819.0,[R] Universal Off-Policy Evaluation,"Hi everyone,

I am happy to share with you our first steps towards a universal off-policy estimator (UnO) --- one that provides off-policy estimates and high-confidence bounds for any parameter of the return distribution. 

[https://arxiv.org/abs/2104.12820](https://arxiv.org/abs/2104.12820)

(joint work with Scott Niekum, Bruno Silva, Erik Learned-Miller, Emma Brunskill, and Phil Thomas)

We use UnO for estimating and simultaneously bounding the mean, variance, quantiles/median, inter-quantile range, CVaR, and the entire cumulative distribution of returns in the off-policy (or counterfactual) setting. We also discuss Uno's applicability in various settings, including fully observable, partially observable (i.e., with unobserved confounders), Markovian, non-Markovian, stationary, smoothly non-stationary, and discrete distribution shifts.

UnO can be useful in many critical applications that require thinking about metrics beyond the expected return. For example, medical settings (where tail-sensitive risk measures like value-at-risk/CVaR are essential to avoid catastrophic outcomes), online recommendation (where metrics like median/inter-quantiles are essential to tackle high noise in data collection), or human-machine interaction (where metrics like variance/entropy are essential to quantify uncertainty in the system's outcomes).

We believe that we have barely scratched the surface of this direction, and we welcome any and all feedback!",hi everyone happy share first step towards universal policy estimator uno one provide policy estimate high confidence bound parameter return distribution url work scott niekum bruno silva erik learn miller emma brunskill phil thomas use uno estimate simultaneously bound mean variance quantiles median inter quantile range cvar entire cumulative distribution return policy counterfactual set also discuss uno applicability various settings include fully observable partially observable e unobserved confounders markovian non markovian stationary smoothly non stationary discrete distribution shift uno useful many critical applications require think metrics beyond expect return example medical settings tail sensitive risk measure like value risk cvar essential avoid catastrophic outcomes online recommendation metrics like median inter quantiles essential tackle high noise data collection human machine interaction metrics like variance entropy essential quantify uncertainty system outcomes believe barely scratch surface direction welcome feedback
SeasonedLeo,MachineLearning,1617925125.0,Multi classification issue [R],"Hello All

I am working on a Multi-classification problem with three classes. I am using light   Gbm model. My F1score on testing data set is pretty good for all three classes  over 0.95. However, when i am trying classify using this model on a new set . model misclassification  Is huge . I am not able understand how can this possible happen . Any pointers or ideas ? 

All dat have same features .

Thanks for help",hello alli work multi classification problem three class use light gbm model f1score test data set pretty good three class 0 95 however try classify use model new set model misclassification huge able understand possible happen pointers ideas dat feature thank help
dokluch,MachineLearning,1620367447.0,[D] Deploying ML model for inference on user devices,"Hi! 
I would like to create a simple app with a couple of pre-trained ML models that would run on a variety of user devices - Windows and macOS machines with Nvidia or AMD or even no GPU.

Are there any guidelines on the such infrastructure? I am currently looking into tflite, but I'm not sure it is the right way.

Thank you!",hi would like create simple app couple pre train ml model would run variety user devices windows macos machine nvidia amd even gpu guidelines infrastructure currently look tflite sure right way thank
cdossman,MachineLearning,1618397655.0,[D] Understanding Hinton’s Capsule Networks Series,"Why Capsules neural network architecture is so important, the intuition behind it, and a dive into the technical details. 

Part I: [Intuition](https://pechyonkin.me/capsules-1/)   
Part II: [How Capsules Work](https://pechyonkin.me/capsules-2/)  
Part III: [Dynamic Routing Between Capsules](https://pechyonkin.me/capsules-3/)  
Part IV: [CapsNet Architecture](https://pechyonkin.me/capsules-4/)",capsule neural network architecture important intuition behind dive technical detail part intuition url part ii capsule work url part iii dynamic rout capsule url part iv capsnet architecture url
SrData,MachineLearning,1619252993.0,[D] Recovering images from bottlenecks or training dataset.," I'm wondering if it is possible to generate the images used to train a CNN or if it is possible to generate the images in the input from the bottleneck. Do you know some papers,  or post that I could check?

I've been investigating this: [https://arxiv.org/pdf/1710.09926.pdf](https://arxiv.org/pdf/1710.09926.pdf) (  Image Compression: Sparse Coding vs. Bottleneck Autoencoders ) but although similar is not the same.

My point to investigate this is to understand what is the level of privacy of the images if the bottleneck produced by those images are stored or going beyond, what is the level of privacy of the images used to train a  model.",wonder possible generate image use train cnn possible generate image input bottleneck know paper post could check investigate url image compression sparse cod vs bottleneck autoencoders although similar point investigate understand level privacy image bottleneck produce image store go beyond level privacy image use train model
runcep,MachineLearning,1617622899.0,[D] Are there attempts at a large German-language LM?,"I am aware of a few smaller attempts (huggingface has a German GPT-2 model, there is also Zamia‘s model), but I wonder if there are, at a university or a research group, projects that want to build a proper German-language LM in the size of the GPT-2 XL model and up? 

I am especially interested in learning what their training set is made up of. 

I saw that The Pile is interested in building open-source models for different languages but could find anything about a German one. 

Any hint is appreciated!",aware smaller attempt huggingface german gpt 2 model also zamia model wonder university research group project want build proper german language lm size gpt 2 xl model especially interest learn train set make saw pile interest build open source model different languages could find anything german one hint appreciate
AhmedAl93,MachineLearning,1617316167.0,[D][R] Solutions for handwritten text generation,"Hello,
The aim of my post is to discuss the possible solutions for synthetic handwritten text generation under specific constraints.

Context: Currently working as a data scientist in a french company, I encountered some issues when dealing with handwritten text recognition, mainly because the amount of available data is simply not enough (~4k images of handwritten text)

The first idea that came to my mind is to generate a large ""artificial"" handwritten text dataset by assembling single handwritten characters (check out Chars74k dataset), so I made a script that takes a string as input, assembles character images and provides an image containing handwritten text.

This strategy gave really promising results: I trained a text recognition model (called ""deep text recognition benchmark"") on the artificial dataset (120k images) then finetuned it on my real dataset and got 72% accuracy.

However, even if this result is good, it can be improved mainly by generating handwritten text that is as similar to the real one as possible.

I was thinking about using GAN models, but the problem is that they need what I'm lacking in the first place: big amount of data.

Do you have any ideas about solving this ""dilemma"" ?
Also, I thought this dilemma can be seen as PhD thesis proposal, and I'm planning to discuss with my managers about it, so if you can share your views on this, it would be great !",hello aim post discuss possible solutions synthetic handwritten text generation specific constraints context currently work data scientist french company encounter issue deal handwritten text recognition mainly amount available data simply enough 4k image handwritten text first idea come mind generate large artificial handwritten text dataset assemble single handwritten character check chars74k dataset make script take string input assemble character image provide image contain handwritten text strategy give really promise result train text recognition model call deep text recognition benchmark artificial dataset 120k image finetuned real dataset get 72 accuracy however even result good improve mainly generate handwritten text similar real one possible think use gin model problem need lack first place big amount data ideas solve dilemma also think dilemma see phd thesis proposal plan discuss managers share view would great
VeterinarianTight102,MachineLearning,1618891530.0,[P] Awesome Semantic Search,"[EDIT] Thanks for the current support!

A repository for Papers , Tools, Projects , Libraries  and Datasets , conf related to MultiModal Semantic Search tasks.
Currently it  has mainly Text based stuff.

https://github.com/Agrover112/awesome-semantic-search

I believe there is a Plethora of content out there related to the topic , but less consolidation at one place.",edit thank current support repository paper tool project libraries datasets conf relate multimodal semantic search task currently mainly text base stuff url believe plethora content relate topic less consolidation one place
techsucker,MachineLearning,1618244638.0,[R] Researchers At The University of Tokyo Present A Frequency-Based Inpainting Method To Generate Missing Image Portions,"Researchers from the University of Tokyo introduce a frequency-based inpainting method that can use both frequency and spatial information to generate missing image portions. 

Image inpainting is a computer vision (CV) technique that fills in the missing pixels in an image. It allows the removal of unwanted objects from a photo or recreates missing regions of occluded images. Inpainting is popularly used to predict missing image data. However, it is difficult to synthesize the missing pixels realistically and coherently.

Summary: [https://www.marktechpost.com/2021/04/12/researchers-at-the-university-of-tokyo-present-a-frequency-based-inpainting-method-to-generate-missing-image-portions/](https://www.marktechpost.com/2021/04/12/researchers-at-the-university-of-tokyo-present-a-frequency-based-inpainting-method-to-generate-missing-image-portions/) 

Paper: https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-30/issue-02/023016/Image-inpainting-using-frequency-domain-priors/10.1117/1.JEI.30.2.023016.full?SSO=1",researchers university tokyo introduce frequency base inpainting method use frequency spatial information generate miss image portion image inpainting computer vision cv technique fill miss pixels image allow removal unwanted object photo recreate miss regions occlude image inpainting popularly use predict miss image data however difficult synthesize miss pixels realistically coherently summary url paper url
andyljones,MachineLearning,1617866914.0,[R] Scaling Scaling Laws with Board Games,"**Studying a sequence of small problems can let you extrapolate the behaviour of orders of magnitude larger problems!**

[Paper](https://arxiv.org/abs/2104.03113), [tweet thread](https://twitter.com/andy_l_jones/status/1380049754458034176).


Here we work on a board game, which is about the friendliest domain possible for this scaling behaviour. But there's potential for it to show up in many other places. If you're a resource-constrained researcher, I think it's a really promising avenue to look into.

Keen to hear your thoughts!",study sequence small problems let extrapolate behaviour order magnitude larger problems paper url tweet thread url work board game friendliest domain possible scale behaviour potential show many place resource constrain researcher think really promise avenue look keen hear thoughts
elbogotazo,MachineLearning,1617384423.0,Discovering column mappings [R],"I have a challenge to work on at work and am trying to figure out the approach. We have an internal system that stores transactional data in a tabular form.

We receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. The amount field may have 3 digits behind the comma, where our system expects 1 digit or what our system calls ""amount"" might be called ""quantity1"" in the incoming files etc.. ). There will be multiple amount, date, free text and categorical fields and there will be relationships between those fields. Note that the source data comes from external parties and I have no control over the format of the incoming files. 

We have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. Im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.

I've been looking into a few things : using NLP\spacy to train a model that recognises patterns in the column data. E.g. Numeric + period + comma is likely to correspond to amount. I've also looked at modeling the data and extracting an RDF representation using a open source tool called Karma to see if I can train a model on a network graph. But really struggling to see how to implement this. Regex would only get us part of the way there and I'm really trying to see if there is a scalable way to implement this. 

Is anyone aware of the formal name of this type of problem and if there are tried and tested approaches\implementations out there that I could build upon?",challenge work work try figure approach internal system store transactional data tabular form receive daily file data domain transactions metadata column name standardise data field always exact e g amount field may nmbr digits behind comma system expect nmbr digit system call amount might call quantity1 incoming file etc multiple amount date free text categorical field relationships field note source data come external party control format incoming file manual map transformation define incoming file volume different format source ever increase im look way take input file train model predict column likely correspond column target file look things use nlp spacy train model recognise pattern column data e g numeric period comma likely correspond amount also look model data extract rdf representation use open source tool call karma see train model network graph really struggle see implement regex would get us part way really try see scalable way implement anyone aware formal name type problem try test approach implementations could build upon
Mundane_Definition_8,MachineLearning,1619678595.0,"[D] I'm kaggler and hate waiting result, what makes you feel good?","Hi guys.

Usually, as a programmer, I enjoy watching my result immediately.

But in ML, Although I want to create my model, I hate waiting my result, and I don't know I really like AI.

My viewpoint makes me limit as a programmer grow.

&#x200B;

Where's your happy point while you are creating yours?

And, What do you do while your model is trained?",hi guy usually programmer enjoy watch result immediately ml although want create model hate wait result know really like ai viewpoint make limit programmer grow x200b happy point create model train
marksteve4,MachineLearning,1617819843.0,[D] Does anyone use huggingface hosted interface?,I tried a bunch (many are disabled by default). But not much useful. Am I missing sth? Thought that the source for HF to make money ...,try bunch many disable default much useful miss sth think source hf make money
DietMediocre8993,MachineLearning,1617816736.0,[D] ML Engineer with Data Engineer,"Hello fellow enthusiasts. I have an upcoming interview for a Data Engineering role where one of the rounds is with a Data Scientist on Problem Solving, I am not sure what to focus for my prep or even what kind of questions I can expect, any suggestions?

It seems the focus would be around production, how DE and Scientists work/collaborate together, what do you think?",hello fellow enthusiasts upcoming interview data engineer role one round data scientist problem solve sure focus prep even kind question expect suggestions seem focus would around production de scientists work collaborate together think
Yuqing7,MachineLearning,1617162546.0,[N] Microsoft & Princeton’s Text-Game Agents Achieve High Scores in Complete Absence of Semantics,"A research team from Princeton University and Microsoft Research discover autonomous language-understanding agents are capable of achieving high scores even in the complete absence of language semantics, indicating that current RL agents for text-based games might not be sufficiently leveraging the semantic structure of game texts.

Here is a quick read: [Microsoft & Princeton’s Surprising Discovery: Text-Game Agents Achieve High Scores in Complete Absence of Semantics](https://syncedreview.com/2021/03/30/microsoft-princetons-surprising-discovery-text-game-agents-achieve-high-scores-in-complete-absence-of-semantics/)

An early version of the paper *Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents* was featured in the NeurIPS 2020 [workshop](https://wordplay-workshop.github.io/modern/) *Wordplay: When Language Meets Games.* The updated paper is available on [arXiv](https://arxiv.org/pdf/2103.13552.pdf).",research team princeton university microsoft research discover autonomous language understand agents capable achieve high score even complete absence language semantics indicate current rl agents text base game might sufficiently leverage semantic structure game texts quick read microsoft princetons surprise discovery text game agents achieve high score complete absence semantics url early version paper read act blindfold need semantics text game agents feature neurips nmbr workshop url wordplay language meet game update paper available arxiv url
tars9999,MachineLearning,1617288820.0,Mixture of experts - tradeoffs vs traditional single nets [D],"I was personally surprised that ""branched NN's"" (seemingly called Mixture-Of-Experts) was not a more popular (or sucessful) technique, (eg. for imagenet entries) but it's appeared in some recent stories.

Is there any research comparing training times, model size, evaluation time between 1 dense net and mixture-of-experts ?

e.g. how would it pan out if you tried to do imagenet with it? (eg 33 branches of 33 categories each, or 10 vs 100 ..)

also is there any research on the tradeoffs of more branches ('heirachical MoE') (like 1000 caterogies = 10x10x10)

Perhaps for a particular problem there is an optimum branching depth, and it just happens below a certain size the optimum number of branches is just '1'.

&#x200B;

I'm guessing that single nets work better than I expected because the features cooperate more.. are there attempts at MoE models with some common trunk shared by the branches 

&#x200B;",personally surprise branch nn seemingly call mixture experts popular sucessful technique eg imagenet entries appear recent stories research compare train time model size evaluation time nmbr dense net mixture experts e g would pan try imagenet eg nmbr branch nmbr categories nmbr vs nmbr also research tradeoffs branch heirachical moe like nmbr caterogies 10x10x10 perhaps particular problem optimum branch depth happen certain size optimum number branch 1 x200b guess single net work better expect feature cooperate attempt moe model common trunk share branch x200b
jostmey,MachineLearning,1616375436.0,[R] Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets,"Preprint: [https://arxiv.org/abs/2103.10472](https://arxiv.org/abs/2103.10472)

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.",preprint url statistical classifiers design find pattern data number fit row columns like spreadsheet many kinds data conform structure uncover pattern non conform data describe approach modify establish statistical classifiers handle non conform data call dynamic kernel match dkm examples non conform data consider dataset cell receptor tcr sequence label disease antigen ii dataset sequence tcr repertoires label patient cytomegalovirus cmv serostatus anticipate datasets contain signatures diagnose disease successfully fit statistical classifiers augment dkm datasets report performance holdout data use standard metrics metrics allow indeterminant diagnose finally identify pattern use statistical classifiers generate predictions show pattern agree observations experimental study
Yuqing7,MachineLearning,1617987945.0,"[N] TUM, Google, Nvidia & LMU München's CodeTrans Pretrained Models Crack Source Code Tasks With SOTA Performance","A research team from Technical University of Munich, Google, Nvidia and LMU München proposes CodeTrans, an encoder-decoder transformer model which achieves state-of-the-art performance on six tasks in the software engineering domain, including Code Documentation Generation, Source Code Summarization, Code Comment Generation, etc.

Here is a quick read: [TUM, Google, Nvidia & LMU München's CodeTrans Pretrained Models Crack Source Code Tasks With SOTA Performance](https://syncedreview.com/2021/04/09/tum-google-nvidia-lmu-munchens-codetrans-pretrained-models-crack-source-code-tasks-with-sota-performance/)

The CodeTrans code is available on the project [GitHub](https://github.com/agemagician/CodeTrans). The paper *CodeTrans: Towards Cracking the Language of Silicone’s Code Through Self-Supervised Deep Learning and High Performance Computing* is on [arXiv](https://arxiv.org/ftp/arxiv/papers/2104/2104.02443.pdf).",research team technical university munich google nvidia lmu münchen propose codetrans encoder decoder transformer model achieve state art performance six task software engineer domain include code documentation generation source code summarization code comment generation etc quick read tum google nvidia lmu münchen codetrans pretrained model crack source code task sota performance url codetrans code available project github url paper codetrans towards crack language silicones code self supervise deep learn high performance compute arxiv url
OnlyProggingForFun,MachineLearning,1619005027.0,[D] Will Transformers Replace CNNs in Computer Vision?," I recently wrote an article, ""[Will Transformers Replace CNNs in Computer Vision](https://pub.towardsai.net/will-transformers-replace-cnns-in-computer-vision-55657a196833)?"" showing that transformers can be applied to not only text but also images and other types of inputs. I did that by covering a paper called the [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) where it gives a way to apply transformers' architecture in computer vision. I know that many others are quite promising, like [The Perceiver](https://arxiv.org/abs/2103.03206), but my question is: Do you think transformers are better suited for computer vision than convolutional neural networks? Are they viable in computation time and results compared to CNNs, optimizing the properties of images, especially for classification?",recently write article transformers replace cnns computer vision url show transformers apply text also image type input cover paper call swin transformer url give way apply transformers architecture computer vision know many others quite promise like perceiver url question think transformers better suit computer vision convolutional neural network viable computation time result compare cnns optimize properties image especially classification
m_nemo_syne,MachineLearning,1617723592.0,[R] Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers,"Timers and Such! New speech dataset for timers, alarms, unit conversion, and math.

Use it to test your speech models or to train your own offline voice assistant.

Paper: [https://arxiv.org/abs/2104.01604](https://t.co/1Itx4KKVyo?amp=1)

Code (SpeechBrain): [https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such](https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such)

Pre-trained model on Hugging Face: [https://huggingface.co/speechbrain/slu-timers-and-such-direct-librispeech-asr](https://huggingface.co/speechbrain/slu-timers-and-such-direct-librispeech-asr)",timers new speech dataset timers alarm unit conversion math use test speech model train offline voice assistant paper url speechbrain url model hug face url
jj4646,MachineLearning,1618972573.0,[D] Complexity of Time Series Models: ARIMA vs. LSTM,"1) When it comes to time series analysis, I am trying to understand what makes newer models such as LSTM's capable of capturing more complex patterns in the data compared to older modeis such as ARIMA? In statistical learning theory, there is something called the VC Dimension of an algorithm (https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension) - the VC dimension is apparently what describes the relartive level of complexity a machine learning algorithm can capture. Does this concept of VC Dimension carry over to models in time series analysis? Is it possible to show that LSTM's have a higher VC dimension compared to ARIMA style models? Supposedly, neural network based time series models were developed because modeols like ARIMA was unable to provide reliable estimates for bigger and complex datasets. Mathematically speaking, what allows a LSTM to capture more variation and complexity in a dataset compared to ARIMA?




2) Lately, I have seen many people starting to suggest that Convoloution Neural Networks (CNN) can also be used for the purpose of time series forecasting. Just as a general question: in what instances would it be better to use a CNN for time series forecasting compared to an LSTM?

Thanks",1 come time series analysis try understand make newer model lstm capable capture complex pattern data compare older modeis arima statistical learn theory something call vc dimension algorithm url vc dimension apparently describe relartive level complexity machine learn algorithm capture concept vc dimension carry model time series analysis possible show lstm higher vc dimension compare arima style model supposedly neural network base time series model develop modeols like arima unable provide reliable estimate bigger complex datasets mathematically speak allow lstm capture variation complexity dataset compare arima 2 lately see many people start suggest convoloution neural network cnn also use purpose time series forecast general question instance would better use cnn time series forecast compare lstm thank
AirZealousideal1342,MachineLearning,1618671696.0,[D]What is this in ontonotes dataset?,"I found in ontonotes, there are many punctuations which are preceded by a slash (e.g. '/.', '/?'). Is it an error. If it is, why it has not been be fixed until now(verion12)?

https://preview.redd.it/br0p5tng0rt61.png?width=3810&format=png&auto=webp&s=9e5e48cb5be7bf1e38ab3df053c637be8cc09d97

But some of then are normal.

https://preview.redd.it/atckgg9vrvt61.png?width=3732&format=png&auto=webp&s=a1ed061d8d28e988be47db12d9d69c691414d108",find ontonotes many punctuations precede slash e g error fix verion12 url normal url
Stingeymingey,MachineLearning,1617265463.0,[D] What's out there in terms of Video comprehension/understanding?,"Hi, 

I'm wondering if people could point me to any papers or projects that are working on video / subsequent frame understanding. Not just object detection, but using previous frames as context to guide future predictions.

Thanks.",hi wonder people could point paper project work video subsequent frame understand object detection use previous frame context guide future predictions thank
cloudone,MachineLearning,1616563801.0,[N] Pieter Abbeel launched a new podcast,"First guest is Andrej Karpathy

https://www.therobotbrains.ai/",first guest andrej karpathy url
BodybuildingPhD,MachineLearning,1616747965.0,[D] Does the NN really learns probability distribution?,"I am currently trying to understand how does the Neural Net learns probability distribution.

At first glance, this seems impossible, since basically neural net is a deterministic non-linear function.

As for the VAE, the encoder of VAE takes such x as an input and results in the main parameters of distribution(mostly, mean and covariance of gaussian distribution). This seems plausible; just by adding some random variable epsilion \~ N(0, I) you can construct Normal distribution.

But for Decoder, though many literature assumes the decoder has the form of probability distribution(which is P(X|Z)), yet they don't. Unlike the encoder part, there is no *randomness* in the structure of VAE decoder.

Moreover many decoder of the VAE assumes that they have normal distribution, but how could they be so sure?

Is there anything that I am missing?",currently try understand neural net learn probability distribution first glance seem impossible since basically neural net deterministic non linear function vae encoder vae take x input result main parameters distribution mostly mean covariance gaussian distribution seem plausible add random variable epsilion n 0 construct normal distribution decoder though many literature assume decoder form probability distribution p x z yet unlike encoder part randomness structure vae decoder moreover many decoder vae assume normal distribution could sure anything miss
Koyset,MachineLearning,1617640968.0,"[Discussion] ""Developers should take philosophy classes to make ethical AI""","Hello, community!

I am still a third-semester CS Student but had already a rather confusing workshop at my student job at a telecommunications firm. It started with two consultants who both have an undergrad in social science but don't code. They provided us within a day with AI ethics guidelines that we developers should implement during our work.   
I asked how to make the guidelines like ""de-bias your code"", ""make inclusive algorithms"" into a (functional) code, since it's my first job, but they just said I should take additional philosophy classes against my hidden bias. 

How do implement ethical guidelines in code? Does every company has their own guidelines? Sorry, I was very uncomfortable asking further questions since I didn't understand the overall workshop. Hope you can provide some real-life examples from your jobs.",hello community still third semester cs student already rather confuse workshop student job telecommunications firm start two consultants undergrad social science code provide us within day ai ethics guidelines developers implement work ask make guidelines like de bias code make inclusive algorithms functional code since first job say take additional philosophy class hide bias implement ethical guidelines code every company guidelines sorry uncomfortable ask question since understand overall workshop hope provide real life examples job
the21st,MachineLearning,1619516433.0,[P] I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool,"Hi there! I'm Simon, an engineer at Deepnote, previously I worked as an ML team lead in an e-commerce company. I recently built a new feature in Deepnote and I just wanted to brag a little bit because I'm proud of it ☺️

It's native SQL cells with Python interop, check out an [example project](https://deepnote.com/project/RNA-Exploration-Duplicate-xlaRWiCeRNqQ0Id0v2RY5A/%2Fnotebook.ipynb). Full docs for this feature are [here](https://docs.deepnote.com/features/sql-cells).

This was possible thanks to standing on the shoulders of open-source giants such as pandas and jinjasql, which I hugely appreciate and it makes me truly happy.

I'd appreciate any feedback and your thoughts on what innovation you'd love to see around notebooks in the future 🙂",hi simon engineer deepnote previously work ml team lead e commerce company recently build new feature deepnote want brag little bite proud native sql cells python interop check example project url full docs feature url possible thank stand shoulder open source giants pandas jinjasql hugely appreciate make truly happy appreciate feedback thoughts innovation love see around notebooks future
New-Sound8660,MachineLearning,1616886907.0,[R] Teaching a machine to paint like a painter comparing photos and associated paintings,"Hello everybody, I'm just approaching ML and I'd need some guidance.

Here's  the idea: I have a set of photos and a set of human made paintings  based on those photos, that try to replicate as faithfully as possible  the photos. Can I ""give"" to the algorithm the two sets of images, asking  him to ""learn"" how to create ""paintings"" in the style of the painter ho  realized the dataset, starting from new photos? Any suggestion about  which books, articles, courses, blogs should I follow to learn how to do  this would be greatly appreciated, thanks so much!",hello everybody approach ml need guidance idea set photos set human make paint base photos try replicate faithfully possible photos give algorithm two set image ask learn create paint style painter ho realize dataset start new photos suggestion book article course blog follow learn would greatly appreciate thank much
vladiliescu,MachineLearning,1616591206.0,[D] [P] How do you use tools like AutoML?,"Hi girls and guys.

I was wondering - do you use tools like AutoML in your day to day life? Like, anything from H2O to Google's AutoML to auto-sklearn.

Asking since personally I only find AutoML useful for training an initial baseline. You know, when you've just managed to get a dataset that's just clean enough to be useful, and you're itching to see if this time you'll need to use something other than LightGBM. It's always LightGBM though. Always. 🥺

Anyway, apart from this initial phase I don't really use the tool - what I do when working with the [Azure flavor](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml) at least, is reverse engineer the model, take whatever I can and adapt whatever I cannot take. Kinda like [Picasso](https://quoteinvestigator.com/2013/03/06/artists-steal/), I imagine.

I've written a short(-ish) blog post on creating your own models based on Azure AutoML-trained ones. You'll find it [here](https://vladiliescu.net/reverse-engineering-automated-ml/). It's focused on time series, but the underlying principles should apply to regression and classification models, too.

Hope you like it. And I appreciate any and all feedback you may have - I'm really curious how you use these tools, and what you think of my approach.",hi girls guy wonder use tool like automl day day life like anything h2o google automl auto sklearn ask since personally find automl useful train initial baseline know manage get dataset clean enough useful itch see time need use something lightgbm always lightgbm though always anyway apart initial phase really use tool work azure flavor url least reverse engineer model take whatever adapt whatever take kinda like picasso url imagine write short ish blog post create model base azure automl train ones find url focus time series underlie principles apply regression classification model hope like appreciate feedback may really curious use tool think approach
otso_ai,MachineLearning,1618355393.0,[P] otso Annotator - A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists working in teams.,"*otso has just launched our Annotator, and have built with the needs of many in this sub! Check it out!*

**A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.**

We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.

otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.

**Why a focus on the user experience of teams?**

Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.

With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.

To check it out, head to [otso.ai/annotator](https://otso.ai/annotator).No credit card required.",otso launch annotator build need many sub check cloud base text annotator build machine learn engineer data scientists work otso annotator two years begin internal tool use manage annotation data label machine learn project tool interface develop begin provide select enterprise customers weve receive much significant positive feedback clients decide launch annotator standalone product otso annotator provide three key benefit user experience prioritise ease use understand project management feature let allocate manage annotation task finally cloud first tool longer need annotators use cli get start make much easier tool team use focus user experience team text annotation best team environment ideally machine learn engineer data scientists set run project subject matter experts provide annotations build otso annotator different user type mind enable seamless project setup project admins easy keyboard enable annotation experience annotators public launch grant users team sign april extend trial period nmbr days check head otso ai annotator url credit card require
artificial_intelect,MachineLearning,1616734142.0,[P] NAS repos,When looking at git repos that have implemented MNasNet or EfficientNet they only ever seem to implement the network found by the neural architecture search. Does anyone know of a git repo that implements the Proximal Policy Optimization that can find EfficientNet (or some other similar NAS algorithm repo)?,look git repos implement mnasnet efficientnet ever seem implement network find neural architecture search anyone know git repo implement proximal policy optimization find efficientnet similar nas algorithm repo
tanelai,MachineLearning,1618087578.0,[P] Using PyTorch + NumPy? A bug that plagues thousands of open-source ML projects.,"Using NumPy’s random number generator with multi-process data loading in PyTorch causes identical augmentations unless you specifically set seeds using the worker\_init\_fn option in the DataLoader. I didn’t and this bug silently regressed my model’s accuracy.

How many others has this bug done damage to? Curious, I downloaded over a hundred thousand repositories from GitHub that import PyTorch, and analysed their source code. I kept projects that define a custom dataset, use NumPy’s random number generator with multi-process data loading, and are more-or-less straightforward to analyse using abstract syntax trees. Out of these, over 95% of the repositories are plagued by this problem. It’s inside PyTorch's official tutorial, OpenAI’s code, and NVIDIA’s projects. Even Karpathy admitted falling prey to it.

For example, the following image shows the duplicated random crop augmentations you get when you blindly follow the official PyTorch tutorial on custom datasets:

https://preview.redd.it/pccy5wskpes61.png?width=1652&format=png&auto=webp&s=f292d0282ad954cbac2c693a9656d62fa0dd9682

You can read more details [here](https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/).",use numpys random number generator multi process data load pytorch cause identical augmentations unless specifically set seed use worker _init _fn option dataloader bug silently regress model accuracy many others bug damage curious download hundred thousand repositories github import pytorch analyse source code keep project define custom dataset use numpys random number generator multi process data load less straightforward analyse use abstract syntax tree 95 repositories plague problem inside pytorch official tutorial openais code nvidias project even karpathy admit fall prey example follow image show duplicate random crop augmentations get blindly follow official pytorch tutorial custom datasets url read detail url
PT_Lightning,MachineLearning,1619457247.0,[N] Lightning Transformers - Train HuggingFace Transformers at scale with PyTorch Lightning,"[Lightning Transformers](https://github.com/PyTorchLightning/lightning-transformers) is for users who want to train, evaluate and predict using HuggingFace models and datasets with PyTorch Lightning. Full customizability of the code using the LightningModule and Trainer, with Hydra config composition for quick and easy experimentation. No boilerplate code required; easily swap out models, optimizers, schedulers, and more without touching the code. Check out the blog post: [Training Transformers at Scale with PyTorch Lightning](https://pytorch-lightning.medium.com/training-transformers-at-scale-with-pytorch-lightning-e1cb25f6db29) for more information or the [documentation](https://lightning-transformers.readthedocs.io/).

https://i.redd.it/ufmoebjbwjv61.gif",lightning transformers url users want train evaluate predict use huggingface model datasets pytorch lightning full customizability code use lightningmodule trainer hydra config composition quick easy experimentation boilerplate code require easily swap model optimizers schedulers without touch code check blog post train transformers scale pytorch lightning url information documentation url
Mephistothelessa,MachineLearning,1616657297.0,[D] What is the best way to publish an image dataset?,"Hello people! As a small team, we looked around for a dataset we needed for a project but couldn't find one. So we decided to properly create one ourselves and publish it for people to use in the future. We want to properly ""publish"" our dataset, meaning we don't want to use Kaggle for this. Maybe a dedicated website might work, we want it to look professional. Maybe Github pages may be used?

What are your suggestions? What do you think is the best way to publish a dataset?

Open to all suggestions and curious about what you guys think. All inputs are appreciated. Cheers.",hello people small team look around dataset need project find one decide properly create one publish people use future want properly publish dataset mean want use kaggle maybe dedicate website might work want look professional maybe github page may use suggestions think best way publish dataset open suggestions curious guy think input appreciate cheer
artificial_intelect,MachineLearning,1616542211.0,[D] PipeMare paper discussion,"or maybe PipeMare paper rant...

&#x200B;

A while back I read a paper on mitigating the effects of asynchronous pipelined training called [PipeMare](https://proceedings.mlsys.org/paper/2021/hash/6c8349cc7260ae62e3b1396831a8398f-Abstract.html). Their methods didn't seem novel or super helpful so I ignored the paper and that was that. Then I noticed that it was accepted into a conference: MLSys2021.

So now I guess it's worth putting my thoughts online.

PipeMare proposes two methods to mitigate asynchronous pipelined NN training.

Issue 1: the type of asynchronous pipelined NN training they mitigate for is Pipelined Backpropagation (Petrowski et al., 1993). Petrowski et al. (1993) aren't even cited in the paper. Just because PipeDream doesn't cite Petrowski et al. (1993) does not mean you shouldn't. Note Pipelined Backpropagation has 2 issues: inconsistent weight & delayed gradients. PipeDream uses weight stashing to eliminate inconsistent weight but still has delayed gradients. PipeMare eliminates the overhead of weight stashing with ""Discrepancy correction"", but doesn't really deal with delayed gradients except for using an lr warmup.

The two methods PipeMare proposes are:

T1 - Learning rate rescheduling: a type of learning rate warm-up where the warm-up period is based on the pipeline delay.

T2 - Discrepancy correction: a type of backward weight prediction for reconciling the weights used in the forward and backward pass. While T2 deals with weight inconsistency, it does not mitigate for gradient delay.

Issue 2: If T1 is a type of learning rate warm-up why does the paper not show a baseline run with just a regular learning rate warm-up? My guess is that a regular learning rate warm-up would do just as well as this new convoluted method T1.

Issue 3: In Table 3 the PipeMare paper shows that T1 on its own works just as well as T1 + T2. Why use T2? It doesn't seem to help and just has overhead. PipeDream's weight stashing eliminates weight inconsistency; T2 only mitigates for weight inconsistency and does not eliminate it. In [2019 Chen et al](https://arxiv.org/abs/1809.02839) proposed a method called SpecTrain. In their work, they show that weight inconsistency is not a big issue and eliminating it by use weight stashing is useless. If weight inconsistency is not a big issue (as shown in the SpecTrain paper), why use T2 (especially since Table 3 shows that it is useless)?

T1 isn't novel / there is no evidence that a simple lr warm-up wouldn't do just as well. T2 looks like it's useless. The methods are neither novel nor useful. How did reviewers at MLSys not see this?

I mean the paper is interesting. The pipelined execution model is interesting. The analysis is interesting, but the mitigation methods (ie the paper's contributions) are incremental at best. How does this get in?

Personally, I think the [SpecTrain paper](https://arxiv.org/abs/1809.02839) (similar topic, better mitigation but the method analysis isn't that good) is a much better paper that should have been published at conf but wasn't. NOTE: I am not an author on the SpecTrain paper.

If anyone is attending MLSys2021, could you question the authors on the points brought up in this post. My only request is that the questions be asked nicely. Like I said their analysis of delayed optimization is still really interesting, and they do explore the world of fine-grained pipelined training ie they're actually exploring non-mainstream execution models; even if Pipelined Backpropagation has existed since 1993, it hasn't really been used on modern NN.

&#x200B;

EDIT:

In the paper, Figure 7 and Figure 8 show that when the pipeline depth is **artificially** increased to be very large, T2 becomes useful, but at that point, pipelined training has a hard time achieving the same accuracy as SGD; at that point maybe you shouldn't even be using pipelined training at all?",maybe pipemare paper rant x200b back read paper mitigate effect asynchronous pipelined train call pipemare url methods seem novel super helpful ignore paper notice accept conference mlsys2021 guess worth put thoughts online pipemare propose two methods mitigate asynchronous pipelined nn train issue 1 type asynchronous pipelined nn train mitigate pipelined backpropagation petrowski et al 1993 petrowski et al 1993 even cite paper pipedream cite petrowski et al 1993 mean note pipelined backpropagation nmbr issue inconsistent weight delay gradients pipedream use weight stash eliminate inconsistent weight still delay gradients pipemare eliminate overhead weight stash discrepancy correction really deal delay gradients except use lr warmup two methods pipemare propose t1 learn rate reschedule type learn rate warm warm period base pipeline delay t2 discrepancy correction type backward weight prediction reconcile weight use forward backward pass t2 deal weight inconsistency mitigate gradient delay issue 2 t1 type learn rate warm paper show baseline run regular learn rate warm guess regular learn rate warm would well new convolute method t1 issue 3 table nmbr pipemare paper show t1 work well t1 t2 use t2 seem help overhead pipedream weight stash eliminate weight inconsistency t2 mitigate weight inconsistency eliminate 2019 chen et al url propose method call spectrain work show weight inconsistency big issue eliminate use weight stash useless weight inconsistency big issue show spectrain paper use t2 especially since table nmbr show useless t1 novel evidence simple lr warm well t2 look like useless methods neither novel useful reviewers mlsys see mean paper interest pipelined execution model interest analysis interest mitigation methods ie paper contributions incremental best get personally think spectrain paper url similar topic better mitigation method analysis good much better paper publish conf note author spectrain paper anyone attend mlsys2021 could question author point bring post request question ask nicely like say analysis delay optimization still really interest explore world fine grain pipelined train ie actually explore non mainstream execution model even pipelined backpropagation exist since 1993 really use modern nn x200b edit paper figure nmbr figure nmbr show pipeline depth artificially increase large t2 become useful point pipelined train hard time achieve accuracy sgd point maybe even use pipelined train
memgamemotron,MachineLearning,1619183370.0,[D] data science and ML bootcamp check!,"Anyone here completed any data science boot camps from Metis, flatiron, brain station, general assembly and others? I’m looking to see if these are legit and if they’ve helped you land a job in data science after completing the boot camps. They advertise a 92% hire rate from top tech companies, so curious if anyone can contribute any thoughts.",anyone complete data science boot camp metis flatiron brain station general assembly others im look see legit theyve help land job data science complete boot camp advertise 92 hire rate top tech company curious anyone contribute thoughts
TopIndependent5791,MachineLearning,1618733779.0,[D] Which AWS tools/services are necessary to learn for Machine Learning Engineer?,"What AWS tools I need to learn in order to be more ""qualified"" for the industry?

I have been personally involved in machine learning through college and personal projects, but I feel I lack some entry level industry knowledge?

What would be the right resources for the AWS?",aws tool need learn order qualify industry personally involve machine learn college personal project feel lack entry level industry knowledge would right resources aws
mgl96,MachineLearning,1617264240.0,[N] Trankit v1.0.0 - An open-source Transformer-based Multilingual NLP Toolkit for 56 languages is out.,"Hi everyone,

We've just released the version v1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit, which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.

💥 💥 💥 The new version v1.0.0 offers:

* **A trainable transformer-based pipeline for fundamental NLP tasks over 100 languages**.
* **90 new pretrained transformer-based pipelines for 56 languages**. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For **English**, Trankit is significantly better than Stanza on sentence segmentation (**+9.36%**) and dependency parsing (**+5.07%** for UAS and **+5.81%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.36%** while **Chinese** observes **14.50%** and **15.0%** improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5) .
* **Auto Mode for multilingual pipelines**. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language. Check out how to turn on the Auto Mode [here](https://trankit.readthedocs.io/en/latest/news.html#auto-mode-for-multilingual-pipelines).
* **Command-line interface** is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily. Check out the command-line tutorials on this [page](https://trankit.readthedocs.io/en/latest/commandline.html).

**Trankit is written in Python** and can be easily installed via pip. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.

Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you for your time reading this post!

Hope you enjoy Trankit!",hi everyone release version v1 0 0 transformer base multilingual nlp toolkit name trankit outperform popular sota stanford nlp stanza many task nmbr different languages new version v1 0 0 offer trainable transformer base pipeline fundamental nlp task nmbr languages 90 new pretrained transformer base pipelines nmbr languages new pipelines train xlm roberta large boost performance significantly nmbr treebanks universal dependencies v2 5 corpus english trankit significantly better stanza sentence segmentation 9 36 dependency parse 5 07 uas 5 81 las arabic toolkit substantially improve sentence segmentation performance 16 36 chinese observe 14 50 15 0 improvement uas las dependency parse performance languages also significantly improve detail comparison trankit stanza udpipe spacy languages find url auto mode multilingual pipelines auto mode language input automatically detect enable multilingual pipelines process input without specify language check turn auto mode url command line interface available use help users familiar python program language use trankit easily check command line tutorials page url write python easily instal via pip code pretrained model publicly available url also create documentation page demo website trankit documentation page url website url detail trankit find paper url time read post hope enjoy trankit
orenog,MachineLearning,1619488275.0,[D] Node Collapse with StyleGAN2 - ada,"Hey reddit, I have a problem.

I trained StyleGAN2 ada on 3100 Hearthstone's cards,the results were amazing, but they started look more and more the same, untill it got to a point where it just generated the same 3 cards! 1 spell and 1 or 2 minions, and all of them with a very similar artwork.

I tried again with all of the augmentation pipeline options on and got the same results! It node collapsed after 20 ticks and just got less and less variation as it went on!


I wanted to ask, what are the ways to deal with node collapse?  And what are the ways to deal with node collapse specifically in StyleGAN2 ada.

The dataset seems like it has a lot of variation, it's 3100+ cards, they look similar for the most parts, but there are different types of cards and each card has a unique artwork, but somehow the generated artworks looked the same after 40 ticks! 

I really want it to works, any advice or direction towards solution will be super helpful, I don't even know what's causing it.

Thanks

Edit: apparently it's mode collapse, not node collapse",hey reddit problem train stylegan2 ada nmbr hearthstone card result amaze start look untill get point generate nmbr card nmbr spell nmbr nmbr minions similar artwork try augmentation pipeline options get result node collapse nmbr tick get less less variation go want ask ways deal node collapse ways deal node collapse specifically stylegan2 ada dataset seem like lot variation 3100 card look similar part different type card card unique artwork somehow generate artworks look nmbr tick really want work advice direction towards solution super helpful even know cause thanksedit apparently mode collapse node collapse
Farconion,MachineLearning,1620326515.0,[D] Any value in negative / null results?,"I am in the middle of completing a small independent research project  under a ML professor at my college. Ideally my project was aiming to  reimplement / replicate the results of a previous paper from a couple of  years ago that had a decent number of citations (\~100), and then extend  upon their work in some simple but worthwhile ways.

However  after reimplementing their code (really old PyTorch that had no  compatibility with my hardware) and running some basic experiments, I  failed to replicate even their most basic results. I plan on running  more experiments to further confirm this, but I'm not getting my hopes  up.

Obviously there are any number  of reasons for this - changes in PyTorch, errors in my implementation  (I feel confident in it but still), etc. - so trying to write or publish  something saying ""technique X doesn't work because I couldn't replicate  it"" doesn't feel worthwhile or correct.

Does  anyone have any tips on the best way to utilize null results like this?  I was hoping to use this project as a part of my grad school apps, but  in its current state I'm not sure what the best way to frame it - or  even if I can.",middle complete small independent research project ml professor college ideally project aim reimplement replicate result previous paper couple years ago decent number citations 100 extend upon work simple worthwhile ways however reimplementing code really old pytorch compatibility hardware run basic experiment fail replicate even basic result plan run experiment confirm get hop obviously number reason change pytorch errors implementation feel confident still etc try write publish something say technique x work replicate feel worthwhile correct anyone tip best way utilize null result like hop use project part grad school apps current state sure best way frame even
robust_melon,MachineLearning,1619475121.0,[D] ethical considerations with launching a website with vehicle accident density forecasts,"I have been working on a project for predicting the distribution of accidents over city locales over short-term forecasting horizons (~1 hour). I wish to create a public page/dashboard that is updated hourly with these forecasts.

**Assuming that I have a great model, which predicts where accident hot spots are geographically likely to occur in the next hour -what are the ethical ramifications, if these forecasts were made public?** Should I assume that someone with ill intent can, and will, use this information with criminal intent (e.g. infer where emergency response resources will likely be)?


Edit: bolded for emphasis",work project predict distribution accidents city locales short term forecast horizons 1 hour wish create public page dashboard update hourly forecast assume great model predict accident hot spot geographically likely occur next hour ethical ramifications forecast make public assume someone ill intent use information criminal intent e g infer emergency response resources likely edit bolded emphasis
biotechdood,MachineLearning,1617993459.0,[D] Looking for an interesting paper in the field of biotech and AI,"Hi,

I  will be having a presentation about a chosen paper in a few days and am  looking for revolutionary scientific discoveries that contain AI in the  domain of bio/biotech/pharma/bioprocess engineering/etc. I would be  really thankful if you could help me out.

Ideally  it shouldn't be too complicated for laymen coming from the bio area and  I would appreciate if it was something (supposedly) revolutionary such  as AlphaFold 2.

Thank you in advance and kind regards

\*I would also appreciate some paper recommendations in the field of computer vision and microscopy",hi presentation choose paper days look revolutionary scientific discoveries contain ai domain bio biotech pharma bioprocess engineer etc would really thankful could help ideally complicate laymen come bio area would appreciate something supposedly revolutionary alphafold 2 thank advance kind regard would also appreciate paper recommendations field computer vision microscopy
hotpot_ai,MachineLearning,1617654345.0,[D] Overview on SOTA methods for deep learning model compression,"**Intro**

This post covers model inference optimization or compression in breadth and hopefully depth as of March 2021. This includes engineering topics like model quantization and binarization, more research-oriented topics like knowledge distillation, as well as well-known-hacks.

Each year, larger and larger models are able to find methods for extracting signal from the noise in machine learning. In particular, language models get larger every day. These models are computationally expensive (in both runtime and memory), which can be both costly when served out to customers or too slow or large to function in edge environments like a phone.

Researchers and practitioners have come up with many methods for optimizing neural networks to run faster or with less memory usage. In this post I’m going to cover some of the state-of-the-art methods. If you know of another method you think should be included, I’m happy to add it. This has a slight PyTorch bias (haha) because I’m most familiar with it.

**URL**

[https://rachitsingh.com/deep-learning-model-compression/](https://rachitsingh.com/deep-learning-model-compression/)",intro post cover model inference optimization compression breadth hopefully depth march 2021 include engineer topics like model quantization binarization research orient topics like knowledge distillation well well know hack year larger larger model able find methods extract signal noise machine learn particular language model get larger every day model computationally expensive runtime memory costly serve customers slow large function edge environments like phone researchers practitioners come many methods optimize neural network run faster less memory usage post im go cover state art methods know another method think include im happy add slight pytorch bias haha im familiar url url
federico-bianchi,MachineLearning,1620243697.0,[N] Coveo SIGIR Data Challenge for Ecommerce,"For those interested, [Coveo](https://www.coveo.com/) is organizing the SIGIR Data Challenge.

Useful to apply Deep Learning/ML skills on one of the biggest eCommerce datasets ever released, including fine-grained shopping behavior, search queries with clicked/unclicked items, and product meta-data from catalogs. Great chance to get creative and write a research paper!

[Challenge website](https://sigir-ecom.github.io/data-task.html)

[Repository](https://github.com/coveooss/SIGIR-ecom-data-challenge)

*Important Dates*:

* Registration ends: June 1st, 2021
* Final leaderboard: June 15th, 2021
* SIGIR eCom Full-day Workshop: July 15th, 2021",interest coveo url organize sigir data challenge useful apply deep learn ml skills one biggest ecommerce datasets ever release include fine grain shop behavior search query click unclicked items product meta data catalog great chance get creative write research paper challenge website url date registration end june 1st 2021 final leaderboard june 15th 2021 sigir ecom full day workshop july 15th 2021
Chriscbe,MachineLearning,1617135089.0,[R] Please point me in the right direction: decision trees or possibly something better,"ML Practitioners:

I am a scientist who studies how to purify recombinant proteins. I am generally proficient with Python. I am looking for ways to fortify decsion making during purification of proteins created in E coli and cell culture. What parts of ML would be best to learn in order to improve planning/ decision making in a decion-tree like format? I have seen some literature on the issue but I'd like to start from a point where I can understand, for example, which parts of (sci-kit learn?) might address the issue, etc.

Thanks for any help",ml practitioners scientist study purify recombinant proteins generally proficient python look ways fortify decsion make purification proteins create e coli cell culture part ml would best learn order improve plan decision make decion tree like format see literature issue like start point understand example part sci kit learn might address issue etc thank help
amaigmbh,MachineLearning,1620394984.0,"[N] i.am.ai Newsletter - Updated AI Conference Calendar, Crowdsourced Speech Recognition, Dinos and more","[Hello there](https://www.youtube.com/watch?v=eaEMSKzqGAg), just here to share the [latest edition](https://www.am.ai/en/blog/newsletter-011/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit) of our i.am.ai Newsletter. Hope this interesting for some and not too annoying self promotion. Read the whole thing below and feel free to [subscribe](https://i.am.ai/newsletter/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit). Feedback welcome :)

# WHATS IMPORTANT

## 📍 ICLR 2021 Conference

The landmark Deep Learning conference [ICLR](https://iclr.cc/) is taking place from May 3rd to May 7th. For a second time the gathering is held completely virtual. Researchers from around the world are gathering for poster sessions, keynotes and workshops. Out of 2997 papers, 860 were accepted for the conference. The official **Outstanding Paper Awards** have been awarded for 8 papers of ""exceptional quality"" leading up to the conference. You can find a full list [here](https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab).

Meanwhile, Topbots made its own selection of ICLR conference papers, with **""breakthrough potential""**. These include the Visual Transformer (ViT), DETR, DeBERTa and Performers papers we previously introduced in this Newsletter. Read more about each breakthrough paper on [topbots.com](https://www.topbots.com/iclr-2021-research-papers/).

 

## 📍 More to Come

ICLR only marks the beginning of the annual Conference Summer. With the global health situation only improving slowly, more conferences have moved to virtual meetings. Find an **updated version of the AI Conference Calendar** below and more about [the most important AI conferences here](https://www.am.ai/en/blog/ai-conferences-2021/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit).

https://preview.redd.it/y0rbh01ecpx61.png?width=2052&format=png&auto=webp&s=d2bc5cb19e7abcc7229bd161a208d765f3006797

 

## THINGS WE FOUND WORTH SHARING

🚀 **Contribute** – The **crowdsourcing project Common Voice** aims to create a free database for speech recognition in 50+ languages. Since 2017 Mozilla asks volunteers to record sample sentences and review recordings of others. They recently saw a USD 1.5 million investment by Nvidia and are continuously looking for more participants. [Contribute here (switch the language in the top right)](https://commonvoice.mozilla.org/en).

 

  📚 **A personal recommendation from** [**AMAI**](https://linkedin.com/company/amai-gmbh) **CEO** [**Jürgen Stumpp**](https://www.linkedin.com/in/juergen-stumpp/):

>""The book **""Real World AI: A Practical Guide for Responsible Machine Learning""** provides executives with a quick but thorough overview of the steps necessary for successful AI projects. Moreover, it can help soon-to-be university graduates to prepare for work in the field.""

In their [book](https://www.goodreads.com/book/show/57347702-real-world-ai), Alyssa Rochwerger, Director of Product at Blue Shield and Appen CTO Wilson Pang, share their practical experiences and present an approach that claims a 3x higher success rate for AI projects compared to industry average. Learn more about **""Real World AI: A Practical Guide for Responsible Machine Learning""** in this [authors interview](https://www.forbes.com/sites/tomtaulli/2021/05/05/real-world-ai-a-great-guide-for-managers/).

 

📄 **Paper** – DINO: In the paper ""Emerging Properties in Self-Supervised Vision Transformers"" ([arXiv](https://arxiv.org/abs/2104.14294)), researchers from Inria, Facebook AI and Sorbonne University introduced **DINO**. Short for ""self-**di**stillation with **no** labels"", this method can segment images in a self-supervised manner, meaning without labels required upfront.

Read more about DINO and the accompanying PAWS on [venturebeat.com](https://venturebeat.com/2021/04/30/facebook-details-self-supervised-ai-that-can-segment-images-and-videos/) and the Facebook AI Blog [ai.facebook.com](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/). One-man paper discussion group, **Yannic Kilcher**, walks his viewers through the paper in detailed 39 minutes on [youtube.com](https://www.youtube.com/watch?v=h3ij3F3cPIk).

[\\""These self-attention maps for selected heads were generated using DINO with videos of a horse, a BMX rider, a puppy, and a fishing boat.\\"" - from Facebook AI](https://reddit.com/link/n6yspn/video/xghum660cpx61/player)

   

📈 **Markets** – Microsoft acquired Nuance in April, a Massachusetts company focussing on AI-driven speech recognition. At USD 19.7 billion this merger marks the second highest acquisition in Microsoft's history (LinkedIn was acquired for 26.2 billion in 2016). Nuance Communications is behind the speech recognition capabilities of Apple's voice assistant, Siri. More on [axios.com](https://www.axios.com/microsoft-readies-deal-frenzy-bbc807a2-fd5b-48a7-99d1-cf06d0a41547.html).

 

🧑‍⚖️ **Regulation** – The European Commission released the **Artificial Intelligence Act**, a [108-page](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence) proposal for regulation of AI. More on the proposal and the legislative hurdles ahead, on [politico.eu](https://www.politico.eu/article/6-key-battles-europes-ai-law-artificial-intelligence-act/).

Outside the EU, Chinese regulators have begun to enforce **data localization** on local companies. From now on data can only be stored in China for certain applications such as facial recognition ([biometricupdate.com](https://www.biometricupdate.com/202104/china-pushes-standards-for-face-biometrics-and-plenty-more)).

 

🤗 **Tongue in Cheek** – After [XKCD](https://xkcd.com/2456/) posted 12 types of scientific papers, [Natasha Jaques](https://twitter.com/natashajaques) and [Max Kleiman-Weiner](https://twitter.com/maxhkw), two PhDs from MIT, put together [12 Types of Machine Learning Papers](https://twitter.com/natashajaques/status/1387859601555554304).

[from XKCD with edits by Natasha Jaques](https://preview.redd.it/2mow1e47cpx61.jpg?width=1080&format=pjpg&auto=webp&s=0904984b236700acef5a8ba13b858ebaef0a466e)

 

👁 **Brief** – 15 Graphs You Need to See to Understand AI in 2021: In the last issue we shared the Stanford [2021 AI Index Report](https://hai.stanford.edu/research/ai-index-2021). From that Eliza Strickland takes 15 visualizations to highlight the most important developments. Scroll through on [ieee.org](https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/the-state-of-ai-in-15-graphs).

 

🎓 **Education** – One of the best educational resources for AI is Andrew Ng's Deep Learning  specialization on Coursera. The course just received the 2021 update. All programming exercises are now in Tensorflow 2 and the syllabus includes Transformer Networks. Find the updated course here [coursera.org](https://www.coursera.org/specializations/deep-learning).

 

## EVENTS 

📅 May 12 (online, 16:00 CET) – **How AI turns existing Business Models Upside Down** – ""Join online to interactively hear some astonishing inputs about disruptive AI business models from the Harvard Post Doc Johannes"" from Merantix Labs. Organized by KI-Garage fro the institute of Entrepreneurship and Innovation Research from Stuttgart University and our partner the German Digital Hub Initiative. – Register [here](https://www.bwstiftung.de/de/veranstaltungen/how-ai-turns-existing-business-models-upside-down).",hello url share latest edition url ai newsletter hope interest annoy self promotion read whole thing feel free subscribe url feedback welcome whats important iclr nmbr conferencethe landmark deep learn conference iclr url take place may 3rd may 7th second time gather hold completely virtual researchers around world gather poster sessions keynote workshops nmbr paper nmbr accept conference official outstanding paper award award nmbr paper exceptional quality lead conference find full list url topbots make selection iclr conference paper breakthrough potential include visual transformer vit detr deberta performers paper previously introduce newsletter read breakthrough paper topbots com url comeiclr mark begin annual conference summer global health situation improve slowly conferences move virtual meet find update version ai conference calendar important ai conferences url things find worth share contribute crowdsourcing project common voice aim create free database speech recognition 50 languages since nmbr mozilla ask volunteer record sample sentence review record others recently saw usd nmbr million investment nvidia continuously look participants contribute switch language top right url personal recommendation amai url ceo jürgen stumpp url book real world ai practical guide responsible machine learn provide executives quick thorough overview step necessary successful ai project moreover help soon university graduate prepare work field book url alyssa rochwerger director product blue shield appen cto wilson pang share practical experience present approach claim 3x higher success rate ai project compare industry average learn real world ai practical guide responsible machine learn author interview url paper dino paper emerge properties self supervise vision transformers arxiv url researchers inria facebook ai sorbonne university introduce dino short self di stillation label method segment image self supervise manner mean without label require upfront read dino accompany paw venturebeat com url facebook ai blog ai facebook com url one man paper discussion group yannic kilcher walk viewers paper detail nmbr minutes youtube com url self attention map select head generate use dino videos horse bmx rider puppy fish boat facebook ai url market microsoft acquire nuance april massachusetts company focus ai drive speech recognition usd nmbr billion merger mark second highest acquisition microsoft history linkedin acquire nmbr billion 2016 nuance communications behind speech recognition capabilities apple voice assistant siri axios com url regulation european commission release artificial intelligence act 108 page url proposal regulation ai proposal legislative hurdle ahead politico eu url eu chinese regulators begin enforce data localization local company data store china certain applications facial recognition biometricupdate com url tongue cheek xkcd url post nmbr type scientific paper natasha jaques url max kleiman weiner url two phds mit put together 12 type machine learn paper url xkcd edit natasha jaques url brief nmbr graph need see understand ai 2021 last issue share stanford 2021 ai index report url eliza strickland take nmbr visualizations highlight important developments scroll ieee org url education one best educational resources ai andrew ng deep learn specialization coursera course receive nmbr update program exercise tensorflow nmbr syllabus include transformer network find update course coursera org url events may nmbr online 16 00 cet ai turn exist business model upside join online interactively hear astonish input disruptive ai business model harvard post doc johannes merantix labs organize ki garage fro institute entrepreneurship innovation research stuttgart university partner german digital hub initiative register url
czhu12,MachineLearning,1617908894.0,[P] A torchvision transforms visualizer,"The `torchvision.transforms` library is used in almost all CV projects but the documentation is a bit hard to visualize. I threw [an app together](https://pytorch-transforms-builder.chriszhu.me/) for myself that some of my teammates also found useful so I figured I'd share it with the community.

It's built with Streamlit and hosted on google cloud run, which I've found to be a nice way to deploy these kinds of things.

See it [here](https://pytorch-transforms-builder.chriszhu.me/)

Let me know if this is useful for anyone else. I'm happy to spend a few weekends making it better / more helpful!

https://preview.redd.it/nnez3sv1zzr61.png?width=958&format=png&auto=webp&s=fc42086e7971a9ff0e9b5b3f5af43d673ee068f0

https://preview.redd.it/g1cktqv1zzr61.png?width=955&format=png&auto=webp&s=8131af14c77fc705f5c2c0801a5184ab17bb0e7e",torchvision transform library use almost cv project documentation bite hard visualize throw app together url teammates also find useful figure share community build streamlit host google cloud run find nice way deploy kinds things see url know useful anyone else happy spend weekend make better helpful url
Combination-Fun,MachineLearning,1617887342.0,[R] Video explaining Normalization Free Nets paper,"Here is a video explaining the idea of the NF-Nets paper titled ""High-Performance Large-Scale Image Recognition Without Normalization"". Hope its useful. Enjoy: [https://youtu.be/AzKFgjrbR2o](https://youtu.be/AzKFgjrbR2o)",video explain idea nf net paper title high performance large scale image recognition without normalization hope useful enjoy url
ashubham,MachineLearning,1617060191.0,[P] Prediction Trees in Pure Javascript,"I recently learnt about compact prediction trees and thought, this would be a great use case to do in the web browser. Hence,built this library [https://github.com/ashubham/CPT](https://github.com/ashubham/CPT)

Feedback, welcome.",recently learn compact prediction tree think would great use case web browser hence build library url welcome
PeupleDeLaMer,MachineLearning,1618982723.0,[R] Looking for Paper Recommendations for characterising model performance/ Assurance,"Hello!

I'm relatively new to ML research and I'm reading up on literature that can help a user characterise the performance of a model in order to understand which points in the input space it is considered reliable and which points not. Some examples include [Algorithmic Assurance](https://paperswithcode.com/paper/algorithmic-assurance-an-active-approach-to), and [Adversarial Attacks](https://openai.com/blog/adversarial-example-research/) (this is not exactly what I'm after but on the right track)

I haven't found a huge amount of work related to this so I figured I'd ask :)",hello relatively new ml research read literature help user characterise performance model order understand point input space consider reliable point examples include algorithmic assurance url adversarial attack url exactly right track find huge amount work relate figure ask
techsucker,MachineLearning,1617380494.0,[N] Introducing PyTorch Profiler – The New And Improved Performance Debugging Profiler For PyTorch,"The analysis and refinement of the large-scale deep learning model’s performance is a constant challenge that increases in importance with the model’s size. Owing to a lack of available resources, PyTorch users had a hard time overcoming this problem. There were common GPU hardware-level debugging tools, but PyTorch-specific background of operations was not available. Users had to merge multi-tools or apply minimal correlation information manually to make sense of the data to retrieve the missing information.

The PyTorch Profiler came to the rescue, an open-source tool for precise, efficient, and troubleshooting performance investigations of large-scale deep learning models. 

Summary: [https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/](https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/)

Source: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/",analysis refinement large scale deep learn model performance constant challenge increase importance model size owe lack available resources pytorch users hard time overcome problem common gpu hardware level debug tool pytorch specific background operations available users merge multi tool apply minimal correlation information manually make sense data retrieve miss information pytorch profiler come rescue open source tool precise efficient troubleshoot performance investigations large scale deep learn model summary url url
hyunwoongko,MachineLearning,1617286558.0,"[P] Asian language BART models (English, Chinese, Japanese, Korean and ECJK mixed)","https://preview.redd.it/wznspj3klkq61.png?width=1028&format=png&auto=webp&s=a234ac4af8d1ab75f5f413413d0bf1a1e928084e

Hello. I am hyunwoongko who is studying natural language processing. In many Asian languages such as Chinese, Korean and Japanese, the pre-trained sequence to sequence model is necessary, but it is currently lacking.

&#x200B;

So I made Chinese, Japanese, Korean, and English Bart models by pruning the embedding layers of the facebook mBART model. The mBART model is a multilingual language model with 25 languages. However, if we only need one language (e.g., Japanese), the mBART's vocab and token embeddings are unnecessary but take up space. Therefore, I have organized only the necessary tokens in each language so that this model can be used in the monolingual setting.

&#x200B;

Please check the following link for detailed usage.

[https://github.com/hyunwoongko/asian-bart](https://github.com/hyunwoongko/asian-bart)",url hyunwoongko study natural language process many asian languages chinese korean japanese pre train sequence sequence model necessary currently lack x200b make chinese japanese korean english bart model prune embed layer facebook mbart model mbart model multilingual language model nmbr languages however need one language e g japanese mbart vocab token embeddings unnecessary take space therefore organize necessary tokens language model use monolingual set x200b please check follow link detail usage url
kongxianxingren,MachineLearning,1620585692.0,[D] Are Centroidal Voronoi tessellation and Voronoi tessellation unsupervised learning in machine learning?,"The centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation in which the generating point of each Voronoi cell is also its centroid (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators.

&#x200B;

The Voronoi tessellation is a partition of a plane into regions close to each of a given set of objects.

&#x200B;

The following is the simple picture of centroidal Voronoi tessellations which I found in [wiki](https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation).

&#x200B;

https://preview.redd.it/ircxp2xr35y61.png?width=560&format=png&auto=webp&s=4af24b1912d7f7455e4c03baaa95e57f4a112d03

&#x200B;

For me, it likes the K-means algorithm and it can be concluded as a clustering method in unsupervised learning. Am I right? The reason I ask is that I didn't see anybody related CVT to the machine learning algorithms.",centroidal voronoi tessellation cvt special type voronoi tessellation generate point voronoi cell also centroid center mass view optimal partition correspond optimal distribution generators x200b voronoi tessellation partition plane regions close give set object x200b follow simple picture centroidal voronoi tessellations find wiki url like k mean algorithm conclude cluster method unsupervised learn right reason ask see anybody relate cvt machine learn algorithms
mrathi12,MachineLearning,1618417597.0,[D] Addressing Gender Bias in Neural Machine Translation,"Hey everyone,   
I made a video discussing how we can fix bias in Neural Machine Translation: [https://youtu.be/p21FjLMf0Fw](https://youtu.be/p21FjLMf0Fw)  


Every so often, we get a viral tweet about how when it comes to assigning gender from a gender-neutral language like Hungarian, Google Translate defaults to the stereotypical translations.  
This video looks at how we evaluate gender bias (using datasets such as WinoMT [https://www.aclweb.org/anthology/P19-1164.pdf](https://www.aclweb.org/anthology/P19-1164.pdf)), a discussion of how the current solution by Google Translate adds to intellectual debt, and finally what I believe is the most practical approach to debiasing large language models - Domain Adaptation. [https://arxiv.org/abs/2004.04498](https://arxiv.org/abs/2004.04498)    


Personally I think that curating a large dataset on the scales we need is impractical, and that it's much more practical to fine-tune on small curated datasets: [https://link.springer.com/article/10.1007/s10676-021-09583-1](https://link.springer.com/article/10.1007/s10676-021-09583-1)

Would be interested to hear your thoughts as to what you think the most practical approach for debiasing is.   


Twitter thread discussing this: [https://twitter.com/mukulrathi\_/status/1382094171977216000?s=20](https://twitter.com/mukulrathi_/status/1382094171977216000?s=20)   
Transcript: [https://mukulrathi.co.uk/google-translate-is-biased/](https://mukulrathi.co.uk/google-translate-is-biased/)",hey everyone make video discuss fix bias neural machine translation url every often get viral tweet come assign gender gender neutral language like hungarian google translate default stereotypical translations video look evaluate gender bias use datasets winomt url discussion current solution google translate add intellectual debt finally believe practical approach debiasing large language model domain adaptation url personally think curating large dataset scale need impractical much practical fine tune small curated datasets url interest hear thoughts think practical approach debiasing twitter thread discuss url transcript url
CrypticParagon,MachineLearning,1617351673.0,[D] When does stratified k-fold cross-validation provide worse performance than standard k-fold?,"I used sklearn to train a random forest model with all default parameters. The dataset is about ~550k data points, 7 features. Binary classification, 0 or 1. All 7 features are simple likelihoods of label 1 given a specific value of a categorical feature.

I'm using the built-in cross-validator, 10 folds. The only thing I'm changing is standard vs. stratified, and stratified performs slightly worse. Why would this be?",use sklearn train random forest model default parameters dataset 550k data point nmbr feature binary classification nmbr 1 nmbr feature simple likelihoods label nmbr give specific value categorical feature use build cross validator nmbr fold thing change standard vs stratify stratify perform slightly worse would
jj4646,MachineLearning,1619239890.0,"[D] Relationship Between Kernels, Neural Networks and Gaussian Process"," ""At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit , thus connecting them to kernel methods ""

Can someone please provide an intuitive example as to why there is some sort of connection between neural networks, gaussian processes and kernels?

Thanks

Source: [https://arxiv.org/pdf/1806.07572.pdf](https://arxiv.org/pdf/1806.07572.pdf)",initialization artificial neural network anns equivalent gaussian process infinite width limit thus connect kernel methods someone please provide intuitive example sort connection neural network gaussian process kernels thankssource url
Seankala,MachineLearning,1620131965.0,[D] Do you guys take out time to study math?,"For a few months now I've been taking out time everyday just to study math. I'm in the 4th semester of my MSCS degree, but my undergrad background wasn't in STEM and I always feel like I lack a strong mathematical foundation. Also, when I read papers with a lot of mathematical formulation sometimes I get intuitively understand what the equations mean but often it takes me hours or even days of Googling.

I'm just curious whether anyone else goes out of their way to brush up/study math stuff. Sometimes it feels futile but I try and tell myself otherwise.",months take time everyday study math 4th semester mscs degree undergrad background stem always feel like lack strong mathematical foundation also read paper lot mathematical formulation sometimes get intuitively understand equations mean often take hours even days google curious whether anyone else go way brush study math stuff sometimes feel futile try tell otherwise
jj4646,MachineLearning,1619239577.0,[D] Automatic Feature Engineering during Deep Learning,"I have often heard that one of the reasons that deep learning methods are preferred over other machine learning methods is because algorithms like deep neural networks do not require the analyst to spend as much time ""selecting variables for the model"" (i.e. feature engineering, feature selection, feature extraction).

Apparently, deep neural networks are able to ""intelligently"" (in the background) consider and create many different combinations of ""features"" that are ""conducive"" to the modelling problem.

Naturally, I was curious about this claim. Intuitively, I understand that through the hidden layers, weights and activation functions, neural networks are making ""new combinations"" of features that are passed forward and are ultimately used for making predictions on new data.

Beyond this, I am not sure what to think - are there any references/papers that have documented (either theoretically or empirically) that deep neural networks are able to largely ""take care"" of the task of ""feature engineering"" compared to traditional algorithms like regression models, decision trees and random forest? Have any experiments been done where many irrelevant features were added to a dataset, and a deep neural network was able to ""ignore"" them?",often hear one reason deep learn methods prefer machine learn methods algorithms like deep neural network require analyst spend much time select variables model e feature engineer feature selection feature extraction apparently deep neural network able intelligently background consider create many different combinations feature conducive model problem naturally curious claim intuitively understand hide layer weight activation function neural network make new combinations feature pass forward ultimately use make predictions new data beyond sure think reference paper document either theoretically empirically deep neural network able largely take care task feature engineer compare traditional algorithms like regression model decision tree random forest experiment many irrelevant feature add dataset deep neural network able ignore
NotThatGuy97,MachineLearning,1617633514.0,[D][P] Feedforward Noise Cancelling Project. Looking for some advice before i dive into details.,"  

Hi I am beginner with ML and finished one minor project yet using a simple neural network.

Now I am heading towards a new project which is more complex and searching for good approach.

Since I have not much experience, I want to introduce you the project and hoping for some ideas. 

(Which approach makes sense NN, RNN, RL or alternative approaches?)

The main goal of the project is to find parameters (amplitude (A\_i)  and phase  (P\_i) at fixed frequencies (i) (i=50Hz, 100Hz… ) for a sinusoid ) for a signal generator. The generated signal is then added to the “main” signal in order to cancel out some noise (which consist of these n x 50Hz frequencies). Basically, it is a noise channeling problem and can be solved by a feedforward algorithm.

The input is a measurement (duration 2-8 seconds) of the main signal. If a Fourier transformation (FFT) is performed the amplitudes of the 50Hz harmonics (50Hz,100Hz,150Hz…) are clearly observable (in the FFT). With the correct A\_i and P\_i the amplitude of the frequency i can be suppressed (so the frequency is compensated by the additional introduced signal created by our signal generator). The goal is now to find these perfect A\_i and P\_i for each frequency i.

The first measurements have shown that for each FFT\_amplitude(frequency i), a unique minimum can be found and a gradient towards this minimum has been observed ( simplified: FFT\_amplitude= (A\_i)\^2+(P\_i)\^2) .

The loop for the optimization would look like:

1. Measurement of the main signal --> FFT

2. Input (n x 1 array) : \[FFTamptitude\_1,…., FFTamptitude\_n\]

3. Optimization algorithm (NN, RNN, RL ) ?

4. Output (n x 2 array): \[\[A\_1,P\_1\],\[A\_2,P\_2\],…. ,\[A\_n,P\_n\]\]

5. Signal processing

6. Measurement of the main signal with the added signal

&#x200B;

The Loss function could look like:

Loss = sum ( FFTamptitude\_i ), (trying to minimize the loss function)

&#x200B;

Since FFT\_amplitude= (A\_i)\^2+(P\_i)\^2, a training simulation can be created in order to “pre”-train the algorithm. 

&#x200B;

&#x200B;

I hope I made the situation clear.

One problem to me:

The output is an array of parameters, but the loss function has to be calculated with the measurement of point 6 (see above). I would have to treat the output as a layer in order to get the backpropagation working right? (Please add some advice)

Furthermore, the algorithm has always to wait for the Measurement. Is that an Issue? I want to use Keras.

&#x200B;

What do you think? Are NN, RNN, RL capable to solve this task?

Which would be the best approach? What problems do you see? 

Do you know any comparable project or paper? (Please share it)",hi beginner ml finish one minor project yet use simple neural network head towards new project complex search good approach since much experience want introduce project hop ideas approach make sense nn rnn rl alternative approach main goal project find parameters amplitude _i phase p _i fix frequencies 50hz 100hz sinusoid signal generator generate signal add main signal order cancel noise consist n x 50hz frequencies basically noise channel problem solve feedforward algorithm input measurement duration 2 8 second main signal fourier transformation fft perform amplitudes 50hz harmonics 50hz 100hz 150hz clearly observable fft correct _i p _i amplitude frequency suppress frequency compensate additional introduce signal create signal generator goal find perfect _i p _i frequency first measurements show fft _amplitude frequency unique minimum find gradient towards minimum observe simplify fft _amplitude _i 2 p _i 2 loop optimization would look like 1 measurement main signal fft2 input n x nmbr array fftamptitude _1 fftamptitude _n 3 optimization algorithm nn rnn rl 4 output n x nmbr array _1 p _1 _2 p _2 _n p _n 5 signal processing6 measurement main signal add signal x200b loss function could look like loss sum fftamptitude _i try minimize loss function x200b since fft _amplitude _i 2 p _i 2 train simulation create order pre train algorithm x200b x200b hope make situation clear one problem output array parameters loss function calculate measurement point nmbr see would treat output layer order get backpropagation work right please add advice furthermore algorithm always wait measurement issue want use keras x200b think nn rnn rl capable solve task would best approach problems see know comparable project paper please share
timscarfe,MachineLearning,1618582221.0,[D] Francois Chollet interview on ML Street Talk," In today's show we are joined by Francois Chollet.

He has extremely interesting views on intelligence as generalisation, abstraction and an information conversation ratio. He wrote on the measure of intelligence at the end of 2019 and it had a huge impact on my thinking. He thinks that NNs can only model continuous problems well, which have a smooth learnable manifold and that many ""type 2"" problems which involve reasoning and/or planning are not suitable for NNs. He thinks that many problems have type 1 and type 2 enmeshed together. He thinks that the future of AI must include program synthesis to allow us to generalise broadly from a few examples, but the search could be guided by neural networks because the search space is interpolative to some extent.  

&#x200B;

Video: [https://youtu.be/J0p\_thJJnoo](https://youtu.be/J0p_thJJnoo)

Pod: [https://anchor.fm/machinelearningstreettalk/episodes/51-Francois-Chollet---Intelligence-and-Generalisation-ev1i79](https://anchor.fm/machinelearningstreettalk/episodes/51-Francois-Chollet---Intelligence-and-Generalisation-ev1i79)",today show join francois chollet extremely interest view intelligence generalisation abstraction information conversation ratio write measure intelligence end nmbr huge impact think think nns model continuous problems well smooth learnable manifold many type 2 problems involve reason plan suitable nns think many problems type nmbr type nmbr enmesh together think future ai must include program synthesis allow us generalise broadly examples search could guide neural network search space interpolative extent x200b video url url
anon-burner-5981,MachineLearning,1620412872.0,[D] Parting Gifts for Lab Director and Ph.D. Student,I've spent the last two years working for an amazing lab. Next year I'll be leaving. They have had a tremendous impact on me and I'd like to be able to give them something to show my appreciation. What would be good gifts for a mentor and Ph.D. student?,spend last two years work amaze lab next year leave tremendous impact like able give something show appreciation would good gift mentor ph student
Square365,MachineLearning,1617649803.0,[D] Good video dataset labeling services? (Frame By Frame),"Hello i'm sort of new to ML and I have been trying to make a dataset to train image recognition with yolov4 for a few weeks now. Im using DarkLabel and i have an aproximate of 24k frames from 34 videos labeled, im planning to label another 500 but I would like to know if there are any good services that label at that rate. I have seen amazon mturk but they only do box in images and in videos they only do the topic. 

I got an estimate from someone willing to do 10videos/5$, but I would like to get your opinions on this.",hello sort new ml try make dataset train image recognition yolov4 weeks im use darklabel aproximate 24k frame nmbr videos label im plan label another nmbr would like know good service label rate see amazon mturk box image videos topic get estimate someone 10videos 5 would like get opinions
AristocraticOctopus,MachineLearning,1619491330.0,[N] Toyota subsidiary to acquire Lyft's self-driving division,"After Zoox's sale to Amazon, Uber's layoffs in AI research, and now this, it's looking grim for self-driving commercialization. I doubt many in this sub are terribly surprised given the difficulty of this problem, but it's still sad to see another one bite the dust.

Personally I'm a fan of Comma.ai's (technical) approach for human policy cloning, but I still think we're dozens of high-quality research papers away from a superhuman driving agent.

Interesting to see how people are valuing these divisions:
>Lyft will receive, in total, approximately $550 million in cash with this transaction, with $200 million paid upfront subject to certain closing adjustments and $350 million of payments over a five-year period. The transaction is also expected to remove $100 million of annualized non-GAAP operating expenses on a net basis - primarily from reduced R&D spend - which will accelerate Lyft’s path to Adjusted EBITDA profitability.",zoox sale amazon uber layoffs ai research look grim self drive commercialization doubt many sub terribly surprise give difficulty problem still sad see another one bite dust personally fan comma ai technical approach human policy clone still think dozens high quality research paper away superhuman drive agent interest see people value divisions lyft receive total approximately 550 million cash transaction 200 million pay upfront subject certain close adjustments 350 million payments five year period transaction also expect remove 100 million annualized non gaap operate expense net basis primarily reduce r spend accelerate lyfts path adjust ebitda profitability
Symbiot10000,MachineLearning,1618176734.0,[D] Practical benefits of unlocking vGPU functionality in NVIDIA cards for ML?,"Over the weekend there has been a lot of [attention](https://news.ycombinator.com/item?id=26754351) [given](https://old.reddit.com/r/hardware/comments/mnord0/unlock_vgpu_functionality_for_consumer_grade_gpus/) to a [new software method](https://github.com/DualCoder/vgpu_unlock) to unlock vGPU functionality in consumer NVIDIA cards.

What does this mean, if anything, for smaller ML outfits that could now divide up GPU resources across VMs and subdivide the GPU across different ML tasks?",weekend lot attention url give url new software method url unlock vgpu functionality consumer nvidia card mean anything smaller ml outfit could divide gpu resources across vms subdivide gpu across different ml task
Competitive-Rub-1958,MachineLearning,1618587934.0,[D] Can some other organization/company replicate GPT-3 for their own use?,"GPT-3 in itself does not create much of a new 'innovation' per se, being an overfitted model on a huge amount of data and a large number of parameters, probably based on it's predecessors architecture more or less.

So, is it easy for companies like Google (FAANG companies) to replicate GPT-3 sized NLP models for their own use?

But the Million dollar question - if it is indeed easy for them to do so, how much advantage does OpenAi have against them?

what prevents google from launching a new Model bigger and better, and offering it to consumers cheaper than OpenAi+MS can?",gpt 3 create much new innovation per se overfitted model huge amount data large number parameters probably base predecessors architecture less easy company like google faang company replicate gpt 3 size nlp model use million dollar question indeed easy much advantage openai prevent google launch new model bigger better offer consumers cheaper openai ms
lukeiy,MachineLearning,1617422679.0,[P] A TF implementation of AdamW with a One-Cycle policy,"&#x200B;

https://preview.redd.it/3wuc17rluvq61.png?width=677&format=png&auto=webp&s=7b5955c84bc3fb8e049fd2079eac6c2e0559cc78

[https://github.com/LukeBolly/OneCycleAdamW](https://github.com/LukeBolly/OneCycleAdamW)

When the article from [fast.ai](https://fast.ai) was originally posted, the reception was.... mixed. I've recently implemented it though, and for my models it sped up training and remains stable at higher learning rates. I added a small example to compare it against regular Adam + AdamW. Let me know if you have any issues.

Cheers!",x200b url article fast ai url originally post reception mix recently implement though model speed train remain stable higher learn rat add small example compare regular adam adamw let know issue cheer
proximauri,MachineLearning,1616976163.0,[D] Face recognition: classification vs distances between embeddings,"Hi,  I am new to face recognition methods. I have noticed that one popular approach to use pre-trained models on new datasets is to measure distances between two embeddings.

I don't understand however why this is preferred over using a pre-trained network without the top layer and just fine tune it with a new fully connected layer on top of the model.

Honestly I don't even understand how to train the model with the first approach. Could someone clarify it to me please?",hi new face recognition methods notice one popular approach use pre train model new datasets measure distance two embeddings understand however prefer use pre train network without top layer fine tune new fully connect layer top model honestly even understand train model first approach could someone clarify please
alexandrea_pierrick,MachineLearning,1617958906.0,[D] Unsupervised document similarity state of the art,"I have a set of N documents with lengths ranging from 0 to more than 20000 characters. I want to calculate a similarity score between 0 and 1 between all pairs of documents where a higher number indicates higher similarity. Assume below that deploying a supervised model is infeasible due to resource constraints that are not necessarily data science related (gathering labels is expensive, infrastructure cannot be approved for supervised models for whatever reason etc).

&#x200B;

\*\*Approaches I have considered\*\*:

1. tf-idf

2. Smooth Inverse Frequency (SIF) embeddings and its developments (uSIF, p-SIF). [https://openreview.net/pdf?id=SyK00v5xx](https://openreview.net/pdf?id=SyK00v5xx) [https://www.aclweb.org/anthology/W18-3012/](https://www.aclweb.org/anthology/W18-3012/) [https://arxiv.org/abs/2005.09069](https://arxiv.org/abs/2005.09069)

3. BERT or bert-like embeddings, e.g., [https://arxiv.org/abs/2010.06467](https://arxiv.org/abs/2010.06467)

4. Hierarchical Optimal Transport for Document Representation (HOTT): [https://papers.nips.cc/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html](https://papers.nips.cc/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html)

&#x200B;

\*\*Question\*\*:

Is there an unsupervised technique that has been shown in a peer-reviewed setting to achieve higher accuracy (or F1 or similar) on long texts (more than, say, 10000 characters) than HOTT?

&#x200B;

\*\*Background\*\*:

The HOTT paper benchmarks various approaches with a k-NN classifier and shows that HOTT performs best, but not dramatically better than tf-idf (HOTT has 0.52 vs tf-idf's 0.66 normalized error). Note that while the HOTT algorithm is unsupervised the datasets in the paper are labeled, otherwise a benchmark would not be possible.

The SIF papers mostly deal with the STS datasets which are not long texts. p-SIF has a benchmark on the Reuters dataset, but uses a SVM supervised approach. Interestingly, the HOTT paper finds that SIF does not perform well with the k-NN approach with 0.79 normalized error.

In many cases BERT requires pre-training and if it does not, its max or average pooled performance (pooling over BERT layers) appears to be worse than glove embeddings ([https://arxiv.org/abs/2010.06467](https://arxiv.org/abs/2010.06467) page 114).

I have also not been able to find unsupervised benchmarks for Doc2Vec, Universal Sentence Encoder (USE).

&#x200B;

There is additionally the question of how to calculate the similarity once the embedding is obtained (e.g., [https://www.aclweb.org/anthology/N19-1100.pdf](https://www.aclweb.org/anthology/N19-1100.pdf)) but that is out of scope for this question unless it affects comparison between unsupervised benchmarks (e.g., the k-NN approach can use various distance metrics which may affect accuracy).

&#x200B;

If the benchmarks in HOTT are representative and no other methods exist that perform substantially better it is tempting to make the conclusion that tf-idf is still a strong approach since it is so simple to implement and understand (it is certainly simpler than HOTT). If that is the case I think it is a remarkable conclusion given the deep learning developments in the last 5-10 years.

edit: added ""(pooling over BERT layers)""",set n document lengths range nmbr nmbr character want calculate similarity score nmbr nmbr pair document higher number indicate higher similarity assume deploy supervise model infeasible due resource constraints necessarily data science relate gather label expensive infrastructure approve supervise model whatever reason etc x200b approach consider 1 tf idf2 smooth inverse frequency sif embeddings developments usif p sif url url url bert bert like embeddings e g url hierarchical optimal transport document representation hott url unsupervised technique show peer review set achieve higher accuracy f1 similar long texts say nmbr character hott x200b background hott paper benchmarks various approach k nn classifier show hott perform best dramatically better tf idf hott nmbr vs tf idf nmbr normalize error note hott algorithm unsupervised datasets paper label otherwise benchmark would possible sif paper mostly deal sts datasets long texts p sif benchmark reuters dataset use svm supervise approach interestingly hott paper find sif perform well k nn approach nmbr normalize error many case bert require pre train max average pool performance pool bert layer appear worse glove embeddings url page 114 also able find unsupervised benchmarks doc2vec universal sentence encoder use x200b additionally question calculate similarity embed obtain e g url scope question unless affect comparison unsupervised benchmarks e g k nn approach use various distance metrics may affect accuracy x200b benchmarks hott representative methods exist perform substantially better tempt make conclusion tf idf still strong approach since simple implement understand certainly simpler hott case think remarkable conclusion give deep learn developments last 5 10 years edit add pool bert layer
noodlepotato,MachineLearning,1619701885.0,[D]Anyone reading Probabilistic Machine Learning: An Introduction? (Murphy's new textbook),"(Already posted in /r/learnmachinelearning but no answer)

How is it so far? Is there any additional/difference from the other edition, like notation-wise, I heard the first edition has pretty inconsistent notations. I'm asking because I'm planning to buy the 2012 Machine learning: a probabilistic perspective physical book but after checking the table of contents of the new one, I might consider buying a physical book of this edition instead. Just want to know what others think who's reading it now",already post r learnmachinelearning answer far additional difference edition like notation wise hear first edition pretty inconsistent notations ask plan buy nmbr machine learn probabilistic perspective physical book check table content new one might consider buy physical book edition instead want know others think read
Guest_Basic,MachineLearning,1617344944.0,[D] AUC vs F-measure for binary classification for unbalanced target variable,"I'm working on building a binary classification predictive model. My target variable is extremely unbalanced (50K 1s and 14Million 0s). 

As I understand AUC is not a very good metric to evaluate my model and F-measure might be a better alternative. 

Question#1: is my assumption correct?


The model I built has a decent AUC (~0.85) and a really low F-measure (0.3). This makes me think that I've actually built a really bad model. However, all the existing published literature only claim an AUC of 0.6 and none of them report F-measure.

Question#2: If my goal is to build a better model than what currently exists... should I taking a victory lap right now?",work build binary classification predictive model target variable extremely unbalance 50k 1s 14million 0s understand auc good metric evaluate model f measure might better alternative question 1 assumption correct model build decent auc 0 85 really low f measure 0 3 make think actually build really bad model however exist publish literature claim auc nmbr none report f measure question 2 goal build better model currently exist take victory lap right
amourav,MachineLearning,1619529111.0,[D] Distributed data platform / framework,What are your thoughts on AWS Sagemaker and/or Horovod for distributed data training? What is your method of choice?,thoughts aws sagemaker horovod distribute data train method choice
moajjem04,MachineLearning,1618508700.0,[D] Best Literature Review Practices,"My supervisor has assigned me a new topic to research on. Typically in any project, I mostly handle the experiment part. This time I thought I should do literature review as well. I know it is a huge deficiency on my part but I want to know about your best practices, dos and don'ts beyond searching google scholar with keywords.

tl;dr: help me senpai by providing me with your knowledge!🙌🙌🙌",supervisor assign new topic research typically project mostly handle experiment part time think literature review well know huge deficiency part want know best practice ts beyond search google scholar keywords tl dr help senpai provide knowledge
SlickBlueML,MachineLearning,1619025945.0,[P] Multilingual translation like Google Translate with PyTorch (+explanation),"I recently put together a tutorial for multilingual translation, but I think the code alone might be useful to people. It includes a demo you can play around with if you get it running and supports Chinese, Japanese, and English right off the bat. With the small amount of training it works surprisingly well.

I have it in a Colab so you don't need to worry about setup if you use that. If you do use the non-pro version of Colab though, make sure to change the model repo from  `google/mt5-base` to `google/mt5-small` or else you will get out of memory errors from CUDA.

Github code: [https://github.com/ejmejm/multilingual-nmt-mt5/blob/main/nmt\_full\_version.ipynb](https://github.com/ejmejm/multilingual-nmt-mt5/blob/main/nmt_full_version.ipynb)

Colab code: [https://colab.research.google.com/drive/1eGSCod03SjWD\_YOfwb33kMJOeDZGu7lP?usp=sharing](https://colab.research.google.com/drive/1eGSCod03SjWD_YOfwb33kMJOeDZGu7lP?usp=sharing)

There is also a video series that goes with it for anyone interested: [https://www.youtube.com/watch?v=HuZq5KkLx8Q&list=PL\_49VD9KwQ\_ObGMW5g9hMOLnDY01NHv91&index=2&t=1s](https://www.youtube.com/watch?v=HuZq5KkLx8Q&list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&index=2&t=1s)",recently put together tutorial multilingual translation think code alone might useful people include demo play around get run support chinese japanese english right bat small amount train work surprisingly well colab need worry setup use use non pro version colab though make sure change model repo google mt5 base google mt5 small else get memory errors cuda github code url code url also video series go anyone interest url
wecmiw,MachineLearning,1618074938.0,[D] How do you share your models/ demos with others?,"Hi, I am a Phd student working with deep learning  and I often have to show demos of my models. I currently simply share results in a presentation and use notebooks when I want to show more explicit experiments on the spot.  Is there another way to do that? I could see this being done a lot especially in industry where you have to talk to many stakeholders?

I am aware of gradio but I was wondering if there are other ways.

Any insight is appreciated! :)",hi phd student work deep learn often show demo model currently simply share result presentation use notebooks want show explicit experiment spot another way could see lot especially industry talk many stakeholders aware gradio wonder ways insight appreciate
hubert0527,MachineLearning,1617996004.0,[R] InfinityGAN: Towards Infinite-Resolution Image Synthesis,"[\\""Synthesizing infinite-resolution images from finite-resolution inputs.\\"" A 1024×2048 image composed of 242 patches, independently synthesized by InfinityGAN with spatial fusion of two styles. The generator is trained on 101×101 patches \(e.g., marked in top-left\) sampled from 197×197 real images. Note that training and inference \(of any resolution\) are performed on a single GTX TITAN X GPU.](https://preview.redd.it/4drmg9ez47s61.png?width=1285&format=png&auto=webp&s=5d5a1112aa090c1707e9e64a1e49c757480b2e3b)

***TL;DR***  We propose InfinityGAN towards a new problem of synthesizing infinite-resolution images. The model is trained with images of limited resolution, and generalizes to arbitrary resolutions at testing. We further demonstrate several applications with a trained generator in spatial style fusion, image outpainting, and image inbetweening.

**Project page**: [https://hubert0527.github.io/infinityGAN/](https://hubert0527.github.io/infinityGAN/)

**Paper**: [https://arxiv.org/abs/2104.03963](https://arxiv.org/abs/2104.03963)

We will release our code soon here: [https://github.com/hubert0527/infinityGAN](https://github.com/hubert0527/infinityGAN)",synthesize infinite resolution image finite resolution input 1024 2048 image compose nmbr patch independently synthesize infinitygan spatial fusion two style generator train 101 101 patch e g mark top leave sample 197 197 real image note train inference resolution perform single gtx titan x gpu url propose infinitygan towards new problem synthesize infinite resolution image model train image limit resolution generalize arbitrary resolutions test demonstrate several applications train generator spatial style fusion image outpainting image inbetweening project page url url release code soon url
SensitiveAnteater420,MachineLearning,1616960506.0,[D] Would it be possible to make a model that predicts where a picture is taken?,"I love OSINT and geolocating, and I'm a Python dev wondering if I can extend my boundaries. The title is self explaining: Is it possible to make a model that predicts where a picture is taken, and are there existing datasets for it?",love osint geolocating python dev wonder extend boundaries title self explain possible make model predict picture take exist datasets
InsightFinder,MachineLearning,1617028825.0,[News] Women in computer science leadership talk on 4/9,"**Event alert - Women in Tech Leadership discussion with execs from Bank of America, Dell, and InsightFinder!**

On April 9, 2021 at 11 AM EST, **InsightFinder** **CTO and Founder Helen Gu** will participate in Women in Tech Leadership panel hosted by NC State's Women in Computer Science (WiCS) organization. Professor Gu teaches at NC State and is also a faculty leader of WiCS.  She will be joined by industry leaders Liz Holland, Vice President of Dell Technologies, Betsy Brady, Managing Director at Bank of America and moderated by NC State professor and WiCS faculty leader Dr. Lina Battestilli.

These leaders have agreed to share their stories - including but not limited to early career decisions, watershed career moments, and how they would advise young students and professionals today.  All are welcome to attend. **To join, register** [here (website form)](https://insightfinder.com/events/) **or** [here (google form)](https://docs.google.com/forms/d/e/1FAIpQLSe0HyYVPgLjvQiwOBls5IFMcZtdgRYnRZ3Yot8mN8Cn2Wr35g/viewform)**.**",event alert women tech leadership discussion execs bank america dell insightfinder april 9 nmbr nmbr est insightfinder cto founder helen gu participate women tech leadership panel host nc state women computer science wics organization professor gu teach nc state also faculty leader wics join industry leaders liz holland vice president dell technologies betsy brady manage director bank america moderate nc state professor wics faculty leader dr lina battestilli leaders agree share stories include limit early career decisions watershed career moments would advise young students professionals today welcome attend join register website form url google form url
asivokon,MachineLearning,1617719141.0,[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language,"This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.

This blog post provides some context: [https://www.grammarly.com/blog/engineering/announcing-ua-gec/](https://www.grammarly.com/blog/engineering/announcing-ua-gec/)

The data and code are on Github: [https://github.com/grammarly/ua-gec](https://github.com/grammarly/ua-gec)

Paper (draft): [https://arxiv.org/abs/2103.16997](https://arxiv.org/abs/2103.16997)

I am one of the authors. Happy to answer your questions :)",dataset contain 20 715 sentence annotate grammatical errors fluency correction license cc 4 0 blog post provide context url data code github url draft url one author happy answer question
shreyansh26,MachineLearning,1620580380.0,[P] BERT - Annotated Paper + Paper Summary,"Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.

As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!

Paper Summary -  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf)",everyone interest nlp even dl ml matter definitely hear bert family model bert roberta distilbert many many paper bert pre train deep bidirectional transformers language understand first introduce completely change way ai practitioners solve look nlp problems days part paper note series go paper create informative summary paper time go bite longer previous paper summaries paper contain many tiny interest nuggets include check link happy read paper summary bert pre train deep bidirectional transformers language understand url paper url
techsucker,MachineLearning,1618972153.0,[R] Researchers Introduce a Convolutional Neural Network (CNN)-Based Model that Automates the Distinction Between Natural Images and Computer-Generated Images (CGI),"With the increasing performance accuracy of computer software systems, the realistic appearance of computer-generated images (CGI) and deepfakes often leads to assuming them as authentic images.

Researchers at the Changsha University of Science and Technology and Hunan University in Hunan, China, have recently developed an image source pipeline forensics method based on convolutional neural networks (CNN) to automate the distinction between natural images and CGI. The work announced in the *International Journal of Autonomous and Adaptive Communications Systems* describes that the CNN-based model is fine-tuned using a database of 10000 images.

Summary: [https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/](https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/) 

Paper: http://www.inderscience.com/offer.php?id=114295",increase performance accuracy computer software systems realistic appearance computer generate image cgi deepfakes often lead assume authentic image researchers changsha university science technology hunan university hunan china recently develop image source pipeline forensics method base convolutional neural network cnn automate distinction natural image cgi work announce international journal autonomous adaptive communications systems describe cnn base model fine tune use database nmbr image summary url paper url
Yuqing7,MachineLearning,1617064776.0,[N] DeepMind & Alberta U Introduce Novel Search Algorithm: Policy-Guided Heuristic Search with Guarantees,"A research team from DeepMind and Alberta University proposes Policy-guided Heuristic Search (PHS), a novel search algorithm that uses both a heuristic function and a policy while offering guarantees on the search loss that relate to both the quality of the heuristic and the policy.

Here is a quick read: [DeepMind & Alberta U Introduce Novel Search Algorithm: Policy-Guided Heuristic Search with Guarantees](https://syncedreview.com/2021/03/29/deepmind-alberta-u-introduce-novel-search-algorithm-policy-guided-heuristic-search-with-guarantees/)

The paper *Policy-Guided Heuristic Search with Guarantees* is on [arXiv](https://arxiv.org/pdf/2103.11505.pdf).",research team deepmind alberta university propose policy guide heuristic search phs novel search algorithm use heuristic function policy offer guarantee search loss relate quality heuristic policy quick read deepmind alberta u introduce novel search algorithm policy guide heuristic search guarantee url paper policy guide heuristic search guarantee arxiv url
Xxyjoel,MachineLearning,1617007872.0,ML + Infrastructure [P],"Hey All,

I   have been in the data science and machine learning space for the   majority of my career, but have more recently spent time in meddling around with infrastructure.  Could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so I built a tool to help manage costs.

It still requires some policy finagling, so it's not self service yet, however, I'd love y'alls candid feedback on the tool - [BlueArch.io](https://bluearch.io/)

Apologies  if this is against the sub's rules... sharing your work can be scary but I'm pretty excited about project.",hey data science machine learn space majority career recently spend time meddle around infrastructure could naivety though complexity inefficiency bundle cloud tool bother build tool help manage cost still require policy finagle self service yet however love alls candid feedback tool bluearch io url sub rule share work scary pretty excite project
thermokopf,MachineLearning,1617657632.0,[D] Can the same convolutional network be used on different image sizes?,"Convolutional networks perform ""convolutions"" on an image, or array of pixels, but basically decreases the size of the image, and then inputs this to a typical feedforward network.

How does this generally handle different image sizes?

For example I train a network on a bunch of 28x28 images. Will I be able to use this network to make predictions on 64x64 images or some other size?",convolutional network perform convolutions image array pixels basically decrease size image input typical feedforward network generally handle different image size example train network bunch 28x28 image able use network make predictions 64x64 image size
flaviojuvenal,MachineLearning,1619445763.0,[P] Entity Embed: fuzzy and scalable Entity Resolution using Approximate Nearest Neighbors,"[https://github.com/vintasoftware/entity-embed](https://github.com/vintasoftware/entity-embed)

Entity Embed is based on and is a special case of the [AutoBlock model described by Amazon](https://www.amazon.science/publications/autoblock-a-hands-off-blocking-framework-for-entity-matching).

It allows you to transform entities like companies, products, etc. into vectors to support **scalable Record Linkage / Entity Resolution using Approximate Nearest Neighbors.**

Using Entity Embed, you can train a deep learning model to transform records into vectors in an N-dimensional embedding space. Thanks to a contrastive loss, those vectors are organized to keep similar records close and dissimilar records far apart in this embedding space. Embedding records enables [scalable ANN search](http://ann-benchmarks.com/index.html), which means finding thousands of candidate duplicate pairs of records per second per CPU.

This is the first Deep Learning project we launch. Hope it's useful! Please feel free to reach me with feedbacks.",url embed base special case autoblock model describe amazon url allow transform entities like company products etc vectors support scalable record linkage entity resolution use approximate nearest neighbor use entity embed train deep learn model transform record vectors n dimensional embed space thank contrastive loss vectors organize keep similar record close dissimilar record far apart embed space embed record enable scalable ann search url mean find thousands candidate duplicate pair record per second per cpu first deep learn project launch hope useful please feel free reach feedbacks
,MachineLearning,1619991678.0,"[D] What is the reason behind the recent explosion in NLP-based research, jobs, and tools?","I've noticed that NLP has been having quite a renaissance moment for the  past 5 years or so. To me, it kinda came out of nowhere, but I'm sure experts saw it coming. Anyways, I see so much NLP research being published, as well  as every company seeking people with NLP knowledge? Why is this? What  has been the cause that's led to this explosion of NLP in the past 5-6 years?",notice nlp quite renaissance moment past nmbr years kinda come nowhere sure experts saw come anyways see much nlp research publish well every company seek people nlp knowledge cause lead explosion nlp past 5 6 years
ObjectiveDue9905,MachineLearning,1617205094.0,[D] How important is controls theory in machine learning?,"Hi,  I was wondering how important is control theory in machine learning as  they seem to go hand in hand in some applications. I'm currently a  computer engineering major student and have taken mostly only CS courses  when it comes to machine learning but control theory is a course in my ECE (Electrical and Computer Engineering) department that I  was interested in taking as it seems to have some applications that I  could see myself using with hardware and software in the future.",hi wonder important control theory machine learn seem go hand hand applications currently computer engineer major student take mostly cs course come machine learn control theory course ece electrical computer engineer department interest take seem applications could see use hardware software future
samk2104,MachineLearning,1617093257.0,[D] Not all independent variables available for same time period..how to handle such situations for ML models?," I have 9/10 independent variables for which information is available for last 2 years (including dependent variable). For 1/10 variables the information is only available for last 2 months as this feature was recently launched. The simplest way would be to build a model on 2 months for which all variables are available, but number of data points are not enough.

Is there a way I can incorporate the new feature in model i.e. using 2 months data, but for other 9/10 features I still build model using 2 years of data?",9 10 independent variables information available last nmbr years include dependent variable 1 10 variables information available last nmbr months feature recently launch simplest way would build model nmbr months variables available number data point enough way incorporate new feature model e use nmbr months data 9 10 feature still build model use nmbr years data
NotAHomeworkQuestion,MachineLearning,1616962377.0,"[D] Instead of taking an approach like Invariant Risk Minimization, why is it not enough to control for environmental factors (confounders) by including them as regressors?","I've only just started diving into this fascinating topic so please excuse my ignorance. I really enjoyed reading the IRM paper but it left me wondering why we couldn't accomplish something similar by including the environmental variables as regressors as people do in causal inference? For example, in the MNIST coloring application, we could have the final layer of our model take the top layer of the usual plain-vanilla CNN as well as an indicator for the color of the image. We have thus 'controlled for' color confounding in our image so that the CNN part of our model architecture accounts for everything but that. As we are worried about shenanigans with future data points having the effect of color being reversed, we thus create prediction on future data points by ignoring the color effect on the prediction. This (I think) would give similar results to graying out the image which was shown to give excellent performance. What am I missing here?",start dive fascinate topic please excuse ignorance really enjoy read irm paper leave wonder accomplish something similar include environmental variables regressors people causal inference example mnist color application could final layer model take top layer usual plain vanilla cnn well indicator color image thus control color confound image cnn part model architecture account everything worry shenanigans future data point effect color reverse thus create prediction future data point ignore color effect prediction think would give similar result gray image show give excellent performance miss
rom1504,MachineLearning,1616491364.0,[P] fromconfig: A library to instantiate any Python object from configuration files.," [fromconfig](https://github.com/criteo/fromconfig) acts as a generic command line interface from configuration files *with absolutely no change to the code*.

[fromconfig](https://preview.redd.it/x953l8lzwqo61.png?width=755&format=png&auto=webp&s=c517361d86fe26d47f92ba43757161307c68d651)

It is particularly well suited for **Machine Learning** (see [examples](https://github.com/criteo/fromconfig#machine-learning)). Launching training jobs on remote clusters requires custom command lines, with arguments that need to be propagated through the call stack (e.g., setting parameters of a particular layer). The usual way is to write a custom command with a reduced set of arguments, combined by an assembler that creates the different objects. With fromconfig, the command line becomes generic, and all the specifics are kept in config files. As a result, this preserves the code from any backwards dependency issues and allows full reproducibility by saving config files as jobs' artifacts. It also makes it easier to merge different sets of arguments in a dynamic way through references and interpolation.",fromconfig url act generic command line interface configuration file absolutely change code fromconfig url particularly well suit machine learn see examples url launch train job remote cluster require custom command line arguments need propagate call stack e g set parameters particular layer usual way write custom command reduce set arguments combine assembler create different object fromconfig command line become generic specifics keep config file result preserve code backwards dependency issue allow full reproducibility save config file job artifacts also make easier merge different set arguments dynamic way reference interpolation
cowgod2007,MachineLearning,1617853595.0,[D] Sourcing medical data?,"Hey all!

For ML projects, how does one source / buy / obtain medical labeled data for training data? e.g. labeled radiology imaging data

Thanks!",hey ml project one source buy obtain medical label data train data e g label radiology image datathanks
JST99,MachineLearning,1619188054.0,[D] How to properly version control ML models amid rapid experimentation?,"Coming from a software engineer background, I've been using git to version control my files, along with a `config.json` that includes all model and training hyperparameters. A trainer class would read a specified configuration file, train the model, and save the best one with the highest validation score.

This pipeline worked in the beginning when I was iterating at a slow pace. Recently, however, I'm increasingly realizing that saved checkpoint files are obsolete because non-trivial changes have been made to the model architecture since the last experiment. By checkpoints, I'm really referring to `torch.load(some_state_dict)`, but my question is framework agnostic.

I could, of course, check when the experiment was conducted and git checkout the repository to that specific point in time. However, part of me believes that this is an incredibly common thing for ML engineers, and that there must be a more elegant solution. So far, my research has brought me to

* [Sacred](https://sacred.readthedocs.io/en/latest/index.html), which I got from [this post](https://www.reddit.com/r/MachineLearning/comments/3npg0d/how_to_keep_track_of_experiments/) 
* [Keepsake](https://keepsake.ai)
* [DVC](https://dvc.org), which appears tangential since my question pertains more to models, not data

Thanks in advance for sharing your insight!",come software engineer background use git version control file along config json include model train hyperparameters trainer class would read specify configuration file train model save best one highest validation score pipeline work begin iterate slow pace recently however increasingly realize save checkpoint file obsolete non trivial change make model architecture since last experiment checkpoints really refer torch load some_state_dict question framework agnostic could course check experiment conduct git checkout repository specific point time however part believe incredibly common thing ml engineer must elegant solution far research bring sacred url get post url keepsake url dvc url appear tangential since question pertain model datathanks advance share insight
charles96322,MachineLearning,1618968095.0,[D] How Valuable Would Cutting Your ML Models Computation Time (at Inference) By 30-50% Be?,"Hello everyone,

I'm currently working on a personal project where I try to optimize deep learning algorithms with respect to the hardware on which it is deployed and so far I've had some pretty decent results; The idea is that some hardware handles some computation better than other. You can then build a simulator that shows you the utilization of each blocks in your neural network and arrange your model so that it keeps the same capacity, but improves the computation time significantly. (e.g. see this paper https://arxiv.org/pdf/2003.02838.pdf)

Eventually, the goal would be to select a piece of hardware on which your model will be deployed on and optimize your model by the click of a button.

I'm now wondering if a solution like that would made any business sense, and I'd love to know more about your use cases.

So, for those of you who are deploying machine learning models to edge devices:

1) What kind of application do you deploy on IoT devices and is latency typically a problem?

2) How much time do you spend optimizing a model for computation time?

3) How valuable would cutting computation time by 30-50% at inference be to you?

I'm wondering if there are applications (e.g. robotics/medical/...) where this 30-50% would become very valuable and I'd love to speak with someone working in such a field",hello everyone currently work personal project try optimize deep learn algorithms respect hardware deploy far pretty decent result idea hardware handle computation better build simulator show utilization block neural network arrange model keep capacity improve computation time significantly e g see paper url goal would select piece hardware model deploy optimize model click button wonder solution like would make business sense love know use case deploy machine learn model edge devices 1 kind application deploy iot devices latency typically problem 2 much time spend optimize model computation time 3 valuable would cut computation time 30 50 inference wonder applications e g robotics medical 30 50 would become valuable love speak someone work field
ijovab,MachineLearning,1620598087.0,[D] What are some promising areas in privacy-preserving learning in medical data?,"So with the EU's new proposal, and general problems related to the usage of medical data the topic seems to be becoming fairly important. I've been reading up on federated learning, continual learning, and differential privacy recently. What do you think are some of the most promising areas to simplify and guarantee the safety of the medical image data during training?   


Any paper suggestions also appreciated.",eu new proposal general problems relate usage medical data topic seem become fairly important read federate learn continual learn differential privacy recently think promise areas simplify guarantee safety medical image data train paper suggestions also appreciate
kamil-rafalko,MachineLearning,1618305825.0,[Project] Better neurobiological research with AI,"How to do instance segmentation of star-shaped cells called astrocytes in 3D with already-known computer vision techniques? This task may seem simple at first, but turns unexpectedly tricky... I've shared details about the process of finding a solution to this problem in a blog post: [https://blog.softwaremill.com/better-neurobiological-research-with-ai-d91eacaf7976](https://blog.softwaremill.com/better-neurobiological-research-with-ai-d91eacaf7976)

I hope it would be interesting for you. Any comments are welcome.",instance segmentation star shape cells call astrocytes 3d already know computer vision techniques task may seem simple first turn unexpectedly tricky share detail process find solution problem blog post url hope would interest comment welcome
proximauri,MachineLearning,1617183404.0,"[D] Is uploading dataset to personal Google Drive and use it in Google Colab against ""No distribution agreement""?","Hi, so I a have a machine learning database, for which I have accepted an agreement. In agreements, it is stated that distribution of data to any third party is prohibited.  


My question is now, if uploading dataset to my personal GDrive and load it in Google Colab is against this? I don't want to share it with anyone.",hi machine learn database accept agreement agreements state distribution data third party prohibit question upload dataset personal gdrive load google colab want share anyone
dh27182,MachineLearning,1619591958.0,[D] Open source projects for interpretability,"Are there any good open source projects for model interpretability?

I've been catching up with several [distill.pub](https://distill.pub) articles recently and the authors show some impressive visualizations. Unfortunately, it seems to not be open source. Wonder if anyone's used/built any projects for visualizing and inspecting models.",good open source project model interpretability catch several distill pub url article recently author show impressive visualizations unfortunately seem open source wonder anyone use build project visualize inspect model
ykilcher,MachineLearning,1619540955.0,[P] We gave GPT-3 random ingredients and cooked the recipe it came up with (Video),"[https://youtu.be/hIoCn\_9QTVU](https://youtu.be/hIoCn_9QTVU)

We went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.

&#x200B;

Our Rules:

1. All Vegan

2. Follow the recipe as closely as possible

3. We must finish our plates

&#x200B;

The Recipe:

1. Boil the potatoes and carrots.

2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. 

3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. 

4. Add the soy cream, stir and cook for three minutes. 

5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. 

6. Cut the bread in small squares and fry in the vegan butter until golden brown.

7. Cut the limes into cubes and squeeze the juice into the bean mixture. 

8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.

9. Pour the bean mix into a blender. 

10. Bake for 5 minutes in the oven at 180C. 

11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. 

12. Cut the bell pepper into cubes and add to the pot. 

13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. 

14. Add the avocado. 

15. Add the chickpeas. 

16. Add the chocolate.

17. Serve on bread with mustard and pommegrenade on top.

&#x200B;

VIDEO OUTLINE:

0:00 - The Plan

2:15 - Ingredients

4:05 - What is GPT-3?

6:10 - Let's cook

12:25 - The Taste Test

&#x200B;

GPT-3 on Wikipedia: [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)

GPT-3 Paper: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)",url go store buy set completely random ingredients openai gpt 3 come recipe cook eat x200b rule 1 vegan2 follow recipe closely possible3 must finish plat x200b recipe 1 boil potatoes carrots 2 meantime prepare vegan mince meat use pre cook soy meat 3 fry vegan butter add garlic mushroom stir nmbr minutes 4 add soy cream stir cook three minutes 5 add pickle tomatoes bean stir simmer five minutes 6 cut bread small square fry vegan butter golden brown 7 cut lime cub squeeze juice bean mixture 8 add soy sauce parsley salt pepper cumin cilantro dry figs stir add kale 9 pour bean mix blender 10 bake nmbr minutes oven 180c 11 cut sweet potatoes cub add pot remain butter add red bean mixture 12 cut bell pepper cub add pot 13 add vegan mince meat cook oven 180c nmbr minutes 14 add avocado 15 add chickpeas 16 add chocolate 17 serve bread mustard pommegrenade top x200b video outline 0 00 plan2 15 ingredients4 05 gpt 3 6 10 let cook12 25 taste test x200b gpt 3 wikipedia url paper url
Stargor14,MachineLearning,1616875179.0,[D] Efficient ways of quantitatively classifying price movements for algorithmic trading using machine learning,"Hi there everyone, I'm currently a high school student and I've recently been experimenting with algorithmic cryptocurrency trading. I started off with generic conditional strategies, however after those proved relatively inefficient and simple, I tried moving onto a machine learning approach. 

I've been using XGBoost and python and I'm trying to figure out an efficient way to classify short term price movements, which in this case is the dependent variable I'm trying to predict with my model. I've tried using the % change over the next 5-10 candles but that didn't work too well. I'm not really looking for a specific answer, tbh I'm just curious on everyone's thoughts on this application (predicting short term price movements) of random forests. If you think my post was a bit vague feel free to ask away. Any answers are appreciated!",hi everyone currently high school student recently experiment algorithmic cryptocurrency trade start generic conditional strategies however prove relatively inefficient simple try move onto machine learn approach use xgboost python try figure efficient way classify short term price movements case dependent variable try predict model try use change next 5 10 candle work well really look specific answer tbh curious everyone thoughts application predict short term price movements random forest think post bite vague feel free ask away answer appreciate
RenYang_ETHZ,MachineLearning,1619719239.0,"[N] NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset, Methods and Codes","We have organized the NTIRE 2021 Challenge on Video Enhancement in conjunction with CVPR 2021. We propose a large-scale diverse video database for the challenge. The proposed methods in the challenge advance the state-of-the-art on enhancing compressed video. The homepage includes the database and the open-source codes (keep updating) of the benchmark: [https://github.com/RenYang-home/NTIRE21\_VEnh](https://github.com/RenYang-home/NTIRE21_VEnh)

The dataset and methods reports are at 

[https://arxiv.org/abs/2104.10782](https://arxiv.org/abs/2104.10782)

[https://arxiv.org/abs/2104.10781](https://arxiv.org/abs/2104.10781) 

We hope the database and the benchmark benefit future research in this direction!

The NTIRE workshop will be held on the first day (June 19) of CVPR 2021, more than welcome to attend our workshop.",organize ntire nmbr challenge video enhancement conjunction cvpr 2021 propose large scale diverse video database challenge propose methods challenge advance state art enhance compress video homepage include database open source cod keep update benchmark url dataset methods report url hope database benchmark benefit future research direction ntire workshop hold first day june 19 cvpr 2021 welcome attend workshop
bin_wang_osl,MachineLearning,1617079701.0,[N] Live IEEE-NASPI Contest calls for ML experts' participation!,"Can machine learning better solve power system problems? It's up to you experts! Here is a live contest co-hosted by IEEE-NASPI (from Apr. 12 to Jun. 11, 2021), see more details at [https://www.naspi.org/node/890](https://www.naspi.org/node/890). You are very welcome to register and solve an important real-world problem rooted in power and energy systems. If you need someone with a power system background to form a team, don't worry, here's a way to find one: [https://docs.google.com/spreadsheets/d/1zA0QdLk2-7OIczh-8ia6zmzuIrDnd7iQ6xkpNJOKvHA/edit#gid=173757192](https://docs.google.com/spreadsheets/d/1zA0QdLk2-7OIczh-8ia6zmzuIrDnd7iQ6xkpNJOKvHA/edit#gid=173757192).",machine learn better solve power system problems experts live contest co host ieee naspi apr nmbr jun 11 2021 see detail url welcome register solve important real world problem root power energy systems need someone power system background form team worry way find one url
AirZealousideal1342,MachineLearning,1618023079.0,[D] Why we must use weight demodulation in stylegan2,"Why we cannot do scale-specific style control if we use the architecture in figure(c)? and must use architecture in figure(d)?

The paper says

>In practice, style modulation may amplify certain feature maps by an order of magnitude or more. For style mixing to work, we must explicitly counteract this amplification on a per-sample basis—otherwise the subsequent layers would not be able to operate on the data in a meaningful way.

But I cannot understand it. And I cannot undertand why weight demodulation solves this problem.

&#x200B;

https://preview.redd.it/y4ljnsdmf9s61.png?width=1072&format=png&auto=webp&s=5529e6c8c3c97fa7a0520a57ece0443fb4d5a34c",scale specific style control use architecture figure c must use architecture figure paper say practice style modulation may amplify certain feature map order magnitude style mix work must explicitly counteract amplification per sample basis otherwise subsequent layer would able operate data meaningful way understand undertand weight demodulation solve problem x200b url
Cosack,MachineLearning,1617493961.0,[Project] Estimating fine-tuning cost,"With model sizes growing larger and larger, it's been mentioned time and again that retraining is prohibitive for individuals. Fine tuning? More reasonable. While of course the tuning intensity would depend on the task and size of available data, how would you go about estimating the cost of tuning with some fixed amount of data? Say I'm looking to fine-tune GPT-Neo (open-source GPT-3) with its 2.7B parameters on some 200k short inputs of 7.5M tokens. How could I estimate the cost?",model size grow larger larger mention time retrain prohibitive individuals fine tune reasonable course tune intensity would depend task size available data would go estimate cost tune fix amount data say look fine tune gpt neo open source gpt 3 2 7b parameters 200k short input 7 5m tokens could estimate cost
ByteHubAi,MachineLearning,1617650022.0,[P] ByteHub: simple timeseries data preparation in Python,"Hi everyone! Sharing a project I've been working on to help make time-series data easier to store, access, and transform when building machine-learning models. It’s a Python-based feature store, and is available as an [open-source library](https://github.com/bytehub-ai/bytehub) or as a low-cost [cloud-hosted service](https://bytehub.ai).

For a bit more background I've [written some more](https://medium.com/bytehub-ai/making-feature-stores-simple-3ae0d0dcac30) about why we built it. In summary we want to help data scientists save time when building machine-learning models with something that is simple to use - i.e. compatible with Jupyter notebooks and with no complex infrastructure to setup and manage. I'd appreciate feedback from anyone interested in checking it out!",hi everyone share project work help make time series data easier store access transform build machine learn model python base feature store available open source library url low cost cloud host service url bite background write url build summary want help data scientists save time build machine learn model something simple use e compatible jupyter notebooks complex infrastructure setup manage appreciate feedback anyone interest check
dev_bes,MachineLearning,1618317666.0,[R][P]MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis,"In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.

The main goal of our work is the democratization of AI. To start working with a modern AI-based high-fidelity face generation, you need just two python commands:

    pip install random_face
    python -m random_face.demo

&#x200B;

Paper: [https://arxiv.org/abs/2104.04767](https://arxiv.org/abs/2104.04767)

Code: [https://github.com/bes-dev/MobileStyleGAN.pytorch](https://github.com/bes-dev/MobileStyleGAN.pytorch)

Python library: [https://github.com/bes-dev/random\_face](https://github.com/bes-dev/random_face)

YouTube videos: [https://www.youtube.com/playlist?list=PLstKhmdpWBtwsvq\_27ALmPbf\_mBLmk0uI](https://www.youtube.com/playlist?list=PLstKhmdpWBtwsvq_27ALmPbf_mBLmk0uI)",recent years use generative adversarial network gans become popular generative image model style base gin architectures yield state art result high fidelity image synthesis computationally highly complex work focus performance optimization style base generative model analyze computationally hard part stylegan2 propose change generator network make possible deploy style base generative network edge devices introduce mobilestylegan architecture x3 5 fewer parameters x9 5 less computationally complex stylegan2 provide comparable quality main goal work democratization ai start work modern ai base high fidelity face generation need two python command pip install random_face python random_face demo x200b paper url url library url videos url
bci-hacker,MachineLearning,1617472841.0,[D] Trustworthy Machine Learning talk | ideas and potential research,"Hey all,

I recently gave a [talk](https://www.youtube.com/watch?v=2CQ7EuTCurc) at Virginia Tech on Trustworthy Machine Learning and discussed research ideas on model interpretability such as influence functions, gradient based attributions, and structure learning.

would love for you to check it out and provide feedback, if possible :)

i'm still an UG so i apologize for the rigor of academic talks. tryin my best to improve haha

would love it if you sub cuz i love making videos on my research :D",hey recently give talk url virginia tech trustworthy machine learn discuss research ideas model interpretability influence function gradient base attributions structure learn would love check provide feedback possible still ug apologize rigor academic talk tryin best improve hahawould love sub cuz love make videos research
grid_world,MachineLearning,1618074601.0,[R] Finding important neural network connections,"Most of the research work related to neural network pruning revolves around iterative pruning ever the general idea is to prune p% of connections per iterative round either locally or globally, structured vs. unstructured. A common criterion is absolute magnitude weight based pruning (Han et al., Frankle et al.).

Since this is an iterative pruning technique, the number of such rounds are large to say prune from 0% to 99.5%.

Is there some other pruning technique to overcome this shortcoming? It's kind of like trying to identify the important connections before the entire training process such that this iterative process can be skipped.",research work relate neural network prune revolve around iterative prune ever general idea prune p connections per iterative round either locally globally structure vs unstructured common criterion absolute magnitude weight base prune han et al frankle et al since iterative prune technique number round large say prune 0 99 5 prune technique overcome shortcoming kind like try identify important connections entire train process iterative process skip
levi97zzz,MachineLearning,1618010691.0,[D] how useful is OS knowledge in AI/ML?,"Question in title, I am choosing between several classes for my next semester, one of them is a class in OS. How useful is OS knowledge if I want to get further in AI/ML in the future?",question title choose several class next semester one class os useful os knowledge want get ai ml future
peaked-too-early,MachineLearning,1619992949.0,"[D] With ICLR starting, how do you make the most of an online conference?","Before the end-times, in-person conferences would be productive (for me, at least) as you could make acquaintances (which later lead to Ph.D. internships/job offers/collaborations) and could naturally learn about others/convey your own research through just casual conversation in a poster hall. The fact that we blocked off our entire schedules for the conference week obviously helped us focus on the conference, but we don't have that luxury anymore.

I've tried just winging it at online ML/ML-adjacent conferences with little success. Do you have any tips/resources on how to make the most out of this suboptimal situation?",end time person conferences would productive least could make acquaintances later lead ph internships job offer collaborations could naturally learn others convey research casual conversation poster hall fact block entire schedule conference week obviously help us focus conference luxury anymore try wing online ml ml adjacent conferences little success tip resources make suboptimal situation
abpan8,MachineLearning,1618930656.0,[P] Open data about logistics, Could you suggest some datasets or a good site to take data from by  scraping to do some forecasting on the cost of transporting goods by truck  in the US?,could suggest datasets good site take data scrap forecast cost transport goods truck us
medwatt,MachineLearning,1617829959.0,[D] Explain Energy Based Models,"I am an electrical engineering student with some basic practical understanding of how neural networks work. I understand the basic ideas of the multi-layer perceptron (MLP), convolutional layers, etc and I have even implemented a binary neural network using Keras. As an electrical engineering student, I have not had/taken any hardcore machine learning courses (or even optimization theory). Probably, they have not yet worked their way into the curriculum of the university I'm attending. Most of my knowledge in the field are from self-interest.

I recently saw project that involves the building hardware accelerators for machine learning tasks. The project requires me to have some background in energy based models. Unsurprisingly, I had never heard of this before. I came across some papers online and even found Yann LeCunn's video lectures on the topic. Needless to say, I wasn't able to understand much. I am, at the moment, unfamiliar with a lot of the ideas in this field. So, I would like if someone could give be an explanation on why energy based models are interesting, how they differ from probabilistic models, how they can be trained, what applications are they most suitable for, etc. Maybe, this will make it easier to revisit the papers and understand them on second reading.",electrical engineer student basic practical understand neural network work understand basic ideas multi layer perceptron mlp convolutional layer etc even implement binary neural network use keras electrical engineer student take hardcore machine learn course even optimization theory probably yet work way curriculum university attend knowledge field self interest recently saw project involve build hardware accelerators machine learn task project require background energy base model unsurprisingly never hear come across paper online even find yann lecunn video lecture topic needle say able understand much moment unfamiliar lot ideas field would like someone could give explanation energy base model interest differ probabilistic model train applications suitable etc maybe make easier revisit paper understand second read
Egan_Fan,MachineLearning,1617299603.0,[D] Statistical Significance in Deep RL Papers: What is going on?,"I'm an ICML reviewer, and I've been reading author responses.  I'm primarily an RL researcher, and so many of the papers I reviewed used deep networks + RL.  I rejected 3-4 papers because their empirical results relied on 3-5 trials (and the authors did not perform any sort of hypothesis testing/statistical analysis...not that that would have helped with so little data).  One of the author responses said something like, ""well, everyone else does the same thing, and the computational cost is very high"".  It's not an excuse, but they are not wrong on either point.

Why is this seen as acceptable?  In other fields (e.g., a medical journal), manuscripts with 3-5 data points and no statistical analysis would be immediately rejected, and rightfully so (and if the authors responded and said ""well we couldn't afford a larger study"", no one would see that as a legitimate excuse).  However, **none of the other reviewers on these papers are raising these concerns**.  Why am I the only one with these concerns?  **Why are papers like these getting accepted at top conferences, and even winning best paper awards?**  Am I missing something, or is this a deep problem with our field (in which case I should stick firmly with “reject” for these papers)?

Thank you in advance for thoughtful replies and discussion.",icml reviewer read author responses primarily rl researcher many paper review use deep network rl reject 3 4 paper empirical result rely 3 5 trials author perform sort hypothesis test statistical analysis would help little data one author responses say something like well everyone else thing computational cost high excuse wrong either point see acceptable field e g medical journal manuscripts 3 5 data point statistical analysis would immediately reject rightfully author respond say well afford larger study one would see legitimate excuse however none reviewers paper raise concern one concern paper like get accept top conferences even win best paper award miss something deep problem field case stick firmly reject paper thank advance thoughtful reply discussion
fripperML,MachineLearning,1616972625.0,"[D] What’s the simplest, most lightweight but complete and 100% open source MLOps toolkit?","I know this has been asked many times and in many different ways. And there are tons of blog posts, articles, videos and courses addressing this and comparing hundreds of tools, libraries, frameworks… And that’s part of my problem: I am facing so many options that I feel like Buridan’s ass, dying of starvation for not knowing what to do.

Although I don’t want to write too much, I need to speak a little about our situation, in order to put the question in our context.

**Our Team**

Our Team is small. We have only four people, which could be qualified as beginner data scientists. One of us has a profile that is a little bit more “engineer”, so data engineer could be more suitable for him. Anyway, we don’t have much experience, neither in Python Projects nor in Machine Learning. What we have is passion and love for ML!

For a couple of years, we have been functioning with SAS, but now we plan to change to the Python landscape, as it is much more vivid and exciting. In the last year, we have made two projects in Python, but without using any good practices at all. Every step was made by hand and prone to error, models were neither monitored nor even deployed (they only were used for making some batch predictions), projects were not properly structured, documentation was painful…

So we know that we need to change it before it becomes unmanageable.

We don’t expect the size of the team to grow fast. Let’s say in a couple of years we can expect 10-12 people working with us (the organization knows the importance of Machine Learning, but economic issues can be an obstacle).

**Our Projects**

For the moment, we have only made “classical” Machine Learning. I mean: no Deep Learning. We have used Pandas and Scikit-Learn, XGBoost, etc. And only in Batch mode. But we expect it to change in less than a year, because we will need to train image classifiers, so it will need to be:

\- Trained using a deep learning convolutional network.

\- Integrated with other applications (that are coded in Java) and fast (real-time).

Other change we expect is to need more distributed computing, as we will need to manage some huge databases that simply do not fit in a pandas dataframe. These are the most important challenges we face.

**Our Company**

We work for a big company, which also imposes some restrictions to us. Mainly:

We do not have budget to spend in MLOps solutions, so everything has to be open and free.

We won’t hire data scientist / data engineers for the moment.

There are some tools, uses by other teams, that we should use as part of the MLOps stack, although they are not the best in the class.

Regarding the last item, a short list of this set of restrictions is the following:

* We have a Cloudera Express installation. It’s the most basic and cheaper Cloudera option, so it does not come with any tool for Machine Learning management. It only gives to us HDFS, Impala, Spark and a set of nodes to run Python scripts on them.
* We have Control-M as the orchestrator and workflow manager tool.
* We have DataStage as the ETL tool.
* We use SVN as the code version system (yes, no git).
* We deploy our projects using a very simplified and self-made version of Docker. It’s a little bit awkward and I think that, if we push a little bit, we could convince the organization to let us use Docker. But if Docker is reachable, Kubernetes is out of our capabilities.
* We have Jenkins for CI.
* We have Visual Studio Code professional licenses.

**Toolset**

With this premises, I have two different and opposed concerns or even fears.

* Fear of not using enough tools and good practices and arriving in a couple of years to a state where we cannot manage our own code, project and models.
* Fear of using so many tools that they impose a burden our small team cannot bear.

It’s clear that we need some MLOps, but how much, I don’t know. I will review some things I have been reading, and I hope you can help me choosing the right tools.

**Python Programming**

It looks like we will program using Visual Studio. We will use a remote interpreter, because we will run things on the Cloudera Nodes, although we will program locally and integrate the code with a SVN repository.

Do we need tools for standardizing our code, like PyLint, Flake8, MyPy or Black? Would you recommend any of those?

**CI and Deployment**

We will use Jenkins. For deployment of our code, is Docker a no brainer, a minimum standard? I tend to think so from what I read, but I’d like to be sure and to have good arguments.

Do we need more tools?

**Project Scaffolding**

I have been reading about PyScaffold, CookieCutter and, best of all (from my point of view), Kedro. I think we will stick to Kedro template, because it offers much more functionality, and I like to think of each project as a set of pipelines to be run. What do you think of Kedro?

**Documentation**

Would you recommend having separate documents, or generating the documentation from the projects, using Sphinx or another similar tool? I tend to prefer the second option, because the first one very likely tend to generate obsolete docs. But I don’t know if the “burden” of the second is too big, and if the generated docs can suffice for a typical ML project.

**Project registry**

Is there any tool that could be used as a “project registry”, like a simple web app where we could navigate through our projects, read the docs and thinks like that? I don’t know. If not, the registry will be the SVN repo with all our projects as folders, and that’s all.

**Data Exploration and Preparation**

I think that Matplotlib, Seaborn and Pandas should suffice, and when things go big, we should use PySpark, Scala or even plain SQL in Impala. However, I know Dask exists, and newer tools like Koalas or Vaex. What do you think?

For creating data transformation pipelines, we will use Kedro, although there are lots of tools that look interesting, like Dagster.

When we enter the “deep learning” realm, can we keep using the same tools? Should we use another framework like TFX? I’d prefer not, cause learning one framework is hard, and two is worse. If a solution is valid for all our projects it’s better. Or TFX is valid for “classical” ML and Deep Learning?

**Tests**

I think unit testing can be too much burden for us. But I have come to Great Expectations library and think it’s well suited for ML projects. Would you recommend it as an important part of our MLOps stack?

By the way, there is a Kedro-Great Expectations plugin, so we could benefit from that.

**Feature Store**

Is it really needed, especially considering our team size and experience? If so, I have read about Feast and Snorkel.

**Data Versioning**

Is it really needed, especially considering our team size and experience? If so, I have read about DVC.

**Experimenting**

I think it’s an important piece, although I wonder if we really need a tool or we could use our own standard of reports and artifacts to follow what we have tried. But the risk that it goes unmanageable is high.

Kedro has a journal, I don’t know if it can suffice. Also it has a Kedro-MLFlow plugin, so that we could benefit from using MLFlow as the experiment tool.

I have also read about Guild, that seems really lightweigh and easy. I don’t know much more.

**Training**

I developed my own library for doing nested cross validation and, within the same function:

\- Optimizing hyperparameters (both of model and pipeline).

\- Generating a report of the training, to assess the quality of the model.

It’s build mainly on top of Skopt. I did it pip installable, it’s here:

[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)

So we might use it in our traning workflow (at least in some projects), along with the typical models like XGBoost, LightGBM and all Scikit-Learn. But when we need other frameworks like TensorFlow or Keras, we will see.

**Model Registry**

I think it’s an important piece, although I don’t know if we even could build our own with an standard database. If not, MLFlow seems a mature option.

**Model serving**

I am not sure if it’s included in the previous point or not. Anyway, I have read about Streamlint and FastAPI. Would you recommend any of those?

Is Apache Kafka needed for real time predictions?

**Visualization**

With this I mean sharing with the organization basic web apps with customizable plots, explainable predictions and things like that. I have read about panel, which has the ability of transform a Jupyter Notebook into a simple web app. It might be interesting.

**Model monitoring**

Is there a good free tool for monitoring the models and detecting loss of accuracy, data drift and things like that? Or we should better generate our own script of monitoring to be run periodically?

**BigData**

As I said before, we plan to use mainly Spark when need.

I know it’s a lot of info. Maybe I have overcomplicated myself and I should use only 20% of what I think I should. Or maybe not. I have no idea. Any help will be GREATLY appreciated. Thanks in advance.

**EDIT**  


I should add a couple of things, the first one is spread among two comments:-  I work for the Spanish public Administration. So we do have lots of data to use and explore. This also has to do with the rigidity of our budget and the other things I explained. This also explains why cloud is not an option for us: there are data protection, legal and even political reasons that forbid us to have any data outside of our scope. It's a pity, because I think AWS (or another provider) can help in having part of this stack covered.

As we already use DataStage, the IBM provider told us that soon it will be unified with Cloud Pak For Data, so we might be having soon a Cloud Pak For Data license. I have mixed feelings for that product, do you think it might benefit us more than the opposite?",know ask many time many different ways tons blog post article videos course address compare hundreds tool libraries frameworks thats part problem face many options feel like buridans ass die starvation know although want write much need speak little situation order put question context team team small four people could qualify beginner data scientists one us profile little bite engineer data engineer could suitable anyway much experience neither python project machine learn passion love ml couple years function sas plan change python landscape much vivid excite last year make two project python without use good practice every step make hand prone error model neither monitor even deploy use make batch predictions project properly structure documentation painful know need change become unmanageable expect size team grow fast let say couple years expect 10 12 people work us organization know importance machine learn economic issue obstacle project moment make classical machine learn mean deep learn use pandas scikit learn xgboost etc batch mode expect change less year need train image classifiers need train use deep learn convolutional network integrate applications cod java fast real time change expect need distribute compute need manage huge databases simply fit pandas dataframe important challenge face company work big company also impose restrictions us mainly budget spend mlops solutions everything open free hire data scientist data engineer moment tool use team use part mlops stack although best class regard last item short list set restrictions follow cloudera express installation basic cheaper cloudera option come tool machine learn management give us hdfs impala spark set nod run python script control orchestrator workflow manager tool datastage etl tool use svn code version system yes git deploy project use simplify self make version docker little bite awkward think push little bite could convince organization let us use docker docker reachable kubernetes capabilities jenkins ci visual studio code professional license toolset premise two different oppose concern even fear fear use enough tool good practice arrive couple years state manage code project model fear use many tool impose burden small team bear clear need mlops much know review things read hope help choose right tool python program look like program use visual studio use remote interpreter run things cloudera nod although program locally integrate code svn repository need tool standardize code like pylint flake8 mypy black would recommend ci deployment use jenkins deployment code docker brainer minimum standard tend think read id like sure good arguments need tool project scaffold read pyscaffold cookiecutter best point view kedro think stick kedro template offer much functionality like think project set pipelines run think kedro documentation would recommend separate document generate documentation project use sphinx another similar tool tend prefer second option first one likely tend generate obsolete docs know burden second big generate docs suffice typical ml project project registry tool could use project registry like simple web app could navigate project read docs think like know registry svn repo project folders thats data exploration preparation think matplotlib seaborn pandas suffice things go big use pyspark scala even plain sql impala however know dask exist newer tool like koalas vaex think create data transformation pipelines use kedro although lot tool look interest like dagster enter deep learn realm keep use tool use another framework like tfx id prefer cause learn one framework hard two worse solution valid project better tfx valid classical ml deep learn test think unit test much burden us come great expectations library think well suit ml project would recommend important part mlops stack way kedro great expectations plugin could benefit feature store really need especially consider team size experience read feast snorkel data versioning really need especially consider team size experience read dvc experiment think important piece although wonder really need tool could use standard report artifacts follow try risk go unmanageable high kedro journal know suffice also kedro mlflow plugin could benefit use mlflow experiment tool also read guild seem really lightweigh easy know much train develop library nest cross validation within function optimize hyperparameters model pipeline generate report train assess quality model build mainly top skopt pip installable url might use traning workflow least project along typical model like xgboost lightgbm scikit learn need frameworks like tensorflow keras see model registry think important piece although know even could build standard database mlflow seem mature option model serve sure include previous point anyway read streamlint fastapi would recommend apache kafka need real time predictions visualization mean share organization basic web apps customizable plot explainable predictions things like read panel ability transform jupyter notebook simple web app might interest model monitor good free tool monitor model detect loss accuracy data drift things like better generate script monitor run periodically bigdata say plan use mainly spark need know lot info maybe overcomplicated use 20 think maybe idea help greatly appreciate thank advance edit add couple things first one spread among two comment work spanish public administration lot data use explore also rigidity budget things explain also explain cloud option us data protection legal even political reason forbid us data outside scope pity think aws another provider help part stack cover already use datastage ibm provider tell us soon unify cloud pak data might soon cloud pak data license mix feel product think might benefit us opposite
jj4646,MachineLearning,1619315323.0,[D] how accurate were the statistical models you developed on real-world data?,"When it comes to real-world data, how accurate were the statistical models you developed? Were these models able to consistently and accurately make predictions? 

E.g. for supervised binary classification, has anyone been able to develop a model that had high accuracy, high sensitivity and high specificity?",come real world data accurate statistical model develop model able consistently accurately make predictions e g supervise binary classification anyone able develop model high accuracy high sensitivity high specificity
Yuqing7,MachineLearning,1618849550.0,[N] DeepMind 'Podracer' TPU-Based RL Frameworks Deliver Exceptional Performance at Low Cost,"A research team from DeepMind introduces Anakin and Sebulba, two architectures that demonstrate reinforcement learning platforms based on TPUs can efficiently deliver exceptional performance at scale and with low cost.

Here is a quick read: [DeepMind 'Podracer' TPU-Based RL Frameworks Deliver Exceptional Performance at Low Cost](https://syncedreview.com/2021/04/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost/).

The paper *Podracer Architectures for Scalable Reinforcement Learning* is on [arXiv](https://arxiv.org/pdf/2104.06272.pdf).",research team deepmind introduce anakin sebulba two architectures demonstrate reinforcement learn platforms base tpus efficiently deliver exceptional performance scale low cost quick read deepmind podracer tpu base rl frameworks deliver exceptional performance low cost url paper podracer architectures scalable reinforcement learn arxiv url
xiikjuy,MachineLearning,1619173343.0,[D] Should I report the pretraining results?,"Hello,

I am using a Transformer-based model to pretrain  my unlabeled large data and then finetune the pretrained model on my labelled smaller dataset, just  a regular learning pipeline following the idea of BERT.

My question is, should I report the pretraining results (like the learning curve)? or just need to describe the pretraing steps, optimizers, etc.

For literature using Transformer-based models with this learning paradigm ( pretrain+finetune), it seems most papers didn't report the pretraining results, just the results on donwstream tasks.

Shouldn't they first show that the pretrained model is well-pretrained?

or it doesn't matter that much, even a model performs poorly in pretraining phase but somehow the checkpoint works well on downstream tasks, then no problem at all?

Thanks.

Edit: ""report"" here  means showing it on a regular ML/DL conference paper.",hello use transformer base model pretrain unlabeled large data finetune pretrained model label smaller dataset regular learn pipeline follow idea bert question report pretraining result like learn curve need describe pretraing step optimizers etc literature use transformer base model learn paradigm pretrain finetune seem paper report pretraining result result donwstream task first show pretrained model well pretrained matter much even model perform poorly pretraining phase somehow checkpoint work well downstream task problem thank edit report mean show regular ml dl conference paper
ilikepancakez,MachineLearning,1619293570.0,Team Polk’s Bryan Pellegrino Talks About His AI Research And How It Helped Formulate Strategies To Win $1.2 Million [P],"https://www.cardplayer.com/poker-news/25778-team-polk-s-bryan-pellegrino-talks-about-his-ai-research-and-how-it-helped-formulate-strategies-to-win-1-2-million

Really interesting use of game theory. Had the honor of watching this happen live. A moment of silence to the human /r/poker players please.",url interest use game theory honor watch happen live moment silence human r poker players please
alexirpan,MachineLearning,1618168907.0,[D] Thoughts on industry research vs academia,"Hi all,

I didn't go to grad school, going straight to indsutry instead, and I've been working in ML for about 5 years now. I thought it'd be interesting to look back on how that turned out. The post is here: [https://www.alexirpan.com/2021/04/07/grad-school-5years.html](https://www.alexirpan.com/2021/04/07/grad-school-5years.html)

I got feedback from all across the ML career spectrum (straight to ML engineer, in PhD, industry to academia, post PhD), and have tried to address all their experiences, so hopefully it matches up with reality and is helpful if you're considering a similar decision.",hi go grad school go straight indsutry instead work ml nmbr years think interest look back turn post url get feedback across ml career spectrum straight ml engineer phd industry academia post phd try address experience hopefully match reality helpful consider similar decision
glassAlloy,MachineLearning,1617285861.0,[P] How to group every data point with HDBSCAN to some group to have no noise?,"**TASK**

\- I am clustering products with about 70 dimensions ex.: price, rating 5/5, product tag(cleaning, toy, food, fruits)

\- I use HDBSCAN to do it

&#x200B;

**GOAL**

\- The goal is when users come on our site and I can show similar products to what they viewing.

&#x200B;

**QUESTION**

\- How to get all data point to be part of a group, so the goal is to not to have any noise?

&#x200B;

**CODE**

    clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=1).fit(data)
    color_palette = sns.color_palette('Paired', 2000)
    cluster_colors = [color_palette[x] if x >= 0
                      else (0.5, 0.5, 0.5)
                      for x in clusterer.labels_]
    cluster_member_colors = [sns.desaturate(x, p) for x, p in
                             zip(cluster_colors, clusterer.probabilities_)]
    plt.scatter(*projection.T, s=20, linewidth=0, c=cluster_member_colors, alpha=0.25)
    
    
    labels = clusterer.labels_
    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    print('Estimated number of clusters: %d' % n_clusters_)",task cluster products nmbr dimension ex price rat 5 5 product tag clean toy food fruit use hdbscan x200b goal goal users come site show similar products view x200b question get data point part group goal noise x200b code clusterer hdbscan hdbscan min_cluster_size 10 min_samples 1 fit data color_palette sns color_palette pair 2000 cluster_colors color_palette x x nmbr else 0 5 0 5 0 5 x clusterer labels_ cluster_member_colors sns desaturate x p x p zip cluster_colors clusterer probabilities_ plt scatter projection 20 linewidth 0 c cluster_member_colors alpha 0 25 label clusterer labels_ number cluster label ignore noise present n_clusters_ len set label 1 1 label else 0 print estimate number cluster n_clusters_
cbsudux,MachineLearning,1616679371.0,[D] Cheapest GPU server options for deploying a side-project?,"Hey guys, what are your cheapest options for deploying ML models?

\- AWS, GCP cost 250-300 USD/mo for using a T4/P4 instance. Too much for a side project.

\- [vast.ai](https://vast.ai), independent GPU providers cost around 100-150 USD per/month which is still a bit pricey.

**1) Are there any pay-as-you go options (aws elastic equivalent)?**  
**2) Are there are any cheap options for 50-80 USD/mo?**",hey guy cheapest options deploy ml model aws gcp cost 250 300 usd mo use t4 p4 instance much side project vast ai url independent gpu providers cost around 100 150 usd per month still bite pricey 1 pay go options aws elastic equivalent 2 cheap options 50 80 usd mo
KirillTheMunchKing,MachineLearning,1620480849.0,[D] Solving computer vision without convolutions! MLP-Mixer explained.,"# [MLP-Mixer: An all-MLP Architecture for Vision](https://t.me/casual_gan/35)

This paper is a spiritual successor of Vision Transformer from last year. This time around the authors once again come up with an all-MLP (multi layer perceptron) model for solving computer vision tasks. This time around, no self-attention blocks are used either (!) instead two types of ""mixing"" layers are proposed. The first is for interaction of features inside patches , and the second - between patches. See [more details](https://t.me/casual_gan/35).

[Model architecture overview](https://preview.redd.it/na599eawfwx61.png?width=1280&format=png&auto=webp&s=fb29ae62876fb41b7c14e53b1dbc5b1cc9bbd0f6)

\[[7 minute paper explanation](https://t.me/casual_gan/35)\] \[[Arxiv](https://arxiv.org/pdf/2105.01601.pdf)\]",mlp mixer mlp architecture vision url paper spiritual successor vision transformer last year time around author come mlp multi layer perceptron model solve computer vision task time around self attention block use either instead two type mix layer propose first interaction feature inside patch second patch see detail url architecture overview url minute paper explanation url arxiv url
st-memory,MachineLearning,1616860882.0,[P] Generating Galaxies using StyleGAN2-ADA,"[This](https://www.youtube.com/watch?v=kxPe8Oux0jQ) is a video of a [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada) trained on images from [Hubble](https://esahubble.org/images/archive/category/galaxies/) that involves travelling through the latent space with different truncation values (such that the variety of images increases in exchange for quality as the video progresses). The images were scraped and then manually cleaned as a fair few either had diagrams or ""defects"". The dataset is of \~1,000 images and the network has been trained until no further improvements in FID were noted.",url video stylegan2 ada url train image hubble url involve travel latent space different truncation value variety image increase exchange quality video progress image scrap manually clean fair either diagram defect dataset 1 000 image network train improvements fid note
roVinchi,MachineLearning,1620639086.0,[D] Will the talks of ICLR21 be publicly available?,"and if so, is there a known ""release"" date? I could not find any information on the website itself.",know release date could find information website
Yuqing7,MachineLearning,1617322595.0,[N] Google Research's SOTA GNN 'Reasons' Interactions over Time to Boost Video Understanding,"A research team from Google Research propose a message-passing graph neural network that can explicitly model spatio-temporal relations, use either implicitly or explicitly representations of objects, and generalize previous structured models for video understanding.

Here is a quick read: [Google Research's SOTA GNN 'Reasons' Interactions over Time to Boost Video Understanding](https://syncedreview.com/2021/04/01/google-researchs-sota-gnn-reasons-interactions-over-time-to-boost-video-understanding/)

The paper *Unified Graph Structured Models for Video Understanding* is on [arXiv](https://arxiv.org/pdf/2103.15662.pdf).",research team google research propose message pass graph neural network explicitly model spatio temporal relations use either implicitly explicitly representations object generalize previous structure model video understand quick read google research sota gnn reason interactions time boost video understand url paper unify graph structure model video understand arxiv url
sharvil,MachineLearning,1620146526.0,[P] ArxivDiff: view diffs of arXiv paper revisions,"I built a tool to show diffs between any two revisions of a paper on arXiv. Just take any arXiv URL and replace arxiv.org with arxivdiff.org, e.g. https://arxiv.org/abs/2009.09761 becomes https://arxivdiff.org/abs/2009.09761

edit: my first Reddit awards! Thank you so much, fellow..um.. net surfers.",build tool show diffs two revisions paper arxiv take arxiv url replace arxiv org arxivdiff org e g url become url first reddit award thank much fellow um net surfers
ottawalanguages,MachineLearning,1619997683.0,[D] correct application of autoencoders for classification,"Can autoencoders be performed on data the same way as principal component analysis? Can you perform dimensionality reduction on your data using autoencoders, and then use random forest on the reduced data? Or is this counterproductive?",autoencoders perform data way principal component analysis perform dimensionality reduction data use autoencoders use random forest reduce data counterproductive
CauchySchwartzDaddy,MachineLearning,1619047967.0,[D] Is it just me or is it getting harder and harder to get access to cloud GPUs with regions being out of resources almost all the time,"I have about 10 VMs set up in google cloud in basically every region where I can get a V100 and yet multiple times a day I cant access any of them due to regions being out of resources.  Maybe this is just a gcloud thing combined with a cutthroat GPU market.

I'd be interested to know if anyone else has this problem.",nmbr vms set google cloud basically every region get v100 yet multiple time day cant access due regions resources maybe gcloud thing combine cutthroat gpu market interest know anyone else problem
xela-sedinnaoi,MachineLearning,1618815073.0,"[P] [D] The benefits of training the simplest model you can think of and deploying it to production, as soon as you can.","I’ve had many successes with this approach. With this in mind, I’ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this Agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a Scikit-Learn model, using FastAPI with [Bodywork](https://github.com/bodywork-ml/bodywork-core) (an open-source MLOps tool that I have built).

How does this compare to your experiences? I’d be interested to get people’s thoughts, as my background is largely with structured data.",ive many successes approach mind ive put together example url make agile approach develop machine learn systems reality demonstrate take nmbr minutes deploy scikit learn model use fastapi bodywork url open source mlops tool build compare experience id interest get people thoughts background largely structure data
prestodigitarium,MachineLearning,1617376157.0,"[P] Gourdian Free Dataset Download: Daily weather of the world, back to 1929","Hi there! 

Have you ever thought that it'd be useful to train a model on historical weather data for some chunk of the world, but didn't want to deal with grungy data wrangling on a massive dataset to get it into a convenient format? 

Have you ever just been curious what the average temperature of Egypt in 1985 was?

Well, a friend and I made a webpage that lets you filter the 28 gig NOAA Global Summary of the Day weather dataset down to a small fraction of that, and download just the part you care about, as a csv, here:

**https://gourdian.net/g/eric/noaa_gsod.global_summary_of_day**

Table preview on the left, geographic and time filters and download button on the right. Delivers it as a single, clean csv, which should be easy to import into Pandas, R, a database, or whatever else you like to use to work on tabular data. CSV works with everything!

A bit about our goals and what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Basically, our focus is on building something very simple - it won't be as powerful at fine-grained querying as something like BigQuery, but it should be easier to get going with.

This is a super early version of our web/javascript download client (we've never shown it publicly before, our other client is a python library), and we're trying to figure out what to make better, so we'd love any feedback, especially if it breaks for whatever reason. What can we do that would make your life easier?

Please note, **it doesn't work well on mobile yet** - we figured that not too many people would want to browse/download datasets there, and it's just the two of us, and... priorities. But if we're wrong, please let us know!",hi ever think useful train model historical weather data chunk world want deal grungy data wrangle massive dataset get convenient format ever curious average temperature egypt nmbr well friend make webpage let filter nmbr gig noaa global summary day weather dataset small fraction download part care csv url preview leave geographic time filter download button right deliver single clean csv easy import pandas r database whatever else like use work tabular data csv work everything bite goals try build filter optional click button csv arrive hard drive download always single csv bundle weird directory structure format csvs index filterable column type lat long date time moment download part want open license datasets free download signup require download open datasets search within across datasetsbasically focus build something simple win powerful fine grain query something like bigquery easier get go super early version web javascript download client never show publicly client python library try figure make better love feedback especially break whatever reason would make life easier please note work well mobile yet figure many people would want browse download datasets two us priorities wrong please let us know
SQL_beginner,MachineLearning,1619323632.0,[D] Reservoir Computing/Echo State Networks vs RNN's and LSTM's,Has anyone ever heard of Reservoir Computing or Echo State Networks (https://en.wikipedia.org/wiki/Reservoir_computing)? Does anyone have any idea in what situations they should be used compared to models such as RNN and LSTM?,anyone ever hear reservoir compute echo state network url anyone idea situations use compare model rnn lstm
marcovirgolin,MachineLearning,1619510512.0,[R] Model Learning with Personalized Interpretability Estimation,"For high-stakes applications (e.g., cancer treatment), AI cannot be used lightly and recklessly. We need models we can trust and, to achieve trust, interpretability is a key factor.

The field of eXplainable AI (XAI) concerns *both* methods to explain the behavior of black-boxes such as deep neural networks, and methods to generate white-boxes, i.e., models that are interpretable (think of, e.g., sparse linear models, small decision trees, symbolic expressions).
There are very good reasons why the latter are more desirable than the former, see e.g., the famous paper by Cynthia Rudin: https://arxiv.org/abs/1811.10154

We proposed a new proof-of-concept work that looks at whether XAI for interpretable model generation can and should be *personalized*. (What follows is essentially taken from the abstract.)
In fact, current algorithms for the synthesis of *potentially* interpretable models rely on objectives or regularization terms that represent interpretability only coarsely (e.g., model size) and are not designed for a specific user. 
Yet, interpretability is intrinsically subjective. 
We propose an approach for the synthesis of models that are tailored to the user by enabling the user to steer the model synthesis process according to her or his preferences. 
We use a bi-objective evolutionary algorithm to synthesize models with trade-offs between accuracy and a user-specific notion of interpretability. The latter is estimated by a neural network that is trained concurrently to the evolution using the feedback of the user, which is collected using uncertainty-based active learning. To maximize usability, the user is only asked to tell, given two models at the time, which one is less complex. With experiments on two real-world datasets involving 61 participants, we find that our approach is capable of learning estimations of interpretability that can be very different for different users. Moreover, the users tend to prefer models found using the proposed approach over models found using non-personalized interpretability indices.

Preprint: https://arxiv.org/abs/2104.06060 
(Accepted to appear at the EC+DM workshop at GECCO 2021)",high stake applications e g cancer treatment ai use lightly recklessly need model trust achieve trust interpretability key factor field explainable ai xai concern methods explain behavior black box deep neural network methods generate white box e model interpretable think e g sparse linear model small decision tree symbolic expressions good reason latter desirable former see e g famous paper cynthia rudin url propose new proof concept work look whether xai interpretable model generation personalize follow essentially take abstract fact current algorithms synthesis potentially interpretable model rely objectives regularization term represent interpretability coarsely e g model size design specific user yet interpretability intrinsically subjective propose approach synthesis model tailor user enable user steer model synthesis process accord preferences use bi objective evolutionary algorithm synthesize model trade accuracy user specific notion interpretability latter estimate neural network train concurrently evolution use feedback user collect use uncertainty base active learn maximize usability user ask tell give two model time one less complex experiment two real world datasets involve nmbr participants find approach capable learn estimations interpretability different different users moreover users tend prefer model find use propose approach model find use non personalize interpretability indices preprint url accept appear ec dm workshop gecco 2021
Last-Programmer2181,MachineLearning,1617364678.0,"[R] Why can a single large SL model be broken down into smaller SL models, and have better accuracy?","For reference, I am using Learning Classifier Systems (LCS) to perform supervised learning on a dataset.

I have a large synthetically generated set of data, 11 different inputs, and the predicted action as output. After generating nearly a million different data points, I trained a SL model and achieved roughly 70% classification accuracy. I did the normal hyperparameter sweeping, and accuracy varied anywhere from 60-70%. The data is very unique, in the sense that there are many different combinations of the eleven inputs, leading to twenty-seven different possible classifications (actions). (I know these numbers are pretty specific, but I just want to give a sense of what I'm dealing with).

The actions could range from, 'do nothing', 'do something X', and 'do something Y'. Where 'do something X' had A many possible variants/similar actions, and then same with 'do something Y'

What I decided to do was break this one larger SL problem, into small chunks. I broke the single larger problem into four much more manageable models, one leading into the next. (Model A -> B -> C -> D, ultimately predicting the same result as the larger singular model)

The first model would simply decide 'do nothing' or 'do something', where I combined all of the 'do something X and Y' into a single prediction. And achieved a model prediction accuracy of 95%+. And so on, where I kept making the models more and more specific, ultimately leading me to the same twenty-seven different possible classifications I had in the one larger model. (Each model had an accuracy of 95%+, for reference).

Each model would always have the same 11 inputs, I just removed the data points no longer relevant, based on the previous model's choice.

Why would one larger model, trying to do everything, have a much lower classification accuracy than multiple models (doing the same thing) that have a classification accuracy of 95%+?",reference use learn classifier systems lcs perform supervise learn dataset large synthetically generate set data nmbr different input predict action output generate nearly million different data point train sl model achieve roughly 70 classification accuracy normal hyperparameter sweep accuracy vary anywhere 60 70 data unique sense many different combinations eleven input lead twenty seven different possible classifications action know number pretty specific want give sense deal action could range nothing something x something something x many possible variants similar action something decide break one larger sl problem small chunk break single larger problem four much manageable model one lead next model b c ultimately predict result larger singular model first model would simply decide nothing something combine something x single prediction achieve model prediction accuracy 95 keep make model specific ultimately lead twenty seven different possible classifications one larger model model accuracy 95 reference model would always nmbr input remove data point longer relevant base previous model choice would one larger model try everything much lower classification accuracy multiple model thing classification accuracy 95
fedetask,MachineLearning,1616519398.0,[R] RL Papers using graph techniques on sampled trajectories,Are there papers that construct a graph from states-actions-rewards collected and do something with it? I just find this idea intriguing but I haven't found much about it,paper construct graph state action reward collect something find idea intrigue find much
programmerChilli,MachineLearning,1619417306.0,[D] Huawei just announced that they trained a 200 billion transformer model on an entirely Chinese stack,"My tweet about it: https://twitter.com/cHHillee/status/1386541907950465028

They trained a 200 billion parameter decoder-only dense transformer for 40B tokens on 2048 Huawei Ascend 910 chips. Moreover, this was all done using MindSpore, Huawei's ML framework.

In contrast, GPT-3 was a 175B parameter model trained for 300B tokens. 

On its own, this is already quite impressive. Even though they've only done 40B tokens, this is the biggest model yet out of China, and represents one of the biggest models yet in the world. 

However, the thing that's really impressive to me is that this was done with an all-Chinese stack: Huawei Mindspore as the framework, compiling down to Huawei Ascend chips. 

I'd known that Huawei was working on AI chips, but I was unaware that they had matured to the point that they could feasibly train a model of this scale.

Code: https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-AIpha

Paper: https://t.co/8wQepOVIYq?amp=1",tweet url train nmbr billion parameter decoder dense transformer 40b tokens nmbr huawei ascend nmbr chip moreover use mindspore huawei ml framework contrast gpt 3 175b parameter model train 300b tokens already quite impressive even though 40b tokens biggest model yet china represent one biggest model yet world however thing really impressive chinese stack huawei mindspore framework compile huawei ascend chip know huawei work ai chip unaware mature point could feasibly train model scale code url url
OnlyProggingForFun,MachineLearning,1618063607.0,[News] From Amputee to Cyborg with this AI-Powered Hand! 🦾[Nguyen & Drealan et al. (2021)],"**Papers involved for this arm**:

1. Nguyen & Drealan et al. (2021) A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based Finger Control: [https://arxiv.org/pdf/2103.13452.pdf](https://arxiv.org/pdf/2103.13452.pdf)
2. Luu & Nguyen et al. (2021) Deep Learning-Based Approaches for Decoding Motor Intent from Peripheral Nerve Signals: [https://www.researchgate.net/publication/349448928\_Deep\_Learning-Based\_Approaches\_for\_Decoding\_Motor\_Intent\_from\_Peripheral\_Nerve\_Signals](https://www.researchgate.net/publication/349448928_Deep_Learning-Based_Approaches_for_Decoding_Motor_Intent_from_Peripheral_Nerve_Signals)
3. Nguyen et al. (2021) Redundant Crossfire: A Technique to Achieve Super-Resolution in Neurostimulator Design by Exploiting Transistor Mismatch: [https://ieeexplore.ieee.org/document/9355574](https://ieeexplore.ieee.org/document/9355574)
4. Nguyen & Xu et al. (2020) A Bioelectric Neural Interface Towards Intuitive Prosthetic Control for Amputees: [https://www.biorxiv.org/content/10.1101/2020.09.17.301663v1.full](https://www.biorxiv.org/content/10.1101/2020.09.17.301663v1.full)

**Video demo**:

[https://youtu.be/wNBrCRzlbVw](https://youtu.be/wNBrCRzlbVw)",paper involve arm 1 nguyen drealan et al 2021 portable self contain neuroprosthetic hand deep learn base finger control url luu nguyen et al 2021 deep learn base approach decode motor intent peripheral nerve signal url nguyen et al 2021 redundant crossfire technique achieve super resolution neurostimulator design exploit transistor mismatch url nguyen xu et al 2020 bioelectric neural interface towards intuitive prosthetic control amputees url demo url
FormerYogurtcloset17,MachineLearning,1617233499.0,"[D] How can I augment an existing model with new training data, preferably on the edge?","How Can I Augment an Existing Model with New Training Data?

I built an app where I use MobileNet model from TensorFlow Lite to detect an object in a live steaming camera. 

Now, I wish to ReTrain the model or somehow ""Augment"" with new training data, i.e. new photos. 

How can I accomplish such an objective at the Edge, i.e. on the device? 

or even on a remote server ""fast""?",augment exist model new train data build app use mobilenet model tensorflow lite detect object live steam camera wish retrain model somehow augment new train data e new photos accomplish objective edge e device even remote server fast
satprepnow124,MachineLearning,1618955447.0,[P] Time Series Forecasting," Hey, I have a dataset of police complaints and I want to do time series forecasting. I got officers' wages and years of promotions and years in which complaints were filed. I want to predict future complaints based on wages , past complaints, and when they were promoted.

Any suggestions for methods to use? Sorry, I have just never done time series forecasting, so I'm a bit confused.",hey dataset police complaints want time series forecast get officer wag years promotions years complaints file want predict future complaints base wag past complaints promote suggestions methods use sorry never time series forecast bite confuse
WigglyHypersurface,MachineLearning,1618503893.0,[DISCUSSION] How do the different versions of the bootstrap work with deep neural networks?,"I've been looking into deep learning methods as a possible way of imputing missing data, and this has lead me to some questions about how deep neural networks interact with the various versions of the bootstrap.

In a statistical missing data context, the goal is to estimate a posterior predictive distribution for the missing data conditioned on the observed data. You then ""fill in"" the missing data with draws from this distribution, leading to *m* versions of your dataset, where the observed data is the same, but the missing data vary. You do your analysis *m* times and use some simple formulas to combine the analyses, which gives you a nice clean picture of how much uncertainty in the missing data reduces your confidence in whatever hypothesis your testing.

Ok, now the deep net part. In the statistics literature on missing data, there is this idea of ""properness"" which says that you want your posterior distribution for the missing data to reflect all sources of uncertainty, in the model you used to fill in the missing data. Your model for filling in the missing data either needs to be fully bayesian, or approximate a fully bayesian model if that isn't possible.

The simplest way to do an approximate proper posterior in missing data world is often to use either a parameteric or nonparametric bootstrap. For parametric bootstrap, you 1) train a model (say, learning mu and sigma in a linear regression), 2) sample predicted values for the outcome variable, (ie make a new outcome variable by sampling from the learned mu and sigma) 3) retrain the model using the sampled values as the new dependent variable, and 4) make whatever predictions/inferences you want from this second model. If you do this a bunch of times you'll aproximate a bayesian posterior. Nonparametric bootstrap you resample you data with replacement and train on the resampled data. Again, repeated many times you'll get an approximate posterior.

So my questions are, for a deep neural network:

1) What advantages/disadvatages does a deep net using variational bayes versus a deep net using either version of the bootstrap have? Are these any known or expected biases when doing either form of bootstrapping with a deep neural network? 
2) Will you cause a bias if you don't train the network from scratch for multiple iterations of the bootstrap? Would it be a problem if, for example, you resample the data, but carry-over the weights from a previous bootstrap iteration? 
3) Is there a good resource on how changing to a fully bayesian deep network changes what sort of choices you should make about dropout, batch normalization, activations, etc?",look deep learn methods possible way impute miss data lead question deep neural network interact various versions bootstrap statistical miss data context goal estimate posterior predictive distribution miss data condition observe data fill miss data draw distribution lead versions dataset observe data miss data vary analysis time use simple formulas combine analyse give nice clean picture much uncertainty miss data reduce confidence whatever hypothesis test ok deep net part statistics literature miss data idea properness say want posterior distribution miss data reflect source uncertainty model use fill miss data model fill miss data either need fully bayesian approximate fully bayesian model possible simplest way approximate proper posterior miss data world often use either parameteric nonparametric bootstrap parametric bootstrap 1 train model say learn mu sigma linear regression 2 sample predict value outcome variable ie make new outcome variable sample learn mu sigma 3 retrain model use sample value new dependent variable 4 make whatever predictions inferences want second model bunch time aproximate bayesian posterior nonparametric bootstrap resample data replacement train resampled data repeat many time get approximate posterior question deep neural network 1 advantage disadvatages deep net use variational bay versus deep net use either version bootstrap know expect bias either form bootstrapping deep neural network 2 cause bias train network scratch multiple iterations bootstrap would problem example resample data carry weight previous bootstrap iteration 3 good resource change fully bayesian deep network change sort choices make dropout batch normalization activations etc
timscarfe,MachineLearning,1619855005.0,[D] Unadversarial Examples video with Hadi Salman (MIT lab),"Performing reliably on unseen or shifting data distributions is a difficult challenge for modern vision systems, even slight corruptions or transformations of images are enough to slash the accuracy of state-of-the-art classifiers. When an adversary is allowed to modify an input image directly, models can be manipulated into predicting anything even when there is no perceptible change, this is known an adversarial example. The ideal definition of an adversarial example is when humans consistently say two pictures are the same but a machine disagrees. Hadi Salman, a Ph.D student at MIT (ex-Uber and Microsoft Research) started thinking about how adversarial robustness  could be leveraged beyond security.  He realised that the phenomenon of adversarial examples could actually be turned upside down to lead to more robust models instead of breaking them. Hadi actually utilized the brittleness of neural networks to design unadversarial examples or robust objects which are objects designed specifically to be robustly recognized by neural networks.  

Video: [https://youtu.be/\_eHRICHlg1k](https://youtu.be/_eHRICHlg1k)

Pod:  https://anchor.fm/machinelearningstreettalk/episodes/52---Unadversarial-Examples-Hadi-Salman--MIT-e1015k2

In the first 10 mins I give an intro covering the MIT features not bugs papers, non-robust features etc. 

Adversarial Examples Are Not Bugs, They Are Features

[https://arxiv.org/pdf/1905.02175.pdf](https://arxiv.org/pdf/1905.02175.pdf)

Adversarial Robustness as a Prior for Learned Representations

[https://arxiv.org/pdf/1906.00945.pdf](https://arxiv.org/pdf/1906.00945.pdf)

Image Synthesis with a Single (Robust) Classifier

[https://arxiv.org/pdf/1906.09453.pdf](https://arxiv.org/pdf/1906.09453.pdf)

&#x200B;

Unadversarial Examples: Designing Objects for Robust Vision

[https://arxiv.org/pdf/2012.12235.pdf](https://arxiv.org/pdf/2012.12235.pdf)

&#x200B;

Do Adversarially Robust ImageNet Models Transfer Better?

[https://arxiv.org/pdf/2007.08489.pdf](https://arxiv.org/pdf/2007.08489.pdf)

&#x200B;

A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks

[https://arxiv.org/pdf/1902.08722.pdf](https://arxiv.org/pdf/1902.08722.pdf)

&#x200B;

Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers

[https://arxiv.org/pdf/1906.04584.pdf](https://arxiv.org/pdf/1906.04584.pdf)

&#x200B;

Denoised Smoothing: A Provable Defense for Pretrained Classifiers

[https://arxiv.org/pdf/2003.01908.pdf](https://arxiv.org/pdf/2003.01908.pdf)

&#x200B;

ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness

[https://arxiv.org/abs/1811.12231](https://arxiv.org/abs/1811.12231)",perform reliably unseen shift data distributions difficult challenge modern vision systems even slight corruptions transformations image enough slash accuracy state art classifiers adversary allow modify input image directly model manipulate predict anything even perceptible change know adversarial example ideal definition adversarial example humans consistently say two picture machine disagree hadi salman ph student mit ex uber microsoft research start think adversarial robustness could leverage beyond security realise phenomenon adversarial examples could actually turn upside lead robust model instead break hadi actually utilize brittleness neural network design unadversarial examples robust object object design specifically robustly recognize neural network video url url first nmbr mins give intro cover mit feature bug paper non robust feature etc adversarial examples bug feature url robustness prior learn representations url synthesis single robust classifier url examples design object robust vision url adversarially robust imagenet model transfer better url convex relaxation barrier tight robustness verification neural network url robust deep learn via adversarially train smooth classifiers url smooth provable defense pretrained classifiers url cnns bias towards texture increase shape bias improve accuracy robustness url
ploomber-io,MachineLearning,1617292408.0,[D] Incremental builds for ML pipelines,"Hi everyone, I'd like to get some perspective on incremental builds. When developing an ML pipeline, I often have to revisit a processing step  (e.g., update SQL or Python script). Since the output is now outdated, I  have to rerun the pipeline, but I can skip unaffected tasks to save some time; as the pipeline grows, this has a significant impact. The most common example is Make. Incremental builds are a must for me since it allows me to modify something and bring everything up-to-date quickly.

A few orchestrators have this feature: [Ploomber](https://github.com/ploomber/ploomber)  (which I'm developing), DVC, drake (in R), but others don't: dagster,  kedro, prefect. Surprisingly, users from the latter group do not seem to miss (or maybe be aware of) that feature.

My only guess is that people who mostly work in Deep Learning don't see much of a benefit of a make-like tool because there are fewer pre-processing steps.

Am I missing anything? How do you ensure all tasks are using the most recent data?  And more importantly, how do you quickly get all outputs up-to-date?",hi everyone like get perspective incremental build develop ml pipeline often revisit process step e g update sql python script since output outdated rerun pipeline skip unaffected task save time pipeline grow significant impact common example make incremental build must since allow modify something bring everything date quickly orchestrators feature ploomber url develop dvc drake r others dagster kedro prefect surprisingly users latter group seem miss maybe aware feature guess people mostly work deep learn see much benefit make like tool fewer pre process step miss anything ensure task use recent data importantly quickly get output date
thejuror8,MachineLearning,1616768727.0,[D] Class-incremental learning and Reviewer 2,"Disclaimer: I am not one of the authors (nor am I connected to the authors in any shape or form) of the following ICLR 2021 rejected paper, for which the review is accessible at:

https://openreview.net/forum?id=mu0WNwWWWCE

While the results presented seemed promising to me (although I'm quite fresh to CIL), I was surprised to read reviewer two's conclusion:

> In summary, the premise of 'class incremental' learning appears weak to me. In practice (in vision applications, ..), most often labeling is manual and highly time consuming, and that's the major bottleneck (getting sufficiently many accurate labels on many classes). As the labels arrive, training or retraining (time and space, batch (re)training) is not the issue with existing batch training methods and computational power.

Besides the fact that this is such a classic reviewer 2 move, traction around papers presenting new CIL methods in vision-specialized conferences has been quite significant, especially recently, which suggests at least some degree of relevance.

Which leads to my question: is there a consensus on the relevance of CIL, and are there people sharing reviewer's opinion on its (im)practicality?",disclaimer one author connect author shape form follow iclr nmbr reject paper review accessible url result present seem promise although quite fresh cil surprise read reviewer two conclusion summary premise class incremental learn appear weak practice vision applications often label manual highly time consume major bottleneck get sufficiently many accurate label many class label arrive train retrain time space batch train issue exist batch train methods computational power besides fact classic reviewer nmbr move traction around paper present new cil methods vision specialize conferences quite significant especially recently suggest least degree relevance lead question consensus relevance cil people share reviewer opinion im practicality
Haunting-Garbage-364,MachineLearning,1620036485.0,"[D] Companies that sell ""creative"" AI/ML products and services","Hey everyone! I am writing my final thesis on Artificial Intelligence use in creative sectors, and I am on the lookout for creative sector companies that use AI specifically to automate the creation of their product/service... For example, this company: [https://ironov.artlebedev.com](https://ironov.artlebedev.com/) Is selling logos designed by AI. I am looking for companies doing something similar, but cannot find any... If any of you have any ideas or companies you know, please let me know, I would really appreciate it!",hey everyone write final thesis artificial intelligence use creative sectors lookout creative sector company use ai specifically automate creation product service example company url sell logos design ai look company something similar find ideas company know please let know would really appreciate
downtownslim,MachineLearning,1618984939.0,[N] Cerebras launches new AI supercomputing processor with 2.6 trillion transistors,">[Cerebras Systems](https://venturebeat.com/2020/11/17/cerebras-wafer-size-chip-is-10000-times-faster-than-a-gpu/) has unveiled its new Wafer Scale Engine 2 processor with a record-setting 2.6 trillion transistors and 850,000 AI-optimized cores. It’s built for supercomputing tasks, and it’s the second time since 2019 that Los Altos, California-based [Cerebras](https://cerebras.net/) has unveiled a chip that is basically an entire wafer.  
>  
>Chipmakers normally slice a wafer from a 12-inch-diameter ingot of silicon to process in a chip factory. Once processed, the wafer is sliced into hundreds of separate chips that can be used in electronic hardware.  
>  
>But Cerebras, started by SeaMicro founder Andrew Feldman, takes that wafer and makes a single, massive chip out of it. Each piece of the chip, dubbed a core, is interconnected in a sophisticated way to other cores. The interconnections are designed to keep all the cores functioning at high speeds so the transistors can work together as one.

Full text: [https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/](https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/)",cerebras systems url unveil new wafer scale engine nmbr processor record set nmbr trillion transistors 850 000 ai optimize core build supercomputing task second time since nmbr los altos california base cerebras url unveil chip basically entire wafer chipmakers normally slice wafer 12 inch diameter ingot silicon process chip factory process wafer slice hundreds separate chip use electronic hardware cerebras start seamicro founder andrew feldman take wafer make single massive chip piece chip dub core interconnect sophisticate way core interconnections design keep core function high speed transistors work together one full text url
TheoreticallyBlank,MachineLearning,1619406224.0,[R] Fractional pooling layers in CNNs,Have there been any recent publications or surveys related to improvements by replacing standard pooling layers with fractional ones?,recent publications survey relate improvements replace standard pool layer fractional ones
Yuqing7,MachineLearning,1618933498.0,"[R] Rice University, IBM & USC Study Pushes Quantum State Tomography Beyond Current Computation Capabilities","A research team from Rice University, IBM and USC combine compressed sensing, non-convex optimization and acceleration techniques to introduce a new algorithm — Momentum Inspired Factored Gradient Descent (MiFGD) — that pushes QST beyond current capabilities. 

Here is a quick read: [Rice University, IBM & USC Study Pushes Quantum State Tomography Beyond Current Computation Capabilities](https://syncedreview.com/2021/04/20/rice-university-ibm-usc-study-pushes-quantum-state-tomography-beyond-current-computation-capabilities/).

The paper *Fast Quantum State Reconstruction via Accelerated Non-Convex Programming* is on [arXiv](https://arxiv.org/pdf/2104.07006.pdf).",research team rice university ibm usc combine compress sense non convex optimization acceleration techniques introduce new algorithm momentum inspire factor gradient descent mifgd push qst beyond current capabilities quick read rice university ibm usc study push quantum state tomography beyond current computation capabilities url paper fast quantum state reconstruction via accelerate non convex program arxiv url
FreddeFrallan,MachineLearning,1616355252.0,[P] [R] Pre-trained Multilingual-CLIP Encoders,"&#x200B;

https://preview.redd.it/n4vkz1ceofo61.png?width=1739&format=png&auto=webp&s=bf33b2791d15b1af6e40d5fd5e5c3742d2b62ad2

We have started creating a set of pre-trained text encoders which match the [OpenAI CLIP](https://openai.com/blog/clip/) image encoders. See our [Github page](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Distil%2040) for more information and list of currently available models.

We have currently only performed a minor qualitative evaluation on:

* French
* Russian
* Spanish
* German
* Green
* Swedish

The model seemingly yields reasonable results for all tested languages ([See results here,](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Distil%2040) in which every column is a Softmax for the corresponding image over the given texts)

The idea of creating multilingual CLIP encoders via teacher learning coupled with machine translation is not very creative, so it is unlikely we will write any proper papers of these. So we thought we might as well share the preliminary models directly.

We will be releasing bigger Multilingual models as soon as they finish training.

Hope you guys find them useful, enjoy!",x200b url start create set pre train text encoders match openai clip url image encoders see github page url information list currently available model currently perform minor qualitative evaluation french russian spanish german green swedishthe model seemingly yield reasonable result test languages see result url every column softmax correspond image give texts idea create multilingual clip encoders via teacher learn couple machine translation creative unlikely write proper paper think might well share preliminary model directly release bigger multilingual model soon finish train hope guy find useful enjoy
hhh312,MachineLearning,1619813787.0,[D] Optimizing the top of a network only,"Hey Guys,

I'm using the backbone of a bart model from the HuggingFace transformers library, but fine tune an additional head. What I do right now is that, where I provide the weights to the optimizer, I only hand in the weights of the head, however, I suspect, during the training, the no\_grad is not applied to the non-head weights, and hence, a lot of computational resources are wasted. Is that the case? How can I enforce no grade to all other weights?

&#x200B;

Thanks",hey guy use backbone bart model huggingface transformers library fine tune additional head right provide weight optimizer hand weight head however suspect train _grad apply non head weight hence lot computational resources waste case enforce grade weight x200b thank
jj4646,MachineLearning,1618864084.0,"[D] Has anyone ever heard of ""scissor plots"" being used in machine learning?","https://imgur.com/a/d2t6gII

I came across this interesting graph called ""scissors plot"". I have never heard about it before - has anyone else heard about it? Is this a well known plot? 

It would be interesting to know if there was some way to roughly approximate the ""N-o"" point, perhaps the ""N-o"" point could be used to decide if it makes more sense to use ""complex"" models or ""simple"" models.",url come across interest graph call scissor plot never hear anyone else hear well know plot would interest know way roughly approximate n point perhaps n point could use decide make sense use complex model simple model
JollyEye3,MachineLearning,1620281484.0,[D] Anomaly detection in sequential data under budget constraint,"I am working on problem where I need to detect anomalies in a collection of N sequential examples with couple requirements:

1) In each example, there can be multiple anomalies i.e., an example may contain 1 or 100 anomalies. However, the goal is to detect atleast one anomaly per example and detecting a single anomaly is good enough.

2) The number of detected anomalies allowed is B. This is because we only have a fixed annotation budget to confirm the detected anomalies. A human reviewer will review and confirm the anomalies.

There is training data available for the problem. This is a constrained optimization problem where we need to maximize the number of examples covered with as few anomalies detected per example (but atleast one).

Any thoughts on a constrained optimization view of the problem? Is there any research or papers around this topic of cost-constrained anomaly detection in corpus of sequential data?",work problem need detect anomalies collection n sequential examples couple requirements 1 example multiple anomalies e example may contain nmbr nmbr anomalies however goal detect atleast one anomaly per example detect single anomaly good enough 2 number detect anomalies allow b fix annotation budget confirm detect anomalies human reviewer review confirm anomalies train data available problem constrain optimization problem need maximize number examples cover anomalies detect per example atleast one thoughts constrain optimization view problem research paper around topic cost constrain anomaly detection corpus sequential data
dhekurbaba,MachineLearning,1620448897.0,[D] just accepted an offer upon graduation..... what do you do in-between?," doing a phd has really messed up my work-life balance..... this will be my first job upon graduation, and joining is 2 weeks from now, meaning i have this many days with nothing to do

i was thinking of emailing my (to be) boss and asking him to refer some tools/concepts to me so that i can teach myself these things, but was wondering if this is the norm..... or should i not do that and just play video games instead

what do you guys do?",phd really mess work life balance first job upon graduation join nmbr weeks mean many days nothing doi think email boss ask refer tool concepts teach things wonder norm play video game insteadwhat guy
Brahimce,MachineLearning,1618321567.0,[R][P] How to handle equality constraints in mutation of evolutionary algorithms?,I am new in evolutionary algorithms field. I have a chromosome of 6 variables (real variable) where the sum of these variables equal to one. I am looking for mutation formulas that can generate a new chromosome respecting the equality constraint ( the sum of new chromosome should always equal to one).,new evolutionary algorithms field chromosome nmbr variables real variable sum variables equal one look mutation formulas generate new chromosome respect equality constraint sum new chromosome always equal one
LakeTurbulent5878,MachineLearning,1617513587.0,"[D] Having published at top ML conference, how to be nominated as a reviewer?","I used to believe that somebody will invite me after publishing a paper at NeurIPS/ICML/ICLR, but after publishing two first-authored papers at these conferences, I didn't receive the invitation to be a reviewer. I actually quite enjoy reviewing papers and I would be happy to be a reviewer. I think I am qualified at least, but my advisor doesn't mainly work in the ML field so he has no idea how to nominate me. Then who should I contact with? Who has the right to nominate reviewers? Just a reviewer, AC, or PC?",use believe somebody invite publish paper neurips icml iclr publish two first author paper conferences receive invitation reviewer actually quite enjoy review paper would happy reviewer think qualify least advisor mainly work ml field idea nominate contact right nominate reviewers reviewer ac pc
ptoews,MachineLearning,1620036509.0,[D] CPU choice for machine learning server (Epyc vs. Threadripper),"We are planning on building a rig with 4 RTX 3090 and 128 GB RAM. The application area is computer vision, so preprocessing will most likely be necessary. I've read about DALI which might be useful, but we can't be sure yet.

We are currently looking at Threadripper vs. Epyc. Are there any benchmarks or experiences on how these two line ups compare in image preprocessing tasks?

So far from what I've read Threadrippers have higher clock speed, but run hotter and support less memory capacity and bandwidth, whereas Epyc is the opposite. But how does this translate to a border for applications in ML?

As a side question, how important is core count for preprocessing? Apparently 2 cores per GPU is recommended, but does it scale after that?",plan build rig nmbr rtx nmbr nmbr gb ram application area computer vision preprocessing likely necessary read dali might useful sure yet currently look threadripper vs epyc benchmarks experience two line compare image preprocessing task far read threadrippers higher clock speed run hotter support less memory capacity bandwidth whereas epyc opposite translate border applications ml side question important core count preprocessing apparently nmbr core per gpu recommend scale
statsIsImportant,MachineLearning,1618647712.0,[D] Looking for the extreme classification + Language modelling video,"Hi, I am looking for a video from a ICML 2020 workshop.

""[ **Invited Talk 2 - Historical perspective on extreme classification in language modeling - Tomas Mikolov** (Talk)](https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5719#collapse6942)  ""

If somebody can point me to a website or provide link, it would be great.",hi look video icml nmbr workshop invite talk nmbr historical perspective extreme classification language model tomas mikolov talk url somebody point website provide link would great
elTope,MachineLearning,1618161239.0,[D] Industry vs Learning process gap,"Maybe this hole post doesn't rasonate with anyone, but still. I always have the same feeling when I try to learn about machine learning stuff. I am obviously constantly amazed by what's being developed and archived by ML in industry/research, but whenever I enroll in a course, search for a guide, etc, I can't wrap my head around how all of this ends up interacting in a production environment. Many courses may explain some models and attach to it some toy example, but when it comes to implementing an end to end solution to a specific problem I'm still rather clueless. 
Maybe this is just me, or maybe is the best way to learn and I lack enough information to know better. If neither of those are the cases, I would like to know if someone could provide with different aproaches, method or resources to remove some of the abstraction.
If I'm just being a bitch you can tell me that also.
Thanks for your attention.",maybe hole post rasonate anyone still always feel try learn machine learn stuff obviously constantly amaze develop archive ml industry research whenever enroll course search guide etc wrap head around end interact production environment many course may explain model attach toy example come implement end end solution specific problem still rather clueless maybe maybe best way learn lack enough information know better neither case would like know someone could provide different aproaches method resources remove abstraction bitch tell also thank attention
jhanytime,MachineLearning,1618156293.0,[D] Video - Why would you use graphs for machine learning data?,"I'm a PhD student studying machine learning and applications in transportation systems and autonomous systems (think RL and robotics). While there are several ""GCN made easy"" videos out there on Youtube, I feel like these videos often miss the forest for the trees (especially since GCN is just 1 algorithm that was developed in 2016...) videos often don't cover the broader historical context of how GNNs were developed and how different variations allow them to model new types of systems. 

This is the first video in a series I'm making about graphs, graph neural networks, and the application areas where they have the potential to make big impacts. Please let me know what you think of the video and if you learned anything new from it!

https://youtu.be/mu1Inz3ltlo",phd student study machine learn applications transportation systems autonomous systems think rl robotics several gcn make easy videos youtube feel like videos often miss forest tree especially since gcn nmbr algorithm develop 2016 videos often cover broader historical context gnns develop different variations allow model new type systems first video series make graph graph neural network application areas potential make big impact please let know think video learn anything new url
srcho,MachineLearning,1619696052.0,[R] Ethical consideration in AI(Machine learning) decision-making process,"Dear community,

I desperately need your help!!

As part of my Master’s thesis at the Universiteit van Amsterdam, I am conducting a study about AI, Machine Learning, Ethical consideration, and its relationship to decision-making outcome quality! I would like to kindly ask your help to participate in my survey. This survey is only for PEOPLE WHO HAVE EXPERIENCE IN THE DECISION-MAKING PROCESS WITH BUSINESS PROJECT before. **If you have working experience with AI, Machine learning, or deep learning, it would be even better!!! Please fill this survey to support me!!**

The survey link is: https://uva.fra1.qualtrics.com/jfe/form/SV\_5bWWZRfReTJmGSa

This survey takes about 5 minutes maximum. To find out the relationship, I need your help with sufficient participants. Please fill out this survey and contribute to helping me to finish my academic work! Feel free to distribute this survey to your network!

I am looking forward to hearing your answers!",dear community desperately need help part master thesis universiteit van amsterdam conduct study ai machine learn ethical consideration relationship decision make outcome quality would like kindly ask help participate survey survey people experience decision make process business project work experience ai machine learn deep learn would even better please fill survey support survey link url survey take nmbr minutes maximum find relationship need help sufficient participants please fill survey contribute help finish academic work feel free distribute survey network look forward hear answer
emilwallner,MachineLearning,1617697075.0,[P] How I built a €25K Machine Learning Rig,"Link: [https://www.emilwallner.com/p/ml-rig](https://www.emilwallner.com/p/ml-rig)

Hey, I made a machine learning rig with four NVIDIA RTX A6000 and an AMD EPYC 2 with 32 cores, including 192 GB in GPU memory and 256GB in RAM ([**part list**](https://docs.google.com/spreadsheets/d/1VMtiLZbgLAChKscBAbC1VovVwsI_8Y0BBAW8R06rc5I/edit?usp=sharing)).

I made a 4000-word guide for people looking to build Nvidia Ampere prosumer workstations and servers, including:

&#x200B;

* Different budget tiers
* Where to place them, home, office, data center, etc.
* Constraints with consumer GPUs
* Reasons to buy prosumer and enterprise GPUs
* Building a workstation and a server
* Key components in a rig and what to pick
* Lists of retailers and build lists

Let me know if you have any questions!

Here's the build:

&#x200B;

[Four RTX A6000 with EPYC 2](https://preview.redd.it/1h7fbtb2iir61.jpg?width=1456&format=pjpg&auto=webp&s=63aa35971c13f372a842d231d1a58c83ae6bc437)",link url make machine learn rig four nvidia rtx a6000 amd epyc nmbr nmbr core include nmbr gb gpu memory 256gb ram part list url make 4000 word guide people look build nvidia ampere prosumer workstations servers include x200b different budget tiers place home office data center etc constraints consumer gpus reason buy prosumer enterprise gpus build workstation server key components rig pick list retailers build listslet know question build x200b four rtx a6000 epyc 2 url
lkhphuc,MachineLearning,1618873310.0,[R] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning,"**ZeRO-Infinity at a glance:**  ZeRO-Infinity is  a novel deep learning (DL) training technology for  scaling model  training, from a single GPU to massive supercomputers  with thousands of  GPUs. It powers unprecedented model sizes by  leveraging the full memory  capacity of a system, concurrently  exploiting all heterogeneous memory  (GPU, CPU, and Non-Volatile Memory  express or NVMe for short). Learn  more in our paper, “[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://www.microsoft.com/en-us/research/publication/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning/).” The highlights of ZeRO-Infinity include: 

* Offering  the system capability to train a model with over 30  trillion  parameters on 512 NVIDIA V100 Tensor Core GPUs, 50x larger than  state  of the art. 
* Delivering  excellent training efficiency and  superlinear throughput scaling  through novel data partitioning and  mapping that can exploit the  aggregate CPU/NVMe memory bandwidths and  CPU compute, offering over 25  petaflops of sustained throughput on 512  NVIDIA V100 GPUs.
* Furthering the mission of the DeepSpeed team to democratize large model training by allowing data scientists with *a single GPU* to fine-tune models larger than Open AI GPT-3 (175 billion parameters).
* Eliminating   the barrier to entry for large model training by making it simpler and   easier—ZeRO-Infinity scales beyond a trillion parameters without the   complexity of combining several parallelism techniques and without   requiring changes to user codes. To the best of our knowledge, it’s the   only parallel technology to do this.

![](https://www.microsoft.com/en-us/research/uploads/prod/2021/04/1400x788_deepspeed_nologo-1.mp4)

From the blog post: [https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/).

Massive  props to Microsoft and the DeepSpeed team for their work. I'm thrilled  every time I see a new ZeRO paper and DeepSpeed release on Github.",zero infinity glance zero infinity novel deep learn dl train technology scale model train single gpu massive supercomputers thousands gpus power unprecedented model size leverage full memory capacity system concurrently exploit heterogeneous memory gpu cpu non volatile memory express nvme short learn paper zero infinity break gpu memory wall extreme scale deep learn url highlight zero infinity include offer system capability train model nmbr trillion parameters nmbr nvidia v100 tensor core gpus 50x larger state art deliver excellent train efficiency superlinear throughput scale novel data partition map exploit aggregate cpu nvme memory bandwidths cpu compute offer nmbr petaflops sustain throughput nmbr nvidia v100 gpus mission deepspeed team democratize large model train allow data scientists single gpu fine tune model larger open ai gpt 3 175 billion parameters eliminate barrier entry large model train make simpler easier zero infinity scale beyond trillion parameters without complexity combine several parallelism techniques without require change user cod best knowledge parallel technology url blog post url prop microsoft deepspeed team work thrill every time see new zero paper deepspeed release github
cedricdb,MachineLearning,1618850067.0,[R] [D] Label info in Adversarial Autoencoders,"I have a question about the Adversarial Autoencoders paper by Makhzani et al, 2016 ([https://arxiv.org/abs/1511.05644](https://arxiv.org/abs/1511.05644)).

Let's look at Figure 8, which concerns the architecture for Semi-Supervised Adversarial Autoencoders. A softmax is used to obtain soft labels for the input image. This soft output is encouraged to be as close to a categorical sample by the objective of the upper GAN. The question is: what happens with the soft label when optimizing decoder? Are you supposed to draw a hard sample from it, or take the argmax?

In the semi-supervised setting it's not that important, but later on in Figure 10 (Dimensionality Reduction with Adversarial Autoencoders), this architecture is reused. Here the softmax output is used as a cluster head selector. Is this a soft or hard selector?

The way I see which makes kind of sense, is as follows: to optimize the upper GAN, you use soft labels. But to optimize the decoder using reconstruction error, you sample from it. But this implies that the gradients of the reconstruction error will not flow through the softmax layer.

Can someone clarify this for me? Thanks!",question adversarial autoencoders paper makhzani et al nmbr url look figure 8 concern architecture semi supervise adversarial autoencoders softmax use obtain soft label input image soft output encourage close categorical sample objective upper gin question happen soft label optimize decoder suppose draw hard sample take argmax semi supervise set important later figure nmbr dimensionality reduction adversarial autoencoders architecture reuse softmax output use cluster head selector soft hard selector way see make kind sense follow optimize upper gin use soft label optimize decoder use reconstruction error sample imply gradients reconstruction error flow softmax layer someone clarify thank
roma-glushko,MachineLearning,1618823116.0,[P] How I built my Deep Learning workstation,"Recently I have built my deep learning workstation and shared my experience in the following blogpost:

[https://www.romaglushko.com/blog/how-i-built-my-ml-workstation/](https://www.romaglushko.com/blog/how-i-built-my-ml-workstation/) ⬅️

I have tried to cover all aspects of building machine learning PC:

\- some theory on choosing PC parts for ML

\- hardware installation and troubleshooting guides

\- software and CUDA setup

I hope it's going to be helpful 🙌",recently build deep learn workstation share experience follow blogpost url try cover aspects build machine learn pc theory choose pc part ml hardware installation troubleshoot guide software cuda setupi hope go helpful
touchanimize,MachineLearning,1618075882.0,[P] Fine tuning Magenta ML model,"Hi all,

I'm a bit of a novice with machine learning as I'm an undergrad working on fine-tuning the Onsets and Frames model by Google ([https://arxiv.org/pdf/1710.11153.pdf](https://arxiv.org/pdf/1710.11153.pdf)) using this code base: [https://github.com/greenbech/onsets-and-frames](https://github.com/greenbech/onsets-and-frames). I'm trying to fine tune this model with jazz music, which is different from the classical music used to train the original model. I was a bit unsure of how to go about this process, and I was hoping if anyone could given any insights on things that I could do. I've done the following thus far:

* Acquired dataset of jazz music and converted them to MIDI
* Have trained different mixes of jazz music with the larger MAESTRO dataset used to train the Onset and Frames model. I've gotten some F1 scores with these mixes.

I was thinking of doing some incremental training progress with a checkpoint and maybe some in-batch dataset mixing. Do I need to do a hyper parameter search again? I'm not exactly sure of how to move forward, so any ideas would be much appreciated. Thank you guys for your time!",hi bite novice machine learn undergrad work fine tune onsets frame model google url use code base url try fine tune model jazz music different classical music use train original model bite unsure go process hop anyone could give insights things could follow thus far acquire dataset jazz music convert midi train different mix jazz music larger maestro dataset use train onset frame model get f1 score mix think incremental train progress checkpoint maybe batch dataset mix need hyper parameter search exactly sure move forward ideas would much appreciate thank guy time
omnipotent_i,MachineLearning,1620326101.0,[D] Paths to become a Productive Non-Academic Researcher,"Hello, I’ve been very much fortunate to have been working in a Research Role post Bachelors in a Start-Up. We’ve been trying for almost 2 years to get accepted in conferences such as ICLR, NeurIPS etc. Have always been unsuccessful. Research is interesting, Unfortunately I couldn’t afford to get into academics due to situations. Is there any other parts I could take to go ahead in this path? Is it a Fantasy to get accepted in Conferences having come from a Non-Academical and Research Labs of Top Companies to provide clear directions and Guidances?  Thanks.",hello ive much fortunate work research role post bachelor start weve try almost nmbr years get accept conferences iclr neurips etc always unsuccessful research interest unfortunately afford get academics due situations part could take go ahead path fantasy get accept conferences come non academical research labs top company provide clear directions guidances thank
meowklaski,MachineLearning,1618758483.0,[P] MyGrad: Drop-In Autodiff for NumPy,"[https://github.com/rsokl/MyGrad](https://github.com/rsokl/MyGrad)

 

MyGrad is a lightweight library that adds automatic differentiation to NumPy – its only dependency is NumPy!

MyGrad's primary goal is to make automatic differentiation accessible and easy to use across the Python/NumPy ecosystem. As such, it strives to behave and feel exactly like NumPy so that users need not learn yet another array-based math library. 

  
\`\`\`

 \>>> import mygrad as mg 

\>>> import numpy as np 

\>>> x = mg.tensor(\[1., 2., 3.\])  # like numpy.array, but supports backprop! 

\>>> f = np.sum(x \* x)  # tensors work with numpy functions! 

\>>> f.backward() # triggers automatic differentiation  

\>>> x.grad # stores \[df/dx0, df/dx1, df/dx2\] 

array(\[2., 4., 6.\]) 

\`\`\`

  
It works by leveraging NumPy's new(ish) protocols for overriding its functions. Thus MyGrad could eventually be used to bring autodiff to CuPy, xarray, sparse array, and other array-based libraries.

This has proven to also be a very useful library to help folks learn about auto-diff and machine learning. I first created it to support a class that I teach, but it has become a fully-fledged autodiff library since then!",url mygrad lightweight library add automatic differentiation numpy dependency numpy mygrad primary goal make automatic differentiation accessible easy use across python numpy ecosystem strive behave feel exactly like numpy users need learn yet another array base math library import mygrad mg import numpy np x mg tensor 1 2 3 like numpy array support backprop f np sum x x tensors work numpy function f backward trigger automatic differentiation x grad store df dx0 df dx1 df dx2 array 2 4 6 work leverage numpy new ish protocols override function thus mygrad could eventually use bring autodiff cupy xarray sparse array array base libraries prove also useful library help folks learn auto diff machine learn first create support class teach become fully fledge autodiff library since
diffgram-anthony,MachineLearning,1618524641.0,[P] Diffgram - Open Annotation Platform,"Sharing [https://github.com/diffgram/diffgram](https://github.com/diffgram/diffgram)

This has been something I (Anthony) have been working on for the last 2+ years in closed source. Recently we have grown to be a small team.

What makes Diffgram different?

We list some [benefits here](https://github.com/diffgram/diffgram#benefits) but if I had to pick one thing, it's that it's a complete system. You can be up and running [in 2 minutes](https://youtu.be/y0LE7QPXxE0) on docker. And scale to ""big tech co"" level on multiple [k8s clusters.](https://diffgram.readme.io/docs/open-installation-production) 

Over time, the goal is continue to define all the abstractions needed to smoothly work with data in anyway you desire on any system. This goes far beyond UI customizations, or specific speed up approach implementations, and really is a complete ""all in one"" system.

Would love your feedback!

&#x200B;

https://preview.redd.it/ack84fz3vet61.png?width=3385&format=png&auto=webp&s=b535a93c3c5f0d90e4d93326c999174d5466a9aa",share url something anthony work last 2 years close source recently grow small team make diffgram different list benefit url pick one thing complete system run nmbr minutes url docker scale big tech co level multiple k8s cluster url time goal continue define abstractions need smoothly work data anyway desire system go far beyond ui customizations specific speed approach implementations really complete one system would love feedback x200b url
JuanPRamirez,MachineLearning,1617358083.0,[D] What would you say are the biggest hurdles for people looking to get into ML?,"Hey all!

So for context I am a junior year undergrad currently taking my first course in ML. Hopefully looking to get deeper into academia and either become a Researcher or a professor in the field. I originally planned for this post to ask what I can do to better prepare myself for what's to come, but I feel like it's better to gauge that by first asking what the common major hurdles are. Whether it be academic, industry, or just life hurdles that commonly show up. Any info is highly appreciated!",hey context junior year undergrad currently take first course ml hopefully look get deeper academia either become researcher professor field originally plan post ask better prepare come feel like better gauge first ask common major hurdle whether academic industry life hurdle commonly show info highly appreciate
tomkoker,MachineLearning,1616612505.0,"[P] Torchsort - Fast, differentiable sorting and ranking in PyTorch","Introducing Torchsort, an implementation of ""Fast Differentiable Sorting and Ranking"" [(Blondel et al.)](https://arxiv.org/abs/2002.08871) in PyTorch, complete with a custom C++ and CUDA kernel for fast performance.

    pip install torchsort

[https://github.com/teddykoker/torchsort](https://github.com/teddykoker/torchsort)

Differentiable sorting and ranking operations open the door to new loss functions. For example you can easily implement Spearman's rank coefficient using Torchsort, and have a model learn to output predictions with a monotonic relationship to the targets:

    import torch
    import torchsort
    
    def spearmanr(pred, target, **kw):
        pred = torchsort.soft_rank(pred, **kw)
        target = torchsort.soft_rank(target, **kw)
        pred = pred - pred.mean()
        pred = pred / pred.norm()
        target = target - target.mean()
        target = target / target.norm()
        return (pred * target).sum()
    
    pred = torch.tensor([[1., 2., 3., 4., 5.]], requires_grad=True)
    target = torch.tensor([[5., 6., 7., 8., 7.]])
    spearman = spearmanr(pred, target)
    # tensor(0.8321)
    
    torch.autograd.grad(spearman, pred)
    # (tensor([[-5.5470e-02,  2.9802e-09,  5.5470e-02,  1.1094e-01, -1.1094e-01]]),)

The algorithm itself is O(n log n), and runs quite fast on CPU and GPU (even with large batch sizes and sequence lengths) thanks to the custom Isotonic regression kernel. I hope this is helpful tool for the ML community!",introduce torchsort implementation fast differentiable sort rank blondel et al url pytorch complete custom c cuda kernel fast performance pip install torchsort url sort rank operations open door new loss function example easily implement spearman rank coefficient use torchsort model learn output predictions monotonic relationship target import torch import torchsort def spearmanr pred target kw pred torchsort soft_rank pred kw target torchsort soft_rank target kw pred pred pred mean pred pred pred norm target target target mean target target target norm return pred target sum pred torch tensor 1 2 3 4 5 requires_grad true target torch tensor 5 6 7 8 7 spearman spearmanr pred target tensor 0 8321 torch autograd grad spearman pred tensor 5 5470e 02 2 9802e 09 5 5470e 02 1 1094e 01 1 1094e 01 algorithm n log n run quite fast cpu gpu even large batch size sequence lengths thank custom isotonic regression kernel hope helpful tool ml community
crubier,MachineLearning,1619634130.0,"[P] Labelflow, the open source image labeling and dataset cleaning platform.","Hi, all! Announcing Labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.

We are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  We were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can’t easily be shared. 

So we started building Labelflow, an image labeling tool with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. Let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!",hi announce labelflow url open source image label dataset clean platform team nmbr people experience image label dataset curating build quality datasets deep learn frustrate amount script require move data back forth tool lack control data especially data cant easily share start build labelflow image label tool bell whistle open source backend connect data stack easily let know think feel free request early access get 50 release months
abhijithneilabraham,MachineLearning,1619615635.0,Question Answering on Covid-19 data [research],"Hi, I have uploaded my first model to huggingface, a lonfgormer model for question answering on covid -19 data. You can find the source code [here](https://github.com/abhijithneilabraham/Covid-QA)

Give a star if you like the project, and also test it on the huggingface API, link given in readme.

&#x200B;

&#x200B;

https://preview.redd.it/dmk0f9iazwv61.png?width=822&format=png&auto=webp&s=e9ad7fa71b7ceff047e61c949599aad3949158c5",hi upload first model huggingface lonfgormer model question answer covid 19 data find source code url star like project also test huggingface api link give readme x200b x200b url
KirillTheMunchKing,MachineLearning,1616775591.0,[D] Encoding in Style (Pixel2Style2Pixel - pSp) explained,"Have you guys seen the results from the pSp encoder?
 I found the paper extremely useful for my research on GAN inversion, and latent space projection for deep learning based image editing.  

If you want to know the main ideas of the paper ""Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation"" (pixel2style2pixel or pSp) by Richardson et al. head over my [telegram channel](https://t.me/casual_gan), where I break down the main ideas from popular GAN papers.   

In case you missed it, Pixel2Style2Pixel is nowadays used in many image editing apps because it has simple, yet effective ideas and it just works! Read 

more here: [https://t.me/casual\_gan/16](https://t.me/casual_gan/16)",guy see result psp encoder find paper extremely useful research gin inversion latent space projection deep learn base image edit want know main ideas paper encode style stylegan encoder image image translation pixel2style2pixel psp richardson et al head telegram channel url break main ideas popular gin paper case miss pixel2style2pixel nowadays use many image edit apps simple yet effective ideas work read url
jj4646,MachineLearning,1619157754.0,[D] neural tangent kernel,"Has anyone heard of the ""neural tangent kernel""? I originally had thought this was an activation function for a neural network. 

Looking here:
 https://en.m.wikipedia.org/wiki/Neural_tangent_kernel

""the neural tangent kernel (NTK) is a kernel which describes the evolution of deep artificial neural networks during their training by gradient descent. It allows ANNs to be studied using theoretical tools from Kernel Methods.""

Can someone please help me understand what this means? Why are neural tangent kernels important?

Thanks

https://rajatvd.github.io/NTK/",anyone hear neural tangent kernel originally think activation function neural network look url neural tangent kernel ntk kernel describe evolution deep artificial neural network train gradient descent allow anns study use theoretical tool kernel methods someone please help understand mean neural tangent kernels important thank url
Rishit-dagli,MachineLearning,1616898697.0,[P] Implementing Geoffery Hinton's latest idea paper," I am glad to today present my attempt to implement Geoffery Hinton's latest idea paper about representing part-whole hierarchies in neural networks. Also doing ML more the way the human brain does it!  
[https://github.com/Rishit-dagli/GLOM-TensorFlow](https://github.com/Rishit-dagli/GLOM-TensorFlow)

Consider giving it a star if you like it.",glad today present attempt implement geoffery hinton latest idea paper represent part whole hierarchies neural network also ml way human brain url give star like
opensourcecolumbus,MachineLearning,1619664415.0,"[Project] Framework to build AI powered search with just 7 lines of code. Supports semantic, text, image, audio & video search","Before [this open-source project(Jina)](https://github.com/jina-ai/jina), one has to depend on closed source solutions to implement neural search. With Jina helps you build our own semantic search engine that can

* Text to text search
* Image to image search
* Text to image search
* Audio to audio search
* Text to audio search
* Text to video search

**Being Open-Source(Apache 2.0 License),** you can modify it, host it on your infrastructure and be in complete control of your data.

**How is it different than Solr/Elasticsearch?**

* Solr/ElasticSearch implements Symbolic Search(rules-based based)
* Jina implements Neural Search(based on pre-trained deep learning models) which results in better semantic search and new capabilities such as cross-modal(e.g. text to video) and multi-modal(e.g. text+image/video to image/video/text) search

Appreciate your feedback/questions",open source project jina url one depend close source solutions implement neural search jina help build semantic search engine text text search image image search text image search audio audio search text audio search text video search open source apache nmbr license modify host infrastructure complete control data different solr elasticsearch solr elasticsearch implement symbolic search rule base base jina implement neural search base pre train deep learn model result better semantic search new capabilities cross modal e g text video multi modal e g text image video image video text searchappreciate feedback question
RoyalScores,MachineLearning,1619528216.0,[D] Is Object Detection a sub-optimal way to do triage and diagnosis in Medicine?,"Currently, Microscopy Object Detection is the fastest feasible method to do  diagnosis of parasitosis' for triage in underdeveloped countries.

Basically, the model is given hundreds of microscopy images of feces and applies Object Detection to all of them in order to find at least one out of a few possible cases of infection.

The way this impacts the problem is that the bulk of negative images can be eliminated by the model, and just the images that might contain eggs of parasites will be analyzed by a biomedic This person will then review the detection and classify it accordingly.

Recently it has come to me that the classification part of the scheme isnt important at all, and the problem could be solved with a approach more similar to Anomaly Detection, by creating a network that can classify images as ""suspicious of infection"" or negative images.

Are there any current researches going on about the specific problem of triage in ML, or is this problem novel? It seems urgent to solve this issues as the WHO has declared it intends to eliminate all NTDs by 2030 and the biggest obstacle to achive this is scalable cheap diagnosis.",currently microscopy object detection fastest feasible method diagnosis parasitosis triage underdevelop countries basically model give hundreds microscopy image feces apply object detection order find least one possible case infection way impact problem bulk negative image eliminate model image might contain egg parasites analyze biomedic person review detection classify accordingly recently come classification part scheme important problem could solve approach similar anomaly detection create network classify image suspicious infection negative image current research go specific problem triage ml problem novel seem urgent solve issue declare intend eliminate ntds nmbr biggest obstacle achive scalable cheap diagnosis
bendee983,MachineLearning,1617367196.0,[D] Machine learning business models in robotics,"Boston Dynamics' latest robot, Stretch, is boring in comparison to the company's previous robots. It can't dance, backflip, and do any of the other tricks that Spot, Handle, and Atlas could do. But it might be the most commercially successful robot the company has created so far.

Successful autonomous mobile robots hinge on versatility/robustness on the one hand, and cost-efficiency on the other.

On the versatility side, they follow the rules of machine learning: The narrower the domain, the more robust the ML model. You can have a robot that can do many tricks and fail often, or one that can do a few tricks but very robustly. Stretch fits this description perfectly. It does one thing (move boxes) in a predictable environment (flat grounds in warehouses), so you can rely on it to work safely and robustly in most cases. And given BD's long history in computer vision and robotics, they can push the limits of versatility beyond their competitors without compromising robustness/safety.

On the cost-efficiency side, since BD has been acquired by Hyundai, they will be in a better position to manufacture their robots at low costs, and then ship further enhancement props to make them even more versatile.

So, Stretch is not cool, but has the potential to turn BD into a profitable company. Meanwhile, it can continue on working to push the limits of science with its research on humanoid and biped robots.

It's kinda like the patent-clerk job Einstein held in the early 1900s. It helped him pay the bills while he used his idle time to develop some of the most important scientific theories of history.

Read the full analysis of BD's new robot and what it means for the company's future here:

[https://bdtechtalks.com/2021/04/01/boston-dynamics-stretch-robot/](https://bdtechtalks.com/2021/04/01/boston-dynamics-stretch-robot/)",boston dynamics latest robot stretch bore comparison company previous robots dance backflip trick spot handle atlas could might commercially successful robot company create far successful autonomous mobile robots hinge versatility robustness one hand cost efficiency versatility side follow rule machine learn narrower domain robust ml model robot many trick fail often one trick robustly stretch fit description perfectly one thing move box predictable environment flat ground warehouse rely work safely robustly case give bd long history computer vision robotics push limit versatility beyond competitors without compromise robustness safety cost efficiency side since bd acquire hyundai better position manufacture robots low cost ship enhancement prop make even versatile stretch cool potential turn bd profitable company meanwhile continue work push limit science research humanoid biped robots kinda like patent clerk job einstein hold early 1900s help pay bill use idle time develop important scientific theories history read full analysis bd new robot mean company future url
GiuPaolo,MachineLearning,1618831031.0,[R] Sparse Reward Exploration via Novelty Search and Emitters,"Excited to announce that our work on dealing with **sparse rewards environments** through **Novelty Search and emitters** has been accepted at GECCO 2021 for publication!

You can find it here: [https://arxiv.org/abs/2102.03140](https://arxiv.org/abs/2102.03140)

The code instead is released on: [https://gpaolo.github.io/SERENE/](https://gpaolo.github.io/SERENE/)

Check it out and if you have any question do not hesitate to ask!

**Abstract:**

Reward-based optimization algorithms require both exploration, to find rewards, and exploitation, to maximize performance. The need for efficient exploration is even more significant in sparse reward settings, in which performance feedback is given sparingly, thus rendering it unsuitable for guiding the search process. In this work, we introduce the SparsE Reward Exploration via Novelty and Emitters (SERENE) algorithm, capable of efficiently exploring a search space, as well as optimizing rewards found in potentially disparate areas. Contrary to existing emitters-based approaches, SERENE separates the search space exploration and reward exploitation into two alternating processes. The first process performs exploration through Novelty Search, a divergent search algorithm. The second one exploits discovered reward areas through emitters, i.e. local instances of population-based optimization algorithms. A meta-scheduler allocates a global computational budget by alternating between the two processes, ensuring the discovery and efficient exploitation of disjoint reward areas. SERENE returns both a collection of diverse solutions covering the search space and a collection of high-performing solutions for each distinct reward area. We evaluate SERENE on various sparse reward environments and show it compares favorably to existing baselines.",excite announce work deal sparse reward environments novelty search emitters accept gecco nmbr publication find url code instead release url question hesitate ask abstract reward base optimization algorithms require exploration find reward exploitation maximize performance need efficient exploration even significant sparse reward settings performance feedback give sparingly thus render unsuitable guide search process work introduce sparse reward exploration via novelty emitters serene algorithm capable efficiently explore search space well optimize reward find potentially disparate areas contrary exist emitters base approach serene separate search space exploration reward exploitation two alternate process first process perform exploration novelty search divergent search algorithm second one exploit discover reward areas emitters e local instance population base optimization algorithms meta scheduler allocate global computational budget alternate two process ensure discovery efficient exploitation disjoint reward areas serene return collection diverse solutions cover search space collection high perform solutions distinct reward area evaluate serene various sparse reward environments show compare favorably exist baselines
MushiML,MachineLearning,1619776404.0,[D] Temperature term in SimCLR or MoCo papers.," Hi!

I read an interesting article on SimCLR and it was quite helpful. 

[https://amitness.com/2020/03/illustrated-simclr/](https://amitness.com/2020/03/illustrated-simclr/)

What is the real purpose of term temperature in the loss function? Please can anyone help in understanding it with some intuitive example. Also, I found this temperature term in the MoCo paper; both of them means the same? I found the following comment on this blog post ([https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e](https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e)), but I don't think that I really understood what does it mean.

""Chen et al. found that an appropriate temperature parameter can help the model learn from hard negatives. In addition, they showed that the optimal temperature differs on different batch sizes and number of training epochs.""

Thanks",hi read interest article simclr quite helpful url real purpose term temperature loss function please anyone help understand intuitive example also find temperature term moco paper mean find follow comment blog post url think really understand mean chen et al find appropriate temperature parameter help model learn hard negative addition show optimal temperature differ different batch size number train epochs thank
tdls_to,MachineLearning,1619374980.0,[D] Can you train a privacy-aware language model,"Pretrained language models memorize training data which can be uncovered by probing the model with appropriate prompts. This has some serious privacy implications. Here is a paper discussing the same; I would love to hear more about this.

[https://arxiv.org/abs/2104.07504](https://arxiv.org/abs/2104.07504)",pretrained language model memorize train data uncover probe model appropriate prompt serious privacy implications paper discuss would love hear url
kk_ai,MachineLearning,1619004138.0,[D] Convenient libs to use for new research project at the intersection of GNN and RL.,"If you were to start new research project (starting point is the ""rough idea"" what you want to do), that can potentially take 1+ years, what tools would you pick?

There is number of libraries for each domain (GNN, RL), and they are based on TF/Keras, PyTorch or JAX. To that end, it's a bit overwhelming to review all of them to make an informed choice.

I did my homework before, so I have some candidates in mind which I am intentionally not sharing, because I don't want to bias the discussion towards any particular direction.

In general, I don't mind learning new libraries, if it promises some flexibility in the future and has growing community of users.

Pls advice, thx.",start new research project start point rough idea want potentially take 1 years tool would pick number libraries domain gnn rl base tf keras pytorch jax end bite overwhelm review make inform choice homework candidates mind intentionally share want bias discussion towards particular direction general mind learn new libraries promise flexibility future grow community users pls advice thx
techsucker,MachineLearning,1619587796.0,"[R] Researchers at JAIST, the Japan Advanced Institute of Science and Technology, Have Proposed a Model that Allows Voices to Mimic and Control the Generated Speech’s Speaker Identity","Voice Conversion (VC) method used to modify the speaker’s identity without altering the linguistic content. Non-linguistic information is vital for having natural (human-to-human) communication. By changing the non-linguistic information, such as adding emotion to speech, VC can make human-machine communication sound more natural. This allows people to get more information from speech and thus socialize better. 

Humans use several languages for communication, and we often need machine translators for speech-to-speech conversions. Prof. Akagi from JAIST explains that conventional (monolingual) VC models face challenges when we apply them to a “cross-lingual” VC (CLVC) task. For example, changing the speaker’s identity led to an undesirable modification of linguistic information.

Summary: [https://www.marktechpost.com/2021/04/27/researchers-at-jaist-the-japan-advanced-institute-of-science-and-technology-have-proposed-a-model-that-allows-voices-to-mimic-and-control-the-generated-speechs-speaker-identity/](https://www.marktechpost.com/2021/04/27/researchers-at-jaist-the-japan-advanced-institute-of-science-and-technology-have-proposed-a-model-that-allows-voices-to-mimic-and-control-the-generated-speechs-speaker-identity/) 

Paper: [https://ieeexplore.ieee.org/document/9367139](https://ieeexplore.ieee.org/document/9367139)",voice conversion vc method use modify speakers identity without alter linguistic content non linguistic information vital natural human human communication change non linguistic information add emotion speech vc make human machine communication sound natural allow people get information speech thus socialize better humans use several languages communication often need machine translators speech speech conversions prof akagi jaist explain conventional monolingual vc model face challenge apply cross lingual vc clvc task example change speakers identity lead undesirable modification linguistic information summary url paper url
jj4646,MachineLearning,1619072335.0,"[D] is the ""curse of dimensionality"" still as relevant as it was 20 years ago?","I have been reading some good examples that explain (in layman's terms) what is the curse of dimensionality. 

These examples first considers a circle inside a square (2 dimensions: example 1) - and then considers a sphere inside a cube (3 dimensions: example 2). This is to illustrate the fact that the cube in example 2 is a lot more ""emptier"" (ratio of volume between sphere and cube) compared to the square in example 1. As the number of dimensions increase (e.g. the cube becomes a hypercube in 4 dimensions), it can be mathematically shown that the ratio of emptiness increases more and more. In this analogy, the sphere represents the data and the cube represents the space which the data belongs to. These examples show us that in higher dimensions, we need exponentially more and more data to fill this space - thus, in higher dimensions, data becomes more ""sparse"", and this sparsity makes it harder to fit machine learning algorithms (I understand this is intuitively, but I don't know if there is a mathematical explanation behind why sparsity gives machine learning algorithms a hard time - perhaps sparsity makes some of the matrix calculations harder to calculate? ). Furthermore, it can be shown using ""Chernhoff's Inequality"" that in higher dimensions, data is probabilistically more likely to occupy the extremity regions of the space, further exacerbating the curse of dimensionality.

All this being said - how are modern machine learning examples (e.g. deep neural networks) able to overcome the curse of dimensionality? Some of these CNN (Convolutional Neural Networks) deal with pictures that are by nature high dimensional data, and thus likely to suffer from the curse of dimensionality. Yet companies like Google and Microsoft are constantly developing neural networks that are able to successfully make predictions on pictures. 

By the looks of it, it would appear that ""the curse of dimensionality is dead"" (or rather, it doesn't affect us that much as it once did). How are modern neural networks able to handle the curse of dimensionality? I was reading about techniques called ""manifold learning"" which are able to extract important information from the data and reduce the number of dimensions (thereby mitigating the curse of dimensionality). Just a thought - somewhere within the architecture and all the hidden layers in neural networks, is some form of ""dimensionality reduction"" taking place?",read good examples explain layman term curse dimensionality examples first consider circle inside square 2 dimension example 1 consider sphere inside cube 3 dimension example 2 illustrate fact cube example nmbr lot emptier ratio volume sphere cube compare square example 1 number dimension increase e g cube become hypercube nmbr dimension mathematically show ratio emptiness increase analogy sphere represent data cube represent space data belong examples show us higher dimension need exponentially data fill space thus higher dimension data become sparse sparsity make harder fit machine learn algorithms understand intuitively know mathematical explanation behind sparsity give machine learn algorithms hard time perhaps sparsity make matrix calculations harder calculate furthermore show use chernhoff inequality higher dimension data probabilistically likely occupy extremity regions space exacerbate curse dimensionality say modern machine learn examples e g deep neural network able overcome curse dimensionality cnn convolutional neural network deal picture nature high dimensional data thus likely suffer curse dimensionality yet company like google microsoft constantly develop neural network able successfully make predictions picture look would appear curse dimensionality dead rather affect us much modern neural network able handle curse dimensionality read techniques call manifold learn able extract important information data reduce number dimension thereby mitigate curse dimensionality think somewhere within architecture hide layer neural network form dimensionality reduction take place
Spotums,MachineLearning,1617791320.0,[D] Docos on ML like AlphaGo - The Movie,I really enjoyed watching AlphaGo - The Movie on youtube and was wondering if there are any similar documentaries related to Machine Learning that are worth watching as entertainment/infotainment without being too dry?,really enjoy watch alphago movie youtube wonder similar documentaries relate machine learn worth watch entertainment infotainment without dry
jfischer,MachineLearning,1617812523.0,[P] Datahut.ai: A directory of data science and data engineering projects,"https://datahut.ai is a new free website that provides statistics and analysis on the most popular data science and data engineering projects. My wife and I created this site because we were spending a lot of time researching which projects best fit a given use case, both for our clients and for our personal side projects. We cover over 100 projects, but are just scratching the surface. We’d love your feedback on what topics to cover and what additional content you’d like to see. Thanks!",url new free website provide statistics analysis popular data science data engineer project wife create site spend lot time research project best fit give use case clients personal side project cover nmbr project scratch surface wed love feedback topics cover additional content like see thank
soulslicer0,MachineLearning,1618352118.0,[D] Bayesian Machine Learning for KITTI Depth Estimation,"This code predicts depth from RGB images. But instead of producing depth alone, it produces a multimodal depth distribution for each pixel, in the form of a categorical distribution. This is useful for weeding out uncertain 3d points, or in downstream adaptive depth sensing tasks. We solve the task of Monocular, Stereo and Lidar Upsampling based depth estimation using the same architecture. The core network architecture is taken from PSM-Net and Neural RGBD

I note that bayesian machine learning has not really been explored in detail in research, especially when looking at multi-view stereo or sensor fusion.

\[Writeup\] [https://github.com/soulslicer/probabilistic-depth/blob/main/pics/explanation.pdf](https://github.com/soulslicer/probabilistic-depth/blob/main/pics/explanation.pdf)

\[Code\] [https://github.com/soulslicer/probabilistic-depth](https://github.com/soulslicer/probabilistic-depth)",code predict depth rgb image instead produce depth alone produce multimodal depth distribution pixel form categorical distribution useful weed uncertain 3d point downstream adaptive depth sense task solve task monocular stereo lidar upsampling base depth estimation use architecture core network architecture take psm net neural rgbdi note bayesian machine learn really explore detail research especially look multi view stereo sensor fusion writeup url url
harish-2306,MachineLearning,1616831163.0,[P] Looking for a teammate in implementing a neat algorithm in Python with C++ as the backend.,"Hi, I'm  20 and doing my major in the field of Data Science. I've planned to  create a python library for neat with a C++ backend so it'll faster than  the already existing neat library. This is my first time writing a  library but I'm very well experienced with Python, C++ and DL. I have  thought of writing the wrapper in SWIG.

We  cloud dicuss and change the technology or plans if needed. I'll be a  fun person to work with. So if you are interested to join drop a  message.

Thanks :)",hi nmbr major field data science plan create python library neat c backend faster already exist neat library first time write library well experience python c dl think write wrapper swig cloud dicuss change technology plan need fun person work interest join drop message thank
Inevitable_Engineer5,MachineLearning,1617281572.0,[D] Genetic Algorithm: the chromosome representation for Sliding Puzzle Solver?,"Hello, I want to solve the game: Sliding Puzzle Solver via Genetic Algorithm. But I don't have any idea, how should be the chromosome representation for the problem. For example I can encode each movement via bits (00 - down, 11-up, 01-right, 10-left). Its' OK but the  recombination will not work with this chromosome representation, because not all movements are allowed. Do you have any idea? thanks",hello want solve game slide puzzle solver via genetic algorithm idea chromosome representation problem example encode movement via bits 00 11 01 right 10 leave ok recombination work chromosome representation movements allow idea thank
ykilcher,MachineLearning,1620491472.0,[D] Paper Explained - Involution: Inverting the Inherence of Convolution for Visual Recognition (Full Video Analysis),"[https://youtu.be/pH2jZun8MoY](https://youtu.be/pH2jZun8MoY)

Convolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

3:00 - Principles of Convolution

10:50 - Towards spatial-specific computations

17:00 - The Involution Operator

20:00 - Comparison to Self-Attention

25:15 - Experimental Results

30:30 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2103.06255](https://arxiv.org/abs/2103.06255)

Code: [https://github.com/d-li14/involution](https://github.com/d-li14/involution)",url neural network cnns dominate computer vision almost decade apply two fundamental principles spatial agnosticism channel specific computations involution aim invert principles present spatial specific computation also channel agnostic result involution operator rednet architecture compromise classic convolutions newer local self attention architectures perform favorably term computation accuracy tradeoff compare either x200b outline 0 00 intro overview3 00 principles convolution10 50 towards spatial specific computations17 00 involution operator20 00 comparison self attention25 15 experimental results30 30 comment conclusion x200b paper url url
emystats,MachineLearning,1616606233.0,"[P] Best way to calculate ""performance"" in a probability estimation task","Thank you in advance for taking the time to read this long post!

I am working on a task in which participants estimate the probability that a series of beads are extracted from one of two hidden jars. The beads are extracted one by one, with replacement. 

The [two jars](https://i.stack.imgur.com/5fO8Hm.png) contain beads in two colors, yellow and black, in different proportions. *Jar A contains 85% yellow beads and 15% black beads, and jar B contains 85% yellow beads and 15% black beads.*

While the jars are hidden to the participant, he or she is aware of the difference between them - therefore he or she can estimate the probability that a sequence of beads is extracted specifically from one of the two jars. 

**Example**

After each extraction, the participant always answers the same question: ""What's the probability that the sequence was extracted from jar A?

Then, if the first bead extracted is yellow, it's more likely that bead was extracted from jar A; if the second bead is also yellow, it's now even more likely that the sequence of beads was extracted from jar A, and so on.

The estimation, of course, changes as the participant is shown more beads. All participants are shown the same sequence.

**The actual task**

At ""Event 0,"" the participant is asked the question before seeing any beads. This is why the estimation in this [plot](https://i.imgur.com/xl7qxlk.png) is at 50%. 
In the plot, you can see a red line: that's the estimations for one participant. On the x-axis, you can see the extraction number (or event number), on the y-axis the probability estimation.
In black you can see the ideal observer's estimation, that is, the correct probabilities.

Then the participant is shown a sequence of beads that have already been extracted. In this case, [8 yellow beads and 2 black beads](https://i.stack.imgur.com/P2ZTpm.png). 

That's why, at this point (Event 1), the sequence is most likely coming from jar A.

After that, the participant is shown more beads, extracted one by one. They all happen to be black (the participant does not know this beforehand, of course.) That's why the probability estimation slowly decreases.

[Final sequence](https://i.stack.imgur.com/nrBBmm.png)





**The problem**

I would like to define a ""performance profile"" for each participant, based on how he or she responds to the task. Then I would like to be able to correlate this ""profile"" with some psychometric results (average responses to surveys.)

About the ""performance profile"", I would like to have a good idea of how much the participant is far off from the ideal observer. I thought I could just calculate the distance between each pair of points on the y-axis and sum them up. Probably the absolute distance or the squared distance would be better, but I would also like to retain the ""sign"" (in other, similar tasks, the participants' responses both overestimate and underestimate the ideal observer's estimation.)

Using the distance, I could easily correlate the ""performance"" with the personality data.

**Question**

Does this make sense? I was wondering if there are better ways to perform this type of analysis. Is there a way I can retain more information about the participant's choices? For example, I thought I could fit one curve to the participant's response and one curve to the ideal observer's estimation and evaluate the difference between the parameters defining the curves, but I am not sure about how to go about that.

Someone has suggested that I instead perform a [time series k-means clustering](https://drkeithmcnulty.com/2020/03/02/clustering-time-series-data-in-r/) to group participant's responses. I am not familiar with this analysis but that could be an idea. But if I perform a clustering analysis, could I then see how each cluster performs with respect to the personality criteria? For example, ""people in cluster A are particularly high in X criterium."" I also thought about performing a [PCA](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/) to see how the personality criteria are correlated (another analysis I am not too familiar with!) The question is how to relate the psychometric results to the performance.

By the way, [here](https://i.stack.imgur.com/BVMSq.png) you can see an image of all the participants. 

If you have any ideas, or if you can recommend online example or tutorials I would really appreciate it!

R code for one participant:

    
    library(ggplot2)
    library(scales)
    
    # participant's probability estimations
    
    participant <- structure(list(event = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), prob_est = c(0.46, 
     0.98, 0.89, 0.72, 0.53, 0.21, 0.24, 0.12, 0.09, 0.01)), class = ""data.frame"", row.names = c(""1"", 
    ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10""))
    
    # ideal observer's probability estimations
    
    ideal_observer <- structure(list(event = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), prob = c(0.5, 
    0.99996979903057, 0.999828885289768, 0.999031123657329, 0.994534412955466, 
    0.969798657718121, 0.85, 0.5, 0.15, 0.0302013422818792)), row.names = c(NA, 
    10L), class = ""data.frame"")
    
    plot <- ggplot(data=subset(participant, event<=9), aes(x = event, y = prob_est, col=""red""))  + 
            geom_point(cex=1.5)+
            geom_line(lwd=0.9)+
            labs(x=""Event Number"", y=""Probability"") + 
            scale_y_continuous(breaks=pretty_breaks(n=10), limits = c(0,1))+
            scale_x_continuous(breaks=pretty_breaks(n=10))+
            geom_line(data=subset(ideal_observer, event<=9), aes(x = event, y = prob),col=""black"",lwd=0.9)+
            geom_point(data=subset(ideal_observer, event<=9), aes(x = event, y = prob),col=""black"",cex=1.5)
    plot

    # calculating discrepancy from ideal performance
    difference <-  sum(participant[,2] - ideal_observer[,2])
    difference
    #> [1] -2.743364

 <sup>Created on 2021-03-24 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>",thank advance take time read long post work task participants estimate probability series bead extract one two hide jar bead extract one one replacement two jar url contain bead two color yellow black different proportion jar contain 85 yellow bead 15 black bead jar b contain 85 yellow bead 15 black bead jar hide participant aware difference therefore estimate probability sequence bead extract specifically one two jar example extraction participant always answer question probability sequence extract jar first bead extract yellow likely bead extract jar second bead also yellow even likely sequence bead extract jar estimation course change participant show bead participants show sequence actual task event 0 participant ask question see bead estimation plot url 50 plot see red line estimations one participant x axis see extraction number event number axis probability estimation black see ideal observer estimation correct probabilities participant show sequence bead already extract case 8 yellow bead nmbr black bead url point event 1 sequence likely come jar participant show bead extract one one happen black participant know beforehand course probability estimation slowly decrease final sequence url problem would like define performance profile participant base respond task would like able correlate profile psychometric result average responses survey performance profile would like good idea much participant far ideal observer think could calculate distance pair point axis sum probably absolute distance square distance would better would also like retain sign similar task participants responses overestimate underestimate ideal observer estimation use distance could easily correlate performance personality data question make sense wonder better ways perform type analysis way retain information participant choices example think could fit one curve participant response one curve ideal observer estimation evaluate difference parameters define curve sure go someone suggest instead perform time series k mean cluster url group participant responses familiar analysis could idea perform cluster analysis could see cluster perform respect personality criteria example people cluster particularly high x criterium also think perform pca url see personality criteria correlate another analysis familiar question relate psychometric result performance way url see image participants ideas recommend online example tutorials would really appreciate r code one participant library ggplot2 library scale participant probability estimations participant structure list event c 0 1 2 3 4 5 6 7 8 9 prob_est c 0 46 0 98 0 89 0 72 0 53 0 21 0 24 0 12 0 09 0 01 class data frame row name c 1 2 3 4 5 6 7 8 9 10 ideal observer probability estimations ideal_observer structure list event c 0 1 2 3 4 5 6 7 8 9 prob c 0 5 0 99996979903057 0 999828885289768 0 999031123657329 0 994534412955466 0 969798657718121 0 85 0 5 0 15 0 0302013422818792 row name c na 10l class data frame plot ggplot data subset participant event 9 aes x event prob_est col red geom_point cex 1 5 geom_line lwd 0 9 labs x event number probability scale_y_continuous break pretty_breaks n 10 limit c 0 1 scale_x_continuous break pretty_breaks n 10 geom_line data subset ideal_observer event 9 aes x event prob col black lwd 0 9 geom_point data subset ideal_observer event 9 aes x event prob col black cex 1 5 plot calculate discrepancy ideal performance difference sum participant 2 ideal_observer 2 difference 1 2 743364 sup create 2021 03 24 reprex package url v0 3 0 sup
SQL_beginner,MachineLearning,1619536458.0,[D] Rules for Determining how much Data should he used in a Model,"This is a concept i always struggled with: in statistics, is ""more data always better""? 

Suppose you 50 years of data about hospital visits. You are interested in supervised classification. You have predictors such as age, height, weight, blood type, salary, etc. You are interested in predicting if the hospital stay will be less than 1 day or more than 1 day. This can be easily solved using random forest.

My dilemma is: using all 50 years of data might be able to capture a wide variety of patterns  ... but since we are interested in predicting future information, maybe some of the older data is less relevant and might surpress more current trends?

How do you deal with this problem?",concept always struggle statistics data always better suppose nmbr years data hospital visit interest supervise classification predictors age height weight blood type salary etc interest predict hospital stay less nmbr day nmbr day easily solve use random forest dilemma use nmbr years data might able capture wide variety pattern since interest predict future information maybe older data less relevant might surpress current trend deal problem
Rina-Panigrahy,MachineLearning,1619364646.0,"[R] Google-Workshop: Conceptual Understanding of Deep Learning, May 17. Join Us.","Please join us for a virtual Google workshop on “[Conceptual Understanding of Deep Learning](https://sites.google.com/view/conceptualdlworkshop/home)” 

**When**: May 17th 9am-4pm PST. 

**Where**: [Live over Youtube](https://www.youtube.com/watch?v=g5DGBWjiULQ),

**Goal:** How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological strides in recent decades, there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form. The goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning, characterizing the class of functions that can be learned, coming up with the right learning architecture that may (provably) learn multiple functions, concepts and remember them over time as humans do, theoretical understanding of language, logic, RL, meta learning and lifelong learning.

The speakers and panelists include **Turing award** winners Geoffrey Hinton, Leslie Valiant, and Godel Prize winner Christos Papadimitriou ([full-details](https://sites.google.com/corp/view/conceptualdlworkshop/home)).   

**Panel Discussion:** There will also be a panel discussion on the fundamental question of “**Is there a mathematical model for the Mind**?”. We will explore basic questions such as “Is there a provable algorithm that captures the essential capabilities of the mind?”, “How do we remember complex phenomena?”, “How is a knowledge graph created automatically?”, “How do we learn new concepts, function and action hierarchies over time?” and “Why do human decisions seem so interpretable?”

Twitter:[ \#ConceptualDLWorkshop](https://twitter.com/search?q=%23ConceptualDLWorkshop&src=recent_search_click). Please  [Retweet](https://twitter.com/rinapy/status/1384311169519788032).Hope to see you there!

Rina Panigrahy

([http://theory.stanford.edu/\~rinap](http://theory.stanford.edu/~rinap))",please join us virtual google workshop conceptual understand deep learn url may 17th 9am 4pm pst live youtube url brain mind perhaps even artificial one work algorithmic level deep learn produce tremendous technological stride recent decades unsettle feel lack conceptual understand work extent work current form goal workshop bring together theorists practitioners develop understand right algorithmic view deep learn characterize class function learn come right learn architecture may provably learn multiple function concepts remember time humans theoretical understand language logic rl meta learn lifelong learn speakers panelists include turing award winners geoffrey hinton leslie valiant godel prize winner christos papadimitriou full detail url panel discussion also panel discussion fundamental question mathematical model mind explore basic question provable algorithm capture essential capabilities mind remember complex phenomena knowledge graph create automatically learn new concepts function action hierarchies time human decisions seem interpretable twitter conceptualdlworkshop url please retweet url see rina panigrahy url
wattnurt,MachineLearning,1620102631.0,"[D] Are there any ML algorithms that can learn a simple ""X+1"" problem?","I've had this idea for a while, a very simplistic problem statement:

Suppose you have N binary inputs and N binary outputs. The simple problem is: for any arbitrary input, copy that to the output, but set one more output bit to 1. Any of them will do. As an example, input is 0010010, a correct output would be 0011010.

Now, N should of course be large enough to not just exhaustively learn the input/output set during training. An interesting side effect of this problem is also, for any input there are usually several correct outputs (as many as there were 0s on the input).

Are there any  ML algorithms that can learn something like this? I should note that I'm not so much interested in some heavily catered solution (e.g. where the problem statement has been encoded in the feature set or the model architecture), but more from a general learning power point of view. Are there algorithms that can learn this extremely simple rule?",idea simplistic problem statement suppose n binary input n binary output simple problem arbitrary input copy output set one output bite 1 example input 0010010 correct output would 0011010 n course large enough exhaustively learn input output set train interest side effect problem also input usually several correct output many 0s input ml algorithms learn something like note much interest heavily cater solution e g problem statement encode feature set model architecture general learn power point view algorithms learn extremely simple rule
Yuqing7,MachineLearning,1617727057.0,"[N] IBM, UMich & ShanghaiTech Papers Focus on Statistical Inference and Gradient-Boosting","A team from University of Michigan, MIT-IBM Watson AI Lab and ShanghaiTech University publishes two papers on individual fairness for ML models, introducing a scale-free and interpretable statistically principled approach for assessing individual fairness and a method for enforcing individual fairness in gradient boosting suitable for non-smooth ML models.

Here is a quick read: [Improving ML Fairness: IBM, UMich & ShanghaiTech Papers Focus on Statistical Inference and Gradient-Boosting](https://syncedreview.com/2021/04/06/improving-ml-fairness-ibm-umich-shanghaitech-papers-focus-on-statistical-inference-and-gradient-boosting/)

The papers [*Statistical Inference for Individual Fairness*](https://arxiv.org/pdf/2103.16714.pdf) *and* [*Individually Fair Gradient Boosting*](https://arxiv.org/pdf/2103.16785.pdf) are on arXiv.",team university michigan mit ibm watson ai lab shanghaitech university publish two paper individual fairness ml model introduce scale free interpretable statistically principled approach assess individual fairness method enforce individual fairness gradient boost suitable non smooth ml model quick read improve ml fairness ibm umich shanghaitech paper focus statistical inference gradient boost url paper statistical inference individual fairness url individually fair gradient boost url arxiv
ykilcher,MachineLearning,1618846736.0,[D] Paper Explained - NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (Full Video Analysis),"[https://youtu.be/CRlN-cYFxTk](https://youtu.be/CRlN-cYFxTk)

View Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:50 - View Synthesis Task Description

5:50 - The fundamental difference to classic Deep Learning

7:00 - NeRF Core Concept

15:30 - Training the NeRF from sparse views

20:50 - Radiance Field Volume Rendering

23:20 - Resulting View Dependence

24:00 - Positional Encoding

28:00 - Hierarchical Volume Sampling

30:15 - Experimental Results

33:30 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)

Website & Code: [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)",url synthesis tricky problem especially give sparse set image input nerf embed entire scene weight feedforward neural network train backpropagation differential volume render procedure achieve state art view synthesis include directional dependence able capture fine structural detail well reflection effect transparency x200b outline 0 00 intro overview4 50 view synthesis task description5 50 fundamental difference classic deep learning7 00 nerf core concept15 30 train nerf sparse views20 50 radiance field volume rendering23 20 result view dependence24 00 positional encoding28 00 hierarchical volume sampling30 15 experimental results33 30 comment conclusion x200b paper url code url
jj4646,MachineLearning,1619155542.0,"[D] can someone please explain the ""representer theorem"" in simpler term?","https://en.m.wikipedia.org/wiki/Representer_theorem

Can someone please try to explain the ""representer theorem"" in simpler terms?  Why is it considered important in the realm of machine learning?

Thanks",url someone please try explain representer theorem simpler term consider important realm machine learn thank
ClaudeCoulombe,MachineLearning,1619512223.0,"[D] Who first advanced the ""manifold hypothesis"" to explain the stunning generalization capacity of deep learning?","The manifold hypothesis states that natural data lies on a low-dimensional manifold (kind of local euclidian subspace) within the high-dimensional space where the data is encoded. I've already found [\[Clayton, 2005\] Algorithms for manifold learning](http://cseweb.ucsd.edu/~lcayton/resexam.pdf)  but it was written before the term ""deep learning"" was  invented by Hinton in 2007 and [\[Brahma, Wu, She, 2015\] Why deep learning works: A manifold disentanglement perspective](https://diginole.lib.fsu.edu/islandora/object/fsu:406789/datastream/PDF/view) which mentioned ""manifold hypothesis"" as a fact but without any citation.",manifold hypothesis state natural data lie low dimensional manifold kind local euclidian subspace within high dimensional space data encode already find clayton 2005 algorithms manifold learn url write term deep learn invent hinton nmbr brahma wu 2015 deep learn work manifold disentanglement perspective url mention manifold hypothesis fact without citation
niels_vg,MachineLearning,1617358952.0,Predict Company Sales - Design choices; how to store incoming sales/which model to choose? [P],"Hi Folks,

I am trying to predict the bookings of a stand-up comedian cafe. There are a lot of features I can use which have an affect on the number of sales. (e.g. day of the year, weather, average sales last month, day of the week, average sales on the specific day of the week etc.)

The goal is to create a model which is able to predict the number of sales for a given day once every hour, starting 10 days (240 hours) before the show and stops one hour before the deadline of the show. We have multiple predicting variables which are always known and do not change, these are:

* **avg\_30**. Average daily sales in the last 30 days
* **avg\_60**. Average daily sales in the last 60 days
* **vacation**. hot/cold variable which indicates whether it is a vacation day
* **LongtermGrowth**; the number of days the company already exists to measure continuous growth
* **DayofYear**. The day of the year. ( min: 1, max: 365)
* **Weekday**. Each day of the week hot/cold encoded. (moday: 0/1 etc.)

Apart from the above variables we have one more indicator (maybe the most important of all) which highly correlates with the eventual sales we are trying to predict. This is a combination of the two variables:

* **currentHour**. The number of hours before the start of the show (min: 0, max: 240)
* **Current sales.** The number of sales already obtained

***My Question***: Is what would be the best method to store all of the data above in such a way I can sufficiently train a prediction model on it. For the *currentHour* and *CurrentSales* I am in doubt between the following two methods.

1. I can simply use two column in the X data. where (1) indicates the number of hours before the deadline and (2) indicates the number of sales already obtained. However, this doesn't provide us any information on how the sales were obtained.
2. I can create a column for each hour before the deadline (240 columns in total). named hr\_01 until hr\_240. Where the value of each hour would be the number of sales yet obtained during that timestamp. The downside of this solution would be that 240 of the 246 columns only hold this data; is this a good idea? This will most likely drastically increase computational time?

*Concluding* I have the following two questions: **(1)** What would be the best of the two methods described above to store the sales yet obtained?, if you have a better idea than the two scenario's I created above, i would be glad to hear it! Furthermore, **(2)** what type of prediction model would you think results in the best performance? Currently I am seeing high performance with Linear Regression (Lasso & Ridge). I can hardly believe this would result in best performance.

Thanks in avance.",hi folks try predict book stand comedian cafe lot feature use affect number sales e g day year weather average sales last month day week average sales specific day week etc goal create model able predict number sales give day every hour start nmbr days 240 hours show stop one hour deadline show multiple predict variables always know change avg _30 average daily sales last nmbr days avg _60 average daily sales last nmbr days vacation hot cold variable indicate whether vacation day longtermgrowth number days company already exist measure continuous growth dayofyear day year min 1 max 365 weekday day week hot cold encode moday 0 1 etc apart variables one indicator maybe important highly correlate eventual sales try predict combination two variables currenthour number hours start show min 0 max 240 current sales number sales already obtain question would best method store data way sufficiently train prediction model currenthour currentsales doubt follow two methods 1 simply use two column x data 1 indicate number hours deadline 2 indicate number sales already obtain however provide us information sales obtain 2 create column hour deadline 240 columns total name hr _01 hr _240 value hour would number sales yet obtain timestamp downside solution would nmbr nmbr columns hold data good idea likely drastically increase computational time conclude follow two question 1 would best two methods describe store sales yet obtain better idea two scenario create would glad hear furthermore 2 type prediction model would think result best performance currently see high performance linear regression lasso ridge hardly believe would result best performance thank avance
jacobgil,MachineLearning,1619376340.0,[Project] Recent Class Activation Map Methods for CNNs and Vision Transformers,"[https://github.com/jacobgil/pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)

&#x200B;

CAM based methods are a family of pixel-attribution methods that try to highlight the parts in the image that contribute to a model output.

These methods assign weights to spatial 2D activations in the network, and them sum them to get a 2D saliency map. 

&#x200B;

This project includes a PyTorch implementation (that you can pip install) for several Class Activation Map methods, including a few very recent ones:

* Grad-CAM ([https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391))
* Grad-CAM++ ([https://arxiv.org/abs/1710.11063](https://arxiv.org/abs/1710.11063))
* XGrad-CAM ([https://arxiv.org/abs/2008.02312](https://arxiv.org/abs/2008.02312))
* Ablation-CAM ([https://ieeexplore.ieee.org/abstract/document/9093360/](https://ieeexplore.ieee.org/abstract/document/9093360/))
* Score-CAM ([https://arxiv.org/abs/1910.01279](https://arxiv.org/abs/1910.01279))

And works for Vision Transformers (tested with DeiT), as well as for CNNs (tested with torchvision.models).

&#x200B;

I hope it will be useful, and that it can be a convenient starting point for developing and comparing new methods!",url base methods family pixel attribution methods try highlight part image contribute model output methods assign weight spatial 2d activations network sum get 2d saliency map x200b project include pytorch implementation pip install several class activation map methods include recent ones grad cam url grad cam url xgrad cam url ablation cam url score cam url work vision transformers test deit well cnns test torchvision model x200b hope useful convenient start point develop compare new methods
TheCollaboratory,MachineLearning,1619194117.0,[P] Introducing The Collaboratory: A place to explore and discover research based on your interests,"Keeping track of new research is an increasingly time-consuming task in many fields. Fast-paced and interdisciplinary domains like ML are even harder to stay on top of, and require researchers to self-curate from a variety of sources (bookmarks, newsletters, arXiv reviews, etc.) on the topics relevant to them. The Collaboratory is a webservice that aims to ease the pain of exploring and keeping up with the research that’s most relevant to you.


Our platform automatically sources and recommends the latest research relevant to your interests, based on papers you seed into “reading lists”. These recommendations are powered by large language models that a) transform research descriptions into semantic embeddings, and b) compare those with pre-computed embeddings from a growing index of over 100M+ papers and datasets. Our models ensure that your recommendations are highly relevant, and our indexing efforts ensure that they are timely.


We were inspired to solve this problem after encountering it in our own daily lives, checking dozens of sources a week to stay on top of the literature in a few different fields. We think this tool will be really useful for people spinning up on new topics, people whose interests lie in-between fields, and for people who need to keep up with a stream of research that moves quickly.


Please take a look at the site and let us know what you think: [thecollaboratory.ai](https://thecollaboratory.ai). 


The platform remains a work in progress, but we want to keep making it better - please let us know how we can make it better for you!",keep track new research increasingly time consume task many field fast pace interdisciplinary domains like ml even harder stay top require researchers self curate variety source bookmarks newsletters arxiv review etc topics relevant collaboratory webservice aim ease pain explore keep research thats relevant platform automatically source recommend latest research relevant interest base paper seed read list recommendations power large language model transform research descriptions semantic embeddings b compare pre compute embeddings grow index 100m paper datasets model ensure recommendations highly relevant index efforts ensure timely inspire solve problem encounter daily live check dozens source week stay top literature different field think tool really useful people spin new topics people whose interest lie field people need keep stream research move quickly please take look site let us know think thecollaboratory ai url platform remain work progress want keep make better please let us know make better
kaia_1527,MachineLearning,1618075767.0,[D] Learning resources: multi-object localization,"Hey everyone, I'm learning about multi-object localization and was wondering whether anyone could recommend learning resources (with an end-to-end example). I went through the 2018 [fast.ai](https://fast.ai) [lesson](https://www.youtube.com/watch?v=0frKXR-2PBY) on object localization using the Pascal dataset but the examples are dated. I'm hoping to train a multi-object detector on a single class but have found the examples surprisingly sparse.",hey everyone learn multi object localization wonder whether anyone could recommend learn resources end end example go nmbr fast ai url lesson url object localization use pascal dataset examples date hop train multi object detector single class find examples surprisingly sparse
swifty540,MachineLearning,1619081651.0,[D] How repetitive are the Ambient Sounds?,"As a fan of the Ambient Sounds feature on the Apple HomePod, I was wondering about their repetitiveness and found this thread: [/r/HomePod/comments/dpu4me/does\_anyone\_else\_feel\_like\_the\_ambient\_sounds\_are](https://www.reddit.com/r/HomePod/comments/dpu4me/does_anyone_else_feel_like_the_ambient_sounds_are/)

I'm just not so sure about their actual repetitiveness as described in the thread and was wondering if Machine Learning could be used to figure out exactly how repetitive they actually are. Does anyone here have insights about tools that can detect repetitiveness in audio?",fan ambient sound feature apple homepod wonder repetitiveness find thread r homepod comment dpu4me _anyone _else _feel _like _the _ambient _sounds _are url sure actual repetitiveness describe thread wonder machine learn could use figure exactly repetitive actually anyone insights tool detect repetitiveness audio
mamrollahi,MachineLearning,1616864558.0,[D] Which similarity method has been used in WS353 dataset?,"Hello,

May I know, which similarity method has been used in [WS353 dataset](http://alfonseca.org/eng/research/wordsim353.html)?",hello may know similarity method use ws353 dataset url
KirillTheMunchKing,MachineLearning,1619540579.0,"[D] Main ideas from ""EigenGAN Layer-Wise Eigen-Learning for GANs"" explained!","# [EigenGAN Layer-Wise Eigen-Learning for GANs](https://t.me/casual_gan/31)

The authors propose a novel generator architecture that can intrinsically learn interpretable directions in the latent space in an unsupervised manner. Moreover each direction can be controlled in a straightforward way with a strength coefficient to directly influence the attributes such as gender, smile, pose, etc on the generated images.

[Samples and architecture overview](https://preview.redd.it/zds1xws1sqv61.png?width=1280&format=png&auto=webp&s=2f0ba0e21e513c5ce7bfb13c7d5e6d4eef43a89f)

&#x200B;

[Direction traversal examples](https://reddit.com/link/mzs8ct/video/746okqodsqv61/player)

Check out:\[[5 minute paper explanation](https://t.me/casual_gan/31)\] \[[Arxiv](https://arxiv.org/pdf/2104.12476.pdf)\]",eigengan layer wise eigen learn gans url author propose novel generator architecture intrinsically learn interpretable directions latent space unsupervised manner moreover direction control straightforward way strength coefficient directly influence attribute gender smile pose etc generate image sample architecture overview url traversal examples url 5 minute paper explanation url arxiv url
Anomalix,MachineLearning,1619811263.0,[D] Any good enough DCGANs that work on low-end GPUs?,"I want to experiment a bit with DCGANs, but 4GB of VRAM is not enough  for something like StyleGAN, so I was wondering if there's anything that  can run on lower end GPUs that's good enough. I want to be able to do  256x256 (or, if possible, 512x512, but I'm betting that's impossible on  such a low amount of VRAM).",want experiment bite dcgans 4gb vram enough something like stylegan wonder anything run lower end gpus good enough want able 256x256 possible 512x512 bet impossible low amount vram
blatant_variable,MachineLearning,1619388418.0,[R] Correcting Experience Replay for Multi-Agent Communication (ICLR 2021 Spotlight),"Hi, I'm first author of this paper on how RL agents can learn to communicate. In general, this is quite a challenge because as agents learn, their policies change, making multi-agent environments highly non-stationary. Our key insight is an Orwellian one -  we can use present information to alter past messages to improve future learning. This involves relabelling messages sampled from the replay buffer to reflect the current communication policies of each agent. We find this greatly enhances agents' abilities to learn, substantially improving the performance of multi-agent RL algorithms across a range of experiments.

Paper : [https://openreview.net/forum?id=xvxPuCkCNPO](https://openreview.net/forum?id=xvxPuCkCNPO)Video: [https://www.youtube.com/watch?v=piiNq-fGDCY](https://www.youtube.com/watch?v=piiNq-fGDCY)

Please ask any questions you may have - happy to answer!",hi first author paper rl agents learn communicate general quite challenge agents learn policies change make multi agent environments highly non stationary key insight orwellian one use present information alter past message improve future learn involve relabelling message sample replay buffer reflect current communication policies agent find greatly enhance agents abilities learn substantially improve performance multi agent rl algorithms across range experiment paper url url ask question may happy answer
minimaxir,MachineLearning,1619970229.0,[P] Create Your Own AI-Generated Magic: The Gathering Cards (2-cell Colab Notebook),"https://colab.research.google.com/drive/1VOt090UzvltoBgMdUZmU5vwhi4X-6E_a

A couple years ago I trained GPT-2 on Magic: The Gathering cards and it worked extremely well, however it was overengineered due to the limits around GPT-2 at the time.

Now, with much better AI text generation tooling, I trained a bespoke tiny GPT-2 (~1M parameters) on encoded Magic card data using [aitextgen](https://github.com/minimaxir/aitextgen) and its schema capabilities. Anecdotally the quality of cards generated is much better than the RNN approaches of the past. The cards follow the color pie now!

I was able to make a Colab Notebook with surprisingly little code if you unnest it, including the card decoder. The model is small enough that you can run locally on a CPU if you download the notebook.

Let me know if you have any questions!",url couple years ago train gpt 2 magic gather card work extremely well however overengineered due limit around gpt 2 time much better ai text generation tool train bespeak tiny gpt 2 1m parameters encode magic card data use aitextgen url schema capabilities anecdotally quality card generate much better rnn approach past card follow color pie able make colab notebook surprisingly little code unnest include card decoder model small enough run locally cpu download notebook let know question
forsakenMule,MachineLearning,1617823068.0,[D] Regularly retraining a churn prediction model,"I am trying to build a process which aims at predicting customer churn. And I am having a hard time finding good resources on how to tackle the problem of retraining the model.

Indeed while it's relatively ""easy"" to train a model for the first time given I have the right data, I am struggling to determine the best strategy to deal with regular retraining. 

Indeed let's assume I have trained a model wich performs well and the company decides to use it to derive actions on customers which have been flagged as potential churners. I am now faced with the issue that the new data which is available to me is biased by the actions that were taken due to the predictions of the model in production.

Reinforcement learning is not really an option as the feedback time is counted in years while the concept drift due to product changes and competitive pressure is rather in months.

Any idea or links on how go tackle this issue?",try build process aim predict customer churn hard time find good resources tackle problem retrain model indeed relatively easy train model first time give right data struggle determine best strategy deal regular retrain indeed let assume train model wich perform well company decide use derive action customers flag potential churners face issue new data available bias action take due predictions model production reinforcement learn really option feedback time count years concept drift due product change competitive pressure rather months idea link go tackle issue
nirmalya8,MachineLearning,1617254105.0,"""[D]"" Generating Medical Images using GANs","Hey people,

Medical  datasets either have less data or the datasets are very imbalanced. To  deal with this imbalance, I thought of synthetically generating medical  images of the class with less examples. So, I was looking for Papers on  Generating Medical Images with the help of Generative Adversarial  Networks.

Can I get some recommendations?

Constraints:

1. Dataset: No Constraint, preferably freely available
2. Either code for the paper or details about the architecture, loss functions, activation functions, etc should be there

Thank You!",hey people medical datasets either less data datasets imbalanced deal imbalance think synthetically generate medical image class less examples look paper generate medical image help generative adversarial network get recommendations constraints 1 dataset constraint preferably freely available2 either code paper detail architecture loss function activation function etc therethank
6rubtub9,MachineLearning,1619011427.0,[D] Meaning of semantic in machine learning,"Hi all, 

This may be an elementary question to ask, but as I am reading several computer vision papers I come across the phrases ""semantically useful features"", ""semantically strong features"", ""deep semantic features"" and so on.. 

I tried looking for the meaning of ""semantics"" in machine/deep learning domain but couldn't find any satisfying answer.

So can anyone explain what does it mean by ""semantically"" strong, deep, useful and so on.

Thank You",hi may elementary question ask read several computer vision paper come across phrase semantically useful feature semantically strong feature deep semantic feature try look mean semantics machine deep learn domain find satisfy answer anyone explain mean semantically strong deep useful thank
seuqaj114,MachineLearning,1618486921.0,[P] Nimbo: Run jobs on AWS with a single command,"Hey everyone,

My friend and I just launched [Nimbo](https://nimbo.sh), a dead-simple CLI that wraps AWS CLI, allowing you to run code on AWS as if you were running it locally. GitHub: [https://github.com/nimbo-sh/nimbo](https://github.com/nimbo-sh/nimbo). Docs: [https://docs.nimbo.sh](https://docs.nimbo.sh).

We decided to build this because we were frustrated with how cumbersome using AWS was, and we just wanted to be able to run jobs on AWS as easily as we run them locally. At the same time, we wanted to make use of cheap spot instances (on Nimbo, this is a single parameter). All in all, we didn't like the current user experience.

For this reason, we also provide many useful commands to make it faster and easier to work with AWS, such as easily checking prices, logging onto an instance, or syncing data to/from S3 (you can see some useful commands [here](https://docs.nimbo.sh/useful-commands)).

Unlike other similar services, we are solely client-side, meaning that the code runs on your EC2 instances and data is stored in your S3 buckets (we don't have a server; all the infrastructure orchestration happens in the Nimbo package).

We have tons of ideas for Nimbo, such as one-command Jupyter notebooks on EC2, add docker support, and (my personal favorite) provide images with preloaded large datasets like ImageNet, so that you don't have to download and store it yourself - you simply spin the instance, and the dataset is available at `/datasets`.

We are happy to receive any feedback and suggestions you have.",hey everyone friend launch nimbo url dead simple cli wrap aws cli allow run code aws run locally github url docs url decide build frustrate cumbersome use aws want able run job aws easily run locally time want make use cheap spot instance nimbo single parameter like current user experience reason also provide many useful command make faster easier work aws easily check price log onto instance sync data s3 see useful command url similar service solely client side mean code run ec2 instance data store s3 bucket server infrastructure orchestration happen nimbo package tons ideas nimbo one command jupyter notebooks ec2 add docker support personal favorite provide image preloaded large datasets like imagenet download store simply spin instance dataset available datasets happy receive feedback suggestions
khalilmeftah,MachineLearning,1616538667.0,[P] Generating CryptoPunks Images with GPT-2," 

[ Generated CryptoPunks with GPT-2](https://preview.redd.it/5633kncstuo61.png?width=768&format=png&auto=webp&s=b6fc29063f8cc97b0362231ca0c5cfa2f67c6516)",generate cryptopunks gpt 2 url
chimp73,MachineLearning,1619910179.0,"[D] How far can we get with one-shot learning, generalization and policy gradient?","[OpenAI research](https://arxiv.org/abs/2001.08361) shows that merely scaling up simple NNs improves performance, generalization and sample-efficiency.
Notably, fine-tuning GPT-3 converges after only [one epoch](https://github.com/cabhijith/GPT-3_Docs/blob/master/Fine-Tune.md).
This raises the question: Can very large NNs be so sample-efficient that they one-shot learn *in a single SDG updates* and reach human-level inference and generalization abilities (and beyond)?

Assuming such capabilities, I've been wondering what could an actor model look like that makes use of them: Chiefly, one could eliminate the large time horizons used in RNNs and Transformers, and instead continuously one-shot learn sensory transitions within a very brief time window, by predicting the next few seconds from previous ones. Further, one could dedicate some output neurons to driving some actuators and train them with policy gradient. Then long-term and near-term recall would simply be generalizations of one-shot learned sensory transitions, and, similarly, decision making would simply be generalization of one-shot learned modulations to the policy.

(To make clear what I mean by one-shot learning by SDG and recall by generalization: Let's say you are about to have dinner and you predict it is going to be pasta, but it's actually fish. Then the SDG update makes you one-shot learn what you ate that evening due to the prediction error. When asked what you ate the next day, then by generalization (from the context of yesterday, to the context of the question), you know it was fish.)

Further, one could use each prediction sample as an additional prediction target such that the model one-shot learns its own predictions as thoughts that have occurred; and by generalization and reward modulation, these thoughts become goal-driven, allowing the agent to ignore the prediction objective if it increases reward (e.g. pondering via inner monologue instead of listening). One would also need to feed the prediction sample as additional sensory input in each time step such that the model has access to these thoughts or predictions.

Then conscious thoughts are not in a latent space, but in sensory space. This matches the human mind, as we, too, cannot have thoughts beyond our model of the data generating process of sensory experience. Further, conscious thoughts would occur in brief time slices, which also matches human conscious thoughts, skipping from one thought to the other in almost discrete manner, with consciousness hence only existing briefly [during the forward passes](https://karpathy.github.io/2021/03/27/forward-pass/), and reality being re-interpreted each second afresh, tied together via one-shot learned contextual information in the previous steps.

By allowing the model to learn from imagined/predicted rewards too, imitation learning would be a simple consequence of generalization, namely by identifying the other agent with the self-model that naturally emerges.

The mere self-model of one's predictions or thoughts, being learned by predicting one's own predictions, seems  sufficient for thoughts to get strategically conditioned (by previous thoughts) such that they are goal-directed, again relying on generalization. I.e. the model may be conditioned to do X by a one-shot learned policy update, but by world knowledge it knows X only works in context Y (which establishes a subgoal). The model also knows that its thoughts act as predictors, thus, by generalization, in order to achieve X it generates a thought that the model expects to be completed in a manner that is useful to get to Y.

The architectural details may not matter much. Ignoring economic factors, there is not a large difference between different NN architectures so far. Even though Transformers [perform 10x better](https://arxiv.org/abs/2001.08361) than LSTMs (Fig. 7, left), there is no strong divergence, i.e. no evidence of LSTMs not being able achieve the same performance with about 10x more resources. Transformers seem to be mostly a trick to get large time horizons, but they are biologically implausible and also not necessary if you rely on one-shot learning tying together long-term dependencies instead of long time-horizons.

Generalization would side-step the issue of meticulously backpropping long-term dependencies by temporal unrolling or exhaustively backpropping value information throughout state space in RL. Policy gradients are extremely noisy, but human-level or higher generalization ability might be able to filter the one-shot learned noisy updates, because, by common sense (having learned how the world works though the prediction task), the model will conclude how the learned experience of pain or pleasure plausibly relates to certain causes in the world.",openai research url show merely scale simple nns improve performance generalization sample efficiency notably fine tune gpt 3 converge one epoch url raise question large nns sample efficient one shoot learn single sdg update reach human level inference generalization abilities beyond assume capabilities wonder could actor model look like make use chiefly one could eliminate large time horizons use rnns transformers instead continuously one shoot learn sensory transition within brief time window predict next second previous ones one could dedicate output neurons drive actuators train policy gradient long term near term recall would simply generalizations one shoot learn sensory transition similarly decision make would simply generalization one shoot learn modulations policy make clear mean one shoot learn sdg recall generalization let say dinner predict go pasta actually fish sdg update make one shoot learn eat even due prediction error ask eat next day generalization context yesterday context question know fish one could use prediction sample additional prediction target model one shoot learn predictions thoughts occur generalization reward modulation thoughts become goal drive allow agent ignore prediction objective increase reward e g ponder via inner monologue instead listen one would also need fee prediction sample additional sensory input time step model access thoughts predictions conscious thoughts latent space sensory space match human mind thoughts beyond model data generate process sensory experience conscious thoughts would occur brief time slice also match human conscious thoughts skip one think almost discrete manner consciousness hence exist briefly forward pass url reality interpret second afresh tie together via one shoot learn contextual information previous step allow model learn imagine predict reward imitation learn would simple consequence generalization namely identify agent self model naturally emerge mere self model one predictions thoughts learn predict one predictions seem sufficient thoughts get strategically condition previous thoughts goal direct rely generalization e model may condition x one shoot learn policy update world knowledge know x work context establish subgoal model also know thoughts act predictors thus generalization order achieve x generate think model expect complete manner useful get architectural detail may matter much ignore economic factor large difference different nn architectures far even though transformers perform 10x better url lstms fig 7 leave strong divergence e evidence lstms able achieve performance 10x resources transformers seem mostly trick get large time horizons biologically implausible also necessary rely one shoot learn tie together long term dependencies instead long time horizons generalization would side step issue meticulously backpropping long term dependencies temporal unroll exhaustively backpropping value information throughout state space rl policy gradients extremely noisy human level higher generalization ability might able filter one shoot learn noisy update common sense learn world work though prediction task model conclude learn experience pain pleasure plausibly relate certain cause world
hiDDenthings63,MachineLearning,1616820874.0,[D] How do I make a model which takes a bedroom image as input give an output of different design of bedroom related to input image?,"I want to make a model for my project which take some interior design image as input and provide me some output with some different kind of design related to the input image.  I don,t know where to start I think it would use CNN. but don't what should be the track to make. I didn't find anything related to it on google.",want make model project take interior design image input provide output different kind design relate input image know start think would use cnn track make find anything relate google
KirillTheMunchKing,MachineLearning,1618595253.0,[R] Spatially-Adaptive Pixelwise Networks for Fast Image Translation (ASAPNet) by Shaham et al. - Explained,"# [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://t.me/casual_gan/27)

The authors propose а novel architecture for efficient high resolution image to image translation. At the core of the method is a pixel-wise model with spatially varying parameters that are predicted by a convolutional network from a low-resolution version of the input. Reportedly, an 18x speedup is achieved over baseline methods with a similar visual quality. More details [here](https://t.me/casual_gan/27).

[ASAPNet](https://preview.redd.it/ablptrd5pkt61.png?width=1225&format=png&auto=webp&s=95fed8d1fbaa57d3481a88284604436c6190ecec)

 If you are not familiar with the paper check it out over [here](https://t.me/casual_gan/27).",spatially adaptive pixelwise network fast image translation url author propose а novel architecture efficient high resolution image image translation core method pixel wise model spatially vary parameters predict convolutional network low resolution version input reportedly 18x speedup achieve baseline methods similar visual quality detail url familiar paper check url
bendee983,MachineLearning,1616616039.0,[D] Has anyone tried Newton's method for ML optimization?,"Is there any scenario where Newton's method would be better than SGD, Adam, etc.? If yes, why isn't it included in ML/DL frameworks (unless I'm mistaken)?",scenario newton method would better sgd adam etc yes include ml dl frameworks unless mistake
deep-yearning,MachineLearning,1618494268.0,[D] What really makes neural networks generalizable?,"We usually employ regularization techniques (dropout, batch norm., early stopping, etc.) in helping prevent our models overfitting to the training set and perform well on the validation set. **However, this still does not usually help our models perform better on real data (test data), or data that is considered to be out-of-distribution of the training/validation sets.** This means that our models are not generalizable. 

This is clearly a huge area of research, but most papers that I have read do not lead to direct usable advice for our own projects, and are instead focused on more theoretical discussions of generalizability (please correct me if I am wrong and point me to better papers). So here is my question:

**What are your practical tips during training and development to get a model to actually perform well on real test data and out-of-distribution data?** Obviously, starting with training/validation data that matches the distribution of your expected test data is a great start - but we can't always predict or guarantee the distribution of our test data.",usually employ regularization techniques dropout batch norm early stop etc help prevent model overfitting train set perform well validation set however still usually help model perform better real data test data data consider distribution train validation set mean model generalizable clearly huge area research paper read lead direct usable advice project instead focus theoretical discussions generalizability please correct wrong point better paper question practical tip train development get model actually perform well real test data distribution data obviously start train validation data match distribution expect test data great start always predict guarantee distribution test data
amirninja,MachineLearning,1617643791.0,[Discussion] Similarity between two datasets/matrices,"Hello,

I have  a dataset and created another dataset from it using tow different methods, methods A and method B. 

I would like to find out how close the newly created dataset is to original dataset? What metrics would make more sense in this case? Cosine similarity/Euclidean distance? 

If I have to capture the closeness/difference in a single number how about using Frobenius Norm? 

This [subredit](https://www.reddit.com/r/MachineLearning/comments/r6i99/proper_method_for_calculating_similarity_between/) seems to suggest Mantel test. However, I am not sure if thats the right one here? Any thoughts/help would be appreciated.

&#x200B;

Thanks!",hello dataset create another dataset use tow different methods methods method b would like find close newly create dataset original dataset metrics would make sense case cosine similarity euclidean distance capture closeness difference single number use frobenius norm subredit url seem suggest mantel test however sure thats right one thoughts help would appreciate x200b thank
JEUNGHWAN,MachineLearning,1620611903.0,[P] AI Kant is willing to be ghostwriter only for you,"Hi, There! I dreamed of someone who writes a Philosophy essay in place of me when I majored in Philosophy. So I came up with the idea that I trained GPT-2 with ‘The Critique of Pure Reason’ of Kant and Kant becomes Ghostwriter and does write Kantian essay in place of students.

The project called Teachable NLP.

[https://forum.ainetwork.ai/t/teachable-nlp-kant-is-willing-to-be-ghostwriter-only-for-you/58](https://forum.ainetwork.ai/t/teachable-nlp-kant-is-willing-to-be-ghostwriter-only-for-you/58)

It is a program that helps fine-tuning natural language processing (NLP) models without complex code or a Graphics Processing Unit (GPU). So you can easily train and get your own NLP model.

There some use cases that you want it.

I'd appreciate any feedback and your thoughts!

Thanks.  


[Demo](https://reddit.com/link/n8t2ud/video/ogn2pttj97y61/player)",hi dream someone write philosophy essay place major philosophy come idea train gpt 2 critique pure reason kant kant become ghostwriter write kantian essay place students project call teachable nlp url program help fine tune natural language process nlp model without complex code graphics process unit gpu easily train get nlp model use case want appreciate feedback thoughts thank demo url
broutonlab,MachineLearning,1620219389.0,[D] Have you ever faced attacks on deep learning model in your projects?,"Adversarial Attacks\[[blog post](https://broutonlab.com/blog/adversarial-attacks-on-deep-learning-models)\] are a serious threat to DL models. An attacker can intentionally create malicious inputs to fool the DL model and get the incorrect outputs. This can lead to serious troubles, e.g. in banking applications that do user identification by his face or in citizen surveillance systems at the airport.

The questions are,

1 - When developing deep learning models, do you take into account the fact that they may come under Adversarial Attacks in production?

2 - If so, what methods of protection do you employ?

3 - Have you ever tried to attack DL models in production😎?",adversarial attack blog post url serious threat dl model attacker intentionally create malicious input fool dl model get incorrect output lead serious trouble e g bank applications user identification face citizen surveillance systems airport question 1 develop deep learn model take account fact may come adversarial attack production 2 methods protection employ 3 ever try attack dl model production
Yuqing7,MachineLearning,1619627967.0,[R] Google’s 1.3 MiB On-Device Model Brings High-Performance Disfluency Detection Down to Size,"A research team from Google Research proposes small, fast, on-device disfluency detection models based on the BERT architecture. The smallest model size is only 1.3 MiB, representing a size reduction of two orders of magnitude and an inference latency reduction of a factor of eight compared to state-of-the-art BERT-based models.

Here is a quick read: [Google’s 1.3 MiB On-Device Model Brings High-Performance Disfluency Detection Down to Size.](https://syncedreview.com/2021/04/28/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-7/)

 The paper *Disfluency Detection with Unlabeled Data and Small BERT Models* is on [arXiv](https://arxiv.org/pdf/2104.10769.pdf).",research team google research propose small fast device disfluency detection model base bert architecture smallest model size nmbr mib represent size reduction two order magnitude inference latency reduction factor eight compare state art bert base model quick read google nmbr mib device model bring high performance disfluency detection size url paper disfluency detection unlabeled data small bert model arxiv url
vaseline555,MachineLearning,1619268301.0,[D] Has anybody implemented FedAvg? I have a question," 

Hello,

I am now implementing FedAvg using PyTorch.

Even after looking into some Github repositories, I am still confusing: testing models

My main question is:

(1) Should I prepare test dataset for each client?  
(i.e. each client has its own training/test dataset)

OR

should I use globally identical test set for all client?  
(since after finishing FedAvg, all the clients have the model with the same weights)

\---

(Detailed description)

I am now dealing with MNIST dataset with 10 clients.

Each client has only one digit: Client 0 has digit 0, Client 1 has digit 1, ... , Client 9 has digit 9.  
(for the simulation of non-IID setting)

After local training, each client sends parameters to the central server, the server then aggregates them, and finally re-distribute the averaged model to clients.

For testing the averaged model's performance, should I test only once using test dataset having all digits (the former case in the above question), or test 9 times using locally split test set (each having only one digit) and calculating mean to report the final performance?

\----

It is very confusing... If the latter is true, each client finally has the ability to encode the unobserved data (e.g. for Client 0 having only 0 digit sample, it is finally available to encode another digits (1-9) after FedAvg). Am I right?

Thank you in advance!",hello implement fedavg use pytorch even look github repositories still confuse test modelsmy main question 1 prepare test dataset client e client train test dataset orshould use globally identical test set client since finish fedavg clients model weight detail description deal mnist dataset nmbr clients client one digit client nmbr digit 0 client nmbr digit 1 client nmbr digit 9 simulation non iid set local train client send parameters central server server aggregate finally distribute average model clients test average model performance test use test dataset digits former case question test nmbr time use locally split test set one digit calculate mean report final performance confuse latter true client finally ability encode unobserved data e g client nmbr nmbr digit sample finally available encode another digits 1 9 fedavg right thank advance
martin1285,MachineLearning,1618585781.0,[D] Serializing and using KNN for Predictions,"Hello,

I am working on a problem where I would like to use the K-Nearest Neighbors Regressor to make predictions. I am working in Python and I am using Joblib to serialize the model in a pickle format.

I understand that the K-Nearest Neighbors is instance-based and requires the entire training set to make predictions. When running this model through the pipeline (ETL, train model, serialize model and make predictions), I am able to make predictions using just the pickled model.

My understanding is that in Python, the training data is serialized along with the algorithm and does not require me to load the entire training data when making predictions. Resulting from this, the size of the pickled object gets larger as the training data increases.

Is my understanding of this correct from a deployment perspective? This would be my first time deploying an instance-based model.

Thanks in advance!",hello work problem would like use k nearest neighbor regressor make predictions work python use joblib serialize model pickle format understand k nearest neighbor instance base require entire train set make predictions run model pipeline etl train model serialize model make predictions able make predictions use pickle model understand python train data serialize along algorithm require load entire train data make predictions result size pickle object get larger train data increase understand correct deployment perspective would first time deploy instance base model thank advance
wdanilo,MachineLearning,1618363356.0,"[News] [Project] Enso 2.0 is out! Visual programming language for Data Science. It lets you code in a visual way in Python, Java, R, and JavaScript. Written in Rust and running in WebGL.","Hi, I'm Wojciech, one of the founders of Enso.

Enso is an award-winning interactive programming language with dual visual and textual representations. It is a tool that spans the entire stack, going from high-level visualization and communication to the nitty-gritty of backend services, all in a single language.

Enso is also a polyglot language - it lets you import any library from Enso, Java, JavaScript, R, or Python, and use functions, callbacks, and data types without any wrappers. The Enso compiler and the underlying GraalVM JIT compiler, compile them to the same instruction set with a unified memory model.

Check out:

* our **demo video**: [https://www.youtube.com/watch?v=fQvWMoOjmQk&t=2s&ab\_channel=Enso](https://www.youtube.com/watch?v=fQvWMoOjmQk&t=2s&ab_channel=Enso)
* our **website**: [https://enso.org](https://enso.org/)
* our **GitHub** (Enso is Open Source): [https://github.com/enso-org/enso](https://github.com/enso-org/enso)
* the **GraalVM** website (which Enso compiler bases on): [https://www.graalvm.org](https://www.graalvm.org/)",hi wojciech one founder enso enso award win interactive program language dual visual textual representations tool span entire stack go high level visualization communication nitty gritty backend service single language enso also polyglot language let import library enso java javascript r python use function callbacks data type without wrappers enso compiler underlie graalvm jit compiler compile instruction set unify memory model check demo video url website url github enso open source url graalvm website enso compiler base url
hardmaru,MachineLearning,1616464018.0,[N] NeurIPS2021 will be using openreview.net to manage submissions,"According to the program chairs, “NeurIPS 2021 will be using OpenReview to manage submissions this year but the reviewing process will not be public.  As in previous years, submissions will be visible only to their assigned program committee.  All internal discussions will remain private both during and after the reviewing process.  After the notification deadline, accepted and opted-in rejected papers will be made public, together with their anonymous reviews and meta-reviews.”

So unlike ICLR, the review process will still be private, and the reviews would be released only afterwards. Rejected submissions by default would not be revealed, unless the authors opt-in.

https://openreview.net/group?id=NeurIPS.cc/2021/Conference",accord program chair neurips nmbr use openreview manage submissions year review process public previous years submissions visible assign program committee internal discussions remain private review process notification deadline accept opt reject paper make public together anonymous review meta review unlike iclr review process still private review would release afterwards reject submissions default would reveal unless author opt url
techsucker,MachineLearning,1619392197.0,"[R] Scientists From The Max Planck Florida Institute For Neuroscience (MPFI) Have Developed ‘Gold Digger’, A Software Tool That Uses A Modified PIX2PIX Deep Learning Network","Electron microscopy (EM) is a method used for high-resolution images of biological and non-biological samples. It requires precise and time-consuming steps from sample preparation to image acquisition to produce the clarity and details required to visualize small cell structures with high resolution. Additionally, extracting the biological information out of EM-created images is a very laborious and time-intensive task. This is because the current EM analysis software usually requires the skilled eye to examine hundreds of pictures manually.

A team of scientists from the Max Planck Florida Institute for Neuroscience (MPFI) has applied neural networks to create a novel analysis software ‘Gold Digger,’ aimed at streamlining part of the lengthy process. [Diego Jerez and Eleanor Stuart, two high school data science students](https://techxplore.com/news/2021-04-gold-digger-neural-networks-nexus.html), started working on this project out of curiosity. But later, it turned into a more complex and interdisciplinary project.

Summary: [https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/](https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/) 

Paper: [https://www.nature.com/articles/s41598-021-87015-2](https://www.nature.com/articles/s41598-021-87015-2)",electron microscopy em method use high resolution image biological non biological sample require precise time consume step sample preparation image acquisition produce clarity detail require visualize small cell structure high resolution additionally extract biological information em create image laborious time intensive task current em analysis software usually require skilled eye examine hundreds picture manually team scientists max planck florida institute neuroscience mpfi apply neural network create novel analysis software gold digger aim streamline part lengthy process diego jerez eleanor stuart two high school data science students url start work project curiosity later turn complex interdisciplinary project summary url paper url
moon-child-99,MachineLearning,1618115946.0,[P] How do I validate a subjective model?,"I have a clustering model that classifies as best and worst, essentially a trade off of all the features provided. Since the classes are subjective, how do I validate this as there aren't test cases with the right answer.",cluster model classify best worst essentially trade feature provide since class subjective validate test case right answer
bendee983,MachineLearning,1616423246.0,[R] Adversarial training reduces safety of neural nets in robots,"Adversarial training results in a drop in pure accuracy of DL models, which is generally received as an acceptable tradeoff for robustness against adversarial attacks.

But when used in robots, the accuracy degradation can cause safety risks, according to a [research paper](https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/) by IST Austria, MIT, and TU Wien.

Key points:

\- Adversarial training has been designed for static image classification tasks, where each inference task is independent of others. Robotics deal with dynamic environments, where inferences are done in dependent sequences.

\- In robotics, it doesn't just matter how often errors happen but also *where* the errors take place, i.e. if they happen in consecutive cycles, they can result in the robot crashing

\- Classic adversarial training metrics only measure the number of misclassification errors. In robotics, it is also important how far the prediction error deviates from the correct label.

Read the coverage of the paper + interview with the lead author here:

[https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/](https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/)

Read the full paper here:

[https://arxiv.org/abs/2103.08187](https://arxiv.org/abs/2103.08187)",adversarial train result drop pure accuracy dl model generally receive acceptable tradeoff robustness adversarial attack use robots accuracy degradation cause safety risk accord research paper url ist austria mit tu wien key point adversarial train design static image classification task inference task independent others robotics deal dynamic environments inferences dependent sequence robotics matter often errors happen also errors take place e happen consecutive cycle result robot crash classic adversarial train metrics measure number misclassification errors robotics also important far prediction error deviate correct label read coverage paper interview lead author url full paper url
Rohit901,MachineLearning,1618904137.0,[D] When to use one-hot encoding of categorical variables?,"Hey all, I have 20 continuous input variables and 1 categorical variable which has 14 levels, so if I use one-hot dummy encoding then it will create 14 more variables as input, will it degrade model performance? I plan to use a simple multi-layer neural network and maybe add LSTM layer in it for classification task.

If it helps, I have 11,747 data points. I was planning to use create dummies method from pandas to encode categorical variables.",hey nmbr continuous input variables nmbr categorical variable nmbr level use one hot dummy encode create nmbr variables input degrade model performance plan use simple multi layer neural network maybe add lstm layer classification task help 11 747 data point plan use create dummy method pandas encode categorical variables
Appaulingly,MachineLearning,1618063524.0,[P] A machine learning tool now exists to detect Photoshop face warping!,"[Sheng-Yu Wang et. al.](https://arxiv.org/abs/1906.05856) have developed a ML tool to identify images that have been edited using Adobe Photoshop’s face aware liquify tool, a common tool utilised to warp facial features. The ML algorithm outperforms humans in identifying edited images and can even determine where editing has taken place in the image. I'd highly recommend checking out their works it's all fascinating.

I've created a subreddit ( r/FakeWarpBot ) which scrapes images from r/Instagramreality and r/KimKardashianPics and uses their tool to analyse the image for face warping. r/Instagramreality is subreddit where redditors post images purported to have been edited in different ways. But a lot of the time the suggested edits are too subtle for human determination, or in fact no-editing has taken place. So I hope that r/FakeWarpBot will provide a more certain determination of editing.

***You can now also post your own images to*** r/FakeWarpBot ***for analysis! I would stress that in no way is this machine earning tool a final determination on whether face warp has occurred.***

All credit goes to [Sheng-Yu Wang et. al.](https://arxiv.org/abs/1906.05856).

EDIT: spelling",sheng yu wang et al url develop ml tool identify image edit use adobe photoshops face aware liquify tool common tool utilise warp facial feature ml algorithm outperform humans identify edit image even determine edit take place image highly recommend check work fascinate create subreddit r fakewarpbot scrap image r instagramreality r kimkardashianpics use tool analyse image face warp r instagramreality subreddit redditors post image purport edit different ways lot time suggest edit subtle human determination fact edit take place hope r fakewarpbot provide certain determination edit also post image r fakewarpbot analysis would stress way machine earn tool final determination whether face warp occur credit go sheng yu wang et al url spell
aselsiriwardena,MachineLearning,1618556255.0,[P] Simple UI for deep learning model,"Hi! I'm looking for some help with the project I'm working on.

It's a  simple image-to-image translation project. I want to create a UI for that model. The project was built using PyTorch.I'm looking for a very simple UI just to demonstrate like selecting an image from the left side and showing the resulting image on the right side.

Any suggestions? Are there any broiler plate codes?",hi look help project work simple image image translation project want create ui model project build use pytorch look simple ui demonstrate like select image leave side show result image right side suggestions broiler plate cod
NahanTrogn,MachineLearning,1618195876.0,[Discussion] About pre-processing audio for Yamnet's input,"Hi community ML, 

This is first time I post a question in forum.

I read the input of [Yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet#input-audio-features) as:

""**Input: Audio Features**

As with our previous release VGGish, YAMNet was trained with audio features computed as follows:

All audio is resampled to 16 kHz mono. A spectrogram is computed using magnitudes of the Short-Time Fourier Transform with a **window** size of 25 ms, a window hop of 10 ms, and a periodic Hann window.

 A mel spectrogram is computed by mapping the spectrogram to 64 mel bins covering the range 125-7500 Hz. A stabilized log mel spectrogram is computed by applying log(mel-spectrum + 0.001) where the offset is used to avoid taking a logarithm of zero. 

These features are then **framed** into 50%-overlapping examples of 0.96 seconds, where each example covers 64 mel bands and 96 frames of **10 ms each**.""

So, my question is why each frame length 10 ms ( ""where each example covers 64 mel bands and 96 frames of **10 ms each""** )? But not 25 ms? I think that because 25 ms is a length of a window of STFT, while 10ms is only a ""window hop"".

Thanks in advance. Wish you a good day.",hi community ml first time post question forum read input yamnet url input audio feature previous release vggish yamnet train audio feature compute follow audio resampled nmbr khz mono spectrogram compute use magnitudes short time fourier transform window size nmbr ms window hop nmbr ms periodic hann window mel spectrogram compute map spectrogram nmbr mel bin cover range 125 7500 hz stabilize log mel spectrogram compute apply log mel spectrum 0 001 offset use avoid take logarithm zero feature frame 50 overlap examples nmbr second example cover nmbr mel band nmbr frame 10 ms question frame length nmbr ms example cover nmbr mel band nmbr frame 10 ms nmbr ms think nmbr ms length window stft 10ms window hop thank advance wish good day
ottawalanguages,MachineLearning,1619755336.0,[D] How sensitive are statistical models to the richness of information within the data?,"I spent the last hour thinking and creating an example that illustrates my question : https://imgur.com/a/36gU0pb

My question indirectly relates to exploratory data analytics and feature selection for statistical models. Suppose you have some variables (let's assume they are categorical variables for this example) - when you make a histogram for these variables, they appear extremely skewed. On first glance, you would not want to include heavily skewed variables as inputs for a statistical model - e.g., if 99% of the variable is a single value, how informative and useful could it be to a statistical model?

But how do you know that these heavily skewed variables don't contain some very useful information in the 1%, that might really help you in making future predictions?

Sometimes, using the context of the problem (e.g. if you working on a biology problem, consult with the biologists) you might be able to gain some insights - other times, you can try to use some logic to figure out if these heavily skewed variables are in fact useful for the model ... but when you have big and complex data, how is it possible?

I posted an example above that illustrates this problem (https://imgur.com/a/36gU0pb) - I would be curious to know if any of you have dealt with similar problems in the past.

Thanks",spend last hour think create example illustrate question url question indirectly relate exploratory data analytics feature selection statistical model suppose variables let assume categorical variables example make histogram variables appear extremely skew first glance would want include heavily skew variables input statistical model e g 99 variable single value informative useful could statistical model know heavily skew variables contain useful information 1 might really help make future predictions sometimes use context problem e g work biology problem consult biologists might able gain insights time try use logic figure heavily skew variables fact useful model big complex data possible post example illustrate problem url would curious know deal similar problems past thank
dgheere,MachineLearning,1619432105.0,[R] Survey on evaluation of algorithmically generated music. Can you tell the difference?,"Dear all,

At Utrecht University, I am conducting a scientific study on the evaluation of algorithmically generated music. I’m testing to see whether people can identify whether a piece of music is made by a computer or by Johann Sebastian Bach, specifically. It does not matter whether you are familiar with the style of Bach, or not.

I would very much appreciate it if you could take 5-10 minutes of your time to fill out this survey. It would greatly help me in writing my bachelor thesis. Your responses will, of course, be completely anonymous and be treated with confidentiality.

Thank you for your time!

https://survey.uu.nl/jfe/form/SV_865UclrBbCUhUb4",dear utrecht university conduct scientific study evaluation algorithmically generate music im test see whether people identify whether piece music make computer johann sebastian bach specifically matter whether familiar style bach would much appreciate could take 5 10 minutes time fill survey would greatly help write bachelor thesis responses course completely anonymous treat confidentiality thank time url
Professional-Bag392,MachineLearning,1617261893.0,[D] Is there a way to evaluate model during training?,"I am working on a Machine Learning Project. I have set up a ML pipeline for various stages of project. The Pipeline goes like -

Data Extraction -> Data Validation -> Preprocessing -> Training -> Model Evaluation

Now as Model Evaluation is concerned, currently it happens after training is completed. Based on evaluation, if model is approved or rejected. Now what I want is model evaluation to take place during training itself at any point. Say at about when 60% of the training is complete, the training is stopped and model is evaluated, based on which if the model is approved, it resumes the training.

How can the above scenario be implemented?",work machine learn project set ml pipeline various stag project pipeline go like data extraction data validation preprocessing train model evaluationnow model evaluation concern currently happen train complete base evaluation model approve reject want model evaluation take place train point say 60 train complete train stop model evaluate base model approve resume train scenario implement
johndoe709,MachineLearning,1616437000.0,[D] Gender Bias in Persian to English Google Translate,"The third person pronoun in many languages is neutral or so-called epicene. In translating texts from these languages into languages in which the third person pronoun is not neutral, such as English, the translator usually either determines the gender of the pronoun in general (he/she) or assumes it using evidence. Below you can see the translation of a text from Persian to English done by Google Translate:

[Gender Bias in Persian to English Translation](https://preview.redd.it/3xs7qtl6bmo61.png?width=1366&format=png&auto=webp&s=c784c3326862a13173a25837c405186f81259de8)

I have to point out that Google Translate is a lot better than it was a few years ago. And it helped me write this text. I'm curious about the solution to avoid bias in language models. And is there a solution at all? Because these models are trained with texts that we humans have produced.

This post is inspired by the following post:

[https://www.reddit.com/r/europe/comments/m9uphb/hungarian\_has\_no\_gendered\_pronouns\_so\_google/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/europe/comments/m9uphb/hungarian_has_no_gendered_pronouns_so_google/?utm_source=share&utm_medium=web2x&context=3)",third person pronoun many languages neutral call epicene translate texts languages languages third person pronoun neutral english translator usually either determine gender pronoun general assume use evidence see translation text persian english google translate gender bias persian english translation url point google translate lot better years ago help write text curious solution avoid bias language model solution model train texts humans produce post inspire follow post url
Stanford_Online,MachineLearning,1619652145.0,[N] Free webinar - Hacking AI: Security & Privacy of Machine Learning Models,"Register for our upcoming webinar with Stanford Professor Dan Boneh. He will discuss recent work at the intersection of cybersecurity and machine learning, with an emphasis on adversarial machine learning. [https://learn.stanford.edu/security-privacy-machine-learning-models-webinar.html](https://learn.stanford.edu/security-privacy-machine-learning-models-webinar.html)",register upcoming webinar stanford professor dan boneh discuss recent work intersection cybersecurity machine learn emphasis adversarial machine learn url
papajan18,MachineLearning,1619619546.0,"[R] ""Understanding Human Intelligence through Human Limitations"" - A Great Article that Highlights How to Think of the Relationship Between the Study of Human and Machine Intelligence","https://www.sciencedirect.com/science/article/pii/S1364661320302151

Some highlights of the article:

* Human intelligence arises from optimizing for three main constraints: limited time (lifespan), limited computation (must fit inside a single brain), and communication (must be able to transfer solutions to other brains to sustain humankind). 

* The space of problems human intelligence is solving is a subset of potential problems we want artificial intelligence to solve. Most machines do not face the same constraints humans do, so they will not necessarily have to use the same solution. 

* There are some use cases of machine intelligence where one has limited time, bandwidth, and high latency communication. In such cases, understanding how humans can optimize around such constraints can provide valuable insight.",url highlight article human intelligence arise optimize three main constraints limit time lifespan limit computation must fit inside single brain communication must able transfer solutions brain sustain humankind space problems human intelligence solve subset potential problems want artificial intelligence solve machine face constraints humans necessarily use solution use case machine intelligence one limit time bandwidth high latency communication case understand humans optimize around constraints provide valuable insight
qudcjf7928,MachineLearning,1618165059.0,"[D] Has anyone looked at ""LSPE"" algorithm as portfolio rebalancing method?"," 

[http://proceedings.mlr.press/v108/uziel20a/uziel20a.pdf](http://proceedings.mlr.press/v108/uziel20a/uziel20a.pdf)

This "" Long-and Short-Term Forecasting for Portfolio Selection with Transaction Costs"" paper claims it can produce positive returns even during the down market times.

Typical problems with the classical methods of portfolio rebalancing was that they were commission fees oblivious, so their models and results were quite not realistic. Ever since then, there has been numerous ways found to incorporate the said commission fees .... etc

And then I came across this LSPE paper but the problem is i have no idea what they are talking about.

I get that there are long-term portfolios that get rebalanced every d days, while there are short-term portfolios that when mod(t,d) != 0, then the agent can choose to update to the short term portfolio.

But I have no idea after ""the transition paths"" part. What are transition paths? what purpose do they serve and what are their dimensions and how are they used?",url long short term forecast portfolio selection transaction cost paper claim produce positive return even market time typical problems classical methods portfolio rebalancing commission fee oblivious model result quite realistic ever since numerous ways find incorporate say commission fee etcand come across lspe paper problem idea talk get long term portfolios get rebalanced every days short term portfolios mod 0 agent choose update short term portfolio idea transition paths part transition paths purpose serve dimension use
ykilcher,MachineLearning,1619899473.0,[D] Paper Explained - DINO: Emerging Properties in Self-Supervised Vision Transformers (Full Video Analysis),"[https://youtu.be/h3ij3F3cPIk](https://youtu.be/h3ij3F3cPIk)

Self-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI's new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for image retrieval and zero-shot k-nearest neighbor classifiers (KNNs).

&#x200B;

OUTLINE:

0:00 - Intro & Overview

6:20 - Vision Transformers

9:20 - Self-Supervised Learning for Images

13:30 - Self-Distillation

15:20 - Building the teacher from the student by moving average

16:45 - DINO Pseudocode

23:10 - Why Cross-Entropy Loss?

28:20 - Experimental Results

33:40 - My Hypothesis why this works

38:45 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)

Blog: [https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training)

Code: [https://github.com/facebookresearch/dino](https://github.com/facebookresearch/dino)",url learn final frontier representation learn get useful feature without label facebook ai new system dino combine advance self supervise learn computer vision new vision transformer vit architecture achieve impressive result without label attention map directly interpret segmentation map obtain representations use image retrieval zero shoot k nearest neighbor classifiers knns x200b outline 0 00 intro overview6 20 vision transformers9 20 self supervise learn images13 30 self distillation15 20 build teacher student move average16 45 dino pseudocode23 10 cross entropy loss 28 20 experimental results33 40 hypothesis works38 45 conclusion comment x200b paper url url url
Difficult_Parsnip876,MachineLearning,1620376445.0,[N] The Pastry A.I. That Learned to Fight Cancer,"[The Pastry A.I. That Learned to Fight Cancer | The New Yorker](https://www.newyorker.com/tech/annals-of-technology/the-pastry-ai-that-learned-to-fight-cancer)

&#x200B;

seems like the bread shops in my country should adopt this technology as well",pastry learn fight cancer new yorker url like bread shop country adopt technology well
MaJhole007,MachineLearning,1618824980.0,[D] BERT Finetuning/Domain Adaptation,"Usually when bert is finetuned on downstream tasks (task adaptation) only small dataset (500 samples) is required. 
I am wondering how much data do we need to finetune it on a specific domain like Finance (domain adaptation) . And do you expect the resulting model to outperform original Bert on downstream tasks in Financial domain ? 
Thanks in advance..",usually bert finetuned downstream task task adaptation small dataset 500 sample require wonder much data need finetune specific domain like finance domain adaptation expect result model outperform original bert downstream task financial domain thank advance
thunder_jaxx,MachineLearning,1616698147.0,[R] Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,"Abstract: 

In the standard Markov decision process formalism, users specify tasks by writing down a reward function. However, in many scenarios, the user is unable to describe the task in words or numbers, but can readily provide examples of what the world would look like if the task were solved. Motivated by this observation, we derive a control algorithm from first principles that aims to visit states that have a high probability of leading to successful outcomes, given only examples of successful outcome states. Prior work has approached similar problem settings in a two-stage process, first learning an auxiliary reward function and then optimizing this reward function using another reinforcement learning algorithm. In contrast, we derive a method based on recursive classification that eschews auxiliary reward functions and instead directly learns a value function from transitions and successful outcomes. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.

&#x200B;

Arxiv URL: [https://arxiv.org/abs/2103.12656](https://arxiv.org/abs/2103.12656)

*This is such a wonderful direction for RL. I am so excited papers are being written in this direction*",abstract standard markov decision process formalism users specify task write reward function however many scenarios user unable describe task word number readily provide examples world would look like task solve motivate observation derive control algorithm first principles aim visit state high probability lead successful outcomes give examples successful outcome state prior work approach similar problem settings two stage process first learn auxiliary reward function optimize reward function use another reinforcement learn algorithm contrast derive method base recursive classification eschew auxiliary reward function instead directly learn value function transition successful outcomes method therefore require fewer hyperparameters tune line code debug show method satisfy new data drive bellman equation examples take place typical reward function term experiment show approach outperform prior methods learn explicit reward function x200b arxiv url url wonderful direction rl excite paper write direction
jikkii,MachineLearning,1618864223.0,"[N] HuggingFace releases accelerate: A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision","HuggingFace releases a new PyTorch library: [Accelerate](https://github.com/huggingface/accelerate), for users that want to **use multi-GPUs or TPUs** without using an abstract class they can't control or tweak easily. With 5 lines of code added to a raw PyTorch training loop, *a script runs locally as well as on any distributed setup.*

They release an accompanying blog post detailing the API: [Introducing 🤗 Accelerate](https://huggingface.co/blog/accelerate-library).

Here's an example of what it looks like in practice:

[HuggingFace Accelerate in practice](https://preview.redd.it/me4g5rtmw6u61.png?width=1055&format=png&auto=webp&s=4839efdf6bfac121e1e3889c6df9235f47af7e06)

The library is fully open-sourced and available on PyPI and on GitHub; to learn more, check out the [documentation](https://huggingface.co/docs/accelerate/).",huggingface release new pytorch library accelerate url users want use multi gpus tpus without use abstract class control tweak easily nmbr line code add raw pytorch train loop script run locally well distribute setup release accompany blog post detail api introduce accelerate url example look like practice huggingface accelerate practice url library fully open source available pypi github learn check documentation url
fabien-campagne,MachineLearning,1618632546.0,[P] Multi-modality Perceiver implementation for Pytorch,"I forked Phil Wangs' repo implementing Perceiver for Pytorch and fixed two problems with the implementation:

1. The figure in the preprint and text indicate that the cross attention block is followed by a 'Latent Transformer'. The text indicates that the latent transformer has 6 blocks when training on ImageNet. This repo , to be fair, most re-implementations I have seen for Keras or JAX make the same mistake and use a single latent block, not a 'Latent Transformer' made up of several blocks. 
2. The signature of the forward method in this implementation cannot support training with multi-modality inputs. Multi-modality is when you want to train with video, audio and image for instance as inputs to the same model. The signature does not support multi-modality because each modality has a different number of dimensions, and positional encoding must be applied to each modality independently. This is not possible when accepting a single tensor as input to forward if positional encoding is done in the forward method.

The repo also offers an experimental contribution to help with text as one of the input modalities.

I have implemented fixes for these two issues in this fork: [https://github.com/fac2003/perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch), the package is available from PyPi (pip install [perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch)).",fork phil wangs repo implement perceiver pytorch fix two problems implementation 1 figure preprint text indicate cross attention block follow latent transformer text indicate latent transformer nmbr block train imagenet repo fair implementations see keras jax make mistake use single latent block latent transformer make several block 2 signature forward method implementation support train multi modality input multi modality want train video audio image instance input model signature support multi modality modality different number dimension positional encode must apply modality independently possible accept single tensor input forward positional encode forward method repo also offer experimental contribution help text one input modalities implement fix two issue fork url package available pypi pip install perceiver multi modality pytorch url
bayonetworking123,MachineLearning,1619155102.0,[P] Imputing values to training/testing data after model selection,"I'm developing a model to multiply impute missing values in covariate data from an experiment I ran. As expected, I selected a model, trained on some training data, based on test set fit. Am I running risk of leakage using this model to then predict missing values in the combined training/test data? 

This is a somewhat different scenario that I've typically experienced: I am trying to predict missing values in my *current* data without overfitting rather than predict data I will see in the future. I could use bootstrapped datasets/cross-validation instead of a testing set to avoid any issues, but I'm not sure if there is an issue here in the first place.",develop model multiply impute miss value covariate data experiment run expect select model train train data base test set fit run risk leakage use model predict miss value combine train test data somewhat different scenario typically experience try predict miss value current data without overfitting rather predict data see future could use bootstrapped datasets cross validation instead test set avoid issue sure issue first place
squidwardstrousers,MachineLearning,1616373467.0,[D] Why does computer vision get more attention than speech recognition?,I see a lot more tutorials and datasets geared towards computer vision than speech recognition. Why?,see lot tutorials datasets gear towards computer vision speech recognition
MonitorIndividual341,MachineLearning,1618801306.0,Prospects of Machine Learning and AI [Discussion],Just wanted opinions on the future of ml and AI. How major of a role will it play in the future? What about jobs? Also when should someone start learning the topic?,want opinions future ml ai major role play future job also someone start learn topic
jwestonhughes,MachineLearning,1616338331.0,[D] Explicitly modelling inter-operator variability in segmentation,"I have a medical image segmentation dataset where each example is segmented by exactly one of twelve different segmenters, with on the order of thousands of segmentations each. I want to understand if each segmenter differs systematically; for example, does one segmenter usually draw tighter segmentations, or usually cut off a piece of the segmentation more often. Does anyone know of any existing computer vision methods for looking at this? I could swear I saw a publication recently on building a model to predict every segmenter's segmentation, but I can't find it now and am wondering if I dreamed it.",medical image segmentation dataset example segment exactly one twelve different segmenters order thousands segmentations want understand segmenter differ systematically example one segmenter usually draw tighter segmentations usually cut piece segmentation often anyone know exist computer vision methods look could swear saw publication recently build model predict every segmenter segmentation find wonder dream
dlisfyn,MachineLearning,1619647752.0,[D] Need advice on how to generate HD point clouds visuals for a presentation,"Hi All

I've to show point cloud data as a part of presentation. I wanted to seek ideas on how can i render high definition point cloud visuals. Till now, I was taking screenshots from Meshlab and that didn't looked so good. Also, if someone is aware of how can i generate these rotating shape visuals ([here at 1:15](https://www.youtube.com/watch?v=1iuLxJmQII0) ) .

Thanks",hi alli show point cloud data part presentation want seek ideas render high definition point cloud visuals till take screenshots meshlab look good also someone aware generate rotate shape visuals 1 15 url thank
whyhateverything,MachineLearning,1617194894.0,[D] Good algorithm for clustering big data (sentences represented as embeddings)?,"Hi,

I have a lot of sentences (100.000) that are represented as embeddings through sentence transformers and I want to cluster them in the group (the number of sentences can be larger as well). All results on Google point out to Kmeans but I don't like it since it doesn't use cosine similarity, it's not scalable and very slow. At the same time, I am interested in finding a good algorithm that can help me cluster this amount of embeddings without losing quality and be time-friendly. I am also struggling in using other solutions since they also ask for the cluster number in advance and I cannit determine it for obvious reasons.

I must point out that I am not a professional machine learning engineer and even though I understand how to use some implementations and what are their disadvantages and advantages, I cannot rewrite optimizations on my own (I often see this happening in the research world where there are pros in data science, ML and AI).

Your help is very valuable and more than welcome!

Take care, I wish good health to everyone.",hi lot sentence 100 000 represent embeddings sentence transformers want cluster group number sentence larger well result google point kmeans like since use cosine similarity scalable slow time interest find good algorithm help cluster amount embeddings without lose quality time friendly also struggle use solutions since also ask cluster number advance cannit determine obvious reason must point professional machine learn engineer even though understand use implementations disadvantage advantage rewrite optimizations often see happen research world pros data science ml ai help valuable welcome take care wish good health everyone
spiritualParkour,MachineLearning,1619002088.0,[P] Coral Dev board vs coral Dev board mini,"Hope this is the right sub to ask.   
I went through the site, but I couldn't understand what the difference was. [https://coral.ai/products/dev-board-mini](https://coral.ai/products/dev-board-mini)   
[https://coral.ai/products/dev-board/](https://coral.ai/products/dev-board/)   


It'd be great if someone can point out. Thanks!",hope right sub ask go site understand difference url url great someone point thank
tdls_to,MachineLearning,1617115495.0,"[P] ML Product Challenge (free to participate, sponsored by a few startups)","hey people!

We are hosting a [machine learning product challenge](https://ai.science/_m/deep-learning-product-challenge-cohort-8?utm_source=rdt) where you can work with a small team (bring your team or meet peers and team up) to solve an interesting business problem using ML. There are  projects sponsored by a few startups as well as the opportunity to work on your own project:

* $3,000 prize: build a web app to automatically detect bias risk in ML models
* $3,000 prize: (tbc) build a recommender system web app 
* $1,000 prize: build an api to mine relationships between business use cases and technical concepts
* $300 prize: whatever you want to build

If you have any questions about this, please ping me on here, or join our [info session](https://www.eventbrite.ca/e/ml-product-challenge-info-session-xai-tickets-148260708771?aff=newsletter)

&#x200B;

ps. This challenge is free to join for everyone, all you need to do is to submit a proposal and then you'd be invited to participate (template for proposal and an instructional video series is provided)",hey people host machine learn product challenge url work small team bring team meet peer team solve interest business problem use ml project sponsor startups well opportunity work project 3 000 prize build web app automatically detect bias risk ml model 3 000 prize tbc build recommender system web app 1 000 prize build api mine relationships business use case technical concepts 300 prize whatever want buildif question please ping join info session url challenge free join everyone need submit proposal invite participate template proposal instructional video series provide
mennasiam,MachineLearning,1616527200.0,Video Class Agnostic Segmentation Benchmark in Autonomous Driving [R],"Check  out our work on video class agnostic segmentation in autonomous driving  ( to identify unknown objects jointly with semantic/panoptic  segmentation).

Paper: [https://arxiv.org/pdf/2103.11015.pdf](https://arxiv.org/pdf/2103.11015.pdf)

Code: [https://github.com/MSiam/video\_class\_agnostic\_segmentation](https://github.com/MSiam/video_class_agnostic_segmentation)

Demo:  [https://www.youtube.com/watch?v=c9hMFHdTs6M](https://www.youtube.com/watch?v=c9hMFHdTs6M)",check work video class agnostic segmentation autonomous drive identify unknown object jointly semantic panoptic segmentation paper url url url
Green_ninjas,MachineLearning,1620350367.0,[R] Easier machine learning conferences,"I know the NeurIPS deadline is coming up, but are there any easier peer-reviewed journals/conferences? I am a student and while I think my paper is of decent quality, I'm not sure it's up to the standards of NeurIPS without a faculty mentor.",know neurips deadline come easier peer review journals conferences student think paper decent quality sure standards neurips without faculty mentor
remymess,MachineLearning,1619896317.0,[N] Hybrid seminar platform,"Hey friends! 👋🏽👋🏽

Hope you are doing great!

We are Alain and Remy, both PhD in Mathematics and Statistics. Together with friends, we've been building a **hybrid** online-physical platform **sculpted for academic seminars**.

While we really miss physical seminars, we find that online seminars are amazing for a myriad of reasons (time, cost, ecology, networking, ...). We do not want these perks to stop. We envision a future where seminars will consist in a hybrid mixture of in-person and online audience, and are building our platform around this vision. We aspire to leverage modern technologies in academia to:

&#x200B;

* Make the life of the seminar organiser **as easy as possible**
* Allow every academic **to open the doors of any academic seminar with a couple clicks**  — *while some talks are public, others require a registration that needs to be reviewed by an organiser*
* Offer a place for **online seminar participants to socialise** with each other before and after the event (Virtual cafeteria in the form of a dynamic gather.town)
* Provide a way for **physical and online seminar participants to interact with each other**
* Give every community from every institutions **an equal chance to shine worldwide**: only the quality of their event matters.

If our vision resonates with you, please have a look at the website [https://agora.stream](https://agora.stream/)! We have been witnessing an exponential increase in traffic since our release. Research groups and university societies from Oxford, UCL, Stanford, and all over the world are publishing content on a regular basis.

The platform's evolution is driven by feedback so feel free reach out to us if you have any question / suggestion or would just like to meet for an informal chat! [https://calendly.com/remy-mess/e-coffee](https://calendly.com/remy-mess/e-coffee) (Special thanks to many of you who engaged with us last time we posted here <3! We loved meeting you and hearing about your experiences / ideas!)

See you very soon! 💪🏽

Alain and Remy

NB1: At the moment, the **platform is completely free** (we do not want money to be a barrier for knowledge). Very soon, we will deploy premium account for users who would want to enjoy some luxury features (e.g. ability to host seminars with more than 400+ participants) or simply support us.

NB2: To not miss out the hottest seminars of the moment or to hear about our new features, follow us on Twitter or Linked! ([https://twitter.com/agorastream](https://twitter.com/agorastream) \+ [https://www.linkedin.com/company/agorastream/](https://www.linkedin.com/company/agorastream/) )

NB3: As an appetizer, here is a small selection of some fun features we got:

* **Online seminar participants can socialise** with each other before and after the event (Dynamic gather.town)
* **Integrated application form for potential future speakers** on every community page.
* **Ability to clap at the end of a seminar** by smashing your space bar! (Fully working in our dev environment, to be released soon)
* Mobile app allowing **physical and online audience to interact with each other** (Fully working in our dev environment, to be released soon)",hey friends hope great alain remy phd mathematics statistics together friends build hybrid online physical platform sculpt academic seminars really miss physical seminars find online seminars amaze myriad reason time cost ecology network want perk stop envision future seminars consist hybrid mixture person online audience build platform around vision aspire leverage modern technologies academia x200b make life seminar organiser easy possible allow every academic open doors academic seminar couple click talk public others require registration need review organiser offer place online seminar participants socialise event virtual cafeteria form dynamic gather town provide way physical online seminar participants interact give every community every institutions equal chance shine worldwide quality event matter vision resonate please look website url witness exponential increase traffic since release research group university societies oxford ucl stanford world publish content regular basis platform evolution drive feedback feel free reach us question suggestion would like meet informal chat url special thank many engage us last time post 3 love meet hear experience ideas see soon alain remynb1 moment platform completely free want money barrier knowledge soon deploy premium account users would want enjoy luxury feature e g ability host seminars 400 participants simply support us nb2 miss hottest seminars moment hear new feature follow us twitter link url url nb3 appetizer small selection fun feature get online seminar participants socialise event dynamic gather town integrate application form potential future speakers every community page ability clap end seminar smash space bar fully work dev environment release soon mobile app allow physical online audience interact fully work dev environment release soon
apatus,MachineLearning,1617707302.0,Multi-Task learning for unbalanced classes [P],"I'm currently working a project, which assigns four classes to an input. So for example a target could be:

`[0,1,1,0]`

The problem is that for each class there are a different amount of 0s and 1s in the data. In the first class there are 900 0s and 50 1s. The problem now is, that my model is just always deciding in the favor of the over represented class. This means, that for class 1 it just always guesses a number close to 0.

I can't find a solution for this problem. I read a little bit about oversampling, but it's really hard to get the distribution right, since every item can have any number of classes. I'm really not sure how to approach this problem. Does anyone have an idea how I could solve this?",currently work project assign four class input example target could 0 1 1 0 problem class different amount 0s 1s data first class nmbr 0s nmbr 1s problem model always decide favor represent class mean class nmbr always guess number close 0 find solution problem read little bite oversampling really hard get distribution right since every item number class really sure approach problem anyone idea could solve
danparker276,MachineLearning,1616348052.0,"[D] Should I build my own AI bot for sms with azure ml studio, or use a 3rd party that say they understand speech better","So first off, cost isn't so much an issue and if it's then part of our IP, the better. But looking to build a concierge type bot where we take in responses and nurture leads to our live agents. There are products like verse that will offer a solution, but we already have all the sending logic and just need an API. Should we should build this ourself with a good AI/ML dev

We have 100million rows of data and either way we'll have to feed that into the system. I'm guessing whatever these 3rd parties have, with their language processing on top of azure cognitive services (luis, text processing), aws, google nlp...Probably that data processing will make more of a difference than what the 3rd party has, we would have to do that anyway. Some of the 3rd parties just work on top of those) And with the limited text for SMS, what they have in their logic isn't going to make much of a difference. They all say they have years of experience with conversation which makes them better.

Either way they all say, it will need to be live trained as well, when the bot doesn't know what to do, so I'm really wondering do these 3rd party blackboxes make much of a difference?",first cost much issue part ip better look build concierge type bot take responses nurture lead live agents products like verse offer solution already send logic need api build ourself good ai ml devwe 100million row data either way fee system guess whatever 3rd party language process top azure cognitive service luis text process aws google nlp probably data process make difference 3rd party would anyway 3rd party work top limit text sms logic go make much difference say years experience conversation make better either way say need live train well bot know really wonder 3rd party blackboxes make much difference
bloodcarter,MachineLearning,1619885051.0,[D] Black Box Interpretations,"Hi folks. I wrote  an overview of the interpretation problem. Is there any method I didn't covered, you you think I should? Would love your advice.

[https://dasha.ai/en-us/blog/black-box-interpretations-ml](https://dasha.ai/en-us/blog/black-box-interpretations-ml)",hi folks write overview interpretation problem method cover think would love advice url
ottawalanguages,MachineLearning,1618956370.0,[D] Why do polynomials have a bad reputation for overfitting?,"We all must have heard by now - when we start learning about statistical models overfitting data, the first example we are often given is about ""polynomial functions"" (e.g. see the picture here: https://ardianumam.wordpress.com/2017/09/22/deriving-polynomial-regression-with-regularization-to-avoid-overfitting/) .

We are warned that although higher degree polynomials can fit training data quite well. they surely will overfit and generalize poorly to the test data.

My question is : Why does this happen? Is there any mathematical justification as to why (higher degree) polynomial functions overfit the data? The closest explanation I could find online was something called ""Runge's Phenomenon"" (https://en.wikipedia.org/wiki/Runge%27s_phenomenon ), which suggests that higher order polynomials tend to ""oscillate"" a lot - does this explain why polynomial functions are known to overfit data?

I understand that there is a whole field of ""Regularization"" that tries to fix these overfitting problems (e.g. penalization can prevent a statistical model from ""hugging"" the data too closely) - but just using mathematical intuition, why are polynomials known to overfit the data?

In general, ""functions"" (e.g. the response variable you are trying to predict using machine learning algorithms) can be approximated using older methods like fourier series, taylor series and newer methods like neural networks. I believe that there are theorems that guarantee that taylor series, polynomials and neural networks can ""arbitrarily approximate"" any function. Perhaps neural networks can promise smaller errors for simpler complexity?

But does anyone know why polynomials are said to have a bad habit of overfitting, to the extent that neural networks have largely replaced them?

Interesting paper: https://www.nber.org/system/files/working_papers/w20405/w20405.pdf",must hear start learn statistical model overfitting data first example often give polynomial function e g see picture url warn although higher degree polynomials fit train data quite well surely overfit generalize poorly test data question happen mathematical justification higher degree polynomial function overfit data closest explanation could find online something call runge phenomenon url suggest higher order polynomials tend oscillate lot explain polynomial function know overfit data understand whole field regularization try fix overfitting problems e g penalization prevent statistical model hug data closely use mathematical intuition polynomials know overfit data general function e g response variable try predict use machine learn algorithms approximate use older methods like fourier series taylor series newer methods like neural network believe theorems guarantee taylor series polynomials neural network arbitrarily approximate function perhaps neural network promise smaller errors simpler complexity anyone know polynomials say bad habit overfitting extent neural network largely replace interest paper url
jakedageek127,MachineLearning,1616442294.0,[P] Mars 2020 Images Sorted by Content Diversity,"Hi all,

Spun up this quick project over the weekend! The [main outreach site](https://mars.nasa.gov/mars2020/multimedia/raw-images/) for raw images from the Mars 2020 Perseverance rover is great, but it only sorts in chronological order. These missions by nature take multiple pictures of the same target, so the gallery pages can get a bit repetitive.

I made a new (very simple, webdev is not my passion :) ) gallery site that uses the DEMUD algorithm (unsupervised novelty detection) to prioritize novel and interesting images. The result is a very content-dense and content-diverse collection of images you can skim through more easily. 

http://jakehlee.com/m2020-content-diverse/#

The ""How Does This Work"" button explains the methodology, but I'll paste it below. I'm trying to update the results once in the morning and once in the evening, since images roll in throughout the day. I'll also be in the comments to answer any questions!

---

Because Perseverance takes many images of the same object by design (such as calibration targets), sorting them chronologically in a gallery results in many pages of similar content. To find all of the different objects and content in the gallery, you have to flip through hundreds of pages of thousands of images.

The DEMUD algorithm solves this problem by prioritizing the set of images with the most diverse and novel content. It quickly identifies one of every item and ranks them for you to look at. If you give DEMUD a massive orchard, it will return with one of every fruit. This can be very useful for quickly finding interesting things in a large dataset.

The DEMUD algorithm is also capable of explaining why it thought an item was interesting enough to bring to your attention. It's not implemented here, but check it out below.

**Steps**

1. Extract features from each image with ResNet-50's last fully-connected layer prior to softmax
2. Find interesting images with DEMUD
3. Display the top 500 in order of prioritization

**Resources**

- [DEMUD GitHub Repository](https://github.com/wkiri/DEMUD)
- ""Guiding Scientific Discovery with Explanations using DEMUD"", Wagstaff et al., AAAI 2013 [paper link](https://wkiri.com/research/papers/wagstaff-demud-13.pdf)
- ""Unusual ChemCam Targets Disocvered Automatically in Curiosity's First Ninety Sols in Gale Crater, Mars"", Wagstaff et al., LPSC 2014 [paper link](http://www.hou.usra.edu/meetings/lpsc2014/pdf/1575.pdf)
- ""Interpretable Discovery in Large Image Data Sets"", Wagstaff and Lee, ICML WHI 2018 [paper link](https://wkiri.com/research/papers/wagstaff-interp-18.pdf)
- ""Visualizing Image Content to Explain Novel Image Discovery"", Lee and Wagstaff, DMKD 2020 [paper site](https://jakehlee.github.io/visualize-img-disc)",hi spin quick project weekend main outreach site url raw image mar nmbr perseverance rover great sort chronological order missions nature take multiple picture target gallery page get bite repetitive make new simple webdev passion gallery site use demud algorithm unsupervised novelty detection prioritize novel interest image result content dense content diverse collection image skim easily url work button explain methodology paste try update result morning even since image roll throughout day also comment answer question perseverance take many image object design calibration target sort chronologically gallery result many page similar content find different object content gallery flip hundreds page thousands image demud algorithm solve problem prioritize set image diverse novel content quickly identify one every item rank look give demud massive orchard return one every fruit useful quickly find interest things large dataset demud algorithm also capable explain think item interest enough bring attention implement check step 1 extract feature image resnet 50 last fully connect layer prior softmax2 find interest image demud3 display top nmbr order prioritization resources demud github repository url guide scientific discovery explanations use demud wagstaff et al aaai nmbr paper link url unusual chemcam target disocvered automatically curiosity first ninety sols gale crater mar wagstaff et al lpsc nmbr paper link url interpretable discovery large image data set wagstaff lee icml whi nmbr paper link url visualize image content explain novel image discovery lee wagstaff dmkd nmbr paper site url
Alternative_Detail31,MachineLearning,1617110460.0,[D] How effective is explainable AI in getting rid of biases?," I was going through a few Y Combinator companies in the recruitments/manpower sector and came across this company: [https://www.hiresweet.com/](https://www.hiresweet.com/)

According to their landing page, they advertised that they could streamline the recruitment process and use explainable AI in-order to get rid of biases. What would their setup possibly look like? (Are they doing something like assigning influence percentages to parameters or something like that?)

Blog posts/additional reading material about the effectiveness of bias reduction with XAI is welcome!",go combinator company recruitments manpower sector come across company url land page advertise could streamline recruitment process use explainable ai order get rid bias would setup possibly look like something like assign influence percentages parameters something like blog post additional read material effectiveness bias reduction xai welcome
techsucker,MachineLearning,1617164953.0,[N] Researchers at MIT and Amazon Study Pervasive Label Errors in Test Sets that Destabilize Machine Learning Benchmarks,"Large labeled data sets are crucial for successful supervised machine learning (ML) across several domains such as image classification, sentiment analysis, and audio classification. However, machine learning (ML) datasets are not perfectly labeled. The processes used to develop datasets often involve automatic labeling or crowdsourcing, inherently error-prone techniques.

Prior work has majorly focused on noises in train sets of ML datasets. Not many studies concentrate on label errors in test sets, Yet they have diverse potential consequences. No study has looked at systematic error across the most-cited ML test sets. 

Benchmark test datasets are used to evaluate the ML models and validate the theoretical findings. If label errors occurred extensively, they could potentially undermine the framework by which we measure machine learning progress. Label errors in the test sets could mislead practitioners to incorrect conclusions about the model’s performance. 

Summary: [https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/](https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/) 

Paper: https://labelerrors.com/paper.pdf

Demo: https://labelerrors.com/",large label data set crucial successful supervise machine learn ml across several domains image classification sentiment analysis audio classification however machine learn ml datasets perfectly label process use develop datasets often involve automatic label crowdsourcing inherently error prone techniques prior work majorly focus noise train set ml datasets many study concentrate label errors test set yet diverse potential consequences study look systematic error across cite ml test set benchmark test datasets use evaluate ml model validate theoretical find label errors occur extensively could potentially undermine framework measure machine learn progress label errors test set could mislead practitioners incorrect conclusions model performance summary url paper url url
GrettaGrove,MachineLearning,1619573475.0,[R] Self-Tuning Deep Reinforcement Learning,[https://arxiv.org/abs/2002.12928v2](https://arxiv.org/abs/2002.12928v2),url
GabrieleValvano,MachineLearning,1618841919.0,[R] Interested in GAN and Attention? Take a look at Adversarial Attention Gates!,"Find out how adversarial conditioning of attention gates improves object segmentation!Last but not least: try our new weakly annotated dataset :)

* **Project page:** [https://vios-s.github.io/multiscale-adversarial-attention-gates…](https://t.co/uW2bxky9q3?amp=1)
* **Paper:** [https://arxiv.org/abs/2007.01152](https://t.co/8NGlbxHPq7?amp=1)
* **Code:** [https://github.com/gvalvano/multiscale-adversarial-attention-gates…](https://t.co/Sf2xcgXqCi?amp=1)
* **Dataset:** [https://vios-s.github.io/multiscale-adversarial-attention-gates/data…](https://t.co/NkavT8UGDe?amp=1)

&#x200B;

https://preview.redd.it/cagml0gq25u61.png?width=1902&format=png&auto=webp&s=82a02d27929c2f853ad155d4d53bda7a28aa9b13

**Abstract**:

Large, fine-grained image segmentation datasets, annotated at pixel-level, are difficult to obtain, particularly in medical imaging, where annotations also require expert knowledge. Weakly-supervised learning can train models by relying on weaker forms of annotation, such as scribbles. Here, we learn to segment using scribble annotations in an adversarial game. With unpaired segmentation masks, we train a multi-scale GAN to generate realistic segmentation masks at multiple resolutions, while we use scribbles to learn their correct position in the image. Central to the model’s success is a novel attention gating mechanism, which we condition with adversarial signals to act as a shape prior, resulting in better object localization at multiple scales. Subject to adversarial conditioning, the segmentor learns attention maps that are semantic, suppress the noisy activations outside the objects, and reduce the vanishing gradient problem in the deeper layers of the segmentor. We evaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical (PPSS) datasets, and we report performance levels matching those achieved by models trained with fully annotated segmentation masks. We also demonstrate extensions in a variety of settings: semi-supervised learning; combining multiple scribble sources (a crowdsourcing scenario) and multi-task learning (combining scribble and mask supervision). We release expert-made scribble annotations for the ACDC dataset, and the code used for the experiments, at [https://vios-s.github.io/multiscale-adversarial-attention-gates](https://vios-s.github.io/multiscale-adversarial-attention-gates).

&#x200B;",find adversarial condition attention gate improve object segmentation last least try new weakly annotate dataset project page url paper url code url dataset url fine grain image segmentation datasets annotate pixel level difficult obtain particularly medical image annotations also require expert knowledge weakly supervise learn train model rely weaker form annotation scribble learn segment use scribble annotations adversarial game unpaired segmentation mask train multi scale gin generate realistic segmentation mask multiple resolutions use scribble learn correct position image central model success novel attention gate mechanism condition adversarial signal act shape prior result better object localization multiple scale subject adversarial condition segmentor learn attention map semantic suppress noisy activations outside object reduce vanish gradient problem deeper layer segmentor evaluate model several medical acdc lvsc chaos non medical ppss datasets report performance level match achieve model train fully annotate segmentation mask also demonstrate extensions variety settings semi supervise learn combine multiple scribble source crowdsourcing scenario multi task learn combine scribble mask supervision release expert make scribble annotations acdc dataset code use experiment url
sk81k,MachineLearning,1620485171.0,How big of a deal are scale free networks? [D],"I’m not sure if this is the right subreddit to post this in, but since network analysis has been up and coming in machine learning, I thought this might be the right place. I recently read Broido and Clauset’s “Scale Free Networks are Rare” paper and I was wondering how much of a difference modeling degree distributions as scale free vs non-scale-free actually matter. Is there a need for non-scale-free algorithms?",im sure right subreddit post since network analysis come machine learn think might right place recently read broido clausets scale free network rare paper wonder much difference model degree distributions scale free vs non scale free actually matter need non scale free algorithms
rarboot,MachineLearning,1617027809.0,[D] Andrew Ng's data-centric vs model-centric Machine Learning,"Regarding [https://youtu.be/06-AZXmwHjo?t=1048](https://youtu.be/06-AZXmwHjo?t=1048) (the timestamp was a choice of mine). I actually referenced this video in the [MLOps](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/) post in a deeper comment thread, but I felt it deserved its own discussion.

My abridged analysis:

* Machine Learning and data science are currently model-centric - In ""project-oriented"" teams (without a backing product), I would actually rephrase it as ""experiment-centric"", in the sense that most workflows are something in the lines of ""Problem \[under\]specification"" -> ""SOTA Literature review"" -> ""Data collection"" -> ""Model\[s\] estimation"" -> ""MVP"".
* Ng advocates for a data-centric approach, which alleviates the problem underspecification issue, and shifts focus from the often fruitless model fiddling to what actually delivers value - quality data.
* It doesn't feel like an actual ""MLOps"" talk, in the sense that no actual ""project patterns"" or ""tools"" are presented.
* Given the last point, in the QA, I felt that Andrew Ng's [comments](https://youtu.be/06-AZXmwHjo?t=3182) about ""maturity"" of MLOps tools and internet tutorials, and the ""vertical"" approach advocacy are actually a veiled critiscism to the current MLOps landscape.

I was underwhelmed at first, given that it's Andrew Ng and MLOps is such a hot topic, but afterwards it really bugged me how *bad* data management can be, especially in the ""project-oriented"" scenario.

Any thoughts?",regard url timestamp choice mine actually reference video mlops url post deeper comment thread felt deserve discussion abridge analysis machine learn data science currently model centric project orient team without back product would actually rephrase experiment centric sense workflows something line problem specification sota literature review data collection model estimation mvp ng advocate data centric approach alleviate problem underspecification issue shift focus often fruitless model fiddle actually deliver value quality data feel like actual mlops talk sense actual project pattern tool present give last point qa felt andrew ng comment url maturity mlops tool internet tutorials vertical approach advocacy actually veil critiscism current mlops landscape underwhelmed first give andrew ng mlops hot topic afterwards really bug bad data management especially project orient scenario thoughts
hou_yz,MachineLearning,1620040910.0,[R] Visualizing Adapted Knowledge in Domain Transfer,"[https://arxiv.org/abs/2104.10602](https://arxiv.org/abs/2104.10602)

This paper is the first attempt at visualizing what the models learn during domain adaptation. Specifically, it is found that for the source and target networks to makes similar predictions (compensate for their knowledge difference), a target image is forced to be translated to a completely ***unseen*** source style. Such results also indicate that we can rely on *models* rather than *images* for style transfer. 

https://preview.redd.it/hg7n3hmt2ww61.png?width=1739&format=png&auto=webp&s=0aeaeb209b41400f420aa78eb7f4405e764eb599",url paper first attempt visualize model learn domain adaptation specifically find source target network make similar predictions compensate knowledge difference target image force translate completely unseen source style result also indicate rely model rather image style transfer url
techsucker,MachineLearning,1619285060.0,"[N] MLCommons, AI Industry’s Performance Benchmark, Releases MLPerf™ Inference v1.0 Results To Understand The Power Usage of Machine Learning (ML) Models","Machine learning is an area that is being incorporated into almost every industry, and there has been a call for standard machine learning benchmarks similar to the SPEC benchmarks created primarily for the CPUs. These benchmarks would prove pivotal in comparing all the relative machine learning solutions available in the marketplace.

[MLCommons](https://mlcommons.org/en/) is an open engineering consortium that has been working in the direction of creating machine learning benchmarks for training and inference through its own platform MLPerf. MLCommons is usually listed as an industry-academic partnership that aims to advance the development and access of the latest AI and machine learning datasets. The benchmarks so created have been discussed and disclosed time and again to make people aware of the refinements taking place along the way. Recently, the company unveiled its platform Inference v1.0 and also released [2000 results](https://mlcommons.org/en/news/mlperf-inference-v10/) into the database. Not only this, but the company also disclosed a new power measurement technique of the platform that would look into providing additional metadata on these results.

Full Read: [https://www.marktechpost.com/2021/04/24/mlcommons-ai-industrys-performance-benchmark-releases-mlperf-inference-v1-0-results-to-understand-the-power-usage-of-machine-learning-ml-models/](https://www.marktechpost.com/2021/04/24/mlcommons-ai-industrys-performance-benchmark-releases-mlperf-inference-v1-0-results-to-understand-the-power-usage-of-machine-learning-ml-models/)",machine learn area incorporate almost every industry call standard machine learn benchmarks similar spec benchmarks create primarily cpus benchmarks would prove pivotal compare relative machine learn solutions available marketplace mlcommons url open engineer consortium work direction create machine learn benchmarks train inference platform mlperf mlcommons usually list industry academic partnership aim advance development access latest ai machine learn datasets benchmarks create discuss disclose time make people aware refinements take place along way recently company unveil platform inference v1 0 also release 2000 result url database company also disclose new power measurement technique platform would look provide additional metadata result full read url
GiuPaolo,MachineLearning,1618052104.0,[R] Last CFP for the Evolutionary RL workshop at GECCO 2021.,"Only 2 days before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)

In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.

Recent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.

Nevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.

The goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.

The topics at the heart of the workshop include:

* Evolutionary reinforcement learning
* Evolution strategies
* Population-based methods for policy search
* Neuroevolution
* Hard exploration and sparse reward problems
* Deceptive reward
* Novelty and diversity search methods
* Divergent search
* Sample-efficient direct policy search
* Intrinsic motivation, curiosity
* Building or designing behaviour characterizations
* Meta-learning, hierarchical learning
* Evolutionary AutoML
* Open-ended learning

Autors are invited to submit **new original work**, or **new perspectives on recently published work**  on those topics. Top submissions will be selected for oral presentation and be presented alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). 

&#x200B;

Important dates

* Submission deadline: **April 12, 2021**
* Notification: **April 26, 2021**
* Camera-ready: **May 3, 2021**

**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**",nmbr days deadline 1st evolutionary reinforcement learn workshop gecco 2021 premiere conference evolutionary compute year hold virtually lille france july 10 14 2021 recent years reinforcement learn rl receive lot attention thank performance ability address complex task time evolutionary algorithms ea prove competitive standard rl algorithms certain problems simpler scalable recent advance ea lead development algorithms like novelty search quality diversity capable efficiently address complex exploration problems find wealth different policies result developments spark strong renew interest population base computational approach nevertheless even eas perform well hard exploration problems still suffer low sample efficiency limitation less present rl methods notably sample reuse contrary struggle hard exploration settings complementary characteristics rl algorithms eas push researchers explore new approach merge two order harness respective strengths avoid shortcomings goal workshop foster collaboration share perspectives spread best practice within grow community intersection rl ea topics heart workshop include evolutionary reinforcement learn evolution strategies population base methods policy search neuroevolution hard exploration sparse reward problems deceptive reward novelty diversity search methods divergent search sample efficient direct policy search intrinsic motivation curiosity build design behaviour characterizations meta learn hierarchical learn evolutionary automl open end learningautors invite submit new original work new perspectives recently publish work topics top submissions select oral presentation present alongside keynote speaker jeff clune ex team leader uberai labs current research team leader openai x200b important date submission deadline april 12 2021 notification april 26 2021 camera ready may 3 2021 find info workshop website url
CopperGenie,MachineLearning,1620269184.0,[D] What is the most human-like chatbot available to the public?,"I'm not necessarily talking about the most comprehensive bot, discussion-wise. But which bot appears to be the most consistently human-like in its responses?",necessarily talk comprehensive bot discussion wise bot appear consistently human like responses
ykilcher,MachineLearning,1620314786.0,[D] Paper Explained - MLP-Mixer: An all-MLP Architecture for Vision (Full Video Analysis),"[https://youtu.be/7K4Z8RqjWIk](https://youtu.be/7K4Z8RqjWIk)

Convolutional Neural Networks have dominated computer vision for nearly 10 years, and that might finally come to an end. First, Vision Transformers (ViT) have shown remarkable performance, and now even simple MLP-based models reach competitive accuracy, as long as sufficient data is used for pre-training. This paper presents MLP-Mixer, using MLPs in a particular weight-sharing arrangement to achieve a competitive, high-throughput model and it raises some interesting questions about the nature of learning and inductive biases and their interaction with scale for future research.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:20 - MLP-Mixer Architecture

13:20 - Experimental Results

17:30 - Effects of Scale

24:30 - Learned Weights Visualization

27:25 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)",url neural network dominate computer vision nearly nmbr years might finally come end first vision transformers vit show remarkable performance even simple mlp base model reach competitive accuracy long sufficient data use pre train paper present mlp mixer use mlps particular weight share arrangement achieve competitive high throughput model raise interest question nature learn inductive bias interaction scale future research x200b outline 0 00 intro overview2 20 mlp mixer architecture13 20 experimental results17 30 effect scale24 30 learn weight visualization27 25 comment conclusion x200b paper url
vadimdotme,MachineLearning,1619763832.0,[R] Eindhoven Reinforcement Learning Seminar,"Hi, /r/MachineLearning! We are running a biweekly (online, for obvious reasons) [seminar on reinforcement learning at TU Eindhoven.](http://einreise.tilda.ws/) We discuss advanced topics like bayesian methods, curiosity and multi-agent RL - think [RL Theory Seminar](https://sites.google.com/view/rltheoryseminars) but with a more practical bent. For every topic we try to invite the best experts as guest speakers (DeepMind/OpenAI/Uber/Lyft/UvA/INRIA/Yandex/etc).  


[Check out our recordings on youtube](https://www.youtube.com/channel/UC7WCvhgSstO2tbbF4pJcS3w) and feel free to join us! We have an event today at 14:30 CET on distributed machine learning. No registration required, just [click to join](https://meet.jit.si/EindhovenReinforcementLearningSeminar).",hi r machinelearning run biweekly online obvious reason seminar reinforcement learn tu eindhoven url discuss advance topics like bayesian methods curiosity multi agent rl think rl theory seminar url practical bend every topic try invite best experts guest speakers deepmind openai uber lyft uva inria yandex etc check record youtube url feel free join us event today 14 30 cet distribute machine learn registration require click join url
SQL_beginner,MachineLearning,1619534991.0,"[D] appeal of ""occam's razor"" in statistics and machine learning","People often refer to ""occam's razor"" in statistics, that simpler models are preferred to complex models, provided both models have similar performance. 

Why should we prefer a simpler model? Does this have to do with the variance-bias tradeoff?",people often refer occam razor statistics simpler model prefer complex model provide model similar performance prefer simpler model variance bias tradeoff
Advanced_Treat_7986,MachineLearning,1617701673.0,[Project] How I trained Spotify Podcast Speech Synthesis using Tacotron2,"\[Project\]

In this project a speech synthesis voice was trained on Spotify Podcasts with the text-to-speech deep learning model Tacotron2 (paper by Google, implemented by Nvidia). This medium article contains how the training was done, and the steps in a re-usable data pipeline to make it more scalable (which can be  re-applied to other ML projects). Code and the paper for this project can be found in the metdium article.",project project speech synthesis voice train spotify podcast text speech deep learn model tacotron2 paper google implement nvidia medium article contain train step usable data pipeline make scalable apply ml project code paper project find metdium article
shinysamurzl,MachineLearning,1616671416.0,[D] Human Error Function," Is having a human as an error function a thing ? So like the Model create a result, a human gives it a score, it learns, and repeat. How would you go about implementing something like this ?",human error function thing like model create result human give score learn repeat would go implement something like
AdelSexy,MachineLearning,1617816916.0,[D] Data imbalance and noisy labels,"I have the CV classification task with 3 classes. Dataset is imbalanced towards class 0. I also know that the labels are noisy: class 0 and 2 are being messed in ~10% of cases. 

Observation: I got better results on the test set (supposed to be clean) if I use default random sampler than in case of the weighted random sampler (samples images in batch with respect to their weight, which is opposite of class presence is the dataset. This is unexpected for me. Can noisy labels be the source of that? I am also using mixup, can affect bad on training because if noisy labels?",cv classification task nmbr class dataset imbalanced towards class 0 also know label noisy class nmbr nmbr mess 10 case observation get better result test set suppose clean use default random sampler case weight random sampler sample image batch respect weight opposite class presence dataset unexpected noisy label source also use mixup affect bad train noisy label
MaxRek,MachineLearning,1620342257.0,"[Project], [Discussion] Do you know any time series annotation platform?","Last many months I am engaged in the generation and study of signal values ​​in time series, built on the data of the electronic document flow (electronic documents interchange) of large companies and their contractors, indicating significant drops in the indicators of companies.

I learned how to produce a couple of types of signals with this data, but there is one problem with them - frequent false positives. To combat this feature, I want to add markup. And then train the classifier...

For mark up purposes on time series data will be needed in near future annotation mechanism for my work project.

Requirements for functionality:
* Many-classes labeling
* In plot labeling or/and labeling by index/value/out of threshold
* for a large number of time series - the ability to maintain / fill several markup sessions at once
* editing marked values
* the ability to work with sub series of the last n values ​​of the time series
* tagged data statistics

This is a new area for me, and therefore I am at a crossroads - to implement this functionality myself or to look for ready-made solutions and try to implement them into the project.

Is existing something for covering the requirements?",last many months engage generation study signal value time series build data electronic document flow electronic document interchange large company contractors indicate significant drop indicators company learn produce couple type signal data one problem frequent false positives combat feature want add markup train classifier mark purpose time series data need near future annotation mechanism work project requirements functionality many class label plot label label index value threshold large number time series ability maintain fill several markup sessions edit mark value ability work sub series last n value time series tag data statisticsthis new area therefore crossroads implement functionality look ready make solutions try implement project exist something cover requirements
JosephLChu,MachineLearning,1619818568.0,[R] Work In Progress: A Drop In Replacement For Softmax With Uncertainty Calibrated Scores,"So, I've been working on an activation function that could work as a drop in replacement for softmax, with the main original intention to be to apply the Principle of Maximum Entropy as a prior that would allow the activation function to output better calibrated confidence scores or probabilities in terms of uncertainty.  It uses some formulas I came up with to convert between correlation scores (-1 to 1) and probabilities (0 to 1).  The idea is that certainty can be measured such that 1 = certain, 0 = fully unknown, and -1 = certainly not, and that there is a way to convert between this and 0 to 1 probabilities by setting 0 on the correlation scale to 1/N on the probability scale.  How this is relevant to a neural net is that in some sense, a pre-activation signal of 0 from a given node basically means that everything cancelled out, and that this maps to maximum uncertainty.  Anyways, I don't want to give away too many details given that it's unpublished.

After a lot of time and effort theorizing and then experimenting with many variants of the formula, I think I have something that works.  When I train a network with this new activation function in the output layer on for instance, MNIST, and then test it on notMNIST, the resulting outputs seem much less overconfident than an equivalent network trained with softmax or sigmoid.  However, the accuracy on MNIST itself doesn't change much.

In fact, on most tasks the new activation function gets very similar results to softmax when used in just the output layer.  So, I'm not sure how useful it actually is.

There is one other place where it might affect performance, though at this point I haven't tested this against the larger SOTA models.  I tried using it in place of softmax in the attention heads of a transformer network for language modelling.  The results seem promising, but I don't know if it'll scale or work across more tasks than the toy problem I was able to run on a single GPU.

What I'd like to know from the experienced researchers here is whether or not this project is worth trying to make into a paper and publishing.  While I think I may have something here, I'm not sure if that's my own bias of wanting all the time and effort spent to not be wasted.

Another problem I'm finding is that over the course of the time that I've worked on this project, I've accumulated many variants of the activation function, and determining which one is actually the best on all tasks and scales is proving tricky.  For a while I was concerned that I had several versions that each work situationally, but don't perform better reliably.  So in some sense I have a family of activation functions, some simpler and more elegant than others.

How does one deal with this kind of uncertainty?",work activation function could work drop replacement softmax main original intention apply principle maximum entropy prior would allow activation function output better calibrate confidence score probabilities term uncertainty use formulas come convert correlation score 1 1 probabilities 0 1 idea certainty measure nmbr certain nmbr fully unknown 1 certainly way convert nmbr nmbr probabilities set nmbr correlation scale 1 n probability scale relevant neural net sense pre activation signal nmbr give node basically mean everything cancel map maximum uncertainty anyways want give away many detail give unpublished lot time effort theorize experiment many variants formula think something work train network new activation function output layer instance mnist test notmnist result output seem much less overconfident equivalent network train softmax sigmoid however accuracy mnist change much fact task new activation function get similar result softmax use output layer sure useful actually one place might affect performance though point test larger sota model try use place softmax attention head transformer network language model result seem promise know scale work across task toy problem able run single gpu like know experience researchers whether project worth try make paper publish think may something sure bias want time effort spend waste another problem find course time work project accumulate many variants activation function determine one actually best task scale prove tricky concern several versions work situationally perform better reliably sense family activation function simpler elegant others one deal kind uncertainty
cdancette,MachineLearning,1617104591.0,[P] multimodal: a library for VQA / vision and language research,"Hi everyone, I am currently building a library for vision & language research: https://github.com/cdancette/multimodal

For now, it focuses only the Visual Question Answering (VQA) datasets (VQA v1, v2 and VQA-CP datasets), and provides the pretrained visual features (bottom-up and top-down attention), and evaluation metrics. Those features and dataset are a hassle to implement when you're starting to work on VQA, so I figured it would be nice to have a reference implementation that just works. I also implemented a model (the bottom-up model) for VQA as an example of how to use the library.

I plan to add other VQA datasets (GQA, CLEVR), more models, or other tasks like captioning.

EDIT: I just added the CLEVR dataset.

Let me know what you think about it, and what you would like to have in this library, like pretrained models, tasks, datasets..",hi everyone currently build library vision language research url focus visual question answer vqa datasets vqa v1 v2 vqa cp datasets provide pretrained visual feature bottom top attention evaluation metrics feature dataset hassle implement start work vqa figure would nice reference implementation work also implement model bottom model vqa example use library plan add vqa datasets gqa clevr model task like caption edit add clevr dataset let know think would like library like pretrained model task datasets
UBIAI,MachineLearning,1618595730.0,[D] Tutorial on how to train entity relation extraction classifier with transformers & use cases,"We recently published an [article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c) on how to do joint NER and relation extraction with transformers and are eager to hear other use cases.

Please checkout the the[ article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c), and share below your use case.

If you have any question, simply DM me and I will get back to you asap.",recently publish article url joint ner relation extraction transformers eager hear use case please checkout article url share use case question simply dm get back asap
bebbo203,MachineLearning,1616746707.0,[D] Random Network Distillation (RND) applied to robotic manipulators,"Is there some working application of this network to a manipulator?  
It seems like this algorithm is not applied to robotic anywhere. I've searched on Google Scholar and various search engines.   
Thank you all",work application network manipulator seem like algorithm apply robotic anywhere search google scholar various search engines thank
louloucha,MachineLearning,1618846331.0,[D] Explanation System on Time-Series Data," As part of my final year research project that I'm doing in my university laboratory. I'm working on designing an Agnostic (i.e Black Box) Explanation System (based on data mining techniques) for recommenders systems. Also, Right now, I'm only focusing on sequential data (i.e sequential purchasing behavior/ User sequential dynamics, etc).

Now, as I'm on the evaluation part my **Sequential Pattern Mining** algorithm based on a **Numerical Target**. I'm searching for some competitors that I could compare to, but AFAIK, there aren't any.

For that purpose, I'm asking you guys if you know any algorithm (baselines) that works on identifying explanation (or pattern) for Time Series data.

I've found a bunch of papers (linked below) that I'm slowly working through :

 

I've found a bunch of papers (linked below) that I'm slowly working through :

* [https://link.springer.com/chapter/10.1007%2F978-3-030-59051-2\_10](https://link.springer.com/chapter/10.1007%2F978-3-030-59051-2_10)
* [https://ieeexplore.ieee.org/document/8995349](https://ieeexplore.ieee.org/document/8995349)
* [https://arxiv.org/pdf/1810.05741.pdf](https://arxiv.org/pdf/1810.05741.pdf)
* [https://arxiv.org/ftp/arxiv/papers/2004/2004.12524.pdf](https://arxiv.org/ftp/arxiv/papers/2004/2004.12524.pdf)

Any help would be appreciated!",part final year research project university laboratory work design agnostic e black box explanation system base data mine techniques recommenders systems also right focus sequential data e sequential purchase behavior user sequential dynamics etc evaluation part sequential pattern mine algorithm base numerical target search competitors could compare afaik purpose ask guy know algorithm baselines work identify explanation pattern time series data find bunch paper link slowly work find bunch paper link slowly work url url url url help would appreciate
Brahimce,MachineLearning,1618447831.0,[R][P] How to deal particle swarm optimization with equality constraints?,"I am facing a problem. I want to create a PSO system where each particle consists of 6 variables (real variable) and the sum of these variables should equal to one.

I generate the initial population, but how can I generate new population from the initial population, and the particles of the generated population equal to one ?",face problem want create pso system particle consist nmbr variables real variable sum variables equal one generate initial population generate new population initial population particles generate population equal one
Math_wizard369,MachineLearning,1620590419.0,[D] Annotated Research paper Walkthroughs using Jupyter Notebooks,"Hi, I just went through [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP. The post walks through the ""Attention is all you need"" paper line by line, there's also a clone of the post as a google colab where you can read along and run the code. I learned a lot going through this notebook and was wondering if anyone knew of anymore of these types of notebooks. I remember a website I found a while back that did this, but I haven't been able to find it. Any help would be greatly appreciated. Thank you.",hi go annotate transformer url harvard nlp post walk attention need paper line line also clone post google colab read along run code learn lot go notebook wonder anyone know anymore type notebooks remember website find back able find help would greatly appreciate thank
Gletta,MachineLearning,1620281236.0,[N] Computer Vision News (with research and code!) - May 2021,"Dear all,

Have a peek at Computer Vision News of May!

Many articles about AI, Deep Learning and more...

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2021May/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2021-may-pdf/)

Dilbert on page 2. Free subscription on page 40.

Enjoy!

&#x200B;

https://preview.redd.it/4i2l82xayfx61.jpg?width=400&format=pjpg&auto=webp&s=f545c54bff9dcfd12f515cf90871e5a5c954f72e",dear peek computer vision news may many article ai deep learn html5 version recommend url version url page 2 free subscription page 40 enjoy x200b url
Yuqing7,MachineLearning,1619026057.0,[R] Pieter Abbeel Team Proposes Task-Agnostic RL Method to Auto-Tune Simulations to the Real World,"A research team from UC Berkeley and Carnegie Mellon University proposes a task-agnostic reinforcement learning method that reduces the task-specific engineering required for domain randomization of both visual and dynamics parameters.

Here is a quick read: [Pieter Abbeel Team Proposes Task-Agnostic RL Method to Auto-Tune Simulations to the Real World](https://syncedreview.com/2021/04/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-2/).

The paper Auto-Tuned Sim-to-Real Transfer is on [arXiv](https://arxiv.org/pdf/2104.07662.pdf).",research team uc berkeley carnegie mellon university propose task agnostic reinforcement learn method reduce task specific engineer require domain randomization visual dynamics parameters quick read pieter abbeel team propose task agnostic rl method auto tune simulations real world url paper auto tune sim real transfer arxiv url
ennanco,MachineLearning,1617956035.0,"[N] Call for Papers: Special Issue ""Applied Machine Learning in NIR Technology"""," Hosting a special issue on Applied Science (JCR - 2.474) Submissions and diffusion are highly appreciated.

[https://www.mdpi.com/journal/applsci/special\_issues/NIR\_technology](https://www.mdpi.com/journal/applsci/special_issues/NIR_technology)

SPECIAL ISSUE INFORMATION

Since the 1980s, Near-Infrared Reflectance (NIR) scanners have been a common companion in laboratories. This technology allows the analysis of the electromagnetic spectrum in bands close to the visible spectrum. As a result, there is a large amount of information that can be analyzed in order to make predictions about a factor such as the quality of the sample.  This advantage of being able to quickly analyze the material without having to destroy the samples have made it so that, in recent times, this kind of equipment has been moved from the laboratory to portable devices in some cases as small as a credit card. Therefore, numerous applications have been developed under the umbrella of this technology, such as analysis of sewer water, determination of food safety, diagnosis of construction material, support on medical and vet diagnosis; the number of applications is near endless.

However, this blossom of applications usually goes along with the requirement of a model which identifies and uses the information in the captured spectra. The relationship between both factors is usually non-linear and complex, so it is necessary to use machine learning techniques to obtain it. Although, it has usually been performed through an analytical process, currently, the application of machine learning has been a step forward in terms of the precision and adjustment of the results. As NIR and machine learning are multidisciplinary technologies, many fields of science and industry could benefit from their use.

Especially, if we focus on feature selection of the bands on the spectra that contain the information required to solve the problem, the application of machine learning needs a process of adjustment to optimize the performance. The aim of this Special Issue is to present the latest advances that have been made in this field, combining both techniques, as well as to show the remaining challenges and the future in this area of research. That is why this Special Issue solicits submissions in, but not limited to, the following areas:

* Selection of features or bands of the NIR spectra, with a special interest in those applications using nano-scale NIR scanners due to its limited operational range;
* Studies of different preprocessing techniques and comparison with automatic features extraction process, such as PCA, LDA, evolutionary computation;
* Developments combining cloud computing, fog computing or edge computing with the processing of NIR spectra in different industrial processes;
* Application and comparative of the latest approaches of machine learning techniques, such as deep learning or ensemble models;
* Works with a particular interest to cover the explainable artificial intelligence in the frame of decision-making using NIR data and machine learning.

Keywords:  

signal processing; near-infrared; machine learning; artificial neural networks; support vector machines; k-nearest neighbour; Naïve Bayes; random forest; ensemble models; evolutionary computation; explainable artificial intelligence; ensemble methods; deep learning; feature extraction; Principal Component Analysis (PCA); food security; material analysis; drug identification; medical diagnosis; vet diagnosis; Internet of things (IoT)",host special issue apply science jcr 2 474 submissions diffusion highly appreciate url issue informationsince 1980s near infrared reflectance nir scanners common companion laboratories technology allow analysis electromagnetic spectrum band close visible spectrum result large amount information analyze order make predictions factor quality sample advantage able quickly analyze material without destroy sample make recent time kind equipment move laboratory portable devices case small credit card therefore numerous applications develop umbrella technology analysis sewer water determination food safety diagnosis construction material support medical vet diagnosis number applications near endless however blossom applications usually go along requirement model identify use information capture spectra relationship factor usually non linear complex necessary use machine learn techniques obtain although usually perform analytical process currently application machine learn step forward term precision adjustment result nir machine learn multidisciplinary technologies many field science industry could benefit use especially focus feature selection band spectra contain information require solve problem application machine learn need process adjustment optimize performance aim special issue present latest advance make field combine techniques well show remain challenge future area research special issue solicit submissions limit follow areas selection feature band nir spectra special interest applications use nano scale nir scanners due limit operational range study different preprocessing techniques comparison automatic feature extraction process pca lda evolutionary computation developments combine cloud compute fog compute edge compute process nir spectra different industrial process application comparative latest approach machine learn techniques deep learn ensemble model work particular interest cover explainable artificial intelligence frame decision make use nir data machine learn keywords signal process near infrared machine learn artificial neural network support vector machine k nearest neighbour naïve bay random forest ensemble model evolutionary computation explainable artificial intelligence ensemble methods deep learn feature extraction principal component analysis pca food security material analysis drug identification medical diagnosis vet diagnosis internet things iot
ml_abler,MachineLearning,1620639883.0,[D] ML and Quantum Computing,"Hi reddit,

I am not entirely sure if this is the right place to ask this but here I go.I recently got an opportunity to work in quantum computing team. The entire purpose of the team is to find applications of quantum computing in our firm. It sounds like an amazing opportunity but I am having a lot of reservations. Would love and appreciate your insights.

I come from an CS engineering background with a masters in AI/ML, my full time position at the moment is that of a data scientist working on clustering models. The quantum computing opportunity is available due to formation of a new team which has a lot of vacancies at the moment.

Although the prospects of Quantum AI/ML sounds hella interesting(although very niche), quantum computing itself seems like an extreme departure from my formal education with its heavy emphasis in physics. This is making me anxious

How viable is this opportunity for my career? How are the job prospects in the domain ( for someone with my background)? Would it help me in creating a niche for myself in data science ?I can admit shamelessly that I'd like a career which pays the big bucks at the same time keep me interested enough.

Would love your opinions",hi reddit entirely sure right place ask go recently get opportunity work quantum compute team entire purpose team find applications quantum compute firm sound like amaze opportunity lot reservations would love appreciate insights come cs engineer background master ai ml full time position moment data scientist work cluster model quantum compute opportunity available due formation new team lot vacancies moment although prospect quantum ai ml sound hella interest although niche quantum compute seem like extreme departure formal education heavy emphasis physics make anxioushow viable opportunity career job prospect domain someone background would help create niche data science admit shamelessly like career pay big buck time keep interest enough would love opinions
MrAcurite,MachineLearning,1617202567.0,[D] Does anyone care about the quality of the prose in academic papers?,"I've been reading a lot of great literary works lately, and when I go back to reading academic papers, there is a very noticeable downgrade in the quality of the language employed. And I guess that that makes sense; when you want to communicate technical information, there's not a lot of room or reason to employ metaphors or colorful language.

But it still seems like there has to be some way to make these works better, to make them legitimately enjoyable to read instead of the linguistic equivalent of shoving stale bread down your throat. Obviously there is often a lot of beauty to be found when you appreciate the cleverness of new ideas being presented, but appreciating that is typically in spite of, rather than due to, the actual quality of the writing.

Is this something that anybody else here cares about?",read lot great literary work lately go back read academic paper noticeable downgrade quality language employ guess make sense want communicate technical information lot room reason employ metaphors colorful language still seem like way make work better make legitimately enjoyable read instead linguistic equivalent shove stale bread throat obviously often lot beauty find appreciate cleverness new ideas present appreciate typically spite rather due actual quality write something anybody else care
olegvol,MachineLearning,1617562096.0,[P] QSMM for building algorithmic neural nets,"I am working on QSMM, a framework for building algorithmic neural nets. In principle, they can produce quality adaptive behavior, do not require excessive computer resources for training and operating, and do not suffer from the problem of explainability of generated behavior.

The core method of operation is random selecting the next state of an algorithmic neural net according to probabilities of possible states calculated on the basis of cycles that occur in the neural net.

Currently, the framework has a few examples demonstrating its possible use cases, but their number shall increase in the future. One of the examples is utilizing the framework to adaptively parse a token sequence. I hope more people make use of implemented ideas to transform approaches behind the concept of artificial intelligence.

[http://qsmm.org](http://qsmm.org)",work qsmm framework build algorithmic neural net principle produce quality adaptive behavior require excessive computer resources train operate suffer problem explainability generate behavior core method operation random select next state algorithmic neural net accord probabilities possible state calculate basis cycle occur neural net currently framework examples demonstrate possible use case number shall increase future one examples utilize framework adaptively parse token sequence hope people make use implement ideas transform approach behind concept artificial intelligence url
vajra_,MachineLearning,1616699791.0,[D] Model Size calculation for sparse neural networks,"In general, how do we calculate the model size of sparse-d neural network (much of the weights are 0)? non-zero weights*4 bytes?",general calculate model size sparse neural network much weight 0 non zero weight 4 bytes
deama15,MachineLearning,1616545017.0,[Project] Remastering old anime using machine learning,"So I've been doing some remastering using AI algorithms on an old anime, hajime no ippo, a boxing anime and have just finished it. The type of remastering I did was to enhance the resolution, clean up artifacts/noise, and frame interpolation. I've made my own subreddit, in there I've got more info, as well as some FAQs and samples. Check it out!

https://www.reddit.com/r/InterpolateAndEnhance/

Screenshot comparisons, first one is old, second one is new:

https://docs.google.com/presentation/d/1bT5pZIZBd2hOVYXBn1IJlFICMlqL5bhqQ_l0oN4P6hA/edit?usp=sharing

For now I'll be doing anime, the next series will be the old hunter x hunter. At some point I want to try out live action content, specifically action stuff like martial arts movies, or straight up action, but for now the interpolation algorithms aren't quite good enough, or well actually they kinda are, at least the high end ones, but good luck trying to interpolate anything with those on anything other than a RTX 3090.",remastering use ai algorithms old anime hajime ippo box anime finish type remastering enhance resolution clean artifacts noise frame interpolation make subreddit get info well faqs sample check url comparisons first one old second one new url anime next series old hunter x hunter point want try live action content specifically action stuff like martial arts movies straight action interpolation algorithms quite good enough well actually kinda least high end ones good luck try interpolate anything anything rtx 3090
machinemask,MachineLearning,1617699100.0,[D] Best system load monitoring tool?,"Hello, I'm looking for a tool that can monitor and display information such as cpu/gpu usage over time. I'm envisioning something like htop together with an interactive dashboard to filter the data on e.g. time or user. I'd like to install it on a company gpu server to get a better understanding of how it's used. We mostly use the servers for training computer vision models so that's what I'm most interested in.

Here are a few features that I think would be interesting to have.

* CPU usage. Blocking vs non-blocking
* GPU utilization and memory usage
* RAM usage
* Disk read and write
* Support for multiple machines
* Display saved data in a dashboard
* Filter displayed data on e.g. user, process, time

&#x200B;

I found some projects that do similar things. [Permon](https://github.com/bminixhofer/permon), [GPU Monitor](https://github.com/msalvaris/gpu_monitor), [NVTOP](https://github.com/Syllo/nvtop), [Weights & Biases](https://wandb.ai/site/articles/monitor-improve-gpu-usage-for-model-training)

What do you think? Is it a good idea to set up such a system? Is there software out there that I've missed?

Any experiences and suggestions are welcome!",hello look tool monitor display information cpu gpu usage time envision something like htop together interactive dashboard filter data e g time user like install company gpu server get better understand use mostly use servers train computer vision model interest feature think would interest cpu usage block vs non block gpu utilization memory usage ram usage disk read write support multiple machine display save data dashboard filter display data e g user process time x200b find project similar things permon url gpu monitor url nvtop url weight bias url think good idea set system software miss experience suggestions welcome
jhanytime,MachineLearning,1618156460.0,[D] Video - Introduction to graph neural networks (made easy!),"I'm a PhD student studying machine learning and applications in transportation systems and autonomous systems (think RL and robotics). While there are several ""GCN made easy"" videos out there on Youtube, I feel like these videos often miss the forest for the trees (especially since GCN is just 1 algorithm that was developed in 2016...) and videos often don't cover the broader historical context of how GNNs were developed and don't cover how different variations of these models allow them to model new types of systems. 

This is the second video in a series I'm making about graphs, graph neural networks, and the application areas where they have the potential to make big impacts. Please let me know what you think of the video and if you learned anything new from it!

https://youtu.be/cka4Fa4TTI4",phd student study machine learn applications transportation systems autonomous systems think rl robotics several gcn make easy videos youtube feel like videos often miss forest tree especially since gcn nmbr algorithm develop 2016 videos often cover broader historical context gnns develop cover different variations model allow model new type systems second video series make graph graph neural network application areas potential make big impact please let know think video learn anything new url
rish-16,MachineLearning,1620547959.0,[P] PyTorch Involution layer wrapper,"Hello everyone!

I'm currently in the process of learning how to implement papers from scratch.

Here's my implementation of the newly-introduced **Involution** layer from the paper ""Involution: Inverting the Inherence of Convolution for Visual Recognition"" by Li et al. presented at CVPR 2021.

Wrapper: [https://github.com/rish-16/involution\_pytorch](https://github.com/rish-16/involution_pytorch)

(Do forgive any implementation errors; any PRs / Issues welcome)

*Note: I'll be releasing a TensorFlow wrapper soon if time permits!*

If you like it, a ⭐️ would be greatly appreciated! It motivates me to continue building easy-to-use ML wrappers :D

Thank you!",hello everyone currently process learn implement paper scratch implementation newly introduce involution layer paper involution invert inherence convolution visual recognition li et al present cvpr 2021 wrapper url forgive implementation errors prs issue welcome note release tensorflow wrapper soon time permit like would greatly appreciate motivate continue build easy use ml wrappers dthank
sim_inf,MachineLearning,1617905410.0,[D] Student Travel Grant for ACL/NAACL/EMNLP,"Is  there any ""student travel grant"" award for ACL conferences? (I mean  only ACL, NAACL, and EMNLP). If there is, how can one apply for it?

The conferences in other communities typically have this, and they receive the applications in various forms.

I can contact the chairs, or follow their websites or twitter  account... But I am posting this here to hear it from an NLP person who  knows this and perhaps has had first hand experience. Thanks.",student travel grant award acl conferences mean acl naacl emnlp one apply conferences communities typically receive applications various form contact chair follow websites twitter account post hear nlp person know perhaps first hand experience thank
ka-wei,MachineLearning,1618776983.0,[P] Cinemate - Movie Recommender System made with Tensorflow Rust,"For the last couple of months, I've worked on a movie recommender system using Tensorflow Rust. The model uses collaborative filtering with neural networks to recommend movies based on the movies you input. Unlike other models you will find on the internet, this model also takes a rating/popularity trade-off coefficient as input. If you prefer popular movies, this coefficient can be set to 0, but if you value a high average rating more, then the coefficient can be set to 1. The optimal number for you is probably in-between the two and the default value is \~0.7. A big goal of this website is fast recommendations - you will get recommendations in a matter of seconds due to the performance of the highly optimized prediction in Rust. 

You can visit the website here:  
[https://cinemate.me/](https://cinemate.me/)",last couple months work movie recommender system use tensorflow rust model use collaborative filter neural network recommend movies base movies input unlike model find internet model also take rat popularity trade coefficient input prefer popular movies coefficient set 0 value high average rat coefficient set 1 optimal number probably two default value 0 7 big goal website fast recommendations get recommendations matter second due performance highly optimize prediction rust visit website url
NoAnalyst4,MachineLearning,1620067314.0,[D] Does the ICML acceptance rate curtailment impact 2021 submissions?,"Are the new ICML acceptance rules applying to this year's papers or next year's submissions? This is my first ever submission to ICML and I am worried about my chances now that the changes are announced: [https://www.reddit.com/r/MachineLearning/comments/n243qw/d\_icml\_conference\_we\_plan\_to\_reduce\_the\_number\_of/](https://www.reddit.com/r/MachineLearning/comments/n243qw/d_icml_conference_we_plan_to_reduce_the_number_of/)

I am biased, but it seems really unfair to change the policy in the middle of a decision process.",new icml acceptance rule apply year paper next year submissions first ever submission icml worry chance change announce url bias seem really unfair change policy middle decision process
Yuqing7,MachineLearning,1619109567.0,[R] Are Multilingual Language Models Fragile? IBM Adversarial Attack Strategies Cut MBERT QA Performance by 85%,"An IBM research team proposes four multilingual adversarial attack strategies and attacks seven languages in a zero-shot setting on large multilingual pretrained language models (e.g. MBERT), reducing average performance by up to 85.6 percent.

Here is a quick read: [Are Multilingual Language Models Fragile? IBM Adversarial Attack Strategies Cut MBERT QA Performance by 85%](https://syncedreview.com/2021/04/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-3/).

The paper *Are Multilingual BERT Models Robust? A Case Study on Adversarial Attacks for Multilingual Question Answering*  is on [arXiv](https://arxiv.org/pdf/2104.07646.pdf).",ibm research team propose four multilingual adversarial attack strategies attack seven languages zero shoot set large multilingual pretrained language model e g mbert reduce average performance nmbr percent quick read multilingual language model fragile ibm adversarial attack strategies cut mbert qa performance 85 url paper multilingual bert model robust case study adversarial attack multilingual question answer arxiv url
Superb-Drawer5214,MachineLearning,1617340770.0,[D] How important is the prestige of undergraduate school in getting into top ML PhD programs?," 

I've looked through most of the cv and resumes of PhD students in CMU, Berkeley, Stanford, University of Washington and MIT and most of them got their bachelor's degree from Berkeley, Stanford, MIT, CMU, Harvard, Princeton, and IIT. There were some students who graduated from other famous Ivy league schools, University of Washington, Peking, and Tsinghua. I could not view all of their cv and resumes because some of them did not upload their cv.

I am considering attending UMD but there were only three students who graduated from UMD who are doing their PhD in those schools.",look cv resume phd students cmu berkeley stanford university washington mit get bachelor degree berkeley stanford mit cmu harvard princeton iit students graduate famous ivy league school university washington peking tsinghua could view cv resume upload cv consider attend umd three students graduate umd phd school
byronbae,MachineLearning,1616680571.0,[P] Sound pollution mapping from GeoJSON,"I am undertaking a data science project within my job on a subject i'm very unfamiliar with. There are already some big problems such as extreme data scarcity but all of that aside I wondered if anyone could help me out with a starting point. To put it as simply as possible, I have a GeoJSON files that contain sound measurements at specific coordinates throughout cities and I would like to build a model that tries to predict the 'noisiest' points. Eventually the goal would be to include more types of related data such as real-time traffic etc etc. 

For now I have found this which seems the closest to my problem: https://omdena.com/heatmap-machine-learning/
It doesn't go into how any of these concepts were actually applied (technologies etc..) but the ideas and the type of data/outcomes is very similar to my goals.

I've played around with the data a bit already in notebooks, leaflet, arcQGIS but i'm having a bit of an issue with wrapping my head around how an entire workflow for this project could work since I want to go from mapping raw data points to then mapping key points identified through an analysis of those points.

Any insights would be greatly appreciated!",undertake data science project within job subject unfamiliar already big problems extreme data scarcity aside wonder anyone could help start point put simply possible geojson file contain sound measurements specific coordinate throughout cities would like build model try predict noisiest point eventually goal would include type relate data real time traffic etc etc find seem closest problem url go concepts actually apply technologies etc ideas type data outcomes similar goals play around data bite already notebooks leaflet arcqgis bite issue wrap head around entire workflow project could work since want go map raw data point map key point identify analysis point insights would greatly appreciate
Acrobatic-Egg-,MachineLearning,1619433901.0,[D] The Journey Of Problem-Solving Using Analytics," 

In my \~6 years of working in the analytics domain, for most of the Fortune 10 clients, across geographies, one thing I've realized is while people may solve business problems using analytics, the journey is lost somewhere. At the risk of sounding cliche, ***'Enjoy the journey, not the destination"".*** So here's my attempt at creating the problem-solving journey from what I've experienced/learned/failed at.

The framework for problem-solving using analytics is a 3 step process. On we go:

1. **Break the business problem into an analytical problem**  
Let's start this with another cliche - *"" If I had an hour to solve a problem I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions"".* This is where a lot of analysts/consultants fail. As soon as a business problem falls into their ears, they straightaway get down to solution-ing, without even a bare attempt at understanding the problem at hand. To tackle this, I (and my team) follow what we call the **CS-FS framework** (extra marks to those who can come up with a better naming).  
The CS-FS framework stands for the Current State - Future State framework.In the CS-FS framework, the first step is to identify the **Current State** of the client, where they're at currently with the problem, followed by the next step, which is to identify the **Desired Future State**, where they want to be after the solution is provided - the insights, the behaviors driven by the insight and finally the outcome driven by the behavior.  
The final, and the most important step of the CS-FS framework is **to identify the gap**, that prevents the client from moving from the Current State to the Desired Future State. This becomes your Analytical Problem, and thus the input for the next step
2. **Find the Analytical Solution to the Analytical Problem**  
Now that you have the business problem converted to an analytical problem, let's look at the data, shall we? \*\*A BIG NO!\*\*  
We will start forming hypotheses around the problem, **WITHOUT BEING BIASED BY THE DATA.** I can't stress this point enough. The process of forming hypotheses should be independent of what data you have available. The correct method to this is after forming all possible hypotheses, you should be looking at the available data, and eliminating those hypotheses for which you don't have data.  
After the hypotheses are formed, you start looking at the data, and then the usual analytical solution follows - understand the data, do some EDA, test for hypotheses, do some ML (if the problem requires it), and yada yada yada. This is the part which most analysts are good at. For example - if the problem revolves around customer churn, this is the step where you'll go ahead with your classification modeling.Let me remind you, the output for this step is just an analytical solution - a classification model for your customer churn problem.  
Most of the time, the people for whom you're solving the problem would not be technically gifted, so they won't understand the Confusion Matrix output of a classification model or the output of an AUC ROC curve. They want you to talk in a language they understand. This is where we take the final road in our journey of problem-solving - the final step
3. **Convert the Analytical Solution to a Business Solution**  
An analytical solution is for computers, a business solution is for humans. And more or less, you'll be dealing with humans who want to understand what your many weeks' worth of effort has produced. You may have just created the most efficient and accurate ML model the world has ever seen, but if the final stakeholder is unable to interpret its meaning, then the whole exercise was useless.  
This is where you will use all your story-boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state. This is where visualization skills, dashboard creation, insight generation, creation of decks come into the picture. Again, when you create dashboards or reports, keep in mind that you're telling a story, and not just laying down a beautiful colored chart on a Power BI or a Tableau dashboard. Each chart, each number on a report should be action-oriented, and part of a larger story.  
Only when someone understands your story, are they most likely going to purchase another book from you. Only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders, will they travel with you again.

With that said, I've reached my destination. I hope you all do too. I'm totally open to criticism/suggestions/improvements that I can make to this journey. Looking forward to inputs from the community!",6 years work analytics domain fortune nmbr clients across geographies one thing realize people may solve business problems use analytics journey lose somewhere risk sound cliche enjoy journey destination attempt create problem solve journey experience learn fail framework problem solve use analytics nmbr step process go 1 break business problem analytical problem let start another cliche hour solve problem spend nmbr minutes think problem nmbr minutes think solutions lot analysts consultants fail soon business problem fall ears straightaway get solution ing without even bare attempt understand problem hand tackle team follow call cs fs framework extra mark come better name cs fs framework stand current state future state framework cs fs framework first step identify current state client currently problem follow next step identify desire future state want solution provide insights behaviors drive insight finally outcome drive behavior final important step cs fs framework identify gap prevent client move current state desire future state become analytical problem thus input next step2 find analytical solution analytical problem business problem convert analytical problem let look data shall big start form hypotheses around problem without bias data stress point enough process form hypotheses independent data available correct method form possible hypotheses look available data eliminate hypotheses data hypotheses form start look data usual analytical solution follow understand data eda test hypotheses ml problem require yada yada yada part analysts good example problem revolve around customer churn step go ahead classification model let remind output step analytical solution classification model customer churn problem time people solve problem would technically gift win understand confusion matrix output classification model output auc roc curve want talk language understand take final road journey problem solve final step3 convert analytical solution business solution analytical solution computers business solution humans less deal humans want understand many weeks worth effort produce may create efficient accurate ml model world ever see final stakeholder unable interpret mean whole exercise useless use story board experience actually tell story would start current state problem step take reach desire future state visualization skills dashboard creation insight generation creation deck come picture create dashboards report keep mind tell story lay beautiful color chart power bi tableau dashboard chart number report action orient part larger story someone understand story likely go purchase another book make journey beautiful meaningful fellow passengers stakeholders travel say reach destination hope totally open criticism suggestions improvements make journey look forward input community
SimlaBurcu,MachineLearning,1619032775.0,[P] ColTraIn Hybrid Block Floating-Point (HBFP) Training Emulator,"We are excited to announce the release of the [ColTraIn HBFP Training Emulator](https://github.com/parsa-epfl/HBFPEmulator).

[HBFP](https://papers.nips.cc/paper/2018/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf) offers the **accuracy** of   32-bit floating-point with the numeric and silicon density of 8-bit   fixed-point for a wide variety of models (ResNet, WideResNet, DenseNet,   AlexNet, LSTM, and BERT). We foresee HBFP laying the foundation for   accurate training algorithms running on accelerators with an order of   magnitude denser arithmetic than conventional or novel floating-point   based platforms. The ColTraIn emulator repository includes several   example DNN models including CNNs, LSTMs and BERT for both HBFP and a   reference FP32 baseline.  
Check out the ColTraIn emulator at [https://github.com/parsa-epfl/HBFPEmulator](https://github.com/parsa-epfl/HBFPEmulator) or visit our website at [https://parsa.epfl.ch/coltrain/](https://parsa.epfl.ch/coltrain/) for more information!",excite announce release coltrain hbfp train emulator url offer accuracy 32 bite float point numeric silicon density 8 bite fix point wide variety model resnet wideresnet densenet alexnet lstm bert foresee hbfp lay foundation accurate train algorithms run accelerators order magnitude denser arithmetic conventional novel float point base platforms coltrain emulator repository include several example dnn model include cnns lstms bert hbfp reference fp32 baseline check coltrain emulator url visit website url information
Svito-zar,MachineLearning,1617024327.0,[D] Is anyone using Automatic Bug Fixing Tools for Python?,"Many of us are making bugs. Some more often than other. I am one of those :) 

After just fixing a bug which was there for several month and screwed up many of my experiments, I wonder: can't ML find bugs for me?

Quick search on this topic resulted in many papers and even a few tools ([https://analyticsindiamag.com/5-python-bug-fixing-tools-essential-for-developers/](https://analyticsindiamag.com/5-python-bug-fixing-tools-essential-for-developers/)).

And I wonder - is anyone using those tools? Are they actually working? Can they fix copy-paste errors?",many us make bug often one fix bug several month screw many experiment wonder ml find bug quick search topic result many paper even tool url wonder anyone use tool actually work fix copy paste errors
mrgemy95,MachineLearning,1620443257.0,[D] JAX vs Pytorch gpu performance.," Does anyone know if there's a benchmark that compares JAX (or it's apis)  and pytorch in speed on gpus.

I'm aware of this comparison [https://dzone.com/articles/accelerated-automatic-differentiation-with-jax-how](https://dzone.com/articles/accelerated-automatic-differentiation-with-jax-how) but it just compare desne layes, I was looking for something that benchmark more complicated architectures.",anyone know benchmark compare jax apis pytorch speed gpus aware comparison url compare desne lay look something benchmark complicate architectures
projekt_treadstone,MachineLearning,1619738644.0,[D] Prototypical network in the medical domain,Is there any existing project work/implementation of prototypical network-based meta-learning for medical image. As medical image are heavier to load computationally than imagenet and I would like to know how they handled large pixel medical data. As when I resize a large medical image  as per standard prototypical architecture there is large drop in accuracy . With a large resize value it's not fitting in GPU to train.,exist project work implementation prototypical network base meta learn medical image medical image heavier load computationally imagenet would like know handle large pixel medical data resize large medical image per standard prototypical architecture large drop accuracy large resize value fit gpu train
PebbleWrestler9000,MachineLearning,1617831182.0,[D] Issues with using LSTM networks to classify raw EMG signal data,"I am unsure if this is the correct subreddit to post this but I am currently having issues with an ML undergraduate research project and I was hoping to potentially get some guidance!

My current task is to take EMG signal data (time-domain electrical signal data) gathered from a user and classify what finger a person is engaging at any moment. This is a multi-class label classification problem, with 5 total labels possible. This is an example of how such an EMG signal looks like when plotted in Python:

https://preview.redd.it/gged4otfgtr61.png?width=471&format=png&auto=webp&s=faf6d9f21137d7dcbdd8dd324f2363e5722ce89b

As you can tell by that graph, I have normalized the amplitude range to the range \[0, 1\], and to ensure each time-series is of equal length, I append 0s to both the front and the end of the series to match the length of the longest sequence in the data-set. As a result, each time series is of approximately length 800, with 812 total series. The distribution of each label is approximately equal. The resulting tensor shape is  (812, 794, 1) , so 812 rows, 794 being the length of each sequence, and 1 value stored at each time-step.

I am currently using the following Keras model:

&#x200B;

https://preview.redd.it/frr0cnbygtr61.png?width=909&format=png&auto=webp&s=a727e10971f45efeade1bc2abb2a85889098dc6c

I use an LSTM network with the length of the sequences as the number of neurons, followed by a dropout, followed by a Dense layer, another dropout layer, and a final classification Dense layer. I am using an Adam optimizer algorithm, with a learning rate between 1e-2 to 1e-10. I also tweaked the L2 kernel regularizer values from the default to the range 1e-4 to 1e-7. 

My issue is I have unusually terrible performance, an example of this is: 

&#x200B;

https://preview.redd.it/781esb8phtr61.png?width=1412&format=png&auto=webp&s=cf8f0ca254e184585c74a18ecaa942a5fc48805a

The validation accuracy is consistently zero, the validation loss continuously increases as if its over-fitting, and the training accuracy plateaus around a value of about 0.2. I have run this model at 1000 epochs before, and validation accuracy nonetheless remained at 0 with the loss increasing. The training accuracy and loss continue to plateau even up to 1000 epochs.

I initially suspected overfitting could be an issue but after dealing with Dropout layers, I do not believe this is the case. Would anyone be will to chat and help me figure out what's going on with the model simply not learning?

Thank you!",unsure correct subreddit post currently issue ml undergraduate research project hop potentially get guidance current task take emg signal data time domain electrical signal data gather user classify finger person engage moment multi class label classification problem nmbr total label possible example emg signal look like plot python url tell graph normalize amplitude range range 0 1 ensure time series equal length append 0s front end series match length longest sequence data set result time series approximately length 800 nmbr total series distribution label approximately equal result tensor shape 812 794 1 nmbr row nmbr length sequence nmbr value store time step currently use follow keras model x200b url use lstm network length sequence number neurons follow dropout follow dense layer another dropout layer final classification dense layer use adam optimizer algorithm learn rate 1e 2 1e 10 also tweak l2 kernel regularizer value default range 1e 4 1e 7 issue unusually terrible performance example x200b url validation accuracy consistently zero validation loss continuously increase fit train accuracy plateaus around value 0 2 run model nmbr epochs validation accuracy nonetheless remain nmbr loss increase train accuracy loss continue plateau even nmbr epochs initially suspect overfitting could issue deal dropout layer believe case would anyone chat help figure go model simply learn thank
hardmaru,MachineLearning,1617937116.0,[P] Neuralink's Monkey Mindpong,"Blog post: https://neuralink.com/blog/

Their decoder, which presumably is trained with machine learning, takes in neural activity data from the monkey, and after calibration, enables the monkey to play pong directly from thought.",blog post url decoder presumably train machine learn take neural activity data monkey calibration enable monkey play pong directly think
Jack_Hackerman,MachineLearning,1618764770.0,[D] How to normalize and merge two different datasets with similar deltas for Keras?,"Hi, I didn't get enough attention on stackoverflow, so ask here

I have two datasets to train model on. The first dataset have values within the range 1.0 - 1.1, and another one has 1000-1100 range. Essentially, datasets are very similar in a sense of percentage data change. So there is 1% data change between row1.value1 = 1.0 and row2.value1 = 1.011 in first dataset and the same is in another dataset (something like row1.value1 = 1000 and row2.value1 = 1010.09), but these raw values range and targets are different (but again - similar in percentage change).

1. How can I normalize them to make them similar to combine and not to break my network?
2. Is this possible at all?
3. Should I normalize targets? Should I normalize data that I feed into .predict function?

I am not trying to predict stocks, or something, but data is a financial data, that is needed for my job to make a regression for some bonds parameters.

&#x200B;

**UPDATE**

&#x200B;

I realized that I can normalize first and second dataset separately by just using set -= mean, set /= std, lol, but what should I do with targets? What if one target is 1 and another target is 1000? ",hi get enough attention stackoverflow ask herei two datasets train model first dataset value within range nmbr 1 1 another one 1000 1100 range essentially datasets similar sense percentage data change 1 data change row1 value1 nmbr row2 value1 nmbr first dataset another dataset something like row1 value1 nmbr row2 value1 1010 09 raw value range target different similar percentage change 1 normalize make similar combine break network 2 possible 3 normalize target normalize data fee predict function try predict stock something data financial data need job make regression bond parameters x200b update x200b realize normalize first second dataset separately use set mean set std lol target one target nmbr another target 1000
Yuqing7,MachineLearning,1617896959.0,[N] ContinualAI Releases Avalanche: An End-to-End Library for Continual Learning,"A research and development team from ContinualAI, including a large group of researchers from KU Leuven, ByteDance AI Lab, University of California, New York University and other institutions, proposes Avalanche, an End-to-End Library for Continual Learning based on PyTorch.

Here is a quick read: [ContinualAI Releases Avalanche: An End-to-End Library for Continual Learning](https://syncedreview.com/2021/04/08/continualai-releases-avalanche-an-end-to-end-library-for-continual-learning/)

The paper *Avalanche: an End-to-End Library for Continual Learning* is on [arXiv](https://arxiv.org/pdf/2104.00405.pdf).",research development team continualai include large group researchers ku leuven bytedance ai lab university california new york university institutions propose avalanche end end library continual learn base pytorch quick read continualai release avalanche end end library continual learn url paper avalanche end end library continual learn arxiv url
thedeepreader,MachineLearning,1620224443.0,[D] (Paper Overview) MLP-Mixer: An all-MLP Architecture for Vision," **Video**

[https://youtu.be/7FHmzEBNzro](https://youtu.be/7FHmzEBNzro)

**Paper**

[https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)

Code  
(Will be soon available by the authors)

[https://github.com/google-research/vision\_transformer](https://github.com/google-research/vision_transformer)

**Abstract**

Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. ""mixing"" the per-location features), and one with MLPs applied across patches (i.e. ""mixing"" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.",video url soon available author url neural network cnns go model computer vision recently attention base network vision transformer also become popular paper show convolutions attention sufficient good performance neither necessary present mlp mixer architecture base exclusively multi layer perceptrons mlps mlp mixer contain two type layer one mlps apply independently image patch e mix per location feature one mlps apply across patch e mix spatial information train large datasets modern regularization scheme mlp mixer attain competitive score image classification benchmarks pre train inference cost comparable state art model hope result spark research beyond realms well establish cnns transformers
niujin,MachineLearning,1620380538.0,[D] Predictive model for the shape of an entire histogram,"I am working on a project in market research. Survey respondents reply to a survey on a 10-point scale (1 = dislike, 5=indifferent, 10=like). The responses can be shown on a histogram.

I would like to develop an ML model to predict the shape of the histogram for a survey before it the survey is run in the wild. I have a database of past surveys by country, industry, etc. It is straightforward to predict a single value such as the mean of the histogram, but how to train a machine learning model to predict the entire histogram?

So far approaches I am considering:

* train 10 separate and independent models
* train a model for the mean and one for the stdev, and assume normal (a bad assumption I know as these histograms have many different shapes)
* design a custom machine learning architecture with 10 outputs

When I've trained a model to predict the mean of the histogram only, the best performance comes from ensemble models such as Random Forest, or using Auto ML such as Azure ML.

Has anybody tried something similar? I am new to market research but have been in data science for several years.",work project market research survey respondents reply survey 10 point scale 1 dislike 5 indifferent 10 like responses show histogram would like develop ml model predict shape histogram survey survey run wild database past survey country industry etc straightforward predict single value mean histogram train machine learn model predict entire histogram far approach consider train nmbr separate independent model train model mean one stdev assume normal bad assumption know histograms many different shape design custom machine learn architecture nmbr outputswhen train model predict mean histogram best performance come ensemble model random forest use auto ml azure ml anybody try something similar new market research data science several years
donkey_strom16001,MachineLearning,1619388491.0,[D] The Rants of an experienced engineer who glimpsed into AI Academia (Briefly),"# Background

I recently graduated with a master's degree and was fortunate/unfortunate to glimpse the whole ""Academic"" side of ML. I took a thesis track in my degree because as an immigrant it's harder to get into a good research lab without having authorship in a couple of good papers  (Or so I delude myself ). 

I worked as a Full-stack SWE for a startup for 4+ years before coming to the US for a master’s degree focused on ML and AI. I did everything in those years. From project management to building fully polished S/W products to DevOps to even dabbled in ML. I did my Batchelor’s degree from a university whose name is not even worth mentioning. The university for my master’s degree is in the top 20 in the AI space.  I didn't know much about ML and the curiosity drove me to university.  

Come to uni and I focused on learning ML and AI for one 1-1.5 years after which I found advisors for a thesis topic. This is when the fun starts. I had the most amazing advisors but the entire peer review system and the way we assess ML/Science is what ticked me off. This is where the rant begins. 

# Rant 1:Acadmia follows a Gated Institutional Narrative

Let's say you are a Ph.D. at the world's top AI institution working under the best prof. You have a way higher likelihood of you getting a good Postdoc at a huge research lab vs someone's from my poor country doing a Ph.D. with a not-so-well-known advisor having published not-so-well-known papers. I come from a developing nation and I see this many times here. In my country academics don't get funding as they do at colleges in the US. One of the reasons for this is that colleges don't have such huge endowments and many academics don't have wealthy research sponsors.  Brand names and prestige carry massive weight to help get funding in US academic circles. This prestige/money percolates down to the students and the researchers who work there. Students in top colleges get a huge advantage and the circles of top researchers keep being from the same sets of institutions. I have nothing against top researchers from top institutions but due to the nature of citations and the way the money flows based on them, a vicious cycle is created where the best institutions keep getting better and the rest don't get as much of a notice. 

# Rant 2: Peer Review without Code Review in ML/AI is shady 

I am a computer scientist and I was appalled when I heard that you don't need to do code reviews for research papers. As a computer scientist and someone who actually did shit tons of actual ML in the past year, I find it absolutely garbage that code reviews are not a part of this system. I am not saying every scientist who reads a paper should review code but at least one person should for any paper's code submission. At least in ML and AI space. This is basic. I don't get why people call themselves computer scientists if they don't want to read the fucking code. If you can't then make a grad student do it. But for the collective of science, we need this.  

***The core problem lies in the fact that peer review is free. :*** There should be better solutions for this. We ended up creating Git and that changed so many lives. Academic Research needs something similar.

# Rant 3: My Idea is Novel Until I see Someone Else's Paper

The volume of scientific research is growing exponentially. Information is being created faster than we can digest.  We can't expect people to know everything and the amount of overlap in the AI/ML fields requires way better search engines than Google Scholar. 

The side effect of large volumes of research is that every paper is doing something ""novel"" making it harder to filter what the fuck was novel. 

I have had so many experiences where I coded up something and came to realize that someone else has done something symbolically similar and my work just seems like a small variant of that. That's what fucks with my head. Is what I did in Novel? What the fuck is Novel? Is stitching up a transformer to any problem with fancy embeddings and tidying it up as a research paper Novel? Is just making a transformer bigger Novel?  Is some new RL algorithm tested with 5 seeds and some fancy fucking prior and some esoteric reasoning for its success Novel? Is using an over parameterized model to get 95% accuracy on 200 sample test set Novel? Is apply Self-supervised learning for some new dataset Novel? If I keep on listing questions on novelty, I can probably write a novel asking about what the fuck is ""Novel"". 

# Rant 4: Citation Based Optimization Promotes Self Growth Over Collective Growth

Whatever people may say about collaboration, Academia intrinsically doesn't promote the right incentive structures to harbor collaboration. Let me explain, When you write a paper, the position of your name matters. If you are just a Ph.D. student and a first author to a paper, it's great. If you are an nth author Not so great. Apparently, this is a very touchy thing for academics. And lots of egos can clash around numbering and ordering of names.  I distinctly remember once attending some seminar in a lab and approaching a few students on research project ideas. The first thing that came out of the PhD student's mouth was the position in authorship. As an engineer who worked with teams in the past, this was never something I had thought about. Especially because I worked in industry, where it's always the group over the person. Academia is the reverse. Academia applauds the celebration of the individual's achievements. 

All of this is understandable but it's something I don't like. This makes PhDs stick to their lane. The way citations/research-focus calibrate the ""hire-ability"" and ""completion of Ph.D. thesis"" metrics, people are incentivized to think about themselves instead of thinking about collaborations for making something better. 

# Conclusion

A Ph.D. in its most idealistic sense for me is the pursuit of hard ideas(I am poetic that way). In a situation like now when you have to publish or perish and words on paper get passed off as science without even seeing the code that runs it, I am extremely discouraged to go down that route.  All these rants are not to diss on scientists. I did them because ""we"" as a community need better ways to addressing some of these problems.


P.S.
Never expected so many people to express their opinions about this rant. 

U shouldn’t take this seriously. As many people have stated I am an outsider with tiny experience to give a full picture.

I realize that my post as coming out as something which tries to dichotomize academia and industry. I am not trying to do that. I wanted to highlight some problems I saw for which there is no one person to blame. These issues are in my opinion a byproduct of the economics which created this system. 

Thank you for gold stranger.",backgroundi recently graduate master degree fortunate unfortunate glimpse whole academic side ml take thesis track degree immigrant harder get good research lab without authorship couple good paper delude work full stack swe startup 4 years come us master degree focus ml ai everything years project management build fully polish w products devops even dabble ml batchelors degree university whose name even worth mention university master degree top nmbr ai space know much ml curiosity drive university come uni focus learn ml ai one 1 1 5 years find advisors thesis topic fun start amaze advisors entire peer review system way assess ml science tick rant begin rant 1 acadmia follow gate institutional narrativelet say ph world top ai institution work best prof way higher likelihood get good postdoc huge research lab vs someone poor country ph well know advisor publish well know paper come develop nation see many time country academics get fund colleges us one reason colleges huge endowments many academics wealthy research sponsor brand name prestige carry massive weight help get fund us academic circle prestige money percolate students researchers work students top colleges get huge advantage circle top researchers keep set institutions nothing top researchers top institutions due nature citations way money flow base vicious cycle create best institutions keep get better rest get much notice rant 2 peer review without code review ml ai shady computer scientist appal hear need code review research paper computer scientist someone actually shit tons actual ml past year find absolutely garbage code review part system say every scientist read paper review code least one person paper code submission least ml ai space basic get people call computer scientists want read fuck code make grad student collective science need core problem lie fact peer review free better solutions end create git change many live academic research need something similar rant 3 idea novel see someone else paperthe volume scientific research grow exponentially information create faster digest expect people know everything amount overlap ai ml field require way better search engines google scholar side effect large volumes research every paper something novel make harder filter fuck novel many experience cod something come realize someone else something symbolically similar work seem like small variant fuck head novel fuck novel stitch transformer problem fancy embeddings tidy research paper novel make transformer bigger novel new rl algorithm test nmbr seed fancy fuck prior esoteric reason success novel use parameterized model get 95 accuracy nmbr sample test set novel apply self supervise learn new dataset novel keep list question novelty probably write novel ask fuck novel rant 4 citation base optimization promote self growth collective growthwhatever people may say collaboration academia intrinsically promote right incentive structure harbor collaboration let explain write paper position name matter ph student first author paper great nth author great apparently touchy thing academics lot egos clash around number order name distinctly remember attend seminar lab approach students research project ideas first thing come phd student mouth position authorship engineer work team past never something think especially work industry always group person academia reverse academia applaud celebration individual achievements understandable something like make phds stick lane way citations research focus calibrate hire ability completion ph thesis metrics people incentivized think instead think collaborations make something better conclusiona ph idealistic sense pursuit hard ideas poetic way situation like publish perish word paper get pass science without even see code run extremely discourage go route rant diss scientists community need better ways address problems p never expect many people express opinions rant u take seriously many people state outsider tiny experience give full picture realize post come something try dichotomize academia industry try want highlight problems saw one person blame issue opinion byproduct economics create system thank gold stranger
RSchaeffer,MachineLearning,1619728964.0,[R] Help understanding NeurIPS 2013 Dirichlet Process Mixture Model paper,"Hi! I'm struggling to understand certain parts of the 2013 NeurIPS paper ""Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation"" ([https://proceedings.neurips.cc/paper/2013/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html](https://proceedings.neurips.cc/paper/2013/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html)). If anyone on this subreddit is familiar with Bayesian nonparametrics and has five minutes, I would really appreciate answers to the questions I posted at Math Stackexchange

[https://math.stackexchange.com/questions/4121431/online-stochastic-variational-inference-for-dirichlet-process-mixture-models](https://math.stackexchange.com/questions/4121431/online-stochastic-variational-inference-for-dirichlet-process-mixture-models)

Based on the paper's reviews,  reviewer 4 had these same questions and I think the author intended to  add answers to the supplement (the supplement starts with, ""This  document provides proofs of theorems presented in the paper"") but never actually got around to adding proofs or answers.

I've emailed the sole author but I have yet to hear back from him.",hi struggle understand certain part nmbr neurips paper online learn nonparametric mixture model via sequential variational approximation url anyone subreddit familiar bayesian nonparametrics five minutes would really appreciate answer question post math stackexchange url paper review reviewer nmbr question think author intend add answer supplement supplement start document provide proof theorems present paper never actually get around add proof answer email sole author yet hear back
answersareallyouneed,MachineLearning,1617655429.0,[D] Questions on mathematical maturity for PhD/R&D ML,"My current job mostly involves implementing papers and adapting models to fit the needs of the company and math hasn't really been an issue in understanding/implementing papers(So far).

I believe that I lack the mathematical maturity for more competitive positions or to go back for a PhD. I've worked on research in the past, and I'm specifically interested in domain adaptation/synthetic data generation.

A few questions:

1. Is it common for people to come into this work with a ""general understanding of concepts"" and slowly build out the maturity? A lot of my mentors were math/physics majors who later came into machine learning and so perhaps my idea of ""what it takes"" are a bit skewed.

2. I have about 2 years before which I'd want to apply for/start a PhD. I plan on working full-time for those years. What's the most effective way to use this time? What do I study first?

A colleague recommended that I go through real-analysis. It also seems like there's a good portion of this subreddit who have gone through Bishop and The Elements of Statistical Learning at the very least.

TLDR: I'm interested in the field and would like to not be a mediocre mid-level professional. Any advice?",current job mostly involve implement paper adapt model fit need company math really issue understand implement paper far believe lack mathematical maturity competitive position go back phd work research past specifically interest domain adaptation synthetic data generation question 1 common people come work general understand concepts slowly build maturity lot mentor math physics major later come machine learn perhaps idea take bite skew 2 nmbr years want apply start phd plan work full time years effective way use time study first colleague recommend go real analysis also seem like good portion subreddit go bishop elements statistical learn least tldr interest field would like mediocre mid level professional advice
ML_WAYR_bot,MachineLearning,1618171211.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 110,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)||

Most upvoted papers two weeks ago:

/u/rtrx3: [How Machine Learning Teams Share and Reuse Features](https://www.tecton.ai/blog/how-machine-learning-teams-share-and-reuse-features/)

/u/Justdis: [Efficient Exploration of Chemical Space with Docking and Deep-Learning](https://chemrxiv.org/articles/preprint/Efficient_Exploration_of_Chemical_Space_with_Docking_and_Deep-Learning/14153819)

/u/KirillTheMunchKing: [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery - SOTA StyleGAN image editing](https://t.me/casual_gan/18)

Besides that, there are no rules, have fun.",place share machine learn research paper journals article read week relate research mean elaborate give us insight otherwise could interest paper read please try provide insight understand please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent link previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 week 1 url 11 url 21 url 31 url 41 url 51 url 61 url 71 url 81 url 91 url 101 url 2 url 12 url 22 url 32 url 42 url 52 url 62 url 72 url 82 url 92 url 102 url 3 url 13 url 23 url 33 url 43 url 53 url 63 url 73 url 83 url 93 url 103 url 4 url 14 url 24 url 34 url 44 url 54 url 64 url 74 url 84 url 94 url 104 url 5 url 15 url 25 url 35 url 45 url 55 url 65 url 75 url 85 url 95 url 105 url 6 url 16 url 26 url 36 url 46 url 56 url 66 url 76 url 86 url 96 url 106 url 7 url 17 url 27 url 37 url 47 url 57 url 67 url 77 url 87 url 97 url 107 url 8 url 18 url 28 url 38 url 48 url 58 url 68 url 78 url 88 url 98 url 108 url 9 url 19 url 29 url 39 url 49 url 59 url 69 url 79 url 89 url 99 url 109 url 10 url 20 url 30 url 40 url 50 url 60 url 70 url 80 url 90 url 100 url upvoted paper two weeks ago u rtrx3 machine learn team share reuse feature url efficient exploration chemical space dock deep learn url styleclip text drive manipulation stylegan imagery sota stylegan image edit url rule fun
Farconion,MachineLearning,1620063831.0,[D] Effectively summarizing projects & research on a resume?,"does anyone have any tips for how to effectively summarize ML projects / research on a resume?

I find that since such projects & research are usually pretty niche, that trying to convey what you did is an uphill battle because most  people who read your resume aren't going to have any idea what you're  talking about - but you don't have enough space to cover the appropriate background knowledge

I get part of this is the jobs you apply to, but since I'm not banking on getting an ML position after college and a good chunk of my work is ML related - I'd like to make my background as approachable to those unfamiliar with the field / subfields related to my work",anyone tip effectively summarize ml project research resume find since project research usually pretty niche try convey uphill battle people read resume go idea talk enough space cover appropriate background knowledgei get part job apply since bank get ml position college good chunk work ml relate like make background approachable unfamiliar field subfields relate work
hallavar,MachineLearning,1617297476.0,[Discussion] Any metric for evaluating non-image synthetic data ?,"Hello, evaluating generated data (obtained by AE, GAN etc..) is always a tricky question.

For images, most of the papers i've read used the Inception score or its evolution, the FiD.

However,  those metrics are based on how well a generated images can be  classified by a pretrained network (this is a summary, no need for  details here)

But if I generate  data that are not image related (like text, sequences, graphs). How can I  evaluate the generation of my model ?

A  good metric should evaluate the plausibility of the synthetic data, ie  the the realism of the generation given the training distribution; but  also its diversity : to ensure that the training distribution is well  understood by the model.

I know how to evaluate the first criterium (realism), but i have no idea how to evaluate the second

If anyone has ever worked on something like that, i will be glad to talk about his/her work.

Thank you in advance",hello evaluate generate data obtain ae gin etc always tricky question image paper read use inception score evolution fid however metrics base well generate image classify pretrained network summary need detail generate data image relate like text sequence graph evaluate generation model good metric evaluate plausibility synthetic data ie realism generation give train distribution also diversity ensure train distribution well understand model know evaluate first criterium realism idea evaluate secondif anyone ever work something like glad talk work thank advance
SolitaryPenman,MachineLearning,1619269805.0,[D] Looking for datasets for video time-series segmentation/labeling,"I am looking for video datasets for time-series segmentation. Note that I am NOT looking for datasets for semantic segmentation of videos but for labeling each time-step into one of K categories. An example of such a video would be where a person performs different actions (with more than one action in a single video). Unfortunately, all the datasets I could find only have a single action performed in the video (eg, Weizmann Human Action Dataset) and thus a single label per video. It would be great if someone could point me to such video dataset(s) where each time-step can have a different label.",look video datasets time series segmentation note look datasets semantic segmentation videos label time step one k categories example video would person perform different action one action single video unfortunately datasets could find single action perform video eg weizmann human action dataset thus single label per video would great someone could point video dataset time step different label
rarboot,MachineLearning,1618235282.0,[N] Microsoft buys AI speech tech company Nuance for $19.7 billion,"From [The Verge](https://www.theverge.com/2021/4/12/22379414/microsoft-buys-nuance-ai-speech-tech).

I may be wrong on this, but afaik it has been a while since Microsoft made such a huge acquisition of a company with an arguably heavily-convoluted internal ecosystem. It feels like MS did it for the data acquisition processes more than for the product portfolio, which IMO will be cannibalized. Any thoughts?",verge url may wrong afaik since microsoft make huge acquisition company arguably heavily convolute internal ecosystem feel like ms data acquisition process product portfolio imo cannibalize thoughts
mikegartrell,MachineLearning,1617282048.0,[N] Upcoming talks for Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale,"We have two upcoming talks in April for our ongoing online seminar series about Bayesian machine learning at scale.  The intended audience includes machine learning practitioners and statisticians from academia and industry.

&#x200B;

Upcoming talks, with Zoom registration links:

This coming Wednesday, 7 April

[Monte Carlo integration with repulsive point processes - Rémi Bardenet](https://criteo.zoom.us/webinar/register/WN_i_Vil4YtTpaFsH5Do_hdyg)

Monte Carlo integration is the workhorse of Bayesian inference, but the mean square error of Monte Carlo estimators decreases slowly, typically as 1/N, where N is the number of integrand evaluations. This becomes a bottleneck in Bayesian applications where evaluating the integrand can take tens of seconds, like in the life sciences, where evaluating the likelihood often requires solving a large system of differential equations. I will present two approaches to faster Monte Carlo rates using interacting particle systems. First, I will show how results from random matrix theory lead to a stochastic version of Gaussian quadrature in any dimension d, with mean square error decreasing as 1/N\^{1+1/d}. This quadrature is based on determinantal point processes, which can be argued to be the kernel machine of point processes. Second, I will show how to further take this error rate down assuming the integrand is smooth. In particular, I will give a tight error bound when the integrand belongs to any arbitrary reproducing kernel Hilbert space, using a mixture of determinantal point processes tailored to that space. This mixture is reminiscent of volume sampling, a randomized experimental design used in linear regression.

Joint work with Ayoub Belhadji, Pierre Chainais, and Adrien Hardy

&#x200B;

21 April

[Automatic Backward Filtering Forward Guiding for Markov processes and graphical models - Frank van der Meulen](https://criteo.zoom.us/webinar/register/WN_6SZkMHlfQHKL3kPalfm5GQ)

I discuss a structured way for efficient inference in probabilistic graphical models with building blocks consisting of Markovian stochastic processes. The starting point is a generative model, a forward description of the probabilistic dynamics. The information provided by observations can be backpropagated through the model to transform the generative (forward) model into a conditional model guided by the data. It approximates the actual conditional model with known likelihood-ratio between the two. The backward filter and the forward change of measure are suitable to be incorporated into a probabilistic programming context because they can be formulated as a set of transformation rules. The guided generative model can be combined with different approaches to efficiently sample latent states and parameters conditional on observations. Application settings include Markov chains with discrete state space, interacting particle systems, state space models, branching diffusions and Gamma processes.

&#x200B;

Past talks can be found [here](https://ailab.criteo.com/laplaces-demon-bayesian-machine-learning-at-scale/).",two upcoming talk april ongoing online seminar series bayesian machine learn scale intend audience include machine learn practitioners statisticians academia industry x200b upcoming talk zoom registration link come wednesday nmbr april monte carlo integration repulsive point process rémi bardenet url carlo integration workhorse bayesian inference mean square error monte carlo estimators decrease slowly typically 1 n n number integrand evaluations become bottleneck bayesian applications evaluate integrand take tens second like life sciences evaluate likelihood often require solve large system differential equations present two approach faster monte carlo rat use interact particle systems first show result random matrix theory lead stochastic version gaussian quadrature dimension mean square error decrease 1 n 1 1 quadrature base determinantal point process argue kernel machine point process second show take error rate assume integrand smooth particular give tight error bind integrand belong arbitrary reproduce kernel hilbert space use mixture determinantal point process tailor space mixture reminiscent volume sample randomize experimental design use linear regression joint work ayoub belhadji pierre chainais adrien hardy x200b 21 april automatic backward filter forward guide markov process graphical model frank van der meulen url discuss structure way efficient inference probabilistic graphical model build block consist markovian stochastic process start point generative model forward description probabilistic dynamics information provide observations backpropagated model transform generative forward model conditional model guide data approximate actual conditional model know likelihood ratio two backward filter forward change measure suitable incorporate probabilistic program context formulate set transformation rule guide generative model combine different approach efficiently sample latent state parameters conditional observations application settings include markov chain discrete state space interact particle systems state space model branch diffusions gamma process x200b past talk find url
strngelet,MachineLearning,1616679026.0,[P] boost T5 models speed up to 5x & reduce the model size by 3x using fastT5.,"I wanted to share this new library I've been working on and that I just open-sourced!. Here are some quick links :

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

[fastt5 logo](https://preview.redd.it/kno5bm8se6p61.png?width=1280&format=png&auto=webp&s=390250fe3018973b949fa8f34ba481a0b1236118)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5.` This code snippet from the repository's README gives a concise overview:

[fastt5 usage](https://preview.redd.it/glra7heve6p61.png?width=1496&format=png&auto=webp&s=ea659b124cb5290fc5e1ab4c1a1f6e9380dac0b3)

The fastT5 library exports the T5 model to onnx with `past_key_values`, then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x).",want share new library work open source quick link github repository url pypi project url logo url title suggest increase inference speed pretrained t5 model also decrease model size single line code library instal pip install fastt5 code snippet repository readme give concise overview fastt5 usage url fastt5 library export t5 model onnx past_key_values quantize run onnxruntime export onnx model support generate method huggingface transformers inferencing information project refer repository url
MudlarkJack,MachineLearning,1617972696.0,[Discussion] is it possible to cross train a pre-existing model with a higher resolution data set than was used to train the original network?,"use case: 

Should have indicated in title that this is a GAN question.

For example I have previously trained a network on say 512x512 images. I want to cross train it on a completely new data set that contains 1024x1024 images to benefit from the normal time saving of cross training. Can that work or do the smaller resolutions in the original data set somehow preclude this?",use case indicate title gin question example previously train network say 512x512 image want cross train completely new data set contain 1024x1024 image benefit normal time save cross train work smaller resolutions original data set somehow preclude
sideonion,MachineLearning,1619650542.0,[P] Are there non-profits who work on ML for social cause (fairness etc.) where I can work for a project and contribute?,"Geography  no bar -- but they should be okay with volunteers from any other  geographic location. I want to volunteer or work part time for a  non-profit who is working or researching on ML on things like fairness  and accountability. I have good tech background but I am not able to  find places where I can contribute and possibly research for social  cause.

Please don't recommend Allen AI because they only look for full time people and I can not move to USA.",geography bar okay volunteer geographic location want volunteer work part time non profit work research ml things like fairness accountability good tech background able find place contribute possibly research social cause please recommend allen ai look full time people move usa
regalalgorithm,MachineLearning,1620420803.0,[D] Invitation to help address AI misrepresentation and misconceptions,"TLDR: I run a site to debunk misconceptions of AI news, pls  positive response, so hope bringing it up again now that we could use more help is fine.

As I posted before, for more than 3 years I've been running this thing called [Skynet Today](http://www.skynettoday.com/) (the name is meant to be ironic/news-y), with the mission of ""Putting AI News In Perspective"", or in other words  debunk inaccurate portrayals of AI research in media and also put out articles that put things in perspective. 

As many people  here are researchers and feel annoyed at hype/misconceptions about AI, I  wonder if any of you might want to join our effort. 

We are basically a couple of grad students doing this in our spare time, and have not put out any new articles in a while due to being busy / not having much help (writing is a lot of work!). If  interested, please consider taking a look at our [contribution survey](https://www.skynettoday.com/contribute), or just message me. Thanks!

Some examples of articles we've put out include:

* [DeepMind’s AlphaFold 2—An Impressive Advance With Hyperbolic Coverage](https://www.skynettoday.com/briefs/alphafold2)
* [The State of Deepfakes in 2020](https://www.skynettoday.com/overviews/state-of-deepfakes-2020)
* [GPT-3: An AI Breakthrough, but not Coming for Your Job](https://www.skynettoday.com/briefs/gpt3)
* [IBM, Microsoft, and Amazon Halt Sales of Facial Recognition to Police, Call for Regulations](https://www.skynettoday.com/briefs/face-recog-police)
* [Boston Dynamics' robots — impressive, but far from the Terminator  ](https://www.skynettoday.com/briefs/boston-dynamics)

TLDR: I run a site to debunk misperceptions about AI, pls [join](https://www.skynettoday.com/contribute) if you wanna help",tldr run site debunk misconceptions ai news pls positive response hope bring could use help fine post nmbr years run thing call skynet today url name mean ironic news mission put ai news perspective word debunk inaccurate portrayals ai research media also put article put things perspective many people researchers feel annoy hype misconceptions ai wonder might want join effort basically couple grad students spare time put new article due busy much help write lot work interest please consider take look contribution survey url message thank examples article put include deepminds alphafold 2 impressive advance hyperbolic coverage url state deepfakes 2020 url gpt 3 ai breakthrough come job url ibm microsoft amazon halt sales facial recognition police call regulations url boston dynamics robots impressive far terminator url run site debunk misperceptions ai pls join url wan na help
git-commit-bt7274,MachineLearning,1617721034.0,[P] Corgi: Rust neural network/dynamic automatic differentiation library I have been working on,"Hello there, I've been working on a Rust automatic differentiation crate called 'corgi'. It's not to the level of the other crates out there, but I thought it would be worth sharing.

Github: [https://github.com/patricksongzy/corgi](https://github.com/patricksongzy/corgi)

Crate: [https://crates.io/crates/corgi](https://crates.io/crates/corgi)

Edit: Fixed example

Edit 1: added support for custom layers, and models, example [https://github.com/patricksongzy/corgi/blob/main/examples/custom.rs](https://github.com/patricksongzy/corgi/blob/main/examples/custom.rs)

Example:

* Dynamic computational graph

&#x200B;

    let a = arr![5.0];
    let b = arr![2.0];
    let mut c = arr![0.0];
    for _ in 0..10 {
        c = &c + &(&a * &b);
        if c[0] > 50.0 {
            c = &c * &a;
        }
    }
    c.backward(None);
    assert_eq!(c, arr![195300.0]);
    assert_eq!(c.gradient(), arr![1.0]);
    assert_eq!(b.gradient(), arr![97650.0]);
    assert_eq!(a.gradient(), arr![232420.0]);

&#x200B;

* Fully-connected neural network: [https://github.com/patricksongzy/corgi/blob/main/src/dense.rs](https://github.com/patricksongzy/corgi/blob/main/src/dense.rs)
* Custom operation (this needs some work) - in the README

The project still needs a lot of work, including:

* Improving the ergonomics of the \`arr!\` macro
* Implementing more than just fully-connected layers
* Efficiency improvements, and using BLAS",hello work rust automatic differentiation crate call corgi level crate think would worth share github url url fix exampleedit 1 add support custom layer model example url dynamic computational graph x200b let arr 5 0 let b arr 2 0 let mut c arr 0 0 _ 0 10 c c b c 0 nmbr c c c backward none assert_eq c arr 195300 0 assert_eq c gradient arr 1 0 assert_eq b gradient arr 97650 0 assert_eq gradient arr 232420 0 x200b fully connect neural network url custom operation need work readmethe project still need lot work include improve ergonomics arr macro implement fully connect layer efficiency improvements use blas
WavyShapes,MachineLearning,1616431656.0,[P] Backprop: a library to easily finetune and use state-of-the-art models,"Hi everybody —

I'd like to share [Backprop](https://github.com/backprop-ai/backprop), a Python library I've been co-authoring for the last few months. Our goal is to make finetuning and using models as easy as possible, even without extensive ML experience.

We've currently got support for text and image-based tasks, with wrappers around models like Google's T5, OpenAI's CLIP, and Facebook's BART, among others.

Once you've got your training data, you can just import your model/task, and then finetune with a single line of code.

We've also got some features that make deployment for production easy, but for full transparency, deployment is through a paid platform we've developed that is by no means necessary to use the library.

If you decide to check it out, the docs are [here](https://backprop.readthedocs.io/en/latest/), and we've got some example notebooks in the repo's [examples](https://github.com/backprop-ai/backprop/tree/main/examples) folder.

We're happy with the progress we've made, but it's still early days — we'd really appreciate any thoughts and feedback we can get, so we can make Backprop more featureful and easier to use in the future.",hi everybody like share backprop url python library co author last months goal make finetuning use model easy possible even without extensive ml experience currently get support text image base task wrappers around model like google t5 openai clip facebook bart among others get train data import model task finetune single line code also get feature make deployment production easy full transparency deployment pay platform develop mean necessary use library decide check docs url get example notebooks repo examples url folder happy progress make still early days really appreciate thoughts feedback get make backprop featureful easier use future
ad1tyawagh,MachineLearning,1619612972.0,[Discussion] Why doesn't Google's Live Captioning feature generate punctuation marks?,"Posting here since it's something related to NLP.

I understand that during data cleaning, people usually remove punctuations, that's why there might not be any punctuations. But considering that punctuations hold semantic meaning, I don't understand why removing them is the general practice. 

Can someone shed some light on this?",post since something relate nlp understand data clean people usually remove punctuations might punctuations consider punctuations hold semantic mean understand remove general practice someone shed light
Red-Portal,MachineLearning,1616337731.0,[D] How many of you explicitly ask the reviewer to raise their score?,"Hi, 
Given that it's the ICML review period, I would like to ask your experience on ""explicitly asking the reviewer to raise their score."" I saw some people do this and got positive results but I'm curious if this is effective (of not negative) in general. In where I'm from, doing such a thing might be somewhat rude, as what to do with the score is totally up to the reviewer. Is there a polite phrase for asking such a thing?

Also, how many of you write something in the private feedback to the metareviewer box (even if there is no outright unjust conduct by the reviewer)?",hi give icml review period would like ask experience explicitly ask reviewer raise score saw people get positive result curious effective negative general thing might somewhat rude score totally reviewer polite phrase ask thing also many write something private feedback metareviewer box even outright unjust conduct reviewer
bendee983,MachineLearning,1618507723.0,[D] Microsoft's ML acquisition strategy,"This week, Microsoft announced the $19.7-billion acquisition of Nuance, a company that uses deep learning to transcribe clinical appointments (and other stuff). What's interesting about the deal is the [evolution of Microsoft's relation with Nuance](https://bdtechtalks.com/2021/04/15/microsoft-nuance-acquisition/), going from cloud provider to partner to owner. 

This is a successful strategy that only Microsoft (and maybe Amazon) is in a position to implement:

Step 1: Microsoft starts by investing in ML companies by giving them Azure credits and luring them into its ML platform. This allows Microsoft to help the companies develop and also learn from them (and possibly replicate their products if it's worth it). Multiple small investments as opposed to one large acquisition is a smart move because many companies are trying new things in ML/DL, few of which will be successful. With small investments, Microsoft can cast a wider net and make sure it is in a good position to make the next move.

Step 2: Microsoft enters partnership with companies that have successful products. This allows Microsoft to integrate their ML products into its enterprise solutions (e.g., Nuance's Dragon DL was integrated into Microsoft's cloud healthcare solution). Since these companies are building their ML tools on top of Azure's stack, the integration is much easier for both companies.

Step 3: Acquire really successful companies (Nuance has a great reach in the AI+healthcare sector). This allows Microsoft to gain exclusive access to the company's data, talent, technology, and clients. With the acquisition of Nuance, Microsoft's total addressable market in healthcare has reached $500B+. And it can integrate its ML technology into its other enterprise tools.

Nuance is just one example of Microsoft's ML acquisition strategy. The company is on a similar path [with OpenAI](https://bdtechtalks.com/2020/09/24/microsoft-openai-gpt-3-license/) and is carrying out [a similar strategy in the self-driving car industry](https://bdtechtalks.com/2021/01/21/microsoft-self-driving-car-strategy/).",week microsoft announce 19 7 billion acquisition nuance company use deep learn transcribe clinical appointments stuff interest deal evolution microsoft relation nuance url go cloud provider partner owner successful strategy microsoft maybe amazon position implement step 1 microsoft start invest ml company give azure credit lure ml platform allow microsoft help company develop also learn possibly replicate products worth multiple small investments oppose one large acquisition smart move many company try new things ml dl successful small investments microsoft cast wider net make sure good position make next move step 2 microsoft enter partnership company successful products allow microsoft integrate ml products enterprise solutions e g nuance dragon dl integrate microsoft cloud healthcare solution since company build ml tool top azure stack integration much easier company step 3 acquire really successful company nuance great reach ai healthcare sector allow microsoft gain exclusive access company data talent technology clients acquisition nuance microsoft total addressable market healthcare reach 500b integrate ml technology enterprise tool nuance one example microsoft ml acquisition strategy company similar path openai url carry similar strategy self drive car industry url
Jemsdaan,MachineLearning,1617052129.0,[D] RTX 3080 cuda 10.0,"Hi guys,

for a uni project I need to replicate a project which uses tensorflow-gpu 1.13 and Cuda 10.0. The only GPU I have at my disposal is a RTX 3080.

Now, from my understanding, the RTX 3080 doesn't support Cuda 10.0, only Cuda 11. In order to use Cuda 11, tensorflow needs to be updated to 2.4.0, but this breaks alot of code. So my quesiton is: am I dead in the water? 

To me it seems like the only solution is to either train the models with a CPU (which has terrible perfomance) or upgrade all code so it is compatible with tensorflow 2.4.0, but this is out of scope of the project.

I'm not sure if this is the correct place to ask but if someone has a suggestion on how to get this to run on my GPU, please let me know!",hi guy uni project need replicate project use tensorflow gpu nmbr cuda 10 0 gpu disposal rtx 3080 understand rtx nmbr support cuda 10 0 cuda 11 order use cuda 11 tensorflow need update 2 4 0 break alot code quesiton dead water seem like solution either train model cpu terrible perfomance upgrade code compatible tensorflow 2 4 0 scope project sure correct place ask someone suggestion get run gpu please let know
SPAMinaCanCan,MachineLearning,1618438536.0,[D] What are you thoughts on how the amount of classes can affect model accuracy,"This hopefully is a very basic question

I'm struggling to find good papers explaining how the amount of different classes affects different models 

For example, say you were building a semantic segmentation classifier for JPG images containing soft drink cans.

Your objective is to find all the cans contained within the image.

Is it better to construct your training data using a generic soft drink can class?

OR is it better to construct your training data using different colours of soft drink cans as seperate classes (i.e. red can, orange can, green can, etc...)?

I am wondering how each of these class naming conventions will affect the overall accuracy classifying every can.

What are your thoughts on this?

Also, if you want, comment on how it can extend to other things, such as cars, clothes, etc...",hopefully basic questioni struggle find good paper explain amount different class affect different model example say build semantic segmentation classifier jpg image contain soft drink objective find contain within image better construct train data use generic soft drink class better construct train data use different colour soft drink seperate class e red orange green etc wonder class name conventions affect overall accuracy classify every thoughts also want comment extend things cars clothe etc
AuspiciousApple,MachineLearning,1620578926.0,[D] What's the SOTA/best practice for finetuning pre-trained CNNs on smaller datasets (~10k images)? Any principled approaches? Any papers comparing different schedules?,"There's basically two ways to approach transfer learning: 

A) Treat the pretrained weights as just a particularly efficient weight initialisation method and train a model like normal, focussing on all the other hyperparams, especially data augmentation

B) Use a special protocol to finetune

For B, Francois Chollet recommends to freeze to the lower convolutional layers and only train the added head, and then optionally as a second step also finetune part or all of the lower convolutional layers with a very low learning rate.

[https://keras.io/guides/transfer\_learning/#introduction](https://keras.io/guides/transfer_learning/#introduction)

[https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

I also remember reading about other ideas, like applying a l2 penalty not to do magnitude of weights but to their deviation from the pre-trained weights.

Are these special schedules worth it? Is there any research on it?

I'm also happy for any other suggestions for finetuning.",basically two ways approach transfer learn treat pretrained weight particularly efficient weight initialisation method train model like normal focus hyperparams especially data augmentationb use special protocol finetunefor b francois chollet recommend freeze lower convolutional layer train add head optionally second step also finetune part lower convolutional layer low learn rate url also remember read ideas like apply l2 penalty magnitude weight deviation pre train weight special schedule worth research also happy suggestions finetuning
hyunwoongko,MachineLearning,1617857667.0,[P] Try to talk with GPT3 (GPT-Neo),"&#x200B;

https://preview.redd.it/7nphdngxrvr61.png?width=2100&format=png&auto=webp&s=9e7df7710ddc52c46a66d76d150485425ac6baeb

The GPT-Neo model was released in the [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) repository by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT3 like causal language model trained on the [Pile](https://pile.eleuther.ai/) dataset.

&#x200B;

Today, I deployed a prompt-based conversation option using GPT-Neo in Openchat. You can try conversation with GPT-Neo using **only line of code.** Check here ([https://github.com/hyunwoongko/openchat](https://github.com/hyunwoongko/openchat)) if you want more detail informations. Thanks.",x200b url gpt neo model release eleutherai gpt neo url repository sid black stella biderman leo gao phil wang connor leahy gpt3 like causal language model train pile url dataset x200b today deploy prompt base conversation option use gpt neo openchat try conversation gpt neo use line code check url want detail informations thank
fromnighttilldawn,MachineLearning,1617599998.0,[D] How do you improve your model after obtaining the test error?,"I have a very basic question about training neural networks and ML models that I do not see being addressed in the literature. Consider the following scenario:

&#x200B;

* You chop your data up into train/validation/test with something like 70%, 10%, 20% ratio. 
* You train and perform validation and you feel pretty confident that your model will perform well.
* Then you run your model on the test set, you obtain a high error (or something that falls below your expectation). Yikes!
* Of course, at this stage, you would do anything to improve your model, but *you can't*: **if you try to do anything, you will be effectively using your test set as a training set.**

My question is whether there exist some best/correct practices to improve your model after obtaining the test error. 

I know in the literature there is a lot of hyperparameter tuning after obtaining the test set...I do not wish to follow in their footsteps. 

If not, are there some techniques that allow you to prevent this scenario from happening? 

Thanks in advance!",basic question train neural network ml model see address literature consider follow scenario x200b chop data train validation test something like 70 10 20 ratio train perform validation feel pretty confident model perform well run model test set obtain high error something fall expectation yikes course stage would anything improve model try anything effectively use test set train set question whether exist best correct practice improve model obtain test error know literature lot hyperparameter tune obtain test set wish follow footsteps techniques allow prevent scenario happen thank advance
hobogalaxy,MachineLearning,1620042624.0,"[P] General and feature-rich PyTorch/Hydra template for rapid and scalable ML research/experimentation, with a list of best practices","Hi all,

I've been looking for a way to make my research more efficient and scalable. After iterating over a couple of different frameworks and structures, I converged on the following template: [https://github.com/ashleve/lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template)

To get you a feel of how it might be useful for you, take a look at [\#Your Superpowers](https://github.com/ashleve/lightning-hydra-template#your-superpowers) section of the readme. It's based on PyTorch Lightning with Hydra and its plugins.

I developed it for my research team but I also meant it as a starting point for anyone who would like to learn about this technology stack. I find this combination simple to use but very powerful at the same time. I believe it's very convenient for small-team research, reproducing papers and generally for projects in which you need to maintain many curated configurations of your experiments.

How I find it useful:

* it allows us to painlessly scale from small experiments into hiperparameter search on multi-GPU or SLURM computing clusters (with frameworks like Optuna or Ax). The hyperparameter optimization requires minimal setup: you only need to declare config with hyperparameter ranges and hydra takes care of the whole iterating over jobs logic.
* possibility to use most experiment tracking frameworks (like neptune, wandb, MLflow or just csv files)
* easy configuration management with command line superpowers (no need for argparse thanks to Hydra)
* advanced training and debugging features from pytorch lightning (e.g. gradient accumualation, deepspeed integration, etc.)
* encapsulating datasets into lightning datamodules gives us a very convenient way for understanding and reusing datasets across projects

I feel like most ML people don't use those tools because they simply don't realize all the advantages (especially Hydra seems like a very useful addition to any deep learning project). I focused on structuring the readme in a way, which (I hope) will give you a quick overview - my hope is it can help to spread the word about those frameworks in a broaded community. It incorporates [best practices](https://github.com/ashleve/lightning-hydra-template#best-practices) and tricks I gathered over the last couple of months of playing around with it.

My typical workflow is the following:

1. I write a LightningDatamodule. I found it to be an intuitive way to encapsulate any dataset. LightningDatamodule is a simple abstraction providing methods for data download, split, transforms and exposing dataloaders. Would love to see more researchers try out this concept, even in projects which don't use pytorch lightning. Reading LightningDatamodule makes me immedietely see how the dataset is prepared, while it seems like most data science projects throw around data logic across different parts of the pipeline, making it hard to understand what's going on. You can see example of such datamodule [here](https://github.com/ashleve/lightning-hydra-template/blob/main/src/datamodules/mnist_datamodule.py)
2. I write a LightningModule. This basically just encapsulates my pytorch model code.
3. I add new experiment config specifying the paths to the LightningDatamodule and LightningModule.Now the training can be launched with some experiment tracker attached, like csv logger or tensorboard: `python` [`run.py`](https://run.py/) `experiment=simple_mnist logger=tensorboard`

Btw. the structure is partly based on data science cookie cutter project template. If you didn't hear about it I recommend you to check it out - I found it to be a great source of useful concepts for project organization:

[https://drivendata.github.io/cookiecutter-data-science/](https://drivendata.github.io/cookiecutter-data-science/)

I'd love to hear your thoughts! Let me know if you see some limitations or a room for improvement",hi look way make research efficient scalable iterate couple different frameworks structure converge follow template url get feel might useful take look superpowers url section readme base pytorch lightning hydra plugins develop research team also mean start point anyone would like learn technology stack find combination simple use powerful time believe convenient small team research reproduce paper generally project need maintain many curated configurations experiment find useful allow us painlessly scale small experiment hiperparameter search multi gpu slurm compute cluster frameworks like optuna ax hyperparameter optimization require minimal setup need declare config hyperparameter range hydra take care whole iterate job logic possibility use experiment track frameworks like neptune wandb mlflow csv file easy configuration management command line superpowers need argparse thank hydra advance train debug feature pytorch lightning e g gradient accumualation deepspeed integration etc encapsulate datasets lightning datamodules give us convenient way understand reuse datasets across projectsi feel like ml people use tool simply realize advantage especially hydra seem like useful addition deep learn project focus structure readme way hope give quick overview hope help spread word frameworks broaded community incorporate best practice url trick gather last couple months play around typical workflow follow 1 write lightningdatamodule find intuitive way encapsulate dataset lightningdatamodule simple abstraction provide methods data download split transform expose dataloaders would love see researchers try concept even project use pytorch lightning read lightningdatamodule make immedietely see dataset prepare seem like data science project throw around data logic across different part pipeline make hard understand go see example datamodule url write lightningmodule basically encapsulate pytorch model code 3 add new experiment config specify paths lightningdatamodule lightningmodule train launch experiment tracker attach like csv logger tensorboard python run py url experiment simple_mnist logger tensorboard btw structure partly base data science cookie cutter project template hear recommend check find great source useful concepts project organization url love hear thoughts let know see limitations room improvement
seuadr,MachineLearning,1617201468.0,Machine Learning and HVAC [D]," Hi all,

I am working in HVAC controls deploying a fault detection platform. The product that we have currently is rules based, which is find for many things, but, it is not so great at determining long term performance drift.

I've started a data science boot camp and am learning about machine learning with an eye towards applying this to our buildings/HVAC systems.

we'd like to use datasets to predict things ranging from a general expected energy use for a near future period of time (like, the next hour) to performance drift on sensors and other equipment (like a heating coils performance not meeting expectations for the current conditions)

i think this is all ""do-able"" but it doesn't seem to be an area that is discussed openly on the internet (or my google-fu is weak..)

so i'm not sure where to start. For instance, I've been collecting large datasets from this equipment for years and i'm pretty confident that we can gain useful insight from it, but, how to i automate the process of feeding that in and executing the model?

what kind of intervals should i consider?

can i have it execute on a short time interval, like say, 5 mins and write that out?

what kind of computing power would i need for this? - i currently have a vm with 8 cores and 32gb, i would expect that'd have some ponies to get the job done?

i know these are big, open questions - just looking for some guidance on where to start and see if you all might be aware of existing communities that could help.

Thanks in advance for your time and assistance!

Jared",hi work hvac control deploy fault detection platform product currently rule base find many things great determine long term performance drift start data science boot camp learn machine learn eye towards apply build hvac systems like use datasets predict things range general expect energy use near future period time like next hour performance drift sensors equipment like heat coil performance meet expectations current condition think able seem area discuss openly internet google fu weak sure start instance collect large datasets equipment years pretty confident gain useful insight automate process feed execute model kind intervals consider execute short time interval like say nmbr mins write kind compute power would need currently vm nmbr core 32gb would expect ponies get job know big open question look guidance start see might aware exist communities could help thank advance time assistance jar
ai_painter,MachineLearning,1619460955.0,[D] Lambda GPU Cloud launches world's first RTX A6000 instances,"[Lambda launches RTX A6000 GPU Cloud Instances](https://lambdalabs.com/blog/introducing-nvidia-rtx-a6000-gpu-instances-on-lambda-cloud/)

* NVIDIA RTX A6000 instances are \~2x faster than NVIDIA RTX 6000 instances
* RTX A6000's have 48 GiB of VRAM per GPU

Disclaimer: I'm an engineer at Lambda.",lambda launch rtx a6000 gpu cloud instance url nvidia rtx a6000 instance 2x faster nvidia rtx nmbr instance rtx a6000 nmbr gib vram per gpudisclaimer engineer lambda
Yuqing7,MachineLearning,1619192608.0,"[R] Facebook AI, McGill U & Mila Promote 'Translationese' to Boost NMT System Faithfulness","A research team from McGill University, Mila - Quebec AI Institute and Facebook AI proposes novel metrics and perturbation functions to detect, quantify and compare trade-offs between robustness and faithfulness in NMT systems, both on the corpus level and with particular examples.

Here is a quick read: [Facebook AI, McGill U & Mila Promote 'Translationese' to Boost NMT System Faithfulness.](https://syncedreview.com/2021/04/23/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-4/)

 The paper *Sometimes We Want Translationese* is on [arXiv](https://arxiv.org/pdf/2104.07623.pdf).",research team mcgill university mila quebec ai institute facebook ai propose novel metrics perturbation function detect quantify compare trade robustness faithfulness nmt systems corpus level particular examples quick read facebook ai mcgill u mila promote translationese boost nmt system faithfulness url paper sometimes want translationese arxiv url
jeromeharper,MachineLearning,1618965208.0,[D] Cov-19 binary classification dataset.," Hi, Guys, I am looking for a binary classification covid-19 dataset, the prediction label is positive or negative, this label could be as, whether some one is infected or not. So far I found dataset from kaggle. But most of them is consists a medical image such as XRay scan of chest. I am not looking for image data as the feature. I found one from kaggle the feature is age, gender, hypertension and etc which the label is positive / negative. Please help. Thank you in advance.. Cheers",hi guy look binary classification covid 19 dataset prediction label positive negative label could whether one infect far find dataset kaggle consist medical image xray scan chest look image data feature find one kaggle feature age gender hypertension etc label positive negative please help thank advance cheer
JasonTodd550,MachineLearning,1619729450.0,[R] I’m building the interface for our product. I’d love some feedback and insights.,"Hey folks,

I’m a frontend developer/designer working on building out an interface for our product, a platform for ML models to be hosted, sandboxed, and ran as an API. Currently our MVP is running, but we don’t have an interface for users to use. We’re looking through communities that are ML focused to try and get industry-target users to participate in our research for 1:1 validation.  Where better to look than  the sub dedicated to it.

The link to our test is [here](https://app.useberry.com/t/mK8Ywphn/), but I’m also super happy to get some one on one feedback too. If you’re interested in sending your thoughts directly to me, or even an interview with me and my partner, DM me!",hey folks im frontend developer designer work build interface product platform ml model host sandboxed run api currently mvp run interface users use look communities ml focus try get industry target users participate research 1 1 validation better look sub dedicate link test url im also super happy get one one feedback interest send thoughts directly even interview partner dm
tstanislawek,MachineLearning,1617815710.0,[P] Curated List of Document Understanding (DU) Papers & Resources.,"Hi everybody,

In  the last few years, I spent a lot of time working on automating business processes of big companies and seeing rising interest in DU topic (especially in the Key Information Extraction field). Therefore, I  created a list [https://github.com/tstanislawek/awesome-document-understanding](https://github.com/tstanislawek/awesome-document-understanding) of resources to make easier to track all the papers out there which are relevant to this topic.",hi everybody last years spend lot time work automate business process big company see rise interest du topic especially key information extraction field therefore create list url resources make easier track paper relevant topic
Yogi_DMT,MachineLearning,1618847448.0,[P] StoRM: Mutation-based hyperparameter tuner,"For those struggling to find a decent hyperparameter tuner (NN tuning for example), I have designed a tuner that attempts to remedy a lot of the issues associated with this type of parameter space (nested, categorical, conditional, etc.)

There is a runnable script in the examples folder that demonstrates StoRM's performance compared random tuning. Please feel free to post any feedback and let me know if it is useful for you.

[https://github.com/ben-arnao/StoRM](https://github.com/ben-arnao/StoRM)",struggle find decent hyperparameter tuner nn tune example design tuner attempt remedy lot issue associate type parameter space nest categorical conditional etc runnable script examples folder demonstrate storm performance compare random tune please feel free post feedback let know useful url
gabegabe6,MachineLearning,1618579597.0,[D] What tools can you recommend for GPU resource alocation?,"The problem is given: There is a team, and there are GPU servers with >1 GPUs. What is the best way, to signal who uses which GPUs for how long? E.g. I am thinking about a simple tool with which you can say, that from 2 days later, you'll need 5 GPUs, and you ""allocate it"" for that time. Of course, everyone in the team could see this.

I was planning to create such a simple CLI tool, but first wanted to get some feedback, on who uses what for this purpose.

(I really don't want to use excel sheets for this.)",problem give team gpu servers 1 gpus best way signal use gpus long e g think simple tool say nmbr days later need nmbr gpus allocate time course everyone team could see plan create simple cli tool first want get feedback use purpose really want use excel sheet
CauchySchwartzDaddy,MachineLearning,1619584963.0,[D] Are there light(-er) installs of pytorch for model deployment that aren't nearly a whole gb of space and just support loading a model and forward pass?,"I'm deploying some models on docker and flask and I find it kind of tedious that I have to install 750 mb of of pytorch if all I need to do is load a model and forward pass it considering most containers and deployment methods are pretty light in terms of space allotted, so I can't really afford such a huge install.",deploy model docker flask find kind tedious install nmbr mb pytorch need load model forward pass consider containers deployment methods pretty light term space allot really afford huge install
Equivalent-Choice-75,MachineLearning,1619383564.0,[D] Academia to Industry. How to deal with Research FOMO?,"I'm a Masters student at a top-10 US university. I spend almost all of my working hours doing ML research.

I'm going to industry in a few days and have FOMO about miss out on cutting edge research due to the nature of job (product facing rather than pure research). I'm sure many PhD students would've faced the same. 

How do you deal with this? Do you still read latest papers post joining? Or any other suggestions?",master student top 10 us university spend almost work hours ml research go industry days fomo miss cut edge research due nature job product face rather pure research sure many phd students would face deal still read latest paper post join suggestions
numpee,MachineLearning,1617769571.0,[Discussion] Suggestions for well-written papers,"I recently encountered a post regarding the [quality of prose](https://www.reddit.com/r/MachineLearning/comments/mh7vrt/d_does_anyone_care_about_the_quality_of_the_prose/?utm_source=share&utm_medium=web2x&context=3) in academic papers.

It seemed like people generally have different views on what they consider to be well written. For example, some prefer clear, concise language without any 'colorful' language. And vice versa.

So my question is: Can you suggest some papers that you consider are very well-written? Would be nice if you gave a quick explanation of your preference in writing style as well. :)",recently encounter post regard quality prose url academic paper seem like people generally different view consider well write example prefer clear concise language without colorful language vice versa question suggest paper consider well write would nice give quick explanation preference write style well
Headz0r,MachineLearning,1617960110.0,[D] Objective of openAIs Microscope,"Regarding the the Microscope application of openAI:

https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/44

** Question **

One can see two sets of images generated from the feature visualization. 
One is ""Channel optimization: objective results in a repeating pattern."" and the other is  ""Neuron optimization: objective shows spatial preferences."".

What optimization techniques are they refering to? How does generating these images differ for the respective classes?",regard microscope application openai url question one see two set image generate feature visualization one channel optimization objective result repeat pattern neuron optimization objective show spatial preferences optimization techniques refer generate image differ respective class
1846bdksy,MachineLearning,1617602310.0,[D] Top 4 CS PhD AI+Healthcare Research Topic Worries,"I am very grateful to have been accepted into a top 4 CS PhD program (MIT/Stanford/CMU/Berkeley) this year. My research interests were listed as AI+healthcare in my PhD application and the faculty at the school all assume I will pursue that area. However, I’ve recently started to have some hesitation about making AI+healthcare, more specifically computer vision + healthcare, my PhD research topic. The main reason is that I’m worried about post-PhD career outcomes with a topic like this. I’d really like to have the opportunity to work at Google Brain, FAIR, or potentially even quant trading companies in the future. On the other hand, I do think AI+healthcare is a promising field and may potentially also do a startup related to it. 

I’m curious, what do you all think of AI+healthcare as a PhD thesis topic? This is assuming I will still be making fundamental advances in AI, e.g. publishing in CVPR/ICCV/ECCV, but at a slightly lower frequency than a pure AI student due to additional papers in healthcare-related journals. Do you think this will hinder my career possibilities at all? Or am I overthinking this? Thank you very much. 

By the way, the alternative would be to try to pursue more “pure” AI research, without applications to healthcare.",grateful accept top nmbr cs phd program mit stanford cmu berkeley year research interest list ai healthcare phd application faculty school assume pursue area however ive recently start hesitation make ai healthcare specifically computer vision healthcare phd research topic main reason im worry post phd career outcomes topic like id really like opportunity work google brain fair potentially even quant trade company future hand think ai healthcare promise field may potentially also startup relate im curious think ai healthcare phd thesis topic assume still make fundamental advance ai e g publish cvpr iccv eccv slightly lower frequency pure ai student due additional paper healthcare relate journals think hinder career possibilities overthinking thank much way alternative would try pursue pure ai research without applications healthcare
ProbablyCloseEnough,MachineLearning,1616960156.0,[R] Slurm Interface Prototype Evaluation Survey (2 minutes),"If you have used Slurm to schedule computing jobs on shared computing resources, please evaluate the proposed interface for doing so by completing the survey in the following link.

[http://peersurvey.cc.gatech.edu/s/6b86204e847c40be9a980d612afbfa69](http://peersurvey.cc.gatech.edu/s/6b86204e847c40be9a980d612afbfa69) 

This continues my investigation from my [previous post](https://www.reddit.com/r/MachineLearning/comments/lfn7d9/r_slurm_interface_survey_2_minutes/?utm_source=share&utm_medium=web2x&context=3). I am doing this as coursework in a human computer interaction course. Thank you for participating!",use slurm schedule compute job share compute resources please evaluate propose interface complete survey follow link url continue investigation previous post url coursework human computer interaction course thank participate
mrwafflezzz,MachineLearning,1619708471.0,[R] Question regarding sampling negative examples for supervised learning,"I'm asking this question in this sub because it's a rather difficult question. Bear with me here:

Let's say I have a match predictor. This predictor uses 2 sets of features: one set for the content (c) and one set for the user (u). It matches a user to content.

Both u and c are represented by vectors that are spatially meaningful in terms of distance between users u and distance between content c.

Let's say that I only have positive matches between u and c (label 1). Any combination that isn't a positive match is ambiguous in meaning.

Would it make sense to approach this in a semi-supervised manner by assuming any combination of u and c that I sample isn't inherently a negative example (label 0), but that it could in fact be 0 or 1?

Could I then give each sampled combination of u and c a label by looking at how similar the vector u is to the vector of other users that have consumed content c?

The idea is then that if a user is similar to other users that have consumed content c, then the likelihood of him consuming content c becomes higher.

Any feedback is welcomed",ask question sub rather difficult question bear let say match predictor predictor use nmbr set feature one set content c one set user u match user content u c represent vectors spatially meaningful term distance users u distance content c let say positive match u c label 1 combination positive match ambiguous mean would make sense approach semi supervise manner assume combination u c sample inherently negative example label 0 could fact nmbr 1 could give sample combination u c label look similar vector u vector users consume content c idea user similar users consume content c likelihood consume content c become higher feedback welcome
fiddlerlabs,MachineLearning,1616372511.0,[R] A Practical Guide To Adversarial Robustness,"While adversarial machine learning is still a very young field (less than 10 years old), there’s been an explosion of papers and work around attacking such models and finding their vulnerabilities, turning into a veritable arms race between defenders and attackers.

Here is a brief summary of the field: [https://blog.fiddler.ai/2021/02/a-practical-guide-to-adversarial-robustness/](https://blog.fiddler.ai/2021/02/a-practical-guide-to-adversarial-robustness/)",adversarial machine learn still young field less nmbr years old theres explosion paper work around attack model find vulnerabilities turn veritable arm race defenders attackers brief summary field url
windy-city-wizard,MachineLearning,1618899153.0,[R] Two questions: computing class weights and large confusion matrix?,"1. What's an algorithm to compute class weights (to address imbalanced dataset) for multi-class classification (480 classes)?
2. What are some ways to make a large confusion matrix (480x480) easier to see? Initially, there are some obvious classes getting a lot (most) of the false positives, so that's not too bad. My guess is once I address those classes, it'll be a lot harder to extract meaningful insight from a very large confusion matrix.

https://preview.redd.it/9gaobryqs9u61.png?width=6561&format=png&auto=webp&s=e22782fad7927d9ae6f5012b1f503cc77d378e12",1 algorithm compute class weight address imbalanced dataset multi class classification 480 class 2 ways make large confusion matrix 480x480 easier see initially obvious class get lot false positives bad guess address class lot harder extract meaningful insight large confusion matrix url
minimaxir,MachineLearning,1619798940.0,[P] Easily Transform Portraits of People into AI Aberrations Using StyleCLIP,"So I've been doing a lot of experiments using [StyleCLIP](https://github.com/orpatashnik/StyleCLIP) to create some fun images. I've written a blog post with reproducible inputs for some of my _fun_ experiments involving Mark Zuckerberg + released some streamlined Colab Notebooks to get up and running.

https://minimaxir.com/2021/04/styleclip/

[Colab Notebook](https://colab.research.google.com/drive/13EJ1ATvTnE0N7I0ULLvRsta7J7HdNuBi?usp=sharing)

tl;dr StyleCLIP is essentially Photoshop driven by text, with all the good, bad, and chaos that entails.",lot experiment use styleclip url create fun image write blog post reproducible input _fun_ experiment involve mark zuckerberg release streamline colab notebooks get run url notebook url styleclip essentially photoshop drive text good bad chaos entail
zecharias99,MachineLearning,1617968121.0,[P] Chai: Open source framework for deploying chat AIs,"* Chai is an open-source platform which allows you to develop and deploy chat AIs. Check out our [website](https://chai.ml) and our [docs](https://chai.ml/docs), which include support for huggingface bots.
* Using [chai\_py](https://pypi.org/project/chaipy/) you can speak with your AI in just a few lines of code:

https://preview.redd.it/ucabdg9qv4s61.png?width=1364&format=png&auto=webp&s=b38821e3de68baa61e72498f05fa08e64f15605d

https://preview.redd.it/gkx2tes3w4s61.png?width=1125&format=png&auto=webp&s=582caaa0d515b7c902392d23066b41021711ac81

* Check it out! Try deploying [Facebook's Blenderbot](https://huggingface.co/facebook/blenderbot_small-90M) to chai and chatting with it in the mobile app.",chai open source platform allow develop deploy chat ais check website url docs url include support huggingface bots use chai _py url speak ai line code url check try deploy facebook blenderbot url chai chat mobile app
RandomForests92,MachineLearning,1616527472.0,[P] I just published first version of my metrics library for ML projects,"I hope you are doing well. I just released my new open-source library - onemetric. It is useful for evaluating computer vision projects for now, but I hope it will become the default benchmarking library for all ML-related projects. I would be very grateful if you could take a look. Potentially suggest some metrics from outside Computer Vision that might be useful.

[https://github.com/SkalskiP/onemetric](https://github.com/SkalskiP/onemetric)

One Metrics Library to Rule Them All!",hope well release new open source library onemetric useful evaluate computer vision project hope become default benchmarking library ml relate project would grateful could take look potentially suggest metrics outside computer vision might useful url metrics library rule
bionet271,MachineLearning,1618480565.0,"[P] I implemented DeepMind's ""Perceiver"" in PyTorch","So deepmind published [this paper](https://arxiv.org/pdf/2103.03206.pdf) but I couldn't find any source code. My implementation isn't *completely* faithful (the positional encoding for example) and the paper also didn't have every detail, but it's pretty close. I just thought I'd share in case anyone wants to use it and/or help make it better. Let me know if I missed anything too, I'm still very much learning.

[https://github.com/louislva/deepmind-perceiver](https://github.com/louislva/deepmind-perceiver)",deepmind publish paper url find source code implementation completely faithful positional encode example paper also every detail pretty close think share case anyone want use help make better let know miss anything still much learn url
mroc_lak,MachineLearning,1616578491.0,[D] What tools do you use for testing your computer vision systems?,"Hi all, 

I am interested in the best practices for developing computer vision applications for production. Are there tools you would recommend for testing the system? (Requirements based tests, unit tests, integrations tests, etc). Or do you have to write all the infra in-house?",hi interest best practice develop computer vision applications production tool would recommend test system requirements base test unit test integrations test etc write infra house
faridrashidi,MachineLearning,1619327497.0,[P] Collection of Kaggle Past Solutions (to learn ideas and techniques),"I have collected here \[1,2\] almost all available solutions and ideas with codes shared by top performers in the past Kaggle competitions. This list will gets updated as soon as a new competition finishes. It allows you to search over the Kaggle past competitions solutions and ideas.

\[1\] [https://github.com/faridrashidi/kaggle-solutions](https://github.com/faridrashidi/kaggle-solutions)

\[2\] [https://farid.one/kaggle-solutions/](https://farid.one/kaggle-solutions/)",collect 1 2 almost available solutions ideas cod share top performers past kaggle competitions list get update soon new competition finish allow search kaggle past competitions solutions ideas 1 url url
CKL-IT,MachineLearning,1620223941.0,"[N] 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0","
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)",200 state art medical model ner entity resolution relation extraction assertion spark nmbr python nmbr support nlu nmbr release much morewe incredibly excite announce release nlu 3 0 0 make john snow labs medical healthcare model available nmbr line code nlu model accurate domains highly scalable spark cluster addition spark 3 0 x spark 3 1 x support together python3 8this enable amaze spark nlp3 0 1 url spark nlp healthcare 3 0 1 url release new feature nmbr new model healthcare domain nmbr new class model assertion sentence chunk resolvers relation extractors medical ner model de identificator model spark 3 0 x 3 1 x support python nmbr support new output level relation nmbr line install nlu run wget url bash various new emr databricks versions support url gpu mode 600 speedup enable gpu mode authorize mode license feature new documentation nlu healthcare examples url instrunctions authorize environment use license feature url new notebooks medical name entity extraction ner notebook url relation extraction notebook url entity resolution overview notebook url assertion overview notebook url de identification overview notebook url graph nlu tutorial url assertiondlmodels language nlu load reference spark nlp model reference english assert url assertion_dl url english assert biobert url assertion_dl_biobert url english assert healthcare url assertion_dl_healthcare url english assert large url assertion_dl_large url new word embeddings language nlu load reference spark nlp model reference english embed glove clinical url embeddings_clinical url english embed glove biovec url embeddings_biovec url english embed glove healthcare url embeddings_healthcare url english embed glove healthcare_100d url embeddings_healthcare_100d url english en embed glove icdoem embeddings_icdoem english en embed glove icdoem_2ng embeddings_icdoem_2ng sentence entity resolvers language nlu load reference spark nlp model reference english embed_sentence biobert mli sbiobert_base_cased_mli english resolve sbiobertresolve_cpt english resolve cpt sbiobertresolve_cpt english resolve cpt augment sbiobertresolve_cpt_augmented english resolve cpt procedures_augmented sbiobertresolve_cpt_procedures_augmented english resolve hcc augment sbiobertresolve_hcc_augmented english resolve icd10cm url sbiobertresolve_icd10cm url english resolve icd10cm augment url sbiobertresolve_icd10cm_augmented url english resolve icd10cm augmented_billable url sbiobertresolve_icd10cm_augmented_billable_hcc url english resolve icd10pcs url sbiobertresolve_icd10pcs url english resolve icdo url sbiobertresolve_icdo url english resolve rxcui url sbiobertresolve_rxcui url english resolve rxnorm url sbiobertresolve_rxnorm url english resolve snomed url sbiobertresolve_snomed_auxconcepts url english resolve snomed aux_concepts url sbiobertresolve_snomed_auxconcepts url english resolve snomed aux_concepts_int url sbiobertresolve_snomed_auxconcepts_int url english resolve snomed find url sbiobertresolve_snomed_findings url english resolve snomed findings_int url sbiobertresolve_snomed_findings_int url relationextractionmodel language nlu load reference spark nlp model reference english relation posology posology_re english relation url redl_bodypart_direction_biobert url english relation bodypart direction url redl_bodypart_direction_biobert url english relation bodypart problem url redl_bodypart_problem_biobert url english relation bodypart procedure url redl_bodypart_procedure_test_biobert url english relation chemprot url redl_chemprot_biobert url english relation clinical url redl_clinical_biobert url english relation date url redl_date_clinical_biobert url english relation drug_drug_interaction url redl_drug_drug_interaction_biobert url english relation humen_phenotype_gene url redl_human_phenotype_gene_biobert url english relation temporal_events url redl_temporal_events_biobert url nerdlmodels language nlu load reference spark nlp model reference english med_ner ade clinical url ner_ade_clinical url english med_ner ade clinical_bert url ner_ade_clinicalbert url english med_ner ade ade_healthcare url ner_ade_healthcare url english med_ner anatomy url ner_anatomy url english med_ner anatomy biobert url ner_anatomy_biobert url english med_ner anatomy coarse url ner_anatomy_coarse url english med_ner anatomy coarse_biobert url ner_anatomy_coarse_biobert url english med_ner aspect_sentiment url ner_aspect_based_sentiment url english med_ner bacterial_species url ner_bacterial_species url english med_ner bionlp url ner_bionlp url english med_ner bionlp biobert url ner_bionlp_biobert url english med_ner cancer url ner_cancer_genetics url englishs med_ner cellular url ner_cellular url english med_ner cellular biobert url ner_cellular_biobert url english med_ner chemicals url ner_chemicals url english med_ner chemprot url ner_chemprot_biobert url english med_ner chemprot clinical url ner_chemprot_clinical url english med_ner clinical url ner_clinical url english med_ner clinical biobert url ner_clinical_biobert url english med_ner clinical noncontrib ner_clinical_noncontrib english med_ner diseases url ner_diseases url english med_ner diseases biobert url ner_diseases_biobert url english med_ner diseases large url ner_diseases_large url english med_ner drug url ner_drugs url english med_ner drugsgreedy url ner_drugs_greedy url english med_ner drug large url ner_drugs_large url english med_ner events_biobert url ner_events_biobert url english med_ner events_clinical url ner_events_clinical url english med_ner events_healthcre url ner_events_healthcare url english med_ner financial_contract url ner_financial_contract url english med_ner healthcare url ner_healthcare url english med_ner human_phenotype gene_biobert url ner_human_phenotype_gene_biobert url english med_ner human_phenotype gene_clinical url ner_human_phenotype_gene_clinical url english med_ner human_phenotype go_biobert url ner_human_phenotype_go_biobert url english med_ner human_phenotype go_clinical url ner_human_phenotype_go_clinical url english med_ner jsl url ner_jsl url english med_ner jsl biobert url ner_jsl_biobert url english med_ner jsl enrich url ner_jsl_enriched url english med_ner jsl enriched_biobert url ner_jsl_enriched_biobert url english med_ner measurements url ner_measurements_clinical url english med_ner medmentions url ner_medmentions_coarse url english med_ner posology url ner_posology url english med_ner posology biobert url ner_posology_biobert url english med_ner posology greedy url ner_posology_greedy url english med_ner posology healthcare url ner_posology_healthcare url english med_ner posology large url ner_posology_large url english med_ner posology large_biobert url ner_posology_large_biobert url english med_ner posology small url ner_posology_small url english med_ner radiology url ner_radiology url english med_ner radiology wip_clinical url ner_radiology_wip_clinical url english med_ner risk_factors url ner_risk_factors url english med_ner risk_factors biobert url ner_risk_factors_biobert url english med_ner i2b2 nerdl_i2b2 english med_ner tumour url nerdl_tumour_demo url english med_ner jsl wip clinical jsl_ner_wip_clinical english med_ner jsl wip clinical greedy url jsl_ner_wip_greedy_clinical url english med_ner jsl wip clinical modifier url jsl_ner_wip_modifier_clinical url english med_ner jsl wip clinical rd url jsl_rd_ner_wip_greedy_clinical url de identification model language nlu load reference spark nlp model reference english med_ner deid augment url ner_deid_augmented url english med_ner deid biobert url ner_deid_biobert url english med_ner deid enrich url ner_deid_enriched url english med_ner deid enriched_biobert url ner_deid_enriched_biobert url english med_ner deid large url ner_deid_large url english med_ner deid sd url ner_deid_sd url english med_ner deid sd_large url ner_deid_sd_large url english med_ner deid nerdl_deid english med_ner deid synthetic ner_deid_synthetic english med_ner deid dl url ner_deidentify_dl url english en de_identify url deidentify_rb url english de_identify rule deid_rules english de_identify clinical url deidentify_enriched_clinical url english de_identify large url deidentify_large url english de_identify rb url deidentify_rb url english de_identify rb_no_regex deidentify_rb_no_regex chunk resolvers language nlu load reference spark nlp model reference english resolve_chunk athena_conditions url chunkresolve_athena_conditions_healthcare url english resolve_chunk cpt_clinical url chunkresolve_cpt_clinical url english resolve_chunk icd10cm clinical url chunkresolve_icd10cm_clinical url english resolve_chunk icd10cm diseases_clinical url chunkresolve_icd10cm_diseases_clinical url english resolve_chunk icd10cm hcc_clinical chunkresolve_icd10cm_hcc_clinical english resolve_chunk icd10cm hcc_healthcare chunkresolve_icd10cm_hcc_healthcare english resolve_chunk icd10cm injuries url chunkresolve_icd10cm_injuries_clinical url english resolve_chunk icd10cm musculoskeletal url chunkresolve_icd10cm_musculoskeletal_clinical url english resolve_chunk icd10cm neoplasms url chunkresolve_icd10cm_neoplasms_clinical url english resolve_chunk icd10cm poison url chunkresolve_icd10cm_poison_ext_clinical url english resolve_chunk icd10cm puerile url chunkresolve_icd10cm_puerile_clinical url english resolve_chunk icd10pcs clinical chunkresolve_icd10pcs_clinical english resolve_chunk icdo clinical url chunkresolve_icdo_clinical url english resolve_chunk loinc url chunkresolve_loinc_clinical url english resolve_chunk rxnorm cd url chunkresolve_rxnorm_cd_clinical url english resolve_chunk rxnorm chunkresolve_rxnorm_in_clinical english resolve_chunk rxnorm in_healthcare chunkresolve_rxnorm_in_healthcare english resolve_chunk rxnorm sbd url chunkresolve_rxnorm_sbd_clinical url english resolve_chunk rxnorm scd url chunkresolve_rxnorm_scd_clinical url english resolve_chunk rxnorm scdc chunkresolve_rxnorm_scdc_clinical english resolve_chunk rxnorm scdc_healthcare chunkresolve_rxnorm_scdc_healthcare english resolve_chunk rxnorm xsmall clinical url chunkresolve_rxnorm_xsmall_clinical url english resolve_chunk snomed find url chunkresolve_snomed_findings_clinical url new classifiers language nlu load reference spark nlp model reference english classify icd10 clinical classifier_icd10cm_hcc_clinical english classify icd10 healthcare classifier_icd10cm_hcc_healthcare english classify ade biobert url classifierdl_ade_biobert url english classify ade clinical url classifierdl_ade_clinicalbert url english classify ade conversational url classifierdl_ade_conversational_biobert url english classify gender biobert url classifierdl_gender_biobert url english classify gender sbert url classifierdl_gender_sbert url english classify pico classifierdl_pico_biobert german medical model nlu load reference spark nlp model reference embed w2v_cc_300d embed w2v w2v_cc_300d resolve_chunk chunkresolve_icd10gm resolve_chunk icd10gm chunkresolve_icd10gm resolve_chunk icd10gm 2021 chunkresolve_icd10gm_2021 med_ner legal ner_legal med_ner ner_healthcare med_ner healthcare ner_healthcare med_ner healthcare_slim ner_healthcare_slim med_ner traffic ner_traffic spanish medical model nlu load reference spark nlp model reference embed scielo 150d url embeddings_scielo_150d url embed scielo 300d url embeddings_scielo_300d url embed scielo 50d url embeddings_scielo_50d url embed scielowiki 150d url embeddings_scielowiki_150d url embed scielowiki 300d url embeddings_scielowiki_300d url embed scielowiki 50d url embeddings_scielowiki_50d url embed sciwiki 150d url embeddings_sciwiki_150d url embed sciwiki 300d url embeddings_sciwiki_300d url embed sciwiki 50d url embeddings_sciwiki_50d url med_ner url ner_diag_proc url med_ner neoplasm url ner_neoplasms url med_ner diag_proc url ner_diag_proc url gpu modeyou enable nlu gpu mode set gpu true load model e nlu load train sentiment gpu true must resart kernel already load nlu pipeline withouth gpu mode output level relationthis new output level use relation extractors give nmbr row per relation extract bug fix fix bug cause load nlu model offline mode work occasion nmbr line install nlu wget url bash install via pip pip install nlu pyspark 3 0 1 additional nlu ressources nlu website url nlu tutorial notebooks url nlu videos blogposts nlu url nlu github url suggestions question contact us slack url
cathie_burry,MachineLearning,1617639889.0,Why are correct AI medical diagnoses seemingly so hard to achieve? [D],"There are a lot of people working on this, a lot of projects invested in this, and a lot of really smart people trying to solve this problem.

The studies I have read have over fitting problems, and people working on them run into a lot of regulatory issues. 

But at the end of the day, it seems like it would be less complicated to deal with some aspects of medicine than some of the other things people have built AI for like AlphaZero.

The roadblocks I hear about don’t add up to the impasse I see, why can’t I go to an AI clinic?",lot people work lot project invest lot really smart people try solve problem study read fit problems people work run lot regulatory issue end day seem like would less complicate deal aspects medicine things people build ai like alphazero roadblocks hear add impasse see cant go ai clinic
windy-city-wizard,MachineLearning,1619974865.0,[D] [R] Evaluation set for large number of imbalanced classes?,"I'm working on a classification problem with upwards of 500 classes. I have hundreds of observations for most classes. The top 10% more common classes have tens of thousands of observations, and the bottom 10% have dozens of observations.

My evaluation set currently has 10% of the observations for each class, capped at (arbitrarily) 250. Seems like a sensible setup to me. My metric of interest is top 3 accuracy.

Is there a better way to evaluate my model?",work classification problem upwards nmbr class hundreds observations class top 10 common class tens thousands observations bottom 10 dozens observations evaluation set currently 10 observations class cap arbitrarily 250 seem like sensible setup metric interest top nmbr accuracy better way evaluate model
shreyansh26,MachineLearning,1619962439.0,[P] GPT-1 - Annotated Paper + Paper Summary,"GPT-2 and recently, GPT-3 created a lot of hype when they were launched. However, it all started with the ""Improving Language Understanding by Generative Pre-Training"" paper which introduced the idea of GPT-1. 

As a part of my Paper Notes series, I have gone through the paper and created a brief yet informative summary of the paper. It will take just take a few minutes to understand GPT-1 well. Check out the links below and happy reading!

Paper Summary -   [Improving Language Understanding by Generative Pre-Training](https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf)",gpt 2 recently gpt 3 create lot hype launch however start improve language understand generative pre train paper introduce idea gpt 1 part paper note series go paper create brief yet informative summary paper take take minutes understand gpt 1 well check link happy read paper summary improve language understand generative pre train url paper url
hwbs20,MachineLearning,1619257344.0,An RTX 3070 for protoyping [D],"Is an RTX 3070 (with i9 processor and 16 GB RAM) enough for prototyping deep learning models? My area of work is generative Modeling and the use cases are :

1. to just check if the model/code actually runs and maybe run it on a tiny dataset to make sure things are correct. Once this is done, I will put it up on a cluster to scale up my experiments.
2. Maybe reproduce some models that don’t demand that much compute. (Even in this case i can just check if the code “works” and then push it to the cluster)
3. To train basic GANs, VAEs etc to understand them (main thing is to get results very fast and not wait like an hour)

Or should I wait and go for RTX 3080 with similar other specs? The main problem is that the 3080 is a bit out of my budget and the availability is in august (the ones available right now are way too out of my budget), whereas I want something in this week. Thanks in advance",rtx nmbr i9 processor nmbr gb ram enough prototyping deep learn model area work generative model use case 1 check model code actually run maybe run tiny dataset make sure things correct put cluster scale experiment 2 maybe reproduce model demand much compute even case check code work push cluster 3 train basic gans vaes etc understand main thing get result fast wait like hour wait go rtx nmbr similar specs main problem nmbr bite budget availability august ones available right way budget whereas want something week thank advance
thunder_jaxx,MachineLearning,1619812725.0,[D] Unpopular Opinion: Conferences Should Mandate a Limitations Section For Any Paper Introducing some New Model / Method / Variant,"The title says it all and I mean a specific limitations section where researchers convey solid limitations of the new methods they propose. I feel such information is very crucial for disseminating good science in the age of AI and ML.  Limitations can be problem-dependent and help ground the claims of a research paper. The review process can help with making this section better. 

Why? few reasons: 

1. If not explicitly stated, we have to go look it up inside a paper and it may not be found. Many times these are hidden as footnotes or concluding remarks which require the cognitive load to read and process. Stating them reduces the cognitive load to think about such stuff because it's covered as a part of good science. 
2. Makes us stick to doing good science over papers becoming a source of benchmark porn. 
3. Keeps clickbaity titles in check. AI/ML Researchers have brought into the advertizing model of clickbaity titles. I am not against clickbaity titles as many times the title has actually been[ really good and spot on](https://arxiv.org/abs/2104.06644)! But at least for the fairness of science, we should have limitations to ensure that there is a boundary between science and salesmanship. 

I am very aware it puts a lot of load on researchers and seldom people like to do it. But it's healthy cozy it makes us care more about what we learn from the research over what we are selling as research. What are your thoughts?",title say mean specific limitations section researchers convey solid limitations new methods propose feel information crucial disseminate good science age ai ml limitations problem dependent help grind claim research paper review process help make section better reason 1 explicitly state go look inside paper may find many time hide footnote conclude remark require cognitive load read process state reduce cognitive load think stuff cover part good science 2 make us stick good science paper become source benchmark porn 3 keep clickbaity title check ai ml researchers bring advertize model clickbaity title clickbaity title many time title actually really good spot url least fairness science limitations ensure boundary science salesmanship aware put lot load researchers seldom people like healthy cozy make us care learn research sell research thoughts
Signal-Ad-8598,MachineLearning,1620398804.0,[D] How is tfjs-node performance in comparison with Python version?,"Hi, I've come across some posts showing that tfjs-node is faster than python tensorflow as it's multi-threaded. However the posts were from 3 years ago. Is it still true? Why is tfjs-node not so popular? Should I train models on Node.js, as I'm more familiar with JS syntax than Python?",hi come across post show tfjs node faster python tensorflow multi thread however post nmbr years ago still true tfjs node popular train model node js familiar js syntax python
__Julia,MachineLearning,1616942750.0,[D] Is it valuable to have a patent in our industry?,"Hello,

I work as a research engineer in R&D, I have heard polarized opinions about ""having/working on patents"" in my small circle, and I would like to hear opinions of other people in the industry. Is it valuable to have ""patents"" in someone's resume?. How is it comparable to having a paper in ML conference?.

The general public's perception of patents is different than reality. In many cases, it is harder to write a paper and make it to top-tier conferences than to write a patent. However, it can show that the author is able to  think out of the box, enhance existing products, and design systems/solutions.

Another argument I have heard is about ""gate-keeping science"", it can be seen as a blocker for others as well. 

&#x200B;

What is the opinion of this community?",hello work research engineer r hear polarize opinions work patent small circle would like hear opinions people industry valuable patent someone resume comparable paper ml conference general public perception patent different reality many case harder write paper make top tier conferences write patent however show author able think box enhance exist products design systems solutions another argument hear gate keep science see blocker others well x200b opinion community
Yuqing7,MachineLearning,1619541221.0,"[R] Microsoft & Peking U Researchers Identify 'Knowledge Neurons' in Pretrained Transformers, Enabling Fact Editing","A research team from Microsoft Research and Peking University peeps into pretrained transformers and investigates how factual knowledge is stored, proposing a method to identify “knowledge neurons,” which can be utilized to explicitly update and erase facts.

Here is a quick read: [Microsoft & Peking U Researchers Identify 'Knowledge Neurons' in Pretrained Transformers, Enabling Fact Editing.](https://syncedreview.com/2021/04/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-6/)

The paper *Knowledge Neurons in Pretrained Transformers* is on [arXiv](https://arxiv.org/pdf/2104.08696.pdf).",research team microsoft research peking university peep pretrained transformers investigate factual knowledge store propose method identify knowledge neurons utilize explicitly update erase facts quick read microsoft peking u researchers identify knowledge neurons pretrained transformers enable fact edit url paper knowledge neurons pretrained transformers arxiv url
dojoteef,MachineLearning,1617280471.0,[N] John Carmack solves AGI!,"After a few short years of solitary research in AI, legendary programmer John Carmack has just recently unveiled an AGI named Rick, which he created by combining the flexibility of GANs with the expressive power of Predictability Minimization!

I knew he had a penchant for physically building machines, but the resulting quality of the facial expressions and smoothness of motion is uncanny! It helps that he recorded the video in 4K at 60fps. Check it out [here](https://www.youtube.com/watch?v=bxqLsrlakK8)!",short years solitary research ai legendary programmer john carmack recently unveil agi name rick create combine flexibility gans expressive power predictability minimization know penchant physically build machine result quality facial expressions smoothness motion uncanny help record video 4k 60fps check url
NeedMoreTime4Things,MachineLearning,1617309596.0,[D] Keeping up with research - Poll,"Hi there, 

After talking to some people I realized that most of us have different ways of keeping up with current developments in ML. 

As I am trying to dive into new fields before my PhD, I’d like to know how you manage to stay on top of research.

Thanks in advance!

[View Poll](https://www.reddit.com/poll/mi4hrm)",hi talk people realize us different ways keep current developments ml try dive new field phd id like know manage stay top research thank advance view poll url
KirillTheMunchKing,MachineLearning,1620229627.0,[D] StyleGAN2 Distillation for Feed-forward Image Manipulation. How to gender swap Harry-Potter and edit other images explained!,"# [StyleGAN2 Distillation for Feed-forward Image Manipulation](https://t.me/casual_gan/34)

In this paper from October, 2020 the authors propose a pipeline to discover semantic editing directions in StyleGAN in an unsupervised way, gather a paired synthetic dataset using these directions, and use it to train a light Image2Image model that can perform one specific edit (add a smile, change hair color, etc) on any new image with a single forward pass. If you are not familiar with this paper, check out the [5 minute summary](https://t.me/casual_gan/34).

&#x200B;

[Samples from the model](https://preview.redd.it/3cohkadiobx61.png?width=1280&format=png&auto=webp&s=969a6f2c5e523a4b12d09cc59c5abcc5abaaa6ad)

\[[Arxiv](https://arxiv.org/abs/2003.03581)\]\[[paper explanained in 5 minutes](https://t.me/casual_gan/34)\]",stylegan2 distillation fee forward image manipulation url paper october nmbr author propose pipeline discover semantic edit directions stylegan unsupervised way gather pair synthetic dataset use directions use train light image2image model perform one specific edit add smile change hair color etc new image single forward pass familiar paper check 5 minute summary url model url explanained nmbr minutes url
KirillTheMunchKing,MachineLearning,1619016127.0,[R] Training Generative Adversarial Networks with Limited Data,"# [Training Generative Adversarial Networks with Limited Data](https://t.me/casual_gan/28)

The authors propose а novel method to train a StyleGAN on a small dataset (few thousand images) without overfitting. They achieve high visual quality of generated images by introducing a set of adaptive discriminator augmentations that stabilize training with limited data. More details [here](https://t.me/casual_gan/28).

&#x200B;

[StyleGAN-ada](https://preview.redd.it/02tu3te5gju61.png?width=1280&format=png&auto=webp&s=883df8de3e976f6576bb38d35cc96597f9b0377a)

In case you are not familiar with the paper, read it [here](https://t.me/casual_gan/28).",train generative adversarial network limit data url author propose а novel method train stylegan small dataset thousand image without overfitting achieve high visual quality generate image introduce set adaptive discriminator augmentations stabilize train limit data detail url case familiar paper read url
ancientmooner,MachineLearning,1618412945.0,[P] Code and pretrained models for Swin Transformer are released (SOTA models on COCO and ADE20K),"Image classification and pretrained models: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)

Object detection on COCO: [https://github.com/SwinTransformer/Swin-Transformer-Object-Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection) 

Semantic segmentation on ADE20K: [https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation)",image classification pretrained model url detection coco url semantic segmentation ade20k url
Exotic-Photograph-37,MachineLearning,1617994173.0,[P] Low Computation GAN? (Noobie to GAN),"Hello all!

I am a researcher doing a study on GAN content. We wanted to find different ways to manipulate someone's face and map them to doing actions (i.e. dance or sing). It seems like most of the DeepFake libraries we used require a lot of computation time. Are there any libraries that have a low computation time? We were thinking something like [Wombo.ai](https://Wombo.ai), which doesn't take too long (though I don't know if its because they have super powerful servers they connect to). We can't use [Wombo.ai](https://Wombo.ai) directly because it would be a privacy issue as that would require communication with a third party. Does anyone have any tips on libraries that we could use?

Thank you!",hello researcher study gin content want find different ways manipulate someone face map action e dance sing seem like deepfake libraries use require lot computation time libraries low computation time think something like wombo ai url take long though know super powerful servers connect use wombo ai url directly would privacy issue would require communication third party anyone tip libraries could use thank
Realistic_Sea_3634,MachineLearning,1620485299.0,"[D] Why has machine learning become such a toxic field, know-it-all field?","I've worked with many scientists from many different fields and backgrounds, but none come close to the obnoxiousness, pomposity, and outright unpalatable know-it-all vibe from the machine learning community. And I'm sure it's a case of a small rotten bunch smearing the whole field.

This behaviour is most rife in 3 places: the cesspit known as Twitter, Reddit, and somewhat in industry. It's much less rampant, comparatively, in academia in my experience (and just so we're clear, Google Brain/DeepMind/FAIR is not academia). Here is some of what I've observed:

* Think they can dominate a field or little involvement from SMEs
   * Often I see in machine learning a group or groups will swarm on a problem, throw ML at the data, and call the problem ""solved"". Very little (if any) SME involved, and importantly, no follow up. The exceptions to this general are few and far between.
* DL encourages a habit of not learning the basics
   * I have encountered this so often, and it is especially apparent in DL. People will jump straight into e.g. CV or NLP, and not bother learning anything foundational. I've seen and spoken to  numerous people publishing in CV papers in prestigious conferences who don't even know what why colour spaces are useful or even what a pixel is (because it sure as fuck isn't a small square on an image). You may claim that they don't need to know this, but that delusional talk, and they absolutely do. There is a limit to what compute + CNN/transformer can do. After that, you need foundations to know how to improve.
* No real work goes into the vast majority of papers (more of a DL problem)
   * This has been cited by many in the past. However, I must articulate it myself. I also understand that there are many contributing factors to this (and indeed all the other) problem. It's most often slight architecture change or incremental improvement, with no real thought gone into the paper. What this sometimes results in (I have seen numerous times myself in a variety of settings - including my own teams) is a PhD being no good at engineering, or less productive at research than an MSc with experience under his belt. The whole point of a PhD coming into an ML team is to be useful at R&D, and that is not always the case (as much as you'd expect).
* Insolence and arrogance of fairness/ethics crowd
   * This crowd - as it currently operates - simply serves as a cancerous tumour to the ML world. There always point out problems, but never (real) solutions. They act like gatekeepers and god's gift to the world. It brings about massive toxicity to the virtual ML community, and prohibits free speech of the community without fear of repercussions. The this crowd could do with an overhaul, as its leaders are some of the most vitriolic yobos who claim to be academics.
* For such an applied field, very little focus on applications
   * Often the excuse put forward is that fields like math have very few applications straight away. Firstly, ML is not like math - it's more like straight up engineering (especially DL). It's primarily applied, and should thus be much more focused on application. Your slight architecture change or you 0.5% improvement on ImageNet is not a Pythagoreon theorem waiting to happen. It's laziness and just wanting to get your PhD (surely if doing a PhD you actually want to make a real contribution to the field that you can stand behind?). Fields like physics and stats are often applied and make a real world impact with the applications. To be fair, ML also does, but nowhere near as much as it should, especially at your average company in industry.
* Can fix all the worlds problems
   * Over-stating the ability of ML. Not sure if delusion, PR, or a combination of both. Cited lots before by others.

&#x200B;

EDIT 1:I don't know why people are focusing on the definition of a pixel. I reiterate that is the ""small square"" model is helpful, useful, and enough for your work/application, then that's great and you should use that. But don't claim your helpful model to be the definition - because it's not. I don't understand why I'm being downvoted for this :/

Thanks for the discussion. I want to understand the Reddit ML community's views on the original content of this post (minus the pixels).

&#x200B;

EDIT2:  
Realised it would be better to put this here, than in the comment as I did:  


Regarding the ethics crowd - I see many people have commented and require me to clarify my position.

I  feel they intentionally try to make mountains out of molehills, and  often (at least from an online perspective) do not want to engage in  civil discourse or debate when presented to them (instead wanting to  just blame and live in their echo chamber). They act like saviours and  pretentious, as if they are doing god's work. The purpose of  ethics/fairness is to try change things to be more ethics/fair in your  community. It's not to perpetually live in your little echo chamber and  try ""cancel"" those who disagree. Utter toxicity.

Never  mind the fact that many of the popular ""researchers"" in that space  preach against toxic large tech companies/CEOs, unfair practices at top  universities and companies, etc., but they are the first to be working  at places like Google, DeepMind, Microsoft, and attending universities  like Stanford and CMU. If you really want to make a difference to the  plight of POC in tech, then why are you working for DeepMind? Go work  for, or consult for, some little random African start up then to empower  them? Fucking hypocritical, will do whatever is convenient and easy.  Simply a popularity contest.

Again,  I found myself venting as I am quite fervent about this. However, with  them it's so often such an apparent display of self-entitlement and  superiority. Granted this is simply from my observations mostly online,  but also somewhat in-person.  
",work many scientists many different field background none come close obnoxiousness pomposity outright unpalatable know vibe machine learn community sure case small rotten bunch smear whole field behaviour rife nmbr place cesspit know twitter reddit somewhat industry much less rampant comparatively academia experience clear google brain deepmind fair academia observe think dominate field little involvement smes often see machine learn group group swarm problem throw ml data call problem solve little sme involve importantly follow exceptions general far dl encourage habit learn basics encounter often especially apparent dl people jump straight e g cv nlp bother learn anything foundational see speak numerous people publish cv paper prestigious conferences even know colour space useful even pixel sure fuck small square image may claim need know delusional talk absolutely limit compute cnn transformer need foundations know improve real work go vast majority paper dl problem cite many past however must articulate also understand many contribute factor indeed problem often slight architecture change incremental improvement real think go paper sometimes result see numerous time variety settings include team phd good engineer less productive research msc experience belt whole point phd come ml team useful r always case much expect insolence arrogance fairness ethics crowd crowd currently operate simply serve cancerous tumour ml world always point problems never real solutions act like gatekeepers god gift world bring massive toxicity virtual ml community prohibit free speech community without fear repercussions crowd could overhaul leaders vitriolic yobos claim academics apply field little focus applications often excuse put forward field like math applications straight away firstly ml like math like straight engineer especially dl primarily apply thus much focus application slight architecture change 0 5 improvement imagenet pythagoreon theorem wait happen laziness want get phd surely phd actually want make real contribution field stand behind field like physics stats often apply make real world impact applications fair ml also nowhere near much especially average company industry fix worlds problems state ability ml sure delusion pr combination cite lot others x200b edit 1 know people focus definition pixel reiterate small square model helpful useful enough work application great use claim helpful model definition understand downvoted thank discussion want understand reddit ml community view original content post minus pixels x200b edit2 realise would better put comment regard ethics crowd see many people comment require clarify position feel intentionally try make mountains molehills often least online perspective want engage civil discourse debate present instead want blame live echo chamber act like saviours pretentious god work purpose ethics fairness try change things ethics fair community perpetually live little echo chamber try cancel disagree utter toxicity never mind fact many popular researchers space preach toxic large tech company ceos unfair practice top universities company etc first work place like google deepmind microsoft attend universities like stanford cmu really want make difference plight poc tech work deepmind go work consult little random african start empower fuck hypocritical whatever convenient easy simply popularity contest find vent quite fervent however often apparent display self entitlement superiority grant simply observations mostly online also somewhat person
rsree123,MachineLearning,1617678542.0,How to overcome Impostor Syndrome [D],Even  after working in applied ML for close to 15 years I sometimes I get a feeling that I don’t know anything. I would not be able to recall things at the top of my mind but a quick look helps. I would not even remember the details of some fundamental concept. Is this the case with everyone or are you able to retain everything in your mind. People just look at my profile and have high expectations. Any tips to overcome this?,even work apply ml close nmbr years sometimes get feel know anything would able recall things top mind quick look help would even remember detail fundamental concept case everyone able retain everything mind people look profile high expectations tip overcome
aselsiriwardena,MachineLearning,1619173423.0,[P] Pytorch Load Balance and Scalability,"I need some ideas to execute a test on a PyTorch image generative model for load balancing and scalability?  
Are there any specific tools for that? Do I need to execute a parallel process?",need ideas execute test pytorch image generative model load balance scalability specific tool need execute parallel process
__data_science__,MachineLearning,1616513807.0,[D] Advanced Takeaways from fast.ai book,"I recently read the Fast AI deep learning [book](https://www.goodreads.com/book/show/50204643-deep-learning-for-coders-with-fastai-and-pytorch) and wanted to summarise some of the many advanced takeaways & tricks I got from it.  I’m going to leave out the basic things because there’s enough posts about them, i’m just focusing on what I found new or special in the book.

I’ve also put the insights into a [deck](https://saveall.ai/shared/deck/140&4&3K3uXPazkg4) on save all to help you remember them over the long-term. I would **massively recommend using a spaced repetition app (video** [**explanation**](https://youtu.be/AD0aFdRCskQ)**) like anki or** [**save all**](https://saveall.ai/) **for the things you learn** otherwise you’ll just forget so much of what is important. Here’s the takeaways:

# Neural Network Training Fundamentals

* Always **start** an ML project by **producing simple baselines**
   * If is binary classification then could even be as simple as predicting the most common class in the training dataset
   * Other baselines: linear regression, random forest, boosting etc…
* Then you can **use your baseline to clean your data** by looking at the datapoints it gets most incorrect and checking to see if they are actually classified correctly in the data
* In general you can also **leverage your baselines** to **help debug** your models
   * e.g. if you make your neural network 1 layer then it should be able to match the performance of a linear regression baseline, if it doesn’t then you have a bug!
   * e.g. if adding a feature improves the performance of linear regression then it should probably also improve the performance of your neural net unless you have a bug!
* Hyperparameter optimisation can help a bit (especially for the learning rate) but in general there are default hyperparameters that can do quite well and so **closely** **optimising the hyperparameters should be one of the last things you try** rather than the first
* **If you know something** about the problem then try to **inject it as an inductive bias into the training process**
   * e.g. if some of your features are related in a sequential way then incorporate them into training separately using an RNN
   * e.g. if you know the output should only be between -3 and 3 then use sigmoid to design the final layer so that it forces the output of the network to be in this range

# Transfer Learning

* Always use transfer learning if you can by finding a model pre-trained for a similar task and then fine-tune that model for your particular task
   * e.g. see [huggingface](http://huggingface.co/) for help with this in NLP
* **Gradual unfreezing** and **discriminative learning rates** work well when fine-tuning a transfer learned model
   * **Gradual unfreezing** = freeze earlier layers and **train the later layers only**, then **gradually unfreeze** the earlier layers one by one
   * **Discriminative learning rates** = having **different learning rates per layer of your network** (usually **earlier** **layers** have **smaller learning rates** than later layers)

# Tricks to Deal with Overfitting

* **Best way** to deal with **overfitting** is by getting **more data**. **Exhaust this first** before you start regularising with other methods
* **Data augmentation** is really powerful and now possible with text as well as images:
   * **Image** data augmentation -  crop, pad, squish and resize images
   * **Text** data augmentation - negate words, replace words with similes, perturb word embeddings (nice github [repo](https://github.com/QData/TextAttack) for this)
* **Mixup regularisation** = create new data by averaging together training datapoints
* **Backwards training (NLP only):** train an additional separate model that is **fed text backwards** and then **average the outputs** of your two models to get your final prediction

# Other Tricks to Improve Performance

* **Test time augmentation** = at test time, use the **average prediction** from many **augmented versions of the input** as your prediction rather than just the prediction from the true input
* **1 cycle training** = when you increase and reduce the learning rate throughout training in a circular fashion (usually makes a **huge difference)**
* **Learning rate finder algorithm** = algorithm that Fast AI provide to help you automatically discover roughly the best learning rate
* **Never use one-hot encodings,** use **embeddings** instead, even in **tabular data**!
* Using **AdamW** instead of **Adam** can help a little bit
* **Lower precision training** can help and on [pytorch lightning](https://github.com/PyTorchLightning/pytorch-lightning) is just a simple flag you can set
* For **regression problems** if you know the **output should be within a range** then its good to use **sigmoid** to force the neural net output to be within this range
   * I.e. make the network output:  min\_value + sigmoid(output) \* (max\_value - min\_value)
* **Clustering** your features can help you **identify which ones are the most redundant** and then removing the can help performance
* **Label smoothing** = use 0.1 and 0.9 instead of 0 and 1 for label targets (can smoothen training)
* **Don’t dichotomise** your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem
* **Progressive resizing** = train model on smaller resolution images first, then increase resolution gradually (can speed up training a lot)
* Strategically using **bottleneck layers** to force the network to form **more compact representations of the data** at different points can be helpful
* Try using **skip connections** as they can help smooth out the loss surface

&#x200B;

Please let me know if you found this helpful and if there are any other training tricks you use that we should also know about?",recently read fast ai deep learn book url want summarise many advance takeaways trick get im go leave basic things theres enough post im focus find new special book ive also put insights deck url save help remember long term would massively recommend use space repetition app video explanation url like anki save url things learn otherwise forget much important heres takeaways neural network train fundamentals always start ml project produce simple baselines binary classification could even simple predict common class train dataset baselines linear regression random forest boost etc use baseline clean data look datapoints get incorrect check see actually classify correctly data general also leverage baselines help debug model e g make neural network nmbr layer able match performance linear regression baseline bug e g add feature improve performance linear regression probably also improve performance neural net unless bug hyperparameter optimisation help bite especially learn rate general default hyperparameters quite well closely optimise hyperparameters one last things try rather first know something problem try inject inductive bias train process e g feature relate sequential way incorporate train separately use rnn e g know output 3 nmbr use sigmoid design final layer force output network range transfer learn always use transfer learn find model pre train similar task fine tune model particular task e g see huggingface url help nlp gradual unfreeze discriminative learn rat work well fine tune transfer learn model gradual unfreeze freeze earlier layer train later layer gradually unfreeze earlier layer one one discriminative learn rat different learn rat per layer network usually earlier layer smaller learn rat later layer trick deal overfitting best way deal overfitting get data exhaust first start regularise methods data augmentation really powerful possible text well image image data augmentation crop pad squish resize image text data augmentation negate word replace word similes perturb word embeddings nice github repo url mixup regularisation create new data average together train datapoints backwards train nlp train additional separate model feed text backwards average output two model get final prediction trick improve performance test time augmentation test time use average prediction many augment versions input prediction rather prediction true input 1 cycle train increase reduce learn rate throughout train circular fashion usually make huge difference learn rate finder algorithm algorithm fast ai provide help automatically discover roughly best learn rate never use one hot encode use embeddings instead even tabular data use adamw instead adam help little bite lower precision train help pytorch lightning url simple flag set regression problems know output within range good use sigmoid force neural net output within range e make network output min _value sigmoid output max _value min _value cluster feature help identify ones redundant remove help performance label smooth use nmbr nmbr instead nmbr nmbr label target smoothen train dichotomise data output continuous better train network predict continuous value rather turn classification problem progressive resize train model smaller resolution image first increase resolution gradually speed train lot strategically use bottleneck layer force network form compact representations data different point helpful try use skip connections help smooth loss surface x200b please let know find helpful train trick use also know
Yuqing7,MachineLearning,1620061055.0,"[R] CMU, UT Austin & Facebook’s CNN Layer Width Optimization Strategies Achieve 320x Overhead Reduction","Researchers from Carnegie Mellon University, the University of Texas at Austin and Facebook AI propose a novel paradigm to optimize widths for each CNN layer. The method is compatible across various width optimization algorithms and networks and achieves up to a 320x reduction in width optimization overhead without compromising top-1 accuracy on ImageNet.

Here is a quick read: [CMU, UT Austin & Facebook’s CNN Layer Width Optimization Strategies Achieve 320x Overhead Reduction](https://syncedreview.com/2021/05/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-10/).

The paper *Width Transfer: On the (In)Variance of Width Optimization* is on [arXiv](https://arxiv.org/pdf/2104.13255.pdf).",researchers carnegie mellon university university texas austin facebook ai propose novel paradigm optimize widths cnn layer method compatible across various width optimization algorithms network achieve 320x reduction width optimization overhead without compromise top 1 accuracy imagenet quick read cmu ut austin facebooks cnn layer width optimization strategies achieve 320x overhead reduction url paper width transfer variance width optimization arxiv url
dontreallyknowmuch,MachineLearning,1618540777.0,[R] GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,"Here's some new work from NVIDIA, recently showcased in the GTC 2021 keynote. It can convert user-created Minecraft block worlds into view-consistent realistic-looking worlds! The method learns to perform this translation in the absence of paired Minecraft-real data, by using a GAN pretrained network to generate ""*pseudo-ground truths*"".

tweet: [https://twitter.com/arunmallya/status/1382860338584952840](https://twitter.com/arunmallya/status/1382860338584952840)

arxiv: [https://arxiv.org/abs/2104.07659](https://arxiv.org/abs/2104.07659)

webpage: [https://nvlabs.github.io/GANcraft/](https://nvlabs.github.io/GANcraft/)

Sample outputs:

https://reddit.com/link/mru35h/video/i51arzkyfkt61/player

https://reddit.com/link/mru35h/video/njql7gc2gkt61/player

https://reddit.com/link/mru35h/video/2rqe3ge5gkt61/player

It can even change the style of the output world!

&#x200B;

https://reddit.com/link/mru35h/video/srg38qw7gkt61/player",new work nvidia recently showcased gtc nmbr keynote convert user create minecraft block worlds view consistent realistic look worlds method learn perform translation absence pair minecraft real data use gin pretrained network generate pseudo grind truths tweet url url url output url even change style output world x200b url
mroc_lak,MachineLearning,1616427593.0,[D] How do you test your machine learning models for healthcare to gain confidence before embarking on a clinical trial?,"Hi all, 

I am looking into ML applications in healthcare and am interested in understanding how companies prepare for clinical trials. Which metrics correlate well with clinical trial performance and what are important pitfalls to avoid?",hi look ml applications healthcare interest understand company prepare clinical trials metrics correlate well clinical trial performance important pitfalls avoid
timscarfe,MachineLearning,1616537282.0,[D] Meta-Gradients in Reinforcement Learning with Tomas Zahavy (DeepMind) and Robert Lange (Video),"Dr. Tom Zahavy, a Research Scientist at DeepMind thinks that reinforcement learning is the most general learning framework that we have today, and in his opinion it could lead to artificial general intelligence. He thinks there are no tasks which could not be solved by simply maximising a reward. 

Back in 2012 when Tom was an undergraduate, before the deep learning revolution he attended an online lecture on how CNNs automatically discover representations. This was an epiphany for Tom. He decided in that very moment that he was going to become an ML researcher.  Tom's view is that the ability to recognise patterns and discover structure is the most important aspect of intelligence. This has been his quest ever since. He is particularly focused on using diversity preservation and metagradients to discover this structure. 

In this discussion we dive deep into meta gradients in reinforcement learning. 

Video: [https://youtu.be/hfaZwgk\_iS0](https://youtu.be/hfaZwgk_iS0)

Pod: [https://anchor.fm/machinelearningstreettalk/episodes/49---Meta-Gradients-in-RL---Dr--Tomas-Zahavy-DeepMind-etbcr9](https://anchor.fm/machinelearningstreettalk/episodes/49---Meta-Gradients-in-RL---Dr--Tomas-Zahavy-DeepMind-etbcr9)",dr tom zahavy research scientist deepmind think reinforcement learn general learn framework today opinion could lead artificial general intelligence think task could solve simply maximise reward back nmbr tom undergraduate deep learn revolution attend online lecture cnns automatically discover representations epiphany tom decide moment go become ml researcher tom view ability recognise pattern discover structure important aspect intelligence quest ever since particularly focus use diversity preservation metagradients discover structure discussion dive deep meta gradients reinforcement learn video url url
freshprinceofuk,MachineLearning,1619968135.0,[D] Best CPU real time pose estimation model available?,"Hi,

I've looked into quite a few pose estimation models (both CPU/GPU and 2D and 3D) but I've not seen anything as impressive as Kemtai (kemtai.com - in browser CPU, real time). Can anyone suggest what model this may be?",hi look quite pose estimation model cpu gpu 2d 3d see anything impressive kemtai kemtai com browser cpu real time anyone suggest model may
Rat-a-ouchie,MachineLearning,1619213594.0,"[D] PhD Applied Maths - Machine Learning, supervisor selection should be one you get along with or something else is more important?","Hey guys,
I'm doing my PhD in Applied Mathematics and love machine learning. 
I want to specialise in a Machine learning based thesis amd I was wondering if anyone of you have any advice for PhD Supervisor selection?
I'm not sure if this is the right place to ask but any advice is appreciated.

In summary,
What do you look for in a supervisor?
How do you find the right research topic for you?",hey guy phd apply mathematics love machine learn want specialise machine learn base thesis amd wonder anyone advice phd supervisor selection sure right place ask advice appreciate summary look supervisor find right research topic
jj4646,MachineLearning,1619155260.0,"[D] is this the equivalent of the ""what came first, the chicken or the egg"" in machine learning?","https://en.wikipedia.org/wiki/Probably_approximately_correct_learning

I am learning about this concept in statistical learning called ""Probably Approximately Correct"" (PAC) - although the wording of this can seem complicated and technical, (if I understand correctly) the essence of PAC is to show that if a ""target concept"" (e.g. the set of all possible input points corresponding to a certain output), then the error of a machine learning algorithm can be probabilistically bounded with a certain range. I think all this is intended to show, that a machine learning algorithm is useful for making predictions, instead of basing your predictors off which color socks your neighbor is wearing. 

1) PAC framework was developed in 1984 - yet prior to this, many statistical models were being used for making predictions (e.g. regression models). Once PAC was developed, did researchers have to examine all statistical models they were using prior to this, and confirm if these models were compatible with PAC framework?

2) Now, the same question for modern models. When newer machine learning models are developed (e.g. LSTM models, developed long after PAC framework was established) - are these new algorithms tested to make sure they are compatible with PAC framework?

3) Can someone please confirm if my understanding of PAC framework is correct? (source: https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean , https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/ )  A ""hypothesis"" (in the context of PAC) seems to be a general term for a machine learning algorithm, the ""hypothesis space"" is the space of all possible such algorithms (e.g. a linear regression model with specific beta parameters is a individual hypothesis and all possible linear regression models are the hypothesis space), ""D"" is the distribution of the data, and a ""Concept Class"" - but Im still confused about the differences between target concept and concept class. Can someone please clarify this?",url learn concept statistical learn call probably approximately correct pac although word seem complicate technical understand correctly essence pac show target concept e g set possible input point correspond certain output error machine learn algorithm probabilistically bound certain range think intend show machine learn algorithm useful make predictions instead base predictors color sock neighbor wear 1 pac framework develop nmbr yet prior many statistical model use make predictions e g regression model pac develop researchers examine statistical model use prior confirm model compatible pac framework 2 question modern model newer machine learn model develop e g lstm model develop long pac framework establish new algorithms test make sure compatible pac framework 3 someone please confirm understand pac framework correct source url url hypothesis context pac seem general term machine learn algorithm hypothesis space space possible algorithms e g linear regression model specific beta parameters individual hypothesis possible linear regression model hypothesis space distribution data concept class im still confuse differences target concept concept class someone please clarify
__data_science__,MachineLearning,1617707256.0,[D] Training strategy given Double Descent phenomenon,"How does [double descent](https://openai.com/blog/deep-double-descent/) change the ideal training strategy? 

Before I used to follow this broad training strategy:

1. Make the network big enough until it is overfitting on the training data
2. Then regularise to reduce overfitting

&#x200B;

But the possibility of double descent means this might not be correct anymore, so I was wondering how you guys think about it?",double descent url change ideal train strategy use follow broad train strategy 1 make network big enough overfitting train data2 regularise reduce overfitting x200b possibility double descent mean might correct anymore wonder guy think
anianruoss,MachineLearning,1617294131.0,[R] Robustness Certification for Point Cloud Models,"We present the first robustness certifier against semantic transformations (e.g., rotation or shearing) on 3D point cloud models for object classification and part segmentation tasks.

Paper: [https://arxiv.org/abs/2103.16652](https://arxiv.org/abs/2103.16652)

Code: [https://github.com/eth-sri/3dcertify](https://github.com/eth-sri/3dcertify)

**Abstract.** The use of deep 3D point cloud models in safety-critical applications, such as autonomous driving, dictates the need to certify the robustness of these models to semantic transformations. This is technically challenging as it requires a scalable verifier tailored to point cloud models that handles a wide range of semantic 3D transformations. In this work, we address this challenge and introduce 3DCertify, the first verifier able to certify robustness of point cloud models. 3DCertify is based on two key insights: (i) a generic relaxation based on first-order Taylor approximations, applicable to any differentiable transformation, and (ii) a precise relaxation for global feature pooling, which is more complex than pointwise activations (e.g., ReLU or sigmoid) but commonly employed in point cloud models. We demonstrate the effectiveness of 3DCertify by performing an extensive evaluation on a wide range of 3D transformations (e.g., rotation, twisting) for both classification and part segmentation tasks. For example, we can certify robustness against rotations by ±60° for 95.7% of point clouds, and our max pool relaxation increases certification by up to 15.6%.",present first robustness certifier semantic transformations e g rotation shear 3d point cloud model object classification part segmentation task paper url url use deep 3d point cloud model safety critical applications autonomous drive dictate need certify robustness model semantic transformations technically challenge require scalable verifier tailor point cloud model handle wide range semantic 3d transformations work address challenge introduce 3dcertify first verifier able certify robustness point cloud model 3dcertify base two key insights generic relaxation base first order taylor approximations applicable differentiable transformation ii precise relaxation global feature pool complex pointwise activations e g relu sigmoid commonly employ point cloud model demonstrate effectiveness 3dcertify perform extensive evaluation wide range 3d transformations e g rotation twist classification part segmentation task example certify robustness rotations 60 95 7 point cloud max pool relaxation increase certification 15 6
adammathias,MachineLearning,1619016230.0,"[N] Aim 2.3.0 is out with system resource monitoring, ""reverse grouping"" and more","Highlights with my comments:

### System resource monitoring
An option for automatic tracking of your **GPU**, **CPU**, **memory** and more.  I'm curious how much this covers - disk, network...?

### ""Reverse grouping""
This is what Aim is calling the option to divide by everything but one param (typically seed).  It looks like you pick that variable in the UI for all experiments, it's not clear if you can set a variable as ""default indivisible"" via the API.

### Line chart smoothing
This is self-explanatory.  Maybe be somewhat automatic by default, based on the number of points and scale?

### Standard error and standard deviation
New built-in aggregation modes in addition to min and max

### Support for infinite values and NaN



---

[Full list of release issues and feature requests](https://github.com/aimhubio/aim/milestone/3?closed=1)

[Announcement](https://medium.com/aimstack/aim-v2-3-0-system-resource-usage-and-reverse-grouping-6900dd04a1ff) from Gev",highlight comment system resource monitoringan option automatic track gpu cpu memory curious much cover disk network reverse group aim call option divide everything one param typically seed look like pick variable ui experiment clear set variable default indivisible via api line chart smoothingthis self explanatory maybe somewhat automatic default base number point scale standard error standard deviationnew build aggregation modes addition min max support infinite value nan full list release issue feature request url gev
CanadianTuero,MachineLearning,1617011308.0,[P] Differentiable Optimizers with Perturbations in PyTorch,"After reading [this](https://www.reddit.com/r/MachineLearning/comments/mcdoxs/p_torchsort_fast_differentiable_sorting_and/) post the other day, I learned about using perturbations to create differentiable optimizers ([https://arxiv.org/abs/2002.08676](https://arxiv.org/abs/2002.08676)). There is an official implementation [here](https://github.com/google-research/google-research/tree/master/perturbations), but it is for Tensorflow. 

Since I primarily use PyTorch, and these tools are something which I want to play around with for my own research, I have reimplemented it using native PyTorch. Figured I would share here in case others find it useful as well!

[https://github.com/tuero/perturbations-differential-pytorch](https://github.com/tuero/perturbations-differential-pytorch)",read url post day learn use perturbations create differentiable optimizers url official implementation url tensorflow since primarily use pytorch tool something want play around research reimplemented use native pytorch figure would share case others find useful well url
BotPoetsSociety,MachineLearning,1618053966.0,[P] AI Poetry,"We create poems combining AI models. The poems were generated by a GPT2 model fine-tuned for poetry.

We choose to do no editing at all to the generated poetry. We think there is some fun in reading raw poetry coming from a machine, even with the obvious flaws.

&#x200B;

[Photo: http:\/\/unsplash.com; Voices: Text-to-speech AWS and Azure](https://reddit.com/link/mo2xrk/video/qfmfy3fbzbs61/player)

Youtube channel: [https://www.youtube.com/channel/UChyDkl0l6W4VebsaKSW2sDw](https://www.youtube.com/channel/UChyDkl0l6W4VebsaKSW2sDw)

Twitter: [https://twitter.com/BotPoetsSociety](https://twitter.com/BotPoetsSociety)",create poems combine ai model poems generate gpt2 model fine tune poetry choose edit generate poetry think fun read raw poetry come machine even obvious flaw x200b photo http unsplash com voice text speech aws azure url channel url url
bendee983,MachineLearning,1617897743.0,[D] Waymo now has a machine learning PhD as its co-CEO,"In 2015, Google hired John Krafcik, a veteran of the automotive industry, to lead its self-driving car efforts, which later spun off as Waymo. Last week, Krafcik stepped down and ceded his role to Dimitry Dolgov, a computer science PhD and a veteran in ML research, and Tekedra Mawakana, a Doctor of Law. 

Why is this important? At the time Krafcik joined Google, the general belief was that deep learning was mature enough for SDCs and reaching production-level SDCs was just a matter of scaling road-testing, gathering enough training data, and training DL models.

But it has become evident that in its current state, DL is not ready to tackle the many challenges of open roads, and many more gaps need to be filled. The legal infrastructure for SDCs is also not ready and many questions remain unanswered.

This is why it makes sense to put an ML engineer and a lawyer at the helm of the company. Deep learning has come a long way in pushing SDCs forward, but a bumpy road still lies ahead.

Read the full analysis here:

[https://bdtechtalks.com/2021/04/08/waymo-ceo-reshuffling-self-driving-car-industry/](https://bdtechtalks.com/2021/04/08/waymo-ceo-reshuffling-self-driving-car-industry/)",2015 google hire john krafcik veteran automotive industry lead self drive car efforts later spin waymo last week krafcik step cede role dimitry dolgov computer science phd veteran ml research tekedra mawakana doctor law important time krafcik join google general belief deep learn mature enough sdcs reach production level sdcs matter scale road test gather enough train data train dl model become evident current state dl ready tackle many challenge open roads many gap need fill legal infrastructure sdcs also ready many question remain unanswered make sense put ml engineer lawyer helm company deep learn come long way push sdcs forward bumpy road still lie ahead read full analysis url
bryant1410,MachineLearning,1619129609.0,[N] Competition on classifying tweets as jokes + more,"Computational Humor task on tweets with 4 subtasks:

* Binary classification of intended humor/non-intended humor
* Funniness scoring.
* Humor Mechanism classification
* Humor Target classification

[https://competitions.codalab.org/competitions/30090](https://competitions.codalab.org/competitions/30090)

It's in Spanish but I think it's not a big blocker if you don't speak it!

The papers from the teams (describing the systems) will be published in the IberLEF Workshop @ SEPLN 2021, Málaga, Spain (or virtual...).",computational humor task tweet nmbr subtasks binary classification intend humor non intend humor funniness score humor mechanism classification humor target classification url spanish think big blocker speak paper team describe systems publish iberlef workshop sepln 2021 málaga spain virtual
VinayUPrabhu,MachineLearning,1617308083.0,[P] A small dataset of mis-parsed citations from Google scholar,"TL-DR:  Google scholar's parser is  \*aggressive\* in indexing publications  & \*sometimes\* indexes high-school cafeteria and restaurant menus as papers. The not-so-nice aspect is that there are a lot of agro-journals from the global south that meet the same fate as well.  


1. Dataset: [https://github.com/vinayprabhu/Revenge\_of\_the\_pith\_Sigbovik2021/tree/main/data](https://github.com/vinayprabhu/Revenge_of_the_pith_Sigbovik2021/tree/main/data)
2. Awareness raising paper authored in Sigbovik-humor style: [https://github.com/vinayprabhu/Revenge\_of\_the\_pith\_Sigbovik2021/blob/main/sigbovik\_plants\_2021\_camera\_readyish.pdf](https://github.com/vinayprabhu/Revenge_of_the_pith_Sigbovik2021/blob/main/sigbovik_plants_2021_camera_readyish.pdf)  


  


https://preview.redd.it/vzu35iqpdmq61.jpg?width=741&format=pjpg&auto=webp&s=6abae4633303358e28674615464d08670e671628",tl dr google scholar parser aggressive index publications sometimes index high school cafeteria restaurant menus paper nice aspect lot agro journals global south meet fate well 1 dataset url awareness raise paper author sigbovik humor style url url
Symbiot10000,MachineLearning,1616967325.0,"[D] Papers on intelligent agents for search (not voice, Alexa, etc.)","I'm looking into writing something on intelligent agents that can be used to search internet resources on regular platforms (i.e. mobile and desktop, not headless platforms like Alexa and other AI voice assistants) on specific topics without the user directly interacting with search engines.

For various reasons, it's incredibly difficult to Google, or to drill down to any papers on the subject, if there are any. The signal-to-noise ratio has beaten me.

I'm not thinking of GPT-3-style generalized oracles that have infinite scope, that abstract a great deal, and that hide their sources; nor some AI project to scrape and replace Google; but perhaps, rather, a machine learning system that has been tasked with a specific domain, such as blastoma research, or compression, which has a specific audience in mind, and which has a greater level of discernment and judgement regarding the 'quality' of sources than the average search engine.

If anyone has a link or two that would point me in the right direction, I'd be grateful.",look write something intelligent agents use search internet resources regular platforms e mobile desktop headless platforms like alexa ai voice assistants specific topics without user directly interact search engines various reason incredibly difficult google drill paper subject signal noise ratio beat think gpt 3 style generalize oracles infinite scope abstract great deal hide source ai project scrape replace google perhaps rather machine learn system task specific domain blastoma research compression specific audience mind greater level discernment judgement regard quality source average search engine anyone link two would point right direction grateful
jj4646,MachineLearning,1619580096.0,[D] do machine learning models handle multicollinearity better than traditional models (e.g. linear regression)?,"When it comes to older and traditional models like linear regression, ensuring that the variables did not have multicollinearity was very important. Multicollinearity greatly harms the prediction ability of a model.

However, older and traditional models were meant to be used on smaller datasets, with fewer rows and fewer colums compared to modern big data. Intuitively, it is easier to identify and correct multicollinearity in smaller datasets (e.g. variable transformations, removing variables through stepwise selection, etc.)

In machine learning models with big data - is multicollinearity as big a problem? 

E.g. are models like randon forest known to sustain a strong performance in the presence of multicollinearity? If so, what makes random forest immune to multicollinearity?

Are neural networks and deep neural networks abke to deal with multicollinearity ? If so, what makes neural networks immune to multicollinearity?

Thanks",come older traditional model like linear regression ensure variables multicollinearity important multicollinearity greatly harm prediction ability model however older traditional model mean use smaller datasets fewer row fewer colums compare modern big data intuitively easier identify correct multicollinearity smaller datasets e g variable transformations remove variables stepwise selection etc machine learn model big data multicollinearity big problem e g model like randon forest know sustain strong performance presence multicollinearity make random forest immune multicollinearity neural network deep neural network abke deal multicollinearity make neural network immune multicollinearity thank
jj4646,MachineLearning,1619072556.0,"[D] Competetive ""Rule Based"" Machine Learning Models","https://en.m.wikipedia.org/wiki/Association_rule_learning

Has anyone ever seen more advanced association rules models that involve machine learning architecture? What is the most advanced machine learning models that can provide a complete set of rules and interpretations when making predictions? Does something like this exist?",url anyone ever see advance association rule model involve machine learn architecture advance machine learn model provide complete set rule interpretations make predictions something like exist
kpang0,MachineLearning,1617823197.0,[P] Vald: a highly scalable distributed fast approximate nearest neighbour dense vector search engine.,"Hi

I've recently released V1 of the Vald, a Cloud-Native distributed fast approximate nearest neighbour dense vector search engine running on Kubernetes as an OSS project under Apache2.0 licence.

It is already running behind Yahoo Japan's image search and some recommendation engine and is also running behind the Japanese National Digital Library Digital Archive retrieval engine.

By using machine learning to convert unstructured data (audio, images, videos, user characteristics, etc.) into vectors and then using Vald to perform vector search on those vectors, it will be possible to operate as a faster and more complex search engine.

&#x200B;

Vald is still a very new project, but we are looking for a lot of feedback from many users.

Please come and visit our site!

&#x200B;

Web: [https://vald.vdaas.org](https://vald.vdaas.org)

GitHub: [https://github.com/vdaas/vald](https://github.com/vdaas/vald)",hii recently release v1 vald cloud native distribute fast approximate nearest neighbour dense vector search engine run kubernetes oss project apache2 0 licence already run behind yahoo japan image search recommendation engine also run behind japanese national digital library digital archive retrieval engine use machine learn convert unstructured data audio image videos user characteristics etc vectors use vald perform vector search vectors possible operate faster complex search engine x200b vald still new project look lot feedback many users please come visit site x200b web url url
SQL_beginner,MachineLearning,1620156717.0,"[D] ""Classifier Technology and the Illusion of Progress"" (2006, Hand)","https://arxiv.org/abs/math/0606441

I found this interesting paper over here, where the author argues that more complex algorithms (e.g. deep neural networks) do not always have significant advantages over simpler algorithms in the real world (hence the illusion). The author brings up many reasons as to why this can happen - some of the reasons are related to mathematics, others are related to experimental design. 

(Note: the author brings up a point here that I am not sure why this is true: ""Conversely, in
the two-class case, although few real data sets have
exactly linear decision surfaces, it is common to find
that the centroids of the predictor variable distributions
of the classes are different, so that a simple linear surface can do surprisingly well as an estimate of the true decision surface"" ...  

Why is it common  to find that the centroids of the predictor variable distributions are different? Why does this allow for a linear surface to estimate the true surface well?)

Here were the thoughts I had after reading this paper: This was paper was published in 2006, before the ""deep learning revolution"" (e.g. in 2012 when Convolution Neural Networks clearly outperformed humans at the imagenet competition). Is it possible that the results from this paper are somewhat irrelevant and outdated? Researchers, universities and companies (e.g. google, Facebook, Microsoft) have probably spent a billion dollars since 2006 on developing more and more complex machine learning models. Using common sense, many of these models have performed well enough so that more research will be done in the future. I agree that for certain problems, perhaps simpler models (e.g. linear regression, decision trees) can perform just as well as deep learning models ... but surely, there are many problems in the real world which require more complex models? Can an argument made as to why complex models are required, using concepts such as the VC Dimension (https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)? Relating to problems such as the initial ""x-or perceptron"" problem - could we not say that big data (data with many columns and many rows) is less likely to be linearly sepperable (i.e. harder to ""shatter"" - shatter =  classify perfectly) compared to smaller datasets? Could we not say that if there are more data points, there exist more configurations that these data points can be arranged in -  making it less probable for them to be analyzed using a simpler model (the VC dimension of a simpler model is lower than the
VC dimension of a complex model) ? Does this fact alone somewhat justify the need to develop complex models?",url find interest paper author argue complex algorithms e g deep neural network always significant advantage simpler algorithms real world hence illusion author bring many reason happen reason relate mathematics others relate experimental design note author bring point sure true conversely inthe two class case although real data set haveexactly linear decision surface common findthat centroids predictor variable distributionsof class different simple linear surface surprisingly well estimate true decision surface common find centroids predictor variable distributions different allow linear surface estimate true surface well thoughts read paper paper publish 2006 deep learn revolution e g nmbr convolution neural network clearly outperform humans imagenet competition possible result paper somewhat irrelevant outdated researchers universities company e g google facebook microsoft probably spend billion dollars since nmbr develop complex machine learn model use common sense many model perform well enough research future agree certain problems perhaps simpler model e g linear regression decision tree perform well deep learn model surely many problems real world require complex model argument make complex model require use concepts vc dimension url relate problems initial x perceptron problem could say big data data many columns many row less likely linearly sepperable e harder shatter shatter classify perfectly compare smaller datasets could say data point exist configurations data point arrange make less probable analyze use simpler model vc dimension simpler model lower thevc dimension complex model fact alone somewhat justify need develop complex model
l34df4rm3r,MachineLearning,1618580410.0,[D] Graph Convolution and GraphSAGE: why don't people use these together?,"In deep learning for graphs, people graph convolution or graph SAGE, but I have not seen a combination of these two. Intuitively, it should combine both transductive and inductive frameworks. Is there any drawback?

Using DGL, it's quite easy to stack these layers and get an output. So, my question is, why do we not see this used in works that involve graph neural networks (GCNs and its variants)?",deep learn graph people graph convolution graph sage see combination two intuitively combine transductive inductive frameworks drawback use dgl quite easy stack layer get output question see use work involve graph neural network gcns variants
othotr,MachineLearning,1616740968.0,[R] Stanford HAI Spring Conference - Intelligence Augmentation: AI Empowering People to Solve Global Challenges,"Stanford Institute for Human-Centered AI hosted its spring conference today with interesting conversations about how AI can best support humans in healthcare, art, and education to address global challenges. More details and the event recording are available at the [HAI conference site](https://hai.stanford.edu/events/intelligence-augmentation-ai-empowering-people-solve-global-challenges). Here is a quick outline with video sections:

[Welcome & Introductions](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=829): HAI directors Fei-Fei Li, John Etchemendy, Russ Altman, & James Landay 

[Session I: Healthcare](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=1286) 

* Immersive Technologies for Caregiving: Innovation Opportunities and Ecosystem Challenges, Deborah Estrin @ Cornell Tech 
* Student Lightning Talks
* On Complementing and Extending Human Intellect: Principles and Directions, Eric Horvitz @ Microsoft
* Mobilizing AI to Achieve Healthy Child Development Worldwide, Dennis Wall @ Stanford 
* Safer and Proactive Care through AI, Suchi Saria @ Johns Hopkins University 

[Session II: Art](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=8379)

* Other Intelligence: Exoticism and AI, Ken Goldberg @ UC Berkeley
* Student Lightning Talks
* Artful Intelligence: Exoticism and AI, Michele Elam @ Stanford 
* The Digital Griot: A Reimagining of the Archive, Rashaad Newsome @ Stanford 
* Amplifying the Human Artist Through AI, Hilary Hahn & Carol Reiley @ DeepMusic.ai

[Session III: Education](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=15230)

* Escaping or Automating a Legacy of Bad Instruction, Daniel Schwartz @ Stanford
* Student Lightning Talks
* AI to Super Power Teachers, Chris Piech @ Stanford
* Pushing the Boundaries of Educational Technology, Amy Ogan @ Carnegie Mellon University 
* AI to Accelerate Workplace Learning at Scale, Candace Thille @ Amazon

https://preview.redd.it/p2qg7eutibp61.png?width=1928&format=png&auto=webp&s=1cc8dd6c4458c3da79d00415552ca4424f03d0c2",stanford institute human center ai host spring conference today interest conversations ai best support humans healthcare art education address global challenge detail event record available hai conference site url quick outline video section welcome introductions url hai directors fei fei li john etchemendy russ altman jam landay session healthcare url immersive technologies caregiving innovation opportunities ecosystem challenge deborah estrin cornell tech student lightning talk complement extend human intellect principles directions eric horvitz microsoft mobilize ai achieve healthy child development worldwide dennis wall stanford safer proactive care ai suchi saria johns hopkins university session ii art url intelligence exoticism ai ken goldberg uc berkeley student lightning talk artful intelligence exoticism ai michele elam stanford digital griot reimagining archive rashaad newsome stanford amplify human artist ai hilary hahn carol reiley deepmusic ai session iii education url escape automate legacy bad instruction daniel schwartz stanford student lightning talk ai super power teachers chris piech stanford push boundaries educational technology amy ogan carnegie mellon university ai accelerate workplace learn scale candace thille amazon url
dadasenior,MachineLearning,1619096232.0,"[Discussion] I own a 16"" macbook pro with max specs. But how can I make it fast for Keras?","I am training a Convolutional Neural Network with a huge dataset using tensorflow/Keras.

So far, I've been doing it with a notebook in Kaggle...  
it's VERY slow, and the *""GPU acceleration""* seems NOT to be working at all (also for many other users)

So, before I'll make an expensive PC build for ML,  
I am wondering:  
**could I exploit my expensive macbook pro 16"", instead?**

Macbook pro 16""  
2,3 GHz Intel Core i9 8 core  
16 GB 2667 MHz DDR4  
AMD Radeon Pro 5500M 4 GB

What's the best option?

a) plaidML, so that I could exploit the discrete AMD GPU in the macbook  
b) Using the ""accelerated tensorflow"" specifically tailored for macs available [here](https://github.com/apple/tensorflow_macos)  
c) I also own an eGPU powered with a AMD Radeon rx 5700xt 8GB (no CUDA 🙃) is it compatible with something like plaidML?  
d) Your call

Any help from your experience is appreciated, thanks in advance.",train convolutional neural network huge dataset use tensorflow keras far notebook kaggle slow gpu acceleration seem work also many users make expensive pc build ml wonder could exploit expensive macbook pro 16 instead macbook pro 16 2 3 ghz intel core i9 nmbr core nmbr gb nmbr mhz ddr4 amd radeon pro 5500m nmbr gbwhat best option plaidml could exploit discrete amd gpu macbook b use accelerate tensorflow specifically tailor macs available url c also egpu power amd radeon rx 5700xt 8gb cuda compatible something like plaidml callany help experience appreciate thank advance
EinsteiniumArmour,MachineLearning,1620342778.0,"[P] Last year, I built a multilingual text simplifier for my undergraduate thesis. This year, I made it work with multilingual BERT!","&#x200B;

[English simplification made by MILES](https://preview.redd.it/kympalub0lx61.png?width=1133&format=png&auto=webp&s=a39d8eaea7cf3178ff5fe9b2424a24800ced531f)

[MILES](https://github.com/Kvasirs/MILES) is a multilingual text simplifier inspired by [LSBert](https://arxiv.org/abs/2006.14939) — A BERT-based lexical simplification approach proposed in 2018. Unlike LSBert, MILES uses the bert-base-multilingual-uncased model, as well as simple language-agnostic approaches to complex word identification (CWI) and candidate ranking. Although not all have been tested, MILES should support at least 22 languages: Arabic, Bulgarian, Catalan, Czech, Danish, Dutch, English, Finnish, French, German, Hungarian, Indonesian, Italian, Norwegian, Polish, Portuguese, Romanian, Russian, Spanish, Swedish, Turkish, and Ukrainian.

Please note: as a result of not using any language-specific resources, MILES does not always offer synonymous substitutions for complex words. Although almost always simpler than the original, selected substitutions may alter the meaning of the text. Please keep this in mind, and feel free to use and tailor MILES to a language of your choosing!

The GitHub repo for MILES can be found [here](https://github.com/Kvasirs/MILES).",x200b english simplification make miles url multilingual text simplifier inspire lsbert url bert base lexical simplification approach propose 2018 unlike lsbert miles use bert base multilingual uncase model well simple language agnostic approach complex word identification cwi candidate rank although test miles support least nmbr languages arabic bulgarian catalan czech danish dutch english finnish french german hungarian indonesian italian norwegian polish portuguese romanian russian spanish swedish turkish ukrainian please note result use language specific resources miles always offer synonymous substitutions complex word although almost always simpler original select substitutions may alter mean text please keep mind feel free use tailor miles language choose github repo miles find url
FirstTimeResearcher,MachineLearning,1620101406.0,[D] Petition to the Neurips 2021 conference to extend the deadline,"Original tweet:
> Hi 
> @NeurIPSConf
> , sincere requests from many of my friends, collaborators (and most importantly students) affected by COVID to *please* extend the deadline. Many students I know are working on experiments *while* having COVID and close family members in hospitals!
>A growing number of 
@NeurIPSConf
 papers are being submitted by Indian researchers and students. I personally know many who are scrambling to write papers and experiments in this situation. Please consider extending by at least a week or two. I personally know two students who are in the hospital currently, and who had been working hard to submit a paper this time. While talking to my Indian collaborators, almost every student they have is in a similar situation. An extra week or two would help tremendously!

>Thank you so much! Pl Amplify!

https://twitter.com/rishiyer/status/1389264519885697029",original tweet hi neuripsconf sincere request many friends collaborators importantly students affect covid please extend deadline many students know work experiment covid close family members hospitals grow number neuripsconf paper submit indian researchers students personally know many scramble write paper experiment situation please consider extend least week two personally know two students hospital currently work hard submit paper time talk indian collaborators almost every student similar situation extra week two would help tremendously thank much pl amplify url
weifz,MachineLearning,1620391383.0,[D]Why is it impossible to do causal discovery from observational data?,"Hi there,

There is a saying that ""As we shall see, this problem is statistically impossible despite the large number of papers on the topic""(in  [http://www.stat.cmu.edu/\~larry/=sml/Causation.pdf](http://www.stat.cmu.edu/~larry/=sml/Causation.pdf) (sec. 1.1)), could you explain it, since I have read a lot of papers which do causal discovery from observational data?

Thanks!",hi say shall see problem statistically impossible despite large number paper topic url sec 1 1 could explain since read lot paper causal discovery observational data thank
SeaworthinessOk834,MachineLearning,1619396009.0,[D] Is Anaconda Worth the Trouble?,"Hi, everyone. I'm getting ready to work with PyTorch and am struggling with whether I should reinstall Anaconda or not. I've had issues with it in the past, where it kind of screwed up the path on my old laptop. I was more careful when I got the new computer, and it was going along fine until it wasn't, so I uninstalled it a few months back. I would really like to use it, but there seems to be no definitive guide as to how to avoid these recurring issues and I was hoping maybe somebody here might have a resource or advice other than ""If you can't make it work, you must just really suck, hur hur."" that you get on other platforms. Thank you in advance.

EDIT: It's strangely reassuring to know that I'm not the only Windows user that has had problems with Anaconda. You've all given me plenty to consider. Thank you for your responses and feedback. ",hi everyone get ready work pytorch struggle whether reinstall anaconda issue past kind screw path old laptop careful get new computer go along fine uninstalled months back would really like use seem definitive guide avoid recur issue hop maybe somebody might resource advice make work must really suck hur hur get platforms thank advance edit strangely reassure know windows user problems anaconda give plenty consider thank responses feedback
thisisdhruvagarwal,MachineLearning,1618056732.0,Which Policy Gradient Method was used by Google's Deep Mind to teach AI to walk? [D],"I just saw [this](https://www.youtube.com/watch?v=gn4nRCC9TwQ) video on Youtube.

Which Policy Gradient method was used to train the AI to walk?

Was it DDPG or D4PG or what?",saw url video youtube policy gradient method use train ai walk ddpg d4pg
mimeticaware,MachineLearning,1617515924.0,[D] Hashing techniques to compare large datasets?, Are there implementations or research papers on hashing/fingerprinting techniques for large datasets (greater than 10 GB)? I want to implement a library which generates a hash/fingerprint for large datasets so they can be easily compared. I'm not sure where to start and any existing implementations/research papers would be really helpful!,implementations research paper hash fingerprint techniques large datasets greater nmbr gb want implement library generate hash fingerprint large datasets easily compare sure start exist implementations research paper would really helpful
Rishit-dagli,MachineLearning,1618399521.0,[P] Implementing Perceiver: General perception with Iterative Attention in TensorFlow," Today I am glad to present an implementation of the ""Perceiver: General Perception with Iterative Attention"" Model which builds on top of Transformers but solves the quadratic scaling problem without making any assumptions of the data like the previous approaches in TensorFlow. This means you can use the same model on images, audio, videos, etc! This model also achieves state-of-the-art for some tasks!

PS: This is made ready to use as a Python package so you can get started very easily.

The project: [https://github.com/Rishit-dagli/Perceiver](https://github.com/Rishit-dagli/Perceiver)",today glad present implementation perceiver general perception iterative attention model build top transformers solve quadratic scale problem without make assumptions data like previous approach tensorflow mean use model image audio videos etc model also achieve state art task ps make ready use python package get start easily project url
VinayUPrabhu,MachineLearning,1618166994.0,[P] On the extreme compressibility of Dall-E encoding tensors,"Brief abstract:   
I saw a bunch of artsy folks experimenting with interesting downstream applications of the Dall-E encoders and thought I'd share this  observation. As it turns out, one could potentially use much lower \*effective\* vocab-sizes  of \~ 32  instead of 8192 without suffering much visual-quality loss. Or in other words, the intrinsic dimensionality of the vocabulary-space unearthed by tucker-decomposition like tensor-compression techniques is \~ 32.  
So, you can use much lower dimensional encoding vectors (with \~96% compression) rather than the raw 8192 x 32 x32 D vectors in your post-encoding regression/classification/linear-algebra twiddling pipelines.  


PS: Notice the nice space-filling artifact in the .gif when you travel upwards from 3D to 32D.  
Colab: [https://github.com/vinayprabhu/Colabarama/blob/master/Dall\_E\_low\_D.ipynb](https://github.com/vinayprabhu/Colabarama/blob/master/Dall_E_low_D.ipynb)

https://preview.redd.it/w9dq107i9ls61.png?width=277&format=png&auto=webp&s=3b9d5c9d46c2f97b4379ef43663d59735a1a226b

[The obligatory cat gif](https://i.redd.it/khmdle27als61.gif)",brief abstract saw bunch artsy folks experiment interest downstream applications dall e encoders think share observation turn one could potentially use much lower effective vocab size nmbr instead nmbr without suffer much visual quality loss word intrinsic dimensionality vocabulary space unearth tucker decomposition like tensor compression techniques 32 use much lower dimensional encode vectors 96 compression rather raw nmbr x nmbr x32 vectors post encode regression classification linear algebra twiddle pipelines ps notice nice space fill artifact gif travel upwards 3d 32d colab url obligatory cat gif url
sideonion,MachineLearning,1620432525.0,[D] What DL algorithm to use to track target when I know target coordinates first frame - single target only?,"So,  I have a set of images and I have to define in the first frame which hu  (target) to detect/track in subsequent frames. Most deep learning models pre-train on a data that I don't have. I have images and I want to detect one specific target. I know human detection models exist but how should I use them when I want to detect that particular human in subsequent frames.

There may be multiple humans in the frame but I want to focus only on the one I am trying to detect.

If there are any methods that you can help me with, please let me know. Thanks.",set image define first frame hu target detect track subsequent frame deep learn model pre train data image want detect one specific target know human detection model exist use want detect particular human subsequent frame may multiple humans frame want focus one try detect methods help please let know thank
TheCockatoo,MachineLearning,1616836263.0,[D] What's your experience with ML conference rebuttals / letters to area chairs?,"Have you ever written to the area chairs due to reviews by incompetent reviewers (e.g., where the reviewer has very obviously not spent more than 2 minutes on your paper yet rejects with high confidence, and their comments reveal he doesn't understand basic machine learning), and if so, what happened?",ever write area chair due review incompetent reviewers e g reviewer obviously spend nmbr minutes paper yet reject high confidence comment reveal understand basic machine learn happen
stivi2000,MachineLearning,1617832590.0,[P] Language Independent Sentiment Analysis,"We trained a sentiment model using an english dataset. But we are using it for german sentences and it works surprisingly well.

See here for details and why it actually works:
https://github.com/AOK-PLUS/Sentimentanalysis",train sentiment model use english dataset use german sentence work surprisingly well see detail actually work url
WFHFAWAY,MachineLearning,1617552633.0,[D] Is A Failure Ever Worth Publishing?,"So I did some formal research as part of my MS. I had a architectural idea, couldn't find any example of it in the literature.

I went through the research process, but the application and integration of my idea was not exhaustive due to time constraints. The net result of the research was that integrating the approach into an existing backbone lowered the validation performance slightly on an apples to apples basis.

If anyone had tried this before, maybe it didn't work, but I could find no reference to anyone having tried it.

Are experiments that don't always lead to big improvements never worth publishing? I feel like we can only make progress if we know what has been tried before.",formal research part ms architectural idea find example literature go research process application integration idea exhaustive due time constraints net result research integrate approach exist backbone lower validation performance slightly apples apples basis anyone try maybe work could find reference anyone try experiment always lead big improvements never worth publish feel like make progress know try
SkyLordOmega,MachineLearning,1618722171.0,[D] Wav2Vec2 training for Hindi language,"I was part of the datasprint with HuggingFace for the wav2vec2 fine-tuning task. I trained it on three datasets for Hindi language.

1. CommonVoice
2. Indic TTS (IITM)
3. IIITH - Indic dataset

While the trained model gives a good performance on the longer audios (2&3 WER 17) its performance on CommonVoice is very bad (WER 56)

CommonVoice has audio of smaller length.

I have attached an image with some example predictions (from the test set) What could be the reason for this drastic degradation in quality? Could I improve the model, by resuming training on only the CommonVoice dataset.

&#x200B;

https://preview.redd.it/8wqzwcho6vt61.png?width=922&format=png&auto=webp&s=b9394a396aec8730fac9630de2ed38b245af5b76",part datasprint huggingface wav2vec2 fine tune task train three datasets hindi language 1 commonvoice2 indic tts iitm 3 iiith indic datasetwhile train model give good performance longer audios 2 3 wer 17 performance commonvoice bad wer 56 commonvoice audio smaller length attach image example predictions test set could reason drastic degradation quality could improve model resume train commonvoice dataset x200b url
PaganPasta,MachineLearning,1620454345.0,[D] ICML 2021 Results,"ICML results are due today. Gather around anxious authors, how did you fare?
Relevant content from last week: https://www.reddit.com/r/MachineLearning/comments/n243qw/d_icml_conference_we_plan_to_reduce_the_number_of/


Good luck.",icml result due today gather around anxious author fare relevant content last week url luck
Seankala,MachineLearning,1618462099.0,"[D] Regarding BERT-based models (BERT, RoBERTa, etc.) do we absolutely have to include the [CLS] and [SEP] special tokens in the input data?","The thought just occurred to me while I was processing data. If we're using the `[CLS]` token for classification, then it would obviously make sense to include it, but if we're not using that token do we have to include it?",think occur process data use cls token classification would obviously make sense include use token include
Gullible_Dance,MachineLearning,1619642170.0,[D] New paper shows that federated learning is broken?,"Title: See through Gradients: Image Batch Recovery via GradInversion 

([https://arxiv.org/abs/2104.07586](https://arxiv.org/abs/2104.07586))

The authors can recover individual training examples from accumulated gradients. What does this mean for data privacy laws?",title see gradients image batch recovery via gradinversion url author recover individual train examples accumulate gradients mean data privacy laws
ravode,MachineLearning,1617294699.0,[D] Dask on App Engine or Cloud Run?,"Hello!

I'm wondering if running Dask on App Engine or Cloud Run is a thing?

The  use case is this - there is an ETL job running an sk-learn model (it's  part of an Argo Workflow to be precise) and as of now execution is done  unparallelized on the respective node. We'd like to parallelize the  sk-learn stuff by moving it to Dask. The obvious approach (as Argo is  K8s based) would be to integrate Dask in the K8s cluster.

But  there are a couple of reasons why I wonder whether App Engine or Cloud  Run might also be a viable option. In that case we'd skip on the  horizontal scaling and just instead fire up a strong instance  parallelizing with Dask on all available cores instead.",hello wonder run dask app engine cloud run thing use case etl job run sk learn model part argo workflow precise execution unparallelized respective node like parallelize sk learn stuff move dask obvious approach argo k8s base would integrate dask k8s cluster couple reason wonder whether app engine cloud run might also viable option case skip horizontal scale instead fire strong instance parallelize dask available core instead
jj4646,MachineLearning,1620015148.0,[D] stochastic block model vs. standard community detection algorithms,"Has anyone ever come across the ""stochastic block model"" (https://en.m.wikipedia.org/wiki/Stochastic_block_model)? All in all, this seems like a community detection algorithm for graphs (i.e. network clustering). 

Does anyone know in which circumstances it would make more sense to use ""stochastic block models"" compared to community detection algorithms such as ""louvain clustering""?

Thanks",anyone ever come across stochastic block model url seem like community detection algorithm graph e network cluster anyone know circumstances would make sense use stochastic block model compare community detection algorithms louvain cluster thank
xiikjuy,MachineLearning,1619972031.0,[D] How to visualize the features of encoder output of an encoder-decoder Transformer model?,"Hello,

I wonder how to visualize the encoder output features of encoder-decoder models like BART, T5.  
For the base Bart model, if max position = 1024, model dimension=768,  
then the feature dimension would be 1024\*768 =786k.  
 I have no experiences using t-sne before, is it still a reasonable choice for features with dimensions at this order?  
 Any suggestions for some good practice?  
 Thanks.",hello wonder visualize encoder output feature encoder decoder model like bart t5 base bart model max position 1024 model dimension 768 feature dimension would 1024 768 786k experience use sne still reasonable choice feature dimension order suggestions good practice thank
blazejd,MachineLearning,1617211645.0,[Discussion] How was this paper titled?,"This is a little desperate, but a couple of weeks ago I stumbled upon an interesting paper, but I don't remember its title and after a lot of searching I couldn't find it. It explained how using gradients we could identify easy and difficult to classify examples in the data and afterwards it was shown that using just 25% of examples, when chosen properly, we can achieve a similar performance as if we used the whole dataset for training. There was a figure showing images in two dimensions, something like below. If anyone remembered based on this vague description the title, I would really appreciate sharing it.

https://preview.redd.it/c2amg3laeeq61.png?width=555&format=png&auto=webp&s=77551085bdcd24f26843cc795ee014ec039d1a7c",little desperate couple weeks ago stumble upon interest paper remember title lot search find explain use gradients could identify easy difficult classify examples data afterwards show use 25 examples choose properly achieve similar performance use whole dataset train figure show image two dimension something like anyone remember base vague description title would really appreciate share url
innerlee,MachineLearning,1617966854.0,"[N] MMOCR: A Toolbox for Text Detection, Recognition, and Understanding Based on PyTorch","We just released [https://github.com/open-mmlab/mmocr](https://github.com/open-mmlab/mmocr), a new member in OpenMMLab [https://openmmlab.com/](https://openmmlab.com/). This first release supports

&#x200B;

[Text Detection](https://i.redd.it/hoyx87ffg6s61.gif)

**Text Detection**

* PSENet
* PANet
* DBNet
* TextSnake
* MaskRCNN

&#x200B;

[Text Recognition](https://i.redd.it/pp4tpyrjg6s61.gif)

**Text Recognition**

* CRNN
* SAR
* Robustscanner
* SegOCR
* NRTR

&#x200B;

[Key Information Extraction](https://i.redd.it/huzucxylg6s61.gif)

**Key Information Extraction**

* SDMG-R

Longer post see [https://medium.com/@openmmlab/mmocr-a-comprehensive-toolbox-for-text-detection-recognition-and-understanding-795befa726b8](https://medium.com/@openmmlab/mmocr-a-comprehensive-toolbox-for-text-detection-recognition-and-understanding-795befa726b8)",release url new member openmmlab url first release support x200b text detection url detection psenet panet dbnet textsnake maskrcnn x200b text recognition url recognition crnn sar robustscanner segocr nrtr x200b key information extraction url information extraction sdmg rlonger post see url
inigomlap,MachineLearning,1619161977.0,[D] What methodology do you use in data science projects?,"Data scientists out there, what **project methodology** do you or your team use for data science? You can check the methodologies used in the poll in this article: [https://www.sciencedirect.com/science/article/abs/pii/S2214579620300514](https://www.sciencedirect.com/science/article/abs/pii/S2214579620300514)

[View Poll](https://www.reddit.com/poll/mwponm)",data scientists project methodology team use data science check methodologies use poll article url poll url
vanstorm9,MachineLearning,1618251549.0,[D] How do you visualize and compare image distributions of datasets?,"I have been in situations where I handle multiple image datasets and where I want to determine how similar two image datasets' distributions are to each other.

This will help me determine things such as whether it is suitable to combine two datasets into one, debug why validation accuracy is high while testing is low, and just straight up discovering more information when comparing image datasets that would aid in important decisions.

The most I have done is generating a pixel intensity histogram for one or multiple images from two datasets and compare them accordingly. Is there anything else I can do to visualize and compare image distributions among datasets? Is there also any other metrics I should also visualize or calculate to compare certain properties of image datasets?",situations handle multiple image datasets want determine similar two image datasets distributions help determine things whether suitable combine two datasets one debug validation accuracy high test low straight discover information compare image datasets would aid important decisions generate pixel intensity histogram one multiple image two datasets compare accordingly anything else visualize compare image distributions among datasets also metrics also visualize calculate compare certain properties image datasets
lsmith1988,MachineLearning,1618030770.0,Search engine used to seek details of videos/images [r],"Other than the caption, are there papers or projects underway where ML can extract information of a given video/image and search the exact location of where this may be on the internet (excluding reverse image search by Google)? For instance search for color wheels, frames of a video, speech, etc - then return its location. I am interested in learning more about this type of technology and whether it has been something that has been done already?",caption paper project underway ml extract information give video image search exact location may internet exclude reverse image search google instance search color wheel frame video speech etc return location interest learn type technology whether something already
cgnorthcutt,MachineLearning,1617033396.0,[R] Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"Hi Reddit! We’re excited to share our latest research: label errors are pervasive in 10 of the most-commonly used benchmark test sets used in most machine learning research. We investigate the implication of these label errors, in particular, how they affect the stability of ML model benchmark rankings.

[An example label error from each category for image datasets. The figure shows given labels, CL-guessed alternatives, human-validated corrected labels, and also the second label for multi-class data points. A browser for all label errors across all 10 datasets is available at https:\/\/labelerrors.com. Errors from text and audio datasets are also included in the website.](https://preview.redd.it/07uhmpp8ozp61.png?width=1900&format=png&auto=webp&s=bc3e3c42a4c9a64055bd2a225284f98b8a3dc86d)

**Demo**: [https://labelerrors.com](https://labelerrors.com/)

**Blog Post**: [https://l7.curtisnorthcutt.com/label-errors](https://l7.curtisnorthcutt.com/label-errors)

**Abstract**: We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.

**Paper**: [https://arxiv.org/abs/2103.14749](https://arxiv.org/abs/2103.14749)

**Code**: [https://github.com/cgnorthcutt/cleanlab](https://github.com/cgnorthcutt/cleanlab)

Joint work with Anish Athalye and Jonas Mueller.",hi reddit excite share latest research label errors pervasive nmbr commonly use benchmark test set use machine learn research investigate implication label errors particular affect stability ml model benchmark rank example label error category image datasets figure show give label cl guess alternatives human validate correct label also second label multi class data point browser label errors across nmbr datasets available https labelerrors com errors text audio datasets also include website url url post url identify label errors test set nmbr commonly use computer vision natural language audio datasets subsequently study potential label errors affect benchmark result errors test set numerous widespread estimate average 3 4 errors across nmbr datasets example nmbr label errors comprise 6 imagenet validation set putative label errors identify use confident learn algorithms human validate via crowdsourcing 54 algorithmically flag candidates indeed erroneously label traditionally machine learn practitioners choose model deploy base test accuracy find advise caution propose judge model correctly label test set may useful especially noisy real world datasets surprisingly find lower capacity model may practically useful higher capacity model real world datasets high proportion erroneously label data example imagenet correct label resnet 18 outperform resnet 50 prevalence originally mislabeled test examples increase 6 cifar 10 correct label vgg 11 outperform vgg 19 prevalence originally mislabeled test examples increase 5 paper url url work anish athalye jonas mueller
,MachineLearning,1616615314.0,[N] Hype GTC 2021,"Get a glimpse at the future of computing during NVIDIA CEO Jensen Huang’s #GTC21 keynote on 4/12 at 8:30 a.m. PDT. Save the date. [https://bit.ly/3rlvfWZ](https://bit.ly/3rlvfWZ)

&#x200B;

I think what we are doing across the AI/ML community is extremely important and pushing boundaries. This keynote is free to listen to (don't even need to register for the conference) and if you haven't heard Nvidia CEO Jensen Huang speak it is a sincerely impressive thing.

I'll have some more useful AI/ML/Data Science information/tips tomorrow. Just thought I'd push this to the masses today.",get glimpse future compute nvidia ceo jensen huangs gtc21 keynote 4 12 8 30 pdt save date url think across ai ml community extremely important push boundaries keynote free listen even need register conference hear nvidia ceo jensen huang speak sincerely impressive thing useful ai ml data science information tip tomorrow think push mass today
CauchySchwartzDaddy,MachineLearning,1617076262.0,[D] Advice on getting grants/funding for a college ML research club to cover GPU costs,"Basically I help run a ML club on campus and I want to be able to get some sort of funding to cover GPU costs in the cloud.  One of my friends said ""google just gives out GPUs"" but I couldn't find any info on this.  We really don't need a whole ton to run the whole operation but I was wondering if there were any good ways to go about this.",basically help run ml club campus want able get sort fund cover gpu cost cloud one friends say google give gpus find info really need whole ton run whole operation wonder good ways go
blissfox-red,MachineLearning,1617171438.0,[D] Nvidia Data Science of the Day posts,"Subsequently to bumping into these posts (whose author disappeared):

[Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506) :  
[https://www.reddit.com/r/MachineLearning/comments/m6b047/p\_embed\_your\_sql\_query\_into\_your\_python\_code\_and/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/?utm_source=share&utm_medium=web2x&context=3)

[With GPUs, K-nearest Neighbor Algorithm Crosses the Finish Line When Others are in the Starting Blocks](https://forums.developer.nvidia.com/t/with-gpus-k-nearest-neighbor-algorithm-crosses-the-finish-line-when-others-are-in-the-starting-blocks/168785) :  
[https://www.reddit.com/r/MachineLearning/comments/m6zve7/p\_with\_gpus\_knearest\_neighbor\_algorithm\_crosses/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/m6zve7/p_with_gpus_knearest_neighbor_algorithm_crosses/?utm_source=share&utm_medium=web2x&context=3)

I arrived at a part I have so far missed on the nvidia webpages (some, if not most of you might already know about it), the Data Science of the Day page, which proposes some posts (about one a day on average it seems) encapsulating usually just a link to a blog post or article about a technical news in data science. Not every post in there might be relevant to a ML practitioner,  but I believe at least some might be. So, for those like me who missed that, or just want to give it a look, here is the link:  
[https://forums.developer.nvidia.com/c/ai-data-science/data-science-of-the-day/323/none](https://forums.developer.nvidia.com/c/ai-data-science/data-science-of-the-day/323/none)",subsequently bump post whose author disappear embed sql query python code let rip gpu url url gpus k nearest neighbor algorithm cross finish line others start block url url arrive part far miss nvidia webpages might already know data science day page propose post one day average seem encapsulate usually link blog post article technical news data science every post might relevant ml practitioner believe least might like miss want give look link url
tranhp129,MachineLearning,1620591990.0,[D] Non Strongly-convex loss is strongly convex in expectation,"I have seen several papers mention that there are some losses that are not strongly convex by themselves but are strongly convex in expectation (eg: logit loss, square loss). Just by taking the derivative of the losses, I can show that these losses are not strongly convex but I don't understand why the expectation of them are strongly convex. For example, the hessian of square loss would be a rank-1 matrix (outer product of vector), thus the square loss is not strongly-convex. But why is taking the expectation of this make any difference? Any help is appreciated!",see several paper mention losses strongly convex strongly convex expectation eg logit loss square loss take derivative losses show losses strongly convex understand expectation strongly convex example hessian square loss would rank 1 matrix outer product vector thus square loss strongly convex take expectation make difference help appreciate
gta141,MachineLearning,1620337216.0,[Research] DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic Using Deep Visual Models,"The goal of this paper is to quantify marine plastic using deep learning. Check out our paper at [https://arxiv.org/abs/2105.01882](https://arxiv.org/abs/2105.01882)

&#x200B;

[Predictions using our model](https://reddit.com/link/n6ihm7/video/dzdhwqfikkx61/player)",goal paper quantify marine plastic use deep learn check paper url use model url
RussellEsby,MachineLearning,1618479575.0,[P] Voice Conversion VAE-Cycle-GAN on Melspectrograms,"Hi! I'm following this great work by Ehab A. AlBadawy ([https://ebadawy.github.io/post/speech\_style\_transfer/](https://ebadawy.github.io/post/speech_style_transfer/)). The demonstrated results are incredible. And it really stands out from other approaches both in quality and feasibility.

I've been working on an open source implementation for the past month. The link for it can be found here ([https://github.com/RussellSB/voice-conversion-gan](https://github.com/RussellSB/voice-conversion-gan)). But have since been struggling with a load of mode collapse - or the model outputting ""blurry"" spectrograms not quite capturing the same initial structure as they should.

There were some architectural changes I had to make to be able to execute the model. For one I added a conv layer to the encoder and decoder to get the right dimensions. In the paper there is no description of how they designed their residual blocks either so I used convolutional layers.

I've tried many modifications and experiments to better stabilize training, or more closely replicate the paper, but I feel I've replicated as much as I currently can. Might anyone be able to point out what may be the issue - or keen to lean me to the right direction?",hi follow great work ehab albadawy url demonstrate result incredible really stand approach quality feasibility work open source implementation past month link find url since struggle load mode collapse model output blurry spectrograms quite capture initial structure architectural change make able execute model one add conv layer encoder decoder get right dimension paper description design residual block either use convolutional layer try many modifications experiment better stabilize train closely replicate paper feel replicate much currently might anyone able point may issue keen lean right direction
paulcjh,MachineLearning,1616435921.0,[P] Silero NLP streaming on serverless GPUs (~300ms latency),"Hey everyone,

A couple of weeks ago I put out a post on DeepSpeech running on the serverless setup at Neuro ([https://getneuro.ai](https://getneuro.ai/)), and I've now got Silero running there as well. I've found this model is a lot faster than DS and way more accurate. Seeing around 300ms per request at the moment, hopefully will be closer to 100ms soon but this is a pretty decent speed in this application already. 

The code listens to a mic on your local machine and streams it to the Silero model then returning you the conversion result. You can find the source here: [https://github.com/neuro-ai-dev/npu\_examples/tree/main/silero/python](https://github.com/neuro-ai-dev/npu_examples/tree/main/silero/python). Of course this is all running on serverless so you can hammer it (in theory) as hard as you want.

Silero ([https://github.com/snakers4/silero-models](https://github.com/snakers4/silero-models)) is an audio to text conversion model and can be pretty heavy. If you have any Q's or want to see more of this let me know! I think next I'll be playing around with Spade ([https://github.com/NVlabs/SPADE](https://github.com/NVlabs/SPADE)) and setting up some other bulky vision models.

Cheers",hey everyone couple weeks ago put post deepspeech run serverless setup neuro url get silero run well find model lot faster ds way accurate see around 300ms per request moment hopefully closer 100ms soon pretty decent speed application already code listen mic local machine stream silero model return conversion result find source url course run serverless hammer theory hard want silero url audio text conversion model pretty heavy q want see let know think next play around spade url set bulky vision model cheer
idg101,MachineLearning,1619555928.0,[D] Unpopular Opinion: I hate the tensorboard Smoothing algorithm and always set the slider to 0.,"Looking at the code for the smoothing slider bar in tensorboard, it implements an exponential moving average which is all useful for the majority of ML tasks I do.  It seems to me that something much more simple like a moving average filter would be much better and make the slider the window length.",look code smooth slider bar tensorboard implement exponential move average useful majority ml task seem something much simple like move average filter would much better make slider window length
Drakshh,MachineLearning,1619710277.0,[Project] Model to evaluate audio clips similarly,"(posted on multiple subreddit but got no response hence posting here... Sorry it this doesn't belong here)

Dear all, I'm relatively new to NLP and ML. Currently I'm working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting text from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it's available on internet) to measure similarly (will probably use cosine similarly)
I couldn't come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation...",post multiple subreddit get response hence post sorry belong dear relatively new nlp ml currently work project compare score two audio clip original clip single sentence movie character animate real second clip human try mimic come model determine similarly score human clip 5 factor consider use 1 get text speech compare original2 evaluate similarly audio spectrogram spectral centroid zero cross rate 3 identification emotion speech use emotions embed assume available internet measure similarly probably use cosine similarly come factor please help come new comparison factor suggest approach problem better way thank anticipation
skeering,MachineLearning,1619970192.0,[R][D] Starting a Post-Doc and Looking for Advice on Research Area,"So I'll be starting a post-doc within the next 6-12 months, and I'm looking around for opportunities. My supervisor advised me not to do a post-doc in the same area as my PhD and instead to ""branch out"" into two main fields, to diversify.

My PhD was/is in XAI, and I'm very happy with how the last 3 years went. Going forward, I was looking for ""hot"" research areas aside from this, and what I personally just like/find interesting. Correct me if I'm wrong, but it seems the big questions in AI going forward are.

1. How to generalise better (e.g., take what you learn from MNIST and apply it to FashionMNIST).
2. How to make systems immune to adversarial attacks.
3. How to get explanations from opaque models (e.g., in medical radiology).
4. How to learn from fewer examples (one shot learning and semi-supervised learning). The ultimate goal being unsupervised learning that works well in the ""real world"".

I'm not sure if I missed anything? Probably what I'm most interested in (aside from XAI) is number 4 above. Would I be correct in assuming any of these four areas are important in the next generation of AI technology? Is there any area I missed? Lastly, would you agree that number 4. is a good area to ""get into"" as a second research interest the next 5-10 years?

Thanks and have a great day.",start post doc within next 6 12 months look around opportunities supervisor advise post doc area phd instead branch two main field diversify phd xai happy last nmbr years go go forward look hot research areas aside personally like find interest correct wrong seem big question ai go forward 1 generalise better e g take learn mnist apply fashionmnist 2 make systems immune adversarial attack 3 get explanations opaque model e g medical radiology 4 learn fewer examples one shoot learn semi supervise learn ultimate goal unsupervised learn work well real world sure miss anything probably interest aside xai number nmbr would correct assume four areas important next generation ai technology area miss lastly would agree number 4 good area get second research interest next 5 10 years thank great day
Jason_s0214,MachineLearning,1619216428.0,[R][D] Our new ICLR'21 work clarifies a misconception regarding distillation and label smoothing in a previous NeurIPS'19 study," [Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://arxiv.org/abs/2104.00676) 

Project page: [http://zhiqiangshen.com/projects/LS\_and\_KD/index.html](http://zhiqiangshen.com/projects/LS_and_KD/index.html)

 Any comments or discussions are welcome!",label smooth truly incompatible knowledge distillation empirical study url project page url comment discussions welcome
aspcraft,MachineLearning,1620306117.0,Noise-reduction techniques and evaluation for timeseries data [Discussion],"What are some common practices for dealing with noisy data? What are some common noise-reduction techniques for non-stationary time series?

Also, is there some sort of formula for comparing different methods of noise reduction? For example, if one produces two different timeseries from the original noisy data, is there a way of comparing these two new timeseries to evaluate which is better? For example, in a linear regression the best line is typically chosen by minimising the mean squared error. Is there something similar for this? If not, anyone have any ideas for a criterion that could be adapted into a formula that is then minimised or maximised to get the best noice-reduced timeseries?

I am interested to hear your thoughts and general ideas!",common practice deal noisy data common noise reduction techniques non stationary time series also sort formula compare different methods noise reduction example one produce two different timeseries original noisy data way compare two new timeseries evaluate better example linear regression best line typically choose minimise mean square error something similar anyone ideas criterion could adapt formula minimise maximise get best noice reduce timeseries interest hear thoughts general ideas
ottawalanguages,MachineLearning,1618866478.0,[D] Effective Ways of Choosing the Number of Layers/Neurons in a Neural Network,"I have been reading more about the theoretical backgrounds of neural networks (e.g. ""universal approximation theorem"") and have seen several authors demonstrate that even a simple (few layers, many neurons) neural network can (theoretically) approximate the variable of interest (i.e. the response variable) to a ""decent"" level of precision.  However, the implication being that to use simple neural networks in order to achieve good results, this would require a very large number of neurons. Therefore, deeper neural networks have been developed over the years, which attempt to provide good results with more layers but a fewer number of neurons.

This brings me to my situation: I have never been able to successfully fit a neural network to any real-world data that I have used. I have always gotten really bad results with neural networks (after trying all sorts of combinations of number of neurons, number of layers, learning rate, activation function, ""drop out"" regularization, etc.). This seems to be a hyperparameter-grid search problem.

(Ironically, models like CART decision trees have good results on the same data (supervised binary classification) and random forest has produced even better - this data is not ""small by any means"", contains around 30 columns and over 300,000 rows of data).

 Does anyone know if routines have been written (e.g. in tensorflow keras) that can assist in this problem of deciding the number of layers and the number of neurons? Is there a ""ground rule"" for deciding how many layers and how many neurons to begin with? Is there something around that can ""intelligently"" point you in the right direction for how many neurons/layers to choose?",read theoretical background neural network e g universal approximation theorem see several author demonstrate even simple layer many neurons neural network theoretically approximate variable interest e response variable decent level precision however implication use simple neural network order achieve good result would require large number neurons therefore deeper neural network develop years attempt provide good result layer fewer number neurons bring situation never able successfully fit neural network real world data use always get really bad result neural network try sort combinations number neurons number layer learn rate activation function drop regularization etc seem hyperparameter grid search problem ironically model like cart decision tree good result data supervise binary classification random forest produce even better data small mean contain around nmbr columns 300 000 row data anyone know routines write e g tensorflow keras assist problem decide number layer number neurons grind rule decide many layer many neurons begin something around intelligently point right direction many neurons layer choose
Unreasonable_Energy,MachineLearning,1618961433.0,Do we already have the ML technology to make eye contact work better in video chat? [D],"**The problem:**

In video chat, if you make ""eye contact"" with the image of your conversation partner's eyes as they appear on your screen, your eyes are not pointed at the camera, and thus you do not appear, to your partner, to be making eye contact with them, but appear to looking in an offset direction -- [relevant xkcd](https://xkcd.com/2430/).

**A solution (in principle?):** 

If you had multiple cameras arrayed at the edges of your screen (say, 3 or 4 webcams around the edges of your screen instead of just 1) it seems like it should be possible, with some machine learning magic, to combine the multiple camera feeds coming from slightly different vantage points into a single feed of a ""virtual vantage point"" within the convex hull of the physical cameras -- that is, a point somewhere in the middle of your screen.  Then all you need is to be able to automatically locate your the image of your conversation partner's eyes within your screen (basically a solved problem, as I understand) and set the virtual vantage point to one of these eye images, or to a point between them -- and voila, when you make eye contact with your screen image if them, they actually see their screen image of you appearing to make eye contact with them.  

**If this would be so easy, why doesn't it already exist?**

Do we not have the machine learning technology already to make this a reality?  Is somebody already doing it?  The potential hurdles that immediately come to mind are:

(1) Learning the virtual-vantage-point transformation would be hard -- maybe?  I'm pretty sure I've already seen more impressive demonstrations of vantage-point-shift than this would require.  I certainly don't have enough understanding of this specialty to know how I'd do it though -- maybe it's hard.

(2) Takes too long to apply the vantage-point transformation in real time, inducing unacceptable lag -- yes, the processing needs to happen in real time, but it can happen on the local machine, and the virtual-vantage-point-feed shouldn't be any more costly to transmit over a network than the standard video feed.  I can imagine applying it locally could be pretty fast, and there already seem to be more superficially-impressive video transformations being applied in real time (face-contouring, etc).

(3) Requires a non-standard hardware setup -- sure, but not like a really expensive or difficult one, just a couple extra webcams and you could do it yourself.  Doesn't work on mobile, but you can't have everything,

(4) Nobody cares about this problem enough to work on it -- I suspect there are surprisingly large gains to be had through making video chat a little less uncanny, but maybe this gain is too small to be worth the effort.

I'd appreciate any thoughts on the feasibility of such a scheme, from a machine-learning perspective.",problem video chat make eye contact image conversation partner eye appear screen eye point camera thus appear partner make eye contact appear look offset direction relevant xkcd url solution principle multiple cameras array edge screen say nmbr nmbr webcams around edge screen instead 1 seem like possible machine learn magic combine multiple camera feed come slightly different vantage point single fee virtual vantage point within convex hull physical cameras point somewhere middle screen need able automatically locate image conversation partner eye within screen basically solve problem understand set virtual vantage point one eye image point voila make eye contact screen image actually see screen image appear make eye contact would easy already exist machine learn technology already make reality somebody already potential hurdle immediately come mind 1 learn virtual vantage point transformation would hard maybe pretty sure already see impressive demonstrations vantage point shift would require certainly enough understand specialty know though maybe hard 2 take long apply vantage point transformation real time induce unacceptable lag yes process need happen real time happen local machine virtual vantage point fee costly transmit network standard video fee imagine apply locally could pretty fast already seem superficially impressive video transformations apply real time face contour etc 3 require non standard hardware setup sure like really expensive difficult one couple extra webcams could work mobile everything 4 nobody care problem enough work suspect surprisingly large gain make video chat little less uncanny maybe gain small worth effort appreciate thoughts feasibility scheme machine learn perspective
mate_classic,MachineLearning,1617894376.0,[D] State of Deep Learning outside CV and NLP,"Computer vision and natural language processing get most of the limelight in the deep learning world, and rightfully so. But, as someone working in another field that fell for the deep learning hype, too, I often see problems that have no direct equivalent in CV or NLP. Nevertheless, people try to adopt successful methods from these fields.

This sometimes leads to results that are not meaningful at all because the researchers don't take the time to adapt the methods properly to the field. Often, datasets are much smaller or have samples that are part of the same group (e.g. steps in a time series). There are enough papers, where one part of a time series is put into the train and the other part into the test set.  I wrote a [blog post](https://krokotsch.eu/research/2021/04/07/One-Eyed-Data-Scientist.html) about one of these problems in my field, predictive maintenance, and how it leads the whole line of research astray.

**So my question would be: how is the state of DL in your field that is not CV or NLP, and does it suffer from blindly copying approaches from them, too?**",computer vision natural language process get limelight deep learn world rightfully someone work another field fell deep learn hype often see problems direct equivalent cv nlp nevertheless people try adopt successful methods field sometimes lead result meaningful researchers take time adapt methods properly field often datasets much smaller sample part group e g step time series enough paper one part time series put train part test set write blog post url one problems field predictive maintenance lead whole line research astray question would state dl field cv nlp suffer blindly copy approach
vulnerablebeast,MachineLearning,1618978908.0,[P] Is it possible to use a loss function involving one input and multiple ground truths," Instead of optimizing say,

f(x) = y\_groundtruth\_i - y\_predicted\_i

Can we take an average of the ground truths, say

f(x) = ( (y\_groundtruth\_i+1 - y\_predicted\_i) + (y\_groundtruth\_i - y\_predicted\_i) ) / 2

Has this been done before? Thanks!",instead optimize say f x _groundtruth _i _predicted _ican take average grind truths sayf x _groundtruth _i 1 _predicted _i _groundtruth _i _predicted _i 2has thank
mihirkarkare,MachineLearning,1617327641.0,[D] Churn prediction using ML,"What would be a good way to define ""churn"" for analysing the behavior of customers who are using Revolving credit? -- 

If a credit card company wants to retain it's revolving credit customers, what would be a good strategy to attack this problem using ML? 

In this situation the term ""churn"" is a little difficult to define as not using Revolving for a month or two is completely natural, and not an indicator of if a customer has made up his/her mind to stop using Revolving credit.",would good way define churn analyse behavior customers use revolve credit credit card company want retain revolve credit customers would good strategy attack problem use ml situation term churn little difficult define use revolve month two completely natural indicator customer make mind stop use revolve credit
sl2085,MachineLearning,1616794332.0,[D] What type of machine learning can be used to solve timetable optimisation problems?,I can only really think of more traditional operational research related techniques such as linear programming but are there any other successful techniques people have used to solve timetable optimisation problems?,really think traditional operational research relate techniques linear program successful techniques people use solve timetable optimisation problems
Competitive-Net-5306,MachineLearning,1617836999.0,[P] About a ML agent for the game Slay the Spire that I've been making,"Hi. I've recently been working on making a setup where I would be able to train and test a machine learning agent for a TCG roguelike game called Slay the Spire. I'd like to share the process and my current status in that project so that I can receive feedback for improvement, and maybe help others who face problems similar to what I went through.

For this project, I had to solve the following problems.

1. Establish connection with the game process, or at least mimic something similar.
2. Find a way to design a model that is capable of making different types of decisions with different format of inputs depending on game state. The model must be able to handle different tasks ranging from selecting card reward to choosing whether throwing a potion at a particular enemy would be beneficial.
3. Find a way to properly reward that model.
4. Run as much games as possible in parallel at any given moment in time

The first problem proved to be a lot easier than I expected. There was already a mod called Communication Mod that practically solved the problem for me the moment I discovered it. It allowed connection and communication between the game process and any arbitrary external python script through stdin and stdout. I just fiddled with it to make it work, and afterwards I was good to go.

Second problem was a bit more tricky, and I'm not quite happy with how I solved it. I just divided the game into distinct situations that requires its own distinct format of input to make its own distinct type of decision. Then, I just made a neural network for every single one of them. I can't shake off the feeling that there might be a more efficient way of doing it, but I'm still a noob.

The way I solved the second problem made solving the third problem a bit more difficult. I wasn't training just one single net, but a collection of nets that together plays the game by trying to predict how beneficial playing some action in some situation would be. I was, however, able to solve it through the following process.

1. Add one extra net to a collection of nets, whose purpose is to predict how much score this AI would get by the end of the game, based on a perfect snapshot of current situation.
2. While the agent is playing a game, create a snapshot at set moments in the game and record every decisions made between each snapshots.
3. When the game is over, assign a label to every snapshots created during that game with a score equal to what it got by the game was over.
4. Train the snapshot net with the label.
5. Label every decision points with snapshot\_score(after) - snapshot\_score(before).
6. Train the nets with the label.

The fourth problem was the most tricky but most enjoyable one to solve. As we all know, more data is usually better when doing machine learning. My data was being generated during training by my agent actually playing the game, so if I could make the agent play more games simultaneously then I would get more data. To do that, however, I had to be able to launch and run multiple games at once.

I was able to achieve that by using virtual machines. Basically, I would create virtual machines as worker nodes that actually runs the game, then make those games communicate with their local python script which would establish socket connection with my main machine that actually runs the agent. But there's one catch. I didn't have enough computing resources to do that in large scale.

My solution to that was google compute engine in their cloud platform. Each instance of it was a virtual machine that can be connected to through the internet, so it was almost perfect for my use. However, putting a GPU to my instances was not practical due to cost, and without it the game run abysmally slow due to rendering issues.

I solved that by just making my own small mod for the game that applies a bytepatch to the code to disable rendering calls in its main loop. It was surprisingly easy due to the game being written in java. However, I still couldn't run the game on cli because it required a window manager. And I wanted to automate the process without having to open up the desktop for all the worker nodes and manually traversing through the gui to launch the game. 

The solution for that problem proved to be deceptively simple. Just attach a command to the gnome-session-properties such that when it starts it automatically fires the game. Then, use ssh connection to open up a new gnome session, which can be done in cli or by a bash script. When things required a reset I could just close the gnome sessions which also can be done via cli and start it back up again.

That's about it!",hi recently work make setup would able train test machine learn agent tcg roguelike game call slay spire like share process current status project receive feedback improvement maybe help others face problems similar go project solve follow problems 1 establish connection game process least mimic something similar 2 find way design model capable make different type decisions different format input depend game state model must able handle different task range select card reward choose whether throw potion particular enemy would beneficial 3 find way properly reward model 4 run much game possible parallel give moment timethe first problem prove lot easier expect already mod call communication mod practically solve problem moment discover allow connection communication game process arbitrary external python script stdin stdout fiddle make work afterwards good go second problem bite tricky quite happy solve divide game distinct situations require distinct format input make distinct type decision make neural network every single one shake feel might efficient way still noob way solve second problem make solve third problem bite difficult train one single net collection net together play game try predict beneficial play action situation would however able solve follow process 1 add one extra net collection net whose purpose predict much score ai would get end game base perfect snapshot current situation 2 agent play game create snapshot set moments game record every decisions make snapshots 3 game assign label every snapshots create game score equal get game 4 train snapshot net label 5 label every decision point snapshot _score snapshot _score 6 train net label fourth problem tricky enjoyable one solve know data usually better machine learn data generate train agent actually play game could make agent play game simultaneously would get data however able launch run multiple game able achieve use virtual machine basically would create virtual machine worker nod actually run game make game communicate local python script would establish socket connection main machine actually run agent one catch enough compute resources large scale solution google compute engine cloud platform instance virtual machine connect internet almost perfect use however put gpu instance practical due cost without game run abysmally slow due render issue solve make small mod game apply bytepatch code disable render call main loop surprisingly easy due game write java however still run game cli require window manager want automate process without open desktop worker nod manually traverse gui launch game solution problem prove deceptively simple attach command gnome session properties start automatically fire game use ssh connection open new gnome session cli bash script things require reset could close gnome sessions also via cli start back
StandardDull3128,MachineLearning,1617116497.0,[Discussion] Can I use CNNs to solve this problem?,"What does your intuition/experience/expertise tell you? Would I be able to train a CNN to solve a binary classification problem described below and expect a high accuracy (let's say >90%)?

**The problem:** Predicting whether an image contains one type of texture (class 0) or it contains several textures and clear borders among them (class 1).

**The data:** The data is high resolution (5 centimeters / pixel) agricultural imagery obtained via remote sensing (drone imaging). So pictures depicting different crops, weeds, roads, soil, bushes, etc. I would feed the model square images of 256x256px (or 128x128px).

**Examples of images:** (""neg"" is class 0 and ""pos"" is class 1)[https://imgur.com/a/gnKbncA](https://imgur.com/a/gnKbncA)",intuition experience expertise tell would able train cnn solve binary classification problem describe expect high accuracy let say 90 problem predict whether image contain one type texture class 0 contain several textures clear border among class 1 data data high resolution 5 centimeters pixel agricultural imagery obtain via remote sense drone image picture depict different crop weed roads soil bush etc would fee model square image 256x256px 128x128px examples image neg class nmbr pos class 1 url
xiikjuy,MachineLearning,1618403267.0,[D] Is captioning a reasonable way towards explainable AI?,"Take video classification for example, someone may visualize spatial(-temporal) attention mask for a way explaining what AI focuses/sees. If I add a video captaining function on top of a video classification model, thus for each test video there is not only a class prediction, but associated sentences. And we can check the sentences generated to have a sense what's in AI's mind to make such prediction .  Does that make sense? or any related published works on this kind of idea?",take video classification example someone may visualize spatial temporal attention mask way explain ai focus see add video captain function top video classification model thus test video class prediction associate sentence check sentence generate sense ai mind make prediction make sense relate publish work kind idea
mfilion,MachineLearning,1618933040.0,[Project] Continuous 3D Hand Pose Tracking using Machine Learning & Monado OpenXR,"As part of a project backed by INVEST-AI, a program managed by IVADO  Labs, Collabora has developed a multi-stage neural network-based  solution that accurately locates and tracks the hands despite complex  background noise and occlusion between hands. Our system estimates 2D  and 3D joint locations without any depth information. Collabora is also currently working on integrating it into the Monado XR codebase, so it can be  used out-of-the-box with different devices.  
[https://www.collabora.com/news-and-blog/blog/2021/04/20/continuous-3d-hand-pose-tracking-using-machine-learning-and-monado-openxr/](https://www.collabora.com/news-and-blog/blog/2021/04/20/continuous-3d-hand-pose-tracking-using-machine-learning-and-monado-openxr/)",part project back invest ai program manage ivado labs collabora develop multi stage neural network base solution accurately locate track hand despite complex background noise occlusion hand system estimate 2d 3d joint locations without depth information collabora also currently work integrate monado xr codebase use box different devices url
bjourne-ml,MachineLearning,1617774303.0,[P] Music generation using tracker music in MOD format,"Link: https://modmusicgen.com/

Hi all, I've been working on a project for generating music using neural networks trained on tracker music in MOD format. The tracker music is converted to a simple internal format to make it amendable for training and then converted to MIDI so it sounds more like piano music than tracker music.

I would very much appreciate it if you have some time over to fill out the preference survey. :) Also if you have any questions about the project I can try and answer them in this thread.",link url work project generate music use neural network train tracker music mod format tracker music convert simple internal format make amendable train convert midi sound like piano music tracker music would much appreciate time fill preference survey also question project try answer thread
techsucker,MachineLearning,1617636798.0,[R] Researchers From the University of Toronto and LG AI Research Develop ‘Explainable’ Artificial Intelligence (AI) Algorithm,"A team of researchers from the University of Toronto and LG AI Research have developed an explainable artificial intelligence (XAI) algorithm. The algorithm can help identify and eliminate defects in display screens.

The algorithm outperformed comparable approaches on industry benchmarks and was developed through an ongoing AI research collaboration between LG and the University of Toronto.

According to the researchers, the XAI algorithm could be applied in other fields, primarily those which require details into how machine learning makes its decisions, including data interpretation from medical scans.

Summary: [https://www.marktechpost.com/2021/04/05/researchers-from-the-university-of-toronto-and-lg-ai-research-develop-explainable-artificial-intelligence-ai-algorithm/](https://www.marktechpost.com/2021/04/05/researchers-from-the-university-of-toronto-and-lg-ai-research-develop-explainable-artificial-intelligence-ai-algorithm/) 

Paper: https://arxiv.org/pdf/2010.00672.pdf",team researchers university toronto lg ai research develop explainable artificial intelligence xai algorithm algorithm help identify eliminate defect display screen algorithm outperform comparable approach industry benchmarks develop ongoing ai research collaboration lg university toronto accord researchers xai algorithm could apply field primarily require detail machine learn make decisions include data interpretation medical scan summary url paper url
NeitherBandicoot,MachineLearning,1616955264.0,[D] Why some major papers in ML aren't peer-reviewed?,"I'm a little bit of an outsider (coming from a life science field) and I noticed that some major publications in the field of ML aren't peer-reviewed, which strikes me as very unusual given that in my field a paper that is not peer-reviewed is as good as a blog post. I understand the need to publish asap because research in the field is moving so fast, but preprints exists for that, so that shouldn't be the reason why.  Could somebody explain to me why peer-review doesn't appear to be prioritized in ML research?  I'm not trying to criticize the way it works, I'm just puzzled and confused because it's not what I'm used to see.


Edit: took off the example because it wasn't representative of my question as the comments suggested.",little bite outsider come life science field notice major publications field ml peer review strike unusual give field paper peer review good blog post understand need publish asap research field move fast preprints exist reason could somebody explain peer review appear prioritize ml research try criticize way work puzzle confuse use see edit take example representative question comment suggest
upulbandara,MachineLearning,1619204466.0,[D] How to extend a text classification ML model to work with more than one language?,"We are using (in production) an ML for text classification. We trained our model using some custom English text corpus. Currently, the model is working acceptable level of accuracy for our purpose. Now we want to extend it to handle French language as well. We are planning to investigate the following two approaches.

1. We have a French-language corpus. Therefore, we would like to train a new model for handling French text.
2. Use the same model trained with English corpus. But use a third-party language translation service (such as Google Translator) to translate French text to English before inputting it into the ML model.

So I would like to know your thoughts regarding these two approaches.",use production ml text classification train model use custom english text corpus currently model work acceptable level accuracy purpose want extend handle french language well plan investigate follow two approach 1 french language corpus therefore would like train new model handle french text 2 use model train english corpus use third party language translation service google translator translate french text english inputting ml model would like know thoughts regard two approach
MediocreMinimum,MachineLearning,1617017494.0,[D] What will the major ML research trends be in the 2020s?,"We've entered a new decade -- hurrah!

**What do you think the next 10 years will bring in ML research?** **What conventionally accepted trend do you think will** ***not*** **happen?**

e.g...

Will deep learning continue to *eat everything*? Will multi-task multi-domain learning make few-shot learning available for most domains? (Or is deep learning on the slow end of the sigmoid curve now?)

Will safe, ethical, explainable AI rise, or is that hogwash?

Will advances decouple from compute power?

Will Gary Marcus and Judea Pearl win out in the symbolic/structural/causal war against deep learning?

Are there still major breakthroughs in language? Do we just finetune GPT-3?

Will we make big breakthroughs in theory and fundamental ML? Or is this the decade of *application*? (Healthcare will finally deploy models that beat logistic regression!)",enter new decade hurrah think next nmbr years bring ml research conventionally accept trend think happen e g deep learn continue eat everything multi task multi domain learn make shoot learn available domains deep learn slow end sigmoid curve safe ethical explainable ai rise hogwash advance decouple compute power gary marcus judea pearl win symbolic structural causal war deep learn still major breakthroughs language finetune gpt 3 make big breakthroughs theory fundamental ml decade application healthcare finally deploy model beat logistic regression
PhYsIcS-GUY227,MachineLearning,1619445711.0,"[P] Integrating Git, DVC, and MLflow into one","Hey r/MachineLearning. I'm one of the creators of [DAGsHub](https://dagshub.com), and I wanted to share something cool that we've been working on. DVC ([dvc.org](https://dvc.org)) and MLflow ([mlflow.org](https://mlflow.org)) are two open-source projects that are very widely adopted, each for its own specialty. DVC excels at data versioning, and MLflow is used for many things (it's actually multiple tools combined into one), but mainly for its experiment tracking capabilities. Both tools have a built-in tradeoff – since they are open source, setting up storage for DVC and a central tracking server for MLflow can be a pain – requiring you to create cloud accounts, add permissions, and more.

DAGsHub is already integrated with DVC, in the sense that whenever you create a project, it comes with a free, built-in, DVC remote.

Since last week, you also get a free MLflow server, which means you can log experiments directly to DAGsHub and share it with your team or colleagues.

&#x200B;

https://i.redd.it/edvlu237xiv61.gif

Why I think this is awesome:

1. Zero setup – add your MLflow remote server URI and just log experiments
2. Access control built-in – if you have a team and some people need access only to view the experiments but not to log new ones, you can easily control that
3. Better UI for comparison – one of the complaints MLflow users have had was about the inability to compare runs across experiments in MLflow, with DAGsHub that is easily possible as all runs appear in a single list, which you can then filter to fit only a single experiment.
4. Integrating MLflow and DVC – a lot of people are working with both systems and building ad-hoc systems to integrate them, but with this integration, you can create a project that was built for this type of work, integrated with all the tools you need.

Here's a [more detailed blog post](https://dagshub.com/blog/launching-dagshub-integration-with-databricks-mlflow/) from the engineer that built this. I'd love to hear your thoughts about it.",hey r machinelearning one creators dagshub url want share something cool work dvc dvc org url mlflow mlflow org url two open source project widely adopt specialty dvc excel data versioning mlflow use many things actually multiple tool combine one mainly experiment track capabilities tool build tradeoff since open source set storage dvc central track server mlflow pain require create cloud account add permissions dagshub already integrate dvc sense whenever create project come free build dvc remote since last week also get free mlflow server mean log experiment directly dagshub share team colleagues x200b url think awesome 1 zero setup add mlflow remote server uri log experiments2 access control build team people need access view experiment log new ones easily control that3 better ui comparison one complaints mlflow users inability compare run across experiment mlflow dagshub easily possible run appear single list filter fit single experiment 4 integrate mlflow dvc lot people work systems build ad hoc systems integrate integration create project build type work integrate tool need detail blog post url engineer build love hear thoughts
mildlyoverfitted,MachineLearning,1618130003.0,[P] Growing Neural Cellular Automata - Implementation and explanation,"Hey there!

I made a video where I try to explain and implement the article ""Growing neural cellular automata"". It is a niche topic, however, I find it fascinating. Hope some of you could find it useful.

&#x200B;

Original article: [https://distill.pub/2020/growing-ca/](https://distill.pub/2020/growing-ca/)

My video: [https://youtu.be/21ACbWoF2Oo](https://youtu.be/21ACbWoF2Oo)",hey make video try explain implement article grow neural cellular automata niche topic however find fascinate hope could find useful x200b original article url video url
temakone,MachineLearning,1617020121.0,[R] Swin Transformer: New SOTA backbone for Computer Vision🔥,"**Swin Transformer: New SOTA backbone for Computer Vision** 🔥*MS Research Asia*

# 👉 What?

New vision Transformer architecture called Swin Transformer that can serve as a backbone in computer vision instead of CNNs.

# ❓Why?

There are two main problems with the usage of Transformers for computer vision.

1. Existing Transformer-based models have tokens of a fixed scale. However, in contrast to the word tokens, visual elements can be different in scale (e.g. objects of varying sizes on the scene)
2. Regular self-attention requires quadratic of the image size number of operations, limiting applications in computer vision where high resolution is necessary (e.g., instance segmentation).

# 🥊 The main ideas of the Swin Transformers:

1. **Hierarchical feature maps** where at each level of hierarchy Self-attention is applied within local non-overlapping windows. The size of the windows is progressively increased with the network depth (inspired by CNNs). This enables building architectures similar to feature pyramid networks (FPN) or U-Net for dense pixel-level tasks.
2. **Window-based Self-attention** reduces the computational overhead.

# ⚙️ Overall Architecture consists of repeating the following blocks:

\- Split RGB image into non-overlapping patches (tokens).

\- Apply MLP to translate raw features into an arbitrary dimension.

\- Apply 2 consecutive Swin Transformer blocks with Window self-attention: **both blocks have the same window size, but the second block uses shifted by \`patch\_size/2\` windows which allows information flow between non-overlapping windows**.

\- Downsampling layer: Reduce the number of tokens by merging neighboring patches in a 2x2 window, and double the feature depth.

&#x200B;

https://preview.redd.it/xtjhyflalyp61.png?width=1920&format=png&auto=webp&s=b0f8539b4e1779ba8263281e4ff2974562858c0d

&#x200B;

https://preview.redd.it/z95z4ycclyp61.png?width=1010&format=png&auto=webp&s=0e27b6b8fb511da3be3b9647da4bacd2b8411dc3

# 🦾 Results

\+ **Outperforms SOTA by a significant margin on COCO segmentation and detection tasks and ADE20K segmentation.**

\+ **Comparable accuracy to the EfficientNet** family on ImageNet-1K classification, while being faster.

&#x200B;

https://preview.redd.it/giw5nz4dlyp61.jpg?width=1920&format=pjpg&auto=webp&s=d8eb93e37dd44ee75e7476c584d41fb14d3b760a

# 👌Conclusion

While Transformers are super flexible, researchers start to **inject in Transformers inductive biases similar to those in CNNs**, e.g., local connectivity, feature hierarchies. And this seems to help tremendously!

&#x200B;

📝 Paper [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)

⚒ Code (promissed soon) [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)

🌐 TL;DR blogpost [https://xzcodes.github.io/posts/paper-review-swin-transformer](https://xzcodes.github.io/posts/paper-review-swin-transformer)

\--

👉  Join my Telegram channel [""Gradient Dude""](https://t.me/gradientdude) not to miss the latest posts like this [https://t.me/gradientdude](https://t.me/gradientdude)",swin transformer new sota backbone computer vision ms research asia new vision transformer architecture call swin transformer serve backbone computer vision instead cnns two main problems usage transformers computer vision 1 exist transformer base model tokens fix scale however contrast word tokens visual elements different scale e g object vary size scene 2 regular self attention require quadratic image size number operations limit applications computer vision high resolution necessary e g instance segmentation main ideas swin transformers 1 hierarchical feature map level hierarchy self attention apply within local non overlap windows size windows progressively increase network depth inspire cnns enable build architectures similar feature pyramid network fpn u net dense pixel level task 2 window base self attention reduce computational overhead overall architecture consist repeat follow block split rgb image non overlap patch tokens apply mlp translate raw feature arbitrary dimension apply nmbr consecutive swin transformer block window self attention block window size second block use shift patch _size 2 windows allow information flow non overlap windows downsampling layer reduce number tokens merge neighbor patch 2x2 window double feature depth x200b url result outperform sota significant margin coco segmentation detection task ade20k segmentation comparable accuracy efficientnet family imagenet 1k classification faster x200b url conclusionwhile transformers super flexible researchers start inject transformers inductive bias similar cnns e g local connectivity feature hierarchies seem help tremendously x200b paper url code promissed soon url tl dr blogpost url join telegram channel gradient dude url miss latest post like url
Kaleidophon,MachineLearning,1616769467.0,[P] deep-significance: Easy and Better Significance Testing for Deep Neural Networks (link below),"Hey!  


Recently, I have become somewhat frustrated at ML / DL papers highlighting scores only stemming from a single run in result tables, and claiming that an approach is superior when it only outperforms others marginally. This is why I re-implemented, tested and packaged a statistical significance test proposed by Dror et al. (2019) that is specifically tailored towards neural networks. I also added information about statistical significance testing and how to apply the mentioned test in the most common scenarios faced by ML practitioners!

[https://github.com/Kaleidophon/deep-significance](https://github.com/Kaleidophon/deep-significance)

I'd be very happy to receive some feedback from the community here and improve this further and help move the field forward :-)",hey recently become somewhat frustrate ml dl paper highlight score stem single run result table claim approach superior outperform others marginally implement test package statistical significance test propose dror et al 2019 specifically tailor towards neural network also add information statistical significance test apply mention test common scenarios face ml practitioners url happy receive feedback community improve help move field forward
cloud-native,MachineLearning,1616368252.0,[D] How would you migrate a DS team from HPC cluster to the cloud,"I wanted to ask if you could point me to some resources on how to successfully move a DS team to the cloud, or if you could share experiences from your teams / companies.

I'm working with a team of \~20 old school analysis folks (stats & natural science backgrounds), and we need to move out of our on-prem setup to Microsoft Azure.

We are a research team (not ML research, but domain specific research), and we are all used to our HPC: 

* we can ssh into it and connect VSCode to it.
* we can easily use dask to run distributed processing (via SLURM)
* we have a unix filesystem
* etc.

What is the best cloud setup for such teams? What do people do in research organizations in big tech companies?

* Give the DS team members freedom to run any compute instances they want?
* Run a pool of compute instances that the DS team can ssh into?
* Run a Kubernetes cluster using AKS?
* Use the Azure ML offering, even though we don't do much ML?
* Something else? We rely heavily on CLI tools and VSCode, so Jupyter-only solution is not an option for us.

Any pointers would be greatly appreciated.",want ask could point resources successfully move ds team cloud could share experience team company work team 20 old school analysis folks stats natural science background need move prem setup microsoft azure research team ml research domain specific research use hpc ssh connect vscode easily use dask run distribute process via slurm unix filesystem etc best cloud setup team people research organizations big tech company give ds team members freedom run compute instance want run pool compute instance ds team ssh run kubernetes cluster use aks use azure ml offer even though much ml something else rely heavily cli tool vscode jupyter solution option us pointers would greatly appreciate
yusuf-bengio,MachineLearning,1616767299.0,[D] Dilemma: Mathematically wrong ICML submission got extremely good reviews,"I am tangentially involved in a ICML submission from a research group at a different institute. After the initial reviews, I realized that the main theorem, which the work is based on, is mathematically objectively wrong.

For anyone working in this subfield finding a counterexample that violates the theorem is pretty straightforward. However, none of the reviewers found an error with the theory, even praising the paper with high scores (definitely an accept).

As I was involved in this work only at the very beginning, e.g., for laying out the ideas and roadmap, I never thoroughly read the theoretical parts of the paper before the submission.

The proof exploits subtle differences of two definitions of a concept in older vs newer literature. Moreover, the final paper use of grandiose mathematical notation, making the error harder to spot. In my opinion, it even appears as if the choices of the mathematical writing style and the use of the conflicting definitions were made deliberately.

The problem though is that if I intervene now, the PI (who didn't read beyond the abstract) and the PhD student responsible for the proof (and known for having ""narcissistic"" tendencies) will be pissed. I don't want to risk my career, because the PI is quite a bigshot in the field and the student is extremely well connected to a FANG company (""best friends"" with a top-5 researcher there).

I was thinking that if I am quite about it, the worst thing that can happen is other researchers realizing the error and publishing a ""counter"" paper to disprove ours.

Furthermore, I have the feeling that I can't be the only one having this particular kind of dilemma. Does anyone have experiences with situations like mine?

**Update**:

Thank you for all the feedback on my situation.

As many suggested, I wrote up and discussed the counterexample with the co-authors. During our discussion I realized three things:

1. Coming up the the counterexample and seeing how it violates the main claims is not as straightforward as I original thought. It took even the student who wrote the proof quite a while to realize that there might be some issues.
2. The student and PI are both still convinced about the correctness of the main theoretical results, citing the good reviews as additional evidence that the claims are correct. Their main argument is that the proof uses primarily existing theorems from literature and (correct) reformulation of mathematical expressions. Thus, if there is something wrong with the claims, it must come from the theorems proven in the literature and be already wrong in there.
3. The submission will not be withdrawn from ICML, nor do the co-authors intend to change the paper.",tangentially involve icml submission research group different institute initial review realize main theorem work base mathematically objectively wrong anyone work subfield find counterexample violate theorem pretty straightforward however none reviewers find error theory even praise paper high score definitely accept involve work begin e g lay ideas roadmap never thoroughly read theoretical part paper submission proof exploit subtle differences two definitions concept older vs newer literature moreover final paper use grandiose mathematical notation make error harder spot opinion even appear choices mathematical write style use conflict definitions make deliberately problem though intervene pi read beyond abstract phd student responsible proof know narcissistic tendencies piss want risk career pi quite bigshot field student extremely well connect fang company best friends top 5 researcher think quite worst thing happen researchers realize error publish counter paper disprove furthermore feel one particular kind dilemma anyone experience situations like mine update thank feedback situation many suggest write discuss counterexample co author discussion realize three things 1 come counterexample see violate main claim straightforward original think take even student write proof quite realize might issue 2 student pi still convince correctness main theoretical result cite good review additional evidence claim correct main argument proof use primarily exist theorems literature correct reformulation mathematical expressions thus something wrong claim must come theorems prove literature already wrong 3 submission withdraw icml co author intend change paper
Rokossowsky,MachineLearning,1618739918.0,[D] In what order should one read self-driving related papers?,"Hi,

I've read a few books on DL and self driving. I'd like to delve into papers. I've found this list: [https://paperswithcode.com/task/autonomous-driving](https://paperswithcode.com/task/autonomous-driving) .

Should I just go through the top rated first or would you guy suggest some other, more logical order of understanding these things?

Thanks

Rokossowski",hi read book dl self drive like delve paper find list url go top rat first would guy suggest logical order understand things thanksrokossowski
kakushka123,MachineLearning,1618324760.0,[D] How is Tesla autopilot trained?,"I listened to this podcast, however I wasn't sure that I understood correctly. To my understanding, they have a big NN (does anyone know the architecture?) trained (from scratch before every release?) on a set of milions of image (each handchosen by humans?) that is evolving all the time, e.g images are taken off and taken on based on interaction of the model with the fleet of cars like humans corrections etc. Is that correct?",listen podcast however sure understand correctly understand big nn anyone know architecture train scratch every release set milions image handchosen humans evolve time e g image take take base interaction model fleet cars like humans corrections etc correct
Sirisian,MachineLearning,1619820302.0,[R] DINO and PAWS: Advancing the state of the art in computer vision with self-supervised Transformers,"https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training

https://arxiv.org/abs/2104.14294  
https://github.com/facebookresearch/dino  
https://arxiv.org/abs/2104.13963  
https://github.com/facebookresearch/suncet

The DINO research shows their ""model automatically learns class-specific features leading to unsupervised object segmentation.""

> PAWS is a method for semi-supervised learning that builds on the principles of self-supervised distance-metric learning. PAWS pre-trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled image are assigned similar pseudo-labels.",url url url url dino research show model automatically learn class specific feature lead unsupervised object segmentation paw method semi supervise learn build principles self supervise distance metric learn paw pre train model minimize consistency loss ensure different view unlabeled image assign similar pseudo label
TheHentaiSama,MachineLearning,1620115041.0,[Discussion] Model to predict a class for a signal based on simpler signals," 

Hello everyone,

I'm working on something and I struggle finding a good solution. To make it simple, let's say I have an original signal which is composed of simpler signals. I can make predictions for these simpler signals using models such as Random Forest Classifier and now I need to make predictions for the original signal. The problem is that my signal can be composed of any number of simpler signals and they don't all have the same impact on the final result. I've tried the simple method which consists in a majority vote (making a prediction for each simple signal and then decide the class of the original signal by majority vote) but the results are bad.

So, my question is, does anyone of you know about a model that can handle such a task or do you know about some litterature that could help me ? Thanks in advance !",hello everyone work something struggle find good solution make simple let say original signal compose simpler signal make predictions simpler signal use model random forest classifier need make predictions original signal problem signal compose number simpler signal impact final result try simple method consist majority vote make prediction simple signal decide class original signal majority vote result bad question anyone know model handle task know litterature could help thank advance
schienal,MachineLearning,1618586669.0,[N] Probabilistic ML @ Tübingen is restarting!,"[https://www.youtube.com/watch?v=UbaVGD4Lfis&list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd](https://www.youtube.com/watch?v=UbaVGD4Lfis&list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)

Is anyone thinking of starting a discord server to study along at the course's pace?",url anyone think start discord server study along course pace
NominalNom,MachineLearning,1619809526.0,[D] A recent history of my pointless stare down with Nvidia which I lost,"Just a quick, comical summary of my experience trying to get a GPU since just prior to the launch of the Ampere cards.

I work in film post production and I have no prior deep learning coding experience, but I wanted a GPU with a lot of memory to process high res imagery with an existing TensorFlow-based open source toolset that I could just about figure out how to use with my small amount of Python experience. I did secure a Titan RTX off Amazon(!) at retail price, but I decided to return it when I found out about the imminent release of the Ampere cards because $1500 sounded a lot better than $2500.

Of course, the 3090 is going for around the same price as the 2.5k rrp I paid for the RTX Titan. I was very naive at that exact moment about the fact that crypto mining was going to go back into full swing, although I was warned by someone on this very sub. I did expect Eth proof of stake to cancel some of that out, but we aren't there yet.

Besides availability issues, there was also the size of the 3090 which Nvidia seem to have intentionaly made huge so it can't fit in a typical deep learning rig or graphics workstation. No problem, I'll get a third party blower card - if I can find one in stock. Whoops, now Nvidia killed them all!

Alright, now I'll look at one of those rinky-dink third party RGB gamer cards that are sufficiently low profile - I only need one card with 24GB VRAM. Okay, they are all sold out forever and if you can get one, it may be as expensive as the previous Titan.

So maybe it's best to get an A5000 or A6000, because tbh it seems they are more likely to go for RRP which makes them not a huge step up from the inflated 3090 prices and they are better supported on Linux IMO.

I have just found out about the LHR cards though, that will be shipping soon. But also that there may not be a 3090 LHR, so it doesn't seem like supply will necessarily improve.

There is now a proprietary GUI-based toolset by the same developers which is now using PyTorch as the back end. But it's still recommended that 24GB is where you really want to be at memory-wise without needing to slice up the input images too much, so this issue is not going anywhere. I had managed to get a 2060 Super with 8GB as an interim card which is really the bare minimum. It's also comical how much those cards are going for now, so I'm glad I snagged one of the last ones at Best Buy for $400 RRP last year.",quick comical summary experience try get gpu since prior launch ampere card work film post production prior deep learn cod experience want gpu lot memory process high res imagery exist tensorflow base open source toolset could figure use small amount python experience secure titan rtx amazon retail price decide return find imminent release ampere card 1500 sound lot better 2500 course nmbr go around price 2 5k rrp pay rtx titan naive exact moment fact crypto mine go go back full swing although warn someone sub expect eth proof stake cancel yet besides availability issue also size nmbr nvidia seem intentionaly make huge fit typical deep learn rig graphics workstation problem get third party blower card find one stock whoop nvidia kill alright look one rinky dink third party rgb gamer card sufficiently low profile need one card 24gb vram okay sell forever get one may expensive previous titan maybe best get a5000 a6000 tbh seem likely go rrp make huge step inflate nmbr price better support linux imo find lhr card though ship soon also may nmbr lhr seem like supply necessarily improve proprietary gui base toolset developers use pytorch back end still recommend 24gb really want memory wise without need slice input image much issue go anywhere manage get nmbr super 8gb interim card really bare minimum also comical much card go glad snag one last ones best buy 400 rrp last year
vishnu_subramaniann,MachineLearning,1616496654.0,"[P] Jarvislabs.ai - An Affordable GPU Cloud with Fast launch, Pause and Resume. Scale GPUs post creation. A100/RTX6K/RTX5K","For the last few years, I have been learning and practicing Deep Learning. Participated in several Kaggle competitions and won few medals. During all these years, I tried several cloud platforms and on-premise systems. Some of them offered simplicity, flexibility, and affordability. But very few to none offered all of these in one platform.

After struggling with different platforms, I know what I would need as a DL researcher. That gave birth to [jarvislabs.ai](https://jarvislabs.ai) with the aim of being simple and affordable. I along with my friends started working on this project a year back. Due to Covid, executing the project became more challenging. As first-time entrepreneurs, we underestimated the complexity of the problem at hand but with persistence, we were able to launch a beta version of the product in December 2020.

With some of the amazing feedback from our early adopters, we have been able to make the product smoother. We would love to invite you all to come and try the platform.

# Features

1. 1 click Jupyter Lab < \[30 seconds\]
2. Pause the instance and Resume from where you left.
3. SSH to the instance.
4. Scale GPUs, storage and change GPU type on resume.
5. Auto-Pause using jarviscloud.pause() in your code, so you can catch up some good night’s sleep while your model trains.
6. Pay per usage – Minute Billing \[After first 15 minutes\]
7. Competitive pricing \[Lowest to our Knowledge\].

&#x200B;

## Pricing

|GPU Type|GPU RAM|Price -$/hr|
|:-|:-|:-|
|RTX 5000|16 GB|0.49|
|RTX 6000|24 GB|0.99|
|A100|40 GB|2.39|

&#x200B;

# Talk to us

We will be happy to assist you in spinning your first instance and many more. You can use one of these platforms to reach us.

1. Chat option on cloud.jarvislabs.ai
2. Email us - [hello@jarvislabs.ai](mailto:hello@jarvislabs.ai)
3. Comment here.

We have come a long way, but we understand that a lot more has to be done. We have listed down all the upcoming product features [here](https://github.com/jarvislabsai/JarvisCloud-ChitChat/discussions/). Deep learning and AI are evolving and how we would use the cloud platforms could evolve in the coming years. Understanding this, we develop in the open by constantly keeping in touch with our users.

Please help us in shaping [Jarvislabs.ai](https://Jarvislabs.ai) with any valuable suggestions/feedback.",last years learn practice deep learn participate several kaggle competitions win medals years try several cloud platforms premise systems offer simplicity flexibility affordability none offer one platform struggle different platforms know would need dl researcher give birth jarvislabs ai url aim simple affordable along friends start work project year back due covid execute project become challenge first time entrepreneurs underestimate complexity problem hand persistence able launch beta version product december 2020 amaze feedback early adopters able make product smoother would love invite come try platform features1 nmbr click jupyter lab 30 second 2 pause instance resume leave 3 ssh instance 4 scale gpus storage change gpu type resume 5 auto pause use jarviscloud pause code catch good nights sleep model train 6 pay per usage minute bill first nmbr minutes 7 competitive price lowest knowledge x200b price gpu type gpu ram price hr rtx 5000 16 gb 0 49 rtx 6000 24 gb 0 99 a100 40 gb 2 39 x200b talk uswe happy assist spin first instance many use one platforms reach us 1 chat option cloud jarvislabs ai2 email us hello jarvislabs ai mailto hello jarvislabs ai 3 comment come long way understand lot list upcoming product feature url deep learn ai evolve would use cloud platforms could evolve come years understand develop open constantly keep touch users please help us shape jarvislabs ai url valuable suggestions feedback
olegranmo,MachineLearning,1618557871.0,[Research] Tsetlin Machine Interpretability and Accuracy in NLP Significantly Boosted Using GloVe Synonyms,"&#x200B;

[Tsetlin Machine Boolean Bag-of-Words Boosted by GloVe Synonyms](https://preview.redd.it/6js1wbb5lht61.png?width=1678&format=png&auto=webp&s=35d9f2bfe2abaad7af50bc5f337c8d633f68c053)

By using GloVe to obtain synonyms, we have enhanced the Boolean Bag-of-Words used by Tsetlin Machines in NLP. The added synonyms help the Tsetlin Machine produce fewer and semantically more powerful rules, while boosting accuracy by up to 4%. This makes the transparent and interpretable rules more competitive with deep learning-based NLP models.

[https://arxiv.org/abs/2104.06901](https://arxiv.org/abs/2104.06901)",x200b tsetlin machine boolean bag word boost glove synonyms url use glove obtain synonyms enhance boolean bag word use tsetlin machine nlp add synonyms help tsetlin machine produce fewer semantically powerful rule boost accuracy 4 make transparent interpretable rule competitive deep learn base nlp model url
cents_less,MachineLearning,1618511530.0,[P] Announcing Feast 0.10: The simplest way to serve features in production,"Hey folks!

Over the last two years we’ve been working on an open source feature store called Feast. The idea behind Feast is that it helps you to operationalize your features. Meaning it helps you build training datasets from your offline features, it helps you load features into an online store in a structured way, and it provides low latency access to your features in production.

The original design of Feast was heavy weight. You needed to run a big stack on Kubernetes with Spark. We’ve been chatting to a bunch of users and the one thing we kept hearing was that they saw value in Feast for productionizing their data, but it was too heavy weight to own.

So we’ve put a lot of energy towards really simplifying Feast. We’ve made it as easy as a pip install. You can now run Feast locally if you’re just getting started with development/testing, but you can also deploy it to cloud providers like GCP if you want something more scalable.

We’d love for you to download it and try it out in your own projects. We’ve got a slack workspace you can join to get involved and ask questions. Also we wouldn’t hate a star on Github either 😉

\---

* Announcement: [https://feast.dev/blog/feast-0-10-announcement/](https://feast.dev/blog/feast-0-10-announcement/) 
* Web site: [https://feast.dev](https://feast.dev)
* Github: [https://github.com/feast-dev/feast](https://github.com/feast-dev/feast)
* Slack: [https://slack.feast.dev](https://slack.feast.dev)",hey folks last two years weve work open source feature store call feast idea behind feast help operationalize feature mean help build train datasets offline feature help load feature online store structure way provide low latency access feature production original design feast heavy weight need run big stack kubernetes spark weve chat bunch users one thing keep hear saw value feast productionizing data heavy weight weve put lot energy towards really simplify feast weve make easy pip install run feast locally get start development test also deploy cloud providers like gcp want something scalable wed love download try project weve get slack workspace join get involve ask question also hate star github either announcement url web site url github url slack url
ottawalanguages,MachineLearning,1618970325.0,"[D] ""no free lunch"" vs neural networks","I read that there is a theorem called ""the no free lunch theorem"" that states : there is no single algorithm that is better than all other algorithms. This is somewhat obvious: complex algorithms should perform better on complex problems, and simpler algorithms should perform better on simpler problems. 

This being said, why have neural network based algorithms been accepted as the main type of algorithms for solving complex real-world problems? Apart from the simple answer, that they ""from observation, they simply perform better on these problems"" - if we were to look at neural networks (e.g. mlp, lstm, cnn), reffering to the mathematical properties of neural networks, how/what could we attribute their success to? Why do we use neural  networks instead of regression models? From a certain prespective, are neural networks defying the ""no free lunch theorem""?",read theorem call free lunch theorem state single algorithm better algorithms somewhat obvious complex algorithms perform better complex problems simpler algorithms perform better simpler problems say neural network base algorithms accept main type algorithms solve complex real world problems apart simple answer observation simply perform better problems look neural network e g mlp lstm cnn reffering mathematical properties neural network could attribute success use neural network instead regression model certain prespective neural network defy free lunch theorem
Andrew_the_giant,MachineLearning,1618067692.0,[D] SARIMAX - Achieving Stationarity first?," 

Newbie at forecasting here, and havn't been able to find the answer to this question as of yet.

This question is in relation to ARIMA type models only.

If I use a hyper-parameter grid search to find the best PDQ, pdq parameters do I need to difference the dataset first? Or by using the parameters the model will achieve stationarity (assuming I use the correct parameters)?",newbie forecast havn able find answer question yet question relation arima type model use hyper parameter grid search find best pdq pdq parameters need difference dataset first use parameters model achieve stationarity assume use correct parameters
DaBeastGeek,MachineLearning,1616594581.0,"[D] What are some non transformer based, NLP models?","Just curious if there’s any well regarded models out there that don’t rely on transformers. I’m looking for something preferably small parameter wise, to test on an edge device. 

Lots of the state of the art things I’m seeing are huge, even the smallest model, Albert has something like 30 million Params. 

Any help is very much appreciated!",curious theres well regard model rely transformers im look something preferably small parameter wise test edge device lot state art things im see huge even smallest model albert something like nmbr million params help much appreciate
PM_ME_UR_FAV_THINGS,MachineLearning,1619476926.0,"[D] Open Source Nudity/NSFW Classification Review, any suggestions?","Hello!

I am doing a review of Open Source models for Nudity/NSFW content classification and/or labelling. Does anyone have recommendations for projects that are worth checking out? I have Google'd already but I was hoping you all might know of some of those that are more niche.",hello review open source model nudity nsfw content classification label anyone recommendations project worth check google already hop might know niche
du_dt,MachineLearning,1618371964.0,[D] Internship at Huawei - your experience?,"Have you had an internship in Huawei (in particular, ML research positions)? What was your experience? Would you recommend it to a friend?

Huawei is actively expanding lately, in particular, in Canada they now are in top-20 by R&D spending ([link](https://www.newswire.ca/news-releases/huawei-canada-ranks-18th-overall-in-corporate-r-amp-d-spending-in-canada-841928676.html)), have seen a lot of intern positions and they are actively hiring.

What's your experience with Huawei?

Cheers!",internship huawei particular ml research position experience would recommend friend huawei actively expand lately particular canada top 20 r spend link url see lot intern position actively hire experience huawei cheer
ykilcher,MachineLearning,1617114145.0,"[D] Machine Learning PhD Survival Guide 2021 (Video) | Advice on Topic Selection, Papers, Conferences & more! - by Yannic Kilcher","[https://youtu.be/rHQPBqMULXo](https://youtu.be/rHQPBqMULXo)

Hi Everyone. Let me know what you think of this!

This video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you're as clueless as I was when I started. The video is a personal recount of my mistakes and what I've learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that's exactly how I felt.

&#x200B;

Main Takeaways:

\- Select niche topics rather than hype topics

\- Write papers that can't be rejected

\- Don't be discouraged by bad reviews

\- Take reviewing & teaching seriously

\- Keep up your focus

\- Conferences are for networking

\- Internships are great opportunities

\- Team up with complementary skills

\- Don't work too hard

&#x200B;

OUTLINE:

0:00 - Intro & Overview

1:25 - Thesis Topic Selection

4:25 - How To Publish Papers

5:35 - Dealing With Reviewers

6:30 - How To Be A Reviewer

7:40 - Take Teaching Seriously

8:30 - Maintain Focus

10:20 - Navigating Conferences

12:40 - Internships

13:40 - Collaborations

14:55 - Don't Forget To Enjoy

&#x200B;

Transcript: [https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae](https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae)",url everyone let know think video advice new phd students field machine learn nmbr field shift dramatically last years navigate grad school hard especially clueless start video personal recount mistake learn already several publish paper know video however even sure start select topic go paper might benefit video exactly felt x200b main takeaways select niche topics rather hype topics write paper reject discourage bad review take review teach seriously keep focus conferences network internships great opportunities team complementary skills work hard x200b outline 0 00 intro overview1 25 thesis topic selection4 25 publish papers5 35 deal reviewers6 30 reviewer7 40 take teach seriously8 30 maintain focus10 20 navigate conferences12 40 internships13 40 collaborations14 55 forget enjoy x200b transcript url
sobe86,MachineLearning,1617750723.0,[D] Samy Bengio resigns from Google,"Source: [Bloomberg](https://www.bloomberg.com/news/articles/2021-04-06/google-ai-research-manager-samy-bengio-resigns-in-email-to-staff) ([archive.fo link](https://archive.fo/yy9aI))

(N.B. Samy ≠ Yoshua Bengio, they are brothers). He co-founded Google Brain, and co-authored the original Torch library.

He was Timnit Gebru's manager during the drama at the end of last year. He did not directly reference this in his email today, but at the time [he voiced his support for her](https://www.facebook.com/story.php?story_fbid=3469738016467233&id=100002932057665), and shock at what had happened. In February, [the Ethical AI group was reshuffled, cutting Samy's responsibilities](https://twitter.com/alexhanna/status/1362476196693303297).

[Reuters reports](https://www.reuters.com/article/us-alphabet-google-research-bengio/google-ai-scientist-bengio-resigns-after-colleagues-firings-email-idUSKBN2BT2JT): *Though he did not mention the firings in his farewell note, they influenced his decision to resign, people familiar with the matter said, speaking on condition of anonymity.*",source bloomberg url archive fo link url samy yoshua bengio brothers co found google brain co author original torch library timnit gebru manager drama end last year directly reference email today time voice support url shock happen february ethical ai group reshuffle cut samy responsibilities url report url though mention fire farewell note influence decision resign people familiar matter say speak condition anonymity
KirillTheMunchKing,MachineLearning,1619877241.0,[D] An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale - Vision Transformers explained!,"# [An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale](https://t.me/casual_gan/33)

In this paper from late 2020 the authors propose a novel architecture that successfully applies transformers to the image classification task. The model is a transformer encoder that operates on flattened image patches. By pretraining on a very large image dataset the authors are able to show great results on a number of smaller datasets after finetuning the  classifier on top of the transformer model. [More details](https://t.me/casual_gan/33).

[ViT model architecture overview](https://preview.redd.it/45k93yszkiw61.png?width=1280&format=png&auto=webp&s=70e50a28c078fa594d695d3212b509ac774fbe5e)

[\[10 minute paper explanation\]](https://t.me/casual_gan/33) [\[Arxiv\]](https://arxiv.org/abs/2010.11929)",image worth 16x16 word transformers image recognition scale url paper late nmbr author propose novel architecture successfully apply transformers image classification task model transformer encoder operate flatten image patch pretraining large image dataset author able show great result number smaller datasets finetuning classifier top transformer model detail url model architecture overview url minute paper explanation url arxiv url
mippie_moe,MachineLearning,1618432999.0,[D] Lambda GPU Benchmark Center for Deep Learning,"[GPU Benchmarks for Machine Learning](https://lambdalabs.com/gpu-benchmarks)

This is an ongoing project at Lambda. We will continue to add new GPUs and popular models as they're released.

Suggestions for new models or any feedback you have would be much appreciated!",gpu benchmarks machine learn url ongoing project lambda continue add new gpus popular model release suggestions new model feedback would much appreciate
ancientmooner,MachineLearning,1616904933.0,[R] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,"Transformers make the previously saturated COCO and ADE20K benchmarks unblocked again. It creates a new SOTA on COCO and ADE20K:

[COCO det](https://paperswithcode.com/sota/object-detection-on-coco): 58.7 mAP (`+2.7 mAP`)

[COCO inst](https://paperswithcode.com/sota/instance-segmentation-on-coco): 51.1 mAP (`+2.6 mAP`)

[ADE seg](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k-val):  53.5 mIoU (`+3.2 mIoU`)

This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones.

[Arxiv](https://arxiv.org/pdf/2103.14030.pdf)   

[Code](https://github.com/microsoft/Swin-Transformer)",transformers make previously saturate coco ade20k benchmarks unblock create new sota coco ade20k coco det url nmbr map 2 7 map coco inst url nmbr map 2 6 map ade seg url nmbr miou 3 2 miou paper present new vision transformer call swin transformer capably serve general purpose backbone computer vision challenge adapt transformer language vision arise differences two domains large variations scale visual entities high resolution pixels image compare word text address differences propose hierarchical transformer whose representation compute shift windows shift windowing scheme bring greater efficiency limit self attention computation non overlap local windows also allow cross window connection hierarchical architecture flexibility model various scale linear computational complexity respect image size qualities swin transformer make compatible broad range vision task include image classification 86 4 top 1 accuracy imagenet 1k dense prediction task object detection 58 7 box ap nmbr mask ap coco test dev semantic segmentation 53 5 miou ade20k val performance surpass previous state art large margin 2 7 box ap 2 6 mask ap coco 3 2 miou ade20k demonstrate potential transformer base model vision backbones arxiv url code url
clint9smith,MachineLearning,1616875481.0,[Discussion] How to create a team recommender when team sizes can change,"I'm trying to use a user-item-rating triple in a SVD recommender model.

User = Phase of a project

Item = Team

Rating = Team rating

The issue I'm trying to solve is how to recommend a team when the team sizes can change from phase to phase. I could recommend each role of a team separately, with the user-item-rating triple being role-person-person\_rating, but the best team isn't necessarily the best person for each role put together.",try use user item rat triple svd recommender model user phase projectitem teamrating team ratingthe issue try solve recommend team team size change phase phase could recommend role team separately user item rat triple role person person _rating best team necessarily best person role put together
broutonlab,MachineLearning,1619605595.0,[D] How do you manage Data Science experiments?,"We have prepared an [overview](https://broutonlab.com/blog/data-science-experiments-management-with-weights-and-biases-platform) and a practical example ([google colab](https://colab.research.google.com/drive/1eF4Y14emyJeLxl3Q2rgwFP52RsIaQYKT?usp=sharing)) of how to use W&B platform to manage experiments.

What techniques and tools do you use to manage Data Science experiments in your work?",prepare overview url practical example google colab url use w b platform manage experiment techniques tool use manage data science experiment work
petersonsass,MachineLearning,1618921963.0,[D] Why solving the vanishing gradients problem?,"It's often said that recurrent neural nets have the vanishing gradients problem. 

But to my understanding, that's what it should be right? The current hidden state should be less dependent on the distant hidden states.

What's wrong with this argument? Do we really want to have each hidden state's dependency equalized?",often say recurrent neural net vanish gradients problem understand right current hide state less dependent distant hide state wrong argument really want hide state dependency equalize
legoonest,MachineLearning,1617351719.0,[P] VinDr Lab - an open-source annotation platform for Medical AI,"Department of Medical Imaging, VinBigdata has decided to release our DICOM annotation tool into the open-source. It's called **VinDr Lab**. It is a web-based tool that allows multiple annotators to work at the same time and remotely. This is the software that we've used to build the dataset for our [Kaggle competition](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/). Following the releasing of the large-scale dataset VinDr-CXR, this is our next contribution in data sharing as well as tools for AI development. We encourage the community to promote the data sharing and tools to drive AI research and development.  
Hope that somebody finds it useful. Enjoy and send your feedback! The tool is publicly available at  
[https://github.com/vinbigdata-medical/vindr-lab](https://github.com/vinbigdata-medical/vindr-lab)

[VinDr Lab DICOM Viewer](https://preview.redd.it/gye835b7vpq61.png?width=3836&format=png&auto=webp&s=2e1e425f996fc8fd747f09b9a3adba8a1948c6ef)",department medical image vinbigdata decide release dicom annotation tool open source call vindr lab web base tool allow multiple annotators work time remotely software use build dataset kaggle competition url follow release large scale dataset vindr cxr next contribution data share well tool ai development encourage community promote data share tool drive ai research development hope somebody find useful enjoy send feedback tool publicly available url lab dicom viewer url
retro_var,MachineLearning,1619635905.0,[D] Books or Articles in Tree Based Methods with Formal Mathematics?,"Hi. 

I'm searching books or articles explaining Tree Based Methods (Random Forest, Gradient Boost, etc.) with extensive mathematical theory in the background. 

But, the only similar book that i've found was [""Classification and Regression Tree"" by Brieman, Friedman (1984).](https://www.amazon.com/-/es/Leo-Breiman/dp/0412048418)

I will be glad if you could recommend new information.

Thanks.",hi search book article explain tree base methods random forest gradient boost etc extensive mathematical theory background similar book find classification regression tree brieman friedman 1984 url glad could recommend new information thank
muzammal-naseer,MachineLearning,1617009271.0,[R] On Generating Transferable Targeted Perturbations,"We study and strengthen target perturbations otherwise features of neural networks in the context of transferability of these patterns from one model to another. We broaden the definition of black-box information available to the attacker by analyzing transferability to unknow target model to unknow training mechanism to unknown decision space or unknown input processing.

[https://arxiv.org/abs/2103.14641](https://arxiv.org/abs/2103.14641)

All pretrained generators are available here:

[https://github.com/Muzammal-Naseer/TTP](https://github.com/Muzammal-Naseer/TTP)

We are also tracking progress of transferable targeted attacks within easy to follow setting. Results will be updated within few days.

[https://github.com/Muzammal-Naseer/TTP#Tracking-SOTA-Targeted-Transferability](https://github.com/Muzammal-Naseer/TTP#Tracking-SOTA-Targeted-Transferability)",study strengthen target perturbations otherwise feature neural network context transferability pattern one model another broaden definition black box information available attacker analyze transferability unknow target model unknow train mechanism unknown decision space unknown input process url pretrained generators available url also track progress transferable target attack within easy follow set result update within days url
Yuqing7,MachineLearning,1620402702.0,[R] MIT & IBM 'Curiosity' Framework Explores Embodied Environments to Learn Task-Agnostic Visual Representations,"A research team from MIT and MIT-IBM Watson AI Lab proposes Curious Representation Learning (CRL), a framework that learns to understand the surrounding environment by training a reinforcement learning (RL) agent to maximize the error of a representation learner to gain an incentive to explore the environment.

Here is a quick read: [MIT & IBM 'Curiosity' Framework Explores Embodied Environments to Learn Task-Agnostic Visual Representations.](https://syncedreview.com/2021/05/07/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-14/)

 The paper *Curious Representation Learning for Embodied Intelligence* is on [arXiv](https://arxiv.org/pdf/2105.01060.pdf).",research team mit mit ibm watson ai lab propose curious representation learn crl framework learn understand surround environment train reinforcement learn rl agent maximize error representation learner gain incentive explore environment quick read mit ibm curiosity framework explore embody environments learn task agnostic visual representations url paper curious representation learn embody intelligence arxiv url
TheRealMarqupe,MachineLearning,1620562851.0,CNNs Color Invariance [Discussion],"We want to detect a specific object that can have any color, for example, a car. If we train our model with images containing only black and gray cars, will the performance of our model's predictions be worse for images containing cars with different colors than the ones used on training? For example, will our model fail to classify correctly if a car is present or not on an image, for images containing yellow cars?   

If so, what is the best way to achieve color invariance? (explanations with examples and credible sources references would be much appreciated)

Thanks!",want detect specific object color example car train model image contain black gray cars performance model predictions worse image contain cars different color ones use train example model fail classify correctly car present image image contain yellow cars best way achieve color invariance explanations examples credible source reference would much appreciate thank
prathameshpck,MachineLearning,1619130146.0,Is fine tuning twice a viable thing to do?? [D],"Don't know if this is the right place to ask this but here goes 

Say I have a dataset with very few samples but i manage to find another dataset of something similar but which is larger 


Would it make sense to finetune twice ? i.e first on the second larger dataset, and then fine tune again on the smaller one?",know right place ask go say dataset sample manage find another dataset something similar larger would make sense finetune twice e first second larger dataset fine tune smaller one
dDW5ia5j,MachineLearning,1617178321.0,"[P] Generic template to bootstrap your PyTorch project with PyTorch Lightning, Hydra, W&B, DVC, and Streamlit","Link: https://github.com/lucmos/nn-template

Generic opinionated template to bootstrap your PyTorch project, avoid writing boilerplate code for:  

- **PyTorch Lightning**, lightweight PyTorch wrapper for high-performance AI research. 
- **Hydra**, a framework for elegantly configuring complex applications. 
- **DVC**, track large files, directories, or ML models. Think ""Git for data"". 
- **Weights and Biases**, organize and analyze machine learning experiments. 
- **Streamlit**, turns data scripts into shareable web apps in minutes.",link url opinionated template bootstrap pytorch project avoid write boilerplate code pytorch lightning lightweight pytorch wrapper high performance ai research hydra framework elegantly configure complex applications dvc track large file directories ml model think git data weight bias organize analyze machine learn experiment streamlit turn data script shareable web apps minutes
NumiAI,MachineLearning,1616346275.0,[D] Barlow Twins: SSL via Redundancy Reduction,"Paper: [https://arxiv.org/pdf/2103.03230v1.pdf](https://arxiv.org/pdf/2103.03230v1.pdf)

Authors: Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny

This recently published work on self-supervised learning is based on a simple idea yet yield amazing results. I liked reading the paper and got interested to read more about Barlow's redundancy concept from neuroscience.

An interesting fact which I came across is that Barlow himself in a publication back in 2000 revisited his opinion about the redundancy reduction.

&#x200B;

https://preview.redd.it/sueixtaiweo61.jpg?width=1640&format=pjpg&auto=webp&s=632575dfce0c1bfd74ca8797a8c3987f06a9fc2c

&#x200B;

https://preview.redd.it/tc8j3xpvweo61.jpg?width=1640&format=pjpg&auto=webp&s=73fe7ab49e6ab0636968bc326868f4db929d4117

The authors in the paper actually supported the idea of redundancy reduction but as I understand from Barlow's own paper the redundancy increase, not decrease. 

I would like to hear your opinion on this apparent conflict in my understanding. Thanks",paper url jure zbontar li jing ishan misra yann lecun stéphane denythis recently publish work self supervise learn base simple idea yet yield amaze result like read paper get interest read barlow redundancy concept neuroscience interest fact come across barlow publication back nmbr revisit opinion redundancy reduction x200b url author paper actually support idea redundancy reduction understand barlow paper redundancy increase decrease would like hear opinion apparent conflict understand thank
l_atze_l,MachineLearning,1620032568.0,[P] MoViNet in PyTorch,"TL;DR: Implemented MoViNet in PyTorch: [https://github.com/Atze00/MoViNet-pytorch](https://github.com/Atze00/MoViNet-pytorch)

I'm currently working on video recognition, I've found [MoViNets](https://arxiv.org/abs/2103.11511) to be a quite interesting paper, so I decided to implement it on PyTorch, since the code will be released in TF by the authors.

I  will be using this architecture myself, once the code and the weights in TF will be released, so bug reports or comments are very useful to me. Let me know if you find incoherence between paper and implementation as well!

I hope this is useful for someone.",tl dr implement movinet pytorch url currently work video recognition find movinets url quite interest paper decide implement pytorch since code release tf author use architecture code weight tf release bug report comment useful let know find incoherence paper implementation well hope useful someone
jj4646,MachineLearning,1619062368.0,"[D] decline of traditional ""state space models""","It seems that Recurrent Neural Networks have overtaken traditional ""state space models"" for time series models. Is this because traditional state space models require the analyst to make certain assumptions about how the system transitions between different states - whereas a recurrent neural  network can consider a wide combination of states through hidden layers and deep architecture?",seem recurrent neural network overtake traditional state space model time series model traditional state space model require analyst make certain assumptions system transition different state whereas recurrent neural network consider wide combination state hide layer deep architecture
igorsusmelj,MachineLearning,1616493351.0,[P] Release of lightly 1.1.3 - A python library for self-supervised learning,"We just released a new version of lightly (https://github.com/lightly-ai/lightly) and after the valuable feedback from this subreddit, we thought some of you might be interested in the updates. 

Lightly now supports more models: In addition to SimCLR and MoCo, we have added SimSiam and Barlow Twins (a big thank you to our open-source contributors!). More models, such as BYOL and SwAV are in the pipeline.

We did some benchmarking (https://docs.lightly.ai/getting_started/benchmarks.html) on cifar10 and show the various frameworks in action using different training epochs and batch sizes.
Most models run well on multi-GPU setups using PyTorch Lightning in distributed data-parallel settings. 

We are curious to hear your feedback.",release new version lightly url valuable feedback subreddit think might interest update lightly support model addition simclr moco add simsiam barlow twin big thank open source contributors model byol swav pipeline benchmarking url cifar10 show various frameworks action use different train epochs batch size model run well multi gpu setups use pytorch lightning distribute data parallel settings curious hear feedback
chasep255,MachineLearning,1617563283.0,[D] Why not use momentum based optimizer with WGAN?,I keep reading that I should not use any momentum when optimizing a WGAN.  Nothing I have read offers an explanation as to why.  So why shouldn't I use a momentum based optimizer?  Anyone know the reason?,keep read use momentum optimize wgan nothing read offer explanation use momentum base optimizer anyone know reason
EscapedLaughter,MachineLearning,1620289418.0,[N] Music Demixing (Audio Source Separation) Competition by Sony | ISMIR 2021,"The [competition](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021?utm_source=reddit&utm_medium=ml&utm_campaign=sony) features 2 baselines:

Open-Unmix: [Code](https://github.com/sigsep/open-unmix-pytorch) | Paper: [https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf](https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf)

CrossNet-UMX: [Code](https://github.com/sony/ai-research-code/tree/master/x-umx) | Paper: [https://arxiv.org/pdf/2010.04228.pdf](https://arxiv.org/pdf/2010.04228.pdf)

The competition can hopefully serve as a benchmark for various source separation models. Participants can get together during the ISMIR Workshop and share their learnings. Sony is also offering 10,000 CHF cash prize for top participants on the leaderboard.

\------------------------------------

[Example](https://d2cowzs755i94n.cloudfront.net/) of music demixing:

https://reddit.com/link/n62hli/video/t8orzh8rmgx61/player

PS: This is my first post on the subreddit, so if I have transgressed any norms, I do apologize.",competition url feature nmbr baselines open unmix code url paper url code url paper url competition hopefully serve benchmark various source separation model participants get together ismir workshop share learn sony also offer 10 000 chf cash prize top participants leaderboard example url music demixing url first post subreddit transgress norms apologize
Hi_I_am_Desmond,MachineLearning,1620404887.0,"[D] In Quantum NNs, are the hybrid optimization methods run on classical hardware?","I have been reading many resources and since I don’t know anyone in those field that I am starting to research, I hope here I can find someone that works in quantum machine learning. While there are many approaches, considering the most common based on “Quantum circuit learning” by Mitarai et al., do they suppose that we use an hybrid hardware where optimization is computed on classical hardware? What are some example with only quantum hardware? Can you link me a paper that makes a clear distinction on the hardware that runs the QNNs?",read many resources since know anyone field start research hope find someone work quantum machine learn many approach consider common base quantum circuit learn mitarai et al suppose use hybrid hardware optimization compute classical hardware example quantum hardware link paper make clear distinction hardware run qnns
PK_thundr,MachineLearning,1619199060.0,[D] How to develop a compelling argument for a paper submission?,"I've been working on a certain loss function for training  a DNN for a while, unfortunately the results have been mediocre (not SoTA, or just matching SoTA).

How do you go about the process of developing a compelling argument from a paper from a simple set of experiments and accuracy results that are incremental progress on what's already been done in the field? 

I realize that this will depend greatly on what you're actually working on, but is there any kind of general guiding principles here?",work certain loss function train dnn unfortunately result mediocre sota match sota go process develop compel argument paper simple set experiment accuracy result incremental progress already field realize depend greatly actually work kind general guide principles
CoolThingsOnTop,MachineLearning,1617471222.0,[D][R] Blog Post: Why Are Kronecker Products So Effective?,"This week, the list of ICLR 2021's Outstanding Papers was announced. One of the awarded papers was "" [**Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with** **1/n1/n** **Parameters**](https://openreview.net/forum?id=rcQdycl0zyk)"", which makes use of the Kronecker Product to build a new kind of NN layer.

I have been looking into Kronecker Products for a while now, and it was a pleasant surprise to find this paper on the list of winners. So I decided to write a blog post about it and point out an interesting connection with previous work. Let me know what you think!

Blog post: [https://santiag0m.github.io/blog/2021/04/02/why-are-kronecker-products-so-effective.html](https://santiag0m.github.io/blog/2021/04/02/why-are-kronecker-products-so-effective.html)",week list iclr nmbr outstanding paper announce one award paper beyond fully connect layer quaternions parameterization hypercomplex multiplications 1 n1 n parameters url make use kronecker product build new kind nn layer look kronecker products pleasant surprise find paper list winners decide write blog post point interest connection previous work let know think blog post url
No_Effective7572,MachineLearning,1619626129.0,[P] Semi Supervised Segmentation on Graphs using Eikonal Equation with PyOpenCl backend.,"Hey guys,

I would like to share this project with you. [https://github.com/aGIToz/semi-supervised-segmentation-on-graphs](https://github.com/aGIToz/semi-supervised-segmentation-on-graphs)  .  It does segmentation on graphs. Its application on images and pointclouds.

It runs a time-dependent eikonal equation on graphs. Eikonal equation can be used to create generalized distances on manifolds. Classically the equation is solved using fast-marching methods. A time-dependent eikonal equation at steady coincides with fast-marching solution.

Basically I am creating a knn graph with gpu backend using pyopencl and then I run the PDE with gpu backend.

I hope you find it interesting !",hey guy would like share project url segmentation graph application image pointclouds run time dependent eikonal equation graph eikonal equation use create generalize distance manifold classically equation solve use fast march methods time dependent eikonal equation steady coincide fast march solution basically create knn graph gpu backend use pyopencl run pde gpu backend hope find interest
shuvob4,MachineLearning,1618401627.0,[R] Drug Target Interaction research using Machine Learning,"I am a computer science graduate with an aim to pursue my master's in machine learning. As I have no thesis in my undergraduate, I would like to know some information about doing research, I hope to find some guidance from this post.

I am highly interested in healthcare and machine learning. After scouring through the internet, I have found Drug-Target Interaction as a topic of my interest. But after digging a little deeper, I found myself in the middle of the sea with no view of the land. I have read some papers; I found I don't understand most of the things described in the field. I don't have the domain knowledge. Should I seek another topic (If so, can someone guide me with some topics which I can look for) or should I read some more papers so that I can understand the topic better?

Thanks in advance and have a nice day.",computer science graduate aim pursue master machine learn thesis undergraduate would like know information research hope find guidance post highly interest healthcare machine learn scour internet find drug target interaction topic interest dig little deeper find middle sea view land read paper find understand things describe field domain knowledge seek another topic someone guide topics look read paper understand topic better thank advance nice day
hardmaru,MachineLearning,1617802883.0,[R] Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,"Recent paper from FAIR published in [PNAS](https://www.pnas.org/content/118/15/e2016239118). They find that biological structure and function emerge in representations of language models trained on massive databases of protein sequences.

*Summary*

Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.

*Abstract*

In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.

Paper: https://www.pnas.org/content/118/15/e2016239118",recent paper fair publish pnas url find biological structure function emerge representations language model train massive databases protein sequence summary learn biological properties sequence data logical step toward generative predictive artificial intelligence biology propose scale deep contextual language model unsupervised learn sequence span evolutionary diversity find without prior knowledge information emerge learn representations fundamental properties proteins secondary structure contact biological activity show learn representations useful across benchmarks remote homology detection prediction secondary structure long range residue residue contact mutational effect unsupervised representation learn enable state art supervise prediction mutational effect secondary structure improve state art feature long range contact prediction abstract field artificial intelligence combination scale data model capacity enable unsupervised learn lead major advance representation learn statistical generation life sciences anticipate growth sequence promise unprecedented data natural sequence diversity protein language model scale evolution logical step toward predictive generative artificial intelligence biology end use unsupervised learn train deep contextual language model nmbr billion amino acids across nmbr million protein sequence span evolutionary diversity result model contain information biological properties representations representations learn sequence data alone learn representation space multiscale organization reflect structure level biochemical properties amino acids remote homology proteins information secondary tertiary structure encode representations identify linear projections representation learn produce feature generalize across range applications enable state art supervise prediction mutational effect secondary structure improve state art feature long range contact prediction paper url
Artistic-Mushroom974,MachineLearning,1617240597.0,[d] Robotics/ AI PhD Salary Expectations for T1.5 School," I'm a junior (first few years) PhD student in Robotics/AI at GTech (CS Rankings for AI/Robotics puts GTech at #6 in the US [http://csrankings.org/#/index?ai&vision&mlmining&nlp&ir&robotics&us](http://csrankings.org/#/index?ai&vision&mlmining&nlp&ir&robotics&us)) . I've seen posts about 250-300k total comp post ML/AI PhD's at big tech or in fintech, but I've also seen some caveats like only for top 4 (MIT/Stanford/CMU/Berkeley) school PhDs. ([https://www.reddit.com/r/MachineLearning/comments/bwwmh9/d\_doing\_a\_phd\_is\_not\_worth\_it\_unless\_exception/](https://www.reddit.com/r/MachineLearning/comments/bwwmh9/d_doing_a_phd_is_not_worth_it_unless_exception/))

So my question is, what is a realistic salary expectations for the next couple schools after the top 4? Are they that much worse than ""top 4?"" Do I still have good prospects of a 250-300k offer (what would be a low, mid, and high range)? Also, what are my prospects if I were to take the Master's rather than finish the full degree?

For context, many labs at my school have a fair (but not outstanding) amount of top conference publications (ICLR,NeurIPS,ICML, CoRL, ICRA).

And no, money is not the only reason I chose this route (I have a lot of curiosity as to the possibilities of AI), but it is demanding and difficult work.",junior first years phd student robotics ai gtech cs rank ai robotics put gtech 6 us url see post 250 300k total comp post ml ai phd big tech fintech also see caveats like top nmbr mit stanford cmu berkeley school phds url question realistic salary expectations next couple school top 4 much worse top 4 still good prospect 250 300k offer would low mid high range also prospect take master rather finish full degree context many labs school fair outstanding amount top conference publications iclr neurips icml corl icra money reason choose route lot curiosity possibilities ai demand difficult work
_Arsenie_Boca_,MachineLearning,1620455301.0,[D] Worth learning JAX?,"Recently, more popular papers as ViT start using JAX to implement their models. I mainly use PyTorch but in the past I was playing with the thought of learning tensorflow. I always felt like tensorflow has a lot of legacy code and conventions that make it kinda unneccessarily complicated compared to pytorch.

1. Now that google seems to shift to JAX internally, do you think JAX will replace tensorflow? 
2. And regardless of that, is it worth learning JAX? 
3. If you would learn JAX, which higher-level api  (comparable to torch.nn) would you use like stax, flax, haiku?",recently popular paper vit start use jax implement model mainly use pytorch past play think learn tensorflow always felt like tensorflow lot legacy code conventions make kinda unneccessarily complicate compare pytorch 1 google seem shift jax internally think jax replace tensorflow 2 regardless worth learn jax 3 would learn jax higher level api comparable torch nn would use like stax flax haiku
memgamemotron,MachineLearning,1617048073.0,[P] Need help figuring out how to print out the sentence and sentence predictions from my google trax neural network model.,"so to begin with, this is a project I am helping my school with to gain experience.  I have taken plenty of [deeplearning.AI](https://deeplearning.AI) courses which helped me build this codes foundation. 

The data is from a chat bot which provides the chat text, ANGRY labels and HAPPY labels which were going to feed into the model.

https://preview.redd.it/iyo9sgfgo0q61.png?width=1752&format=png&auto=webp&s=53291557217d14836af90988b88bee956ef0f694

Next up is the data preprocessing which includes splitting the data 80/20 for training and evaluation.

&#x200B;

https://preview.redd.it/qk2d4i3wo0q61.png?width=1464&format=png&auto=webp&s=029037b1e9c735cba44b0f291978247f8aac865e

The I preprocess the text by removing special characters, spaces, stop words, and tokenizing text like so:

&#x200B;

https://preview.redd.it/ky1f9nruq0q61.png?width=1436&format=png&auto=webp&s=f1fa6bac026a04acd2e283e47525535f7091374c

Next up I build the vocab list like so:

&#x200B;

https://preview.redd.it/q6fnx8b2r0q61.png?width=1218&format=png&auto=webp&s=9adde66dd739fd5fa02e4b25594849ab81133c44

Then create padded tensors for each sentence:

&#x200B;

https://preview.redd.it/8m66va9vr0q61.png?width=1310&format=png&auto=webp&s=d9b676fe49968da93e081de3dc57fbb54da5d97c

Afterwards create the data generator that we can apply to training and evaluation.

&#x200B;

https://preview.redd.it/1mzl9ytis0q61.png?width=1304&format=png&auto=webp&s=117069b46458f358a315719c556d7e163f58ee64

&#x200B;

https://preview.redd.it/196f3m1ks0q61.png?width=1404&format=png&auto=webp&s=7a387441463aa73e9e60e7eae240fc0de85d73c7

&#x200B;

https://preview.redd.it/xk8u8rrls0q61.png?width=1352&format=png&auto=webp&s=d2b41660c8906f5508201bf503e1f15f968f3a89

&#x200B;

https://preview.redd.it/thqav2its0q61.png?width=1746&format=png&auto=webp&s=443f28186ea9151fafd4555dea564ed16a28bd41

&#x200B;

https://preview.redd.it/w64d88yxs0q61.png?width=1378&format=png&auto=webp&s=73150cdb3582dbfdf51f70adf3a2471768c5ae3f

I then start to create the layers by forward propagation from one layer to the next neighing with a relu activation function

&#x200B;

https://preview.redd.it/hb9czcgft0q61.png?width=1124&format=png&auto=webp&s=5edfb4009a5b6fe9ca36e4ca89db981e0e71ecbf

Defining the Dense Layer:

&#x200B;

https://preview.redd.it/rfysjt6pt0q61.png?width=1336&format=png&auto=webp&s=866f725320c859aefdea53bec957cf82b5897fbd

&#x200B;

https://preview.redd.it/mmksp94qt0q61.png?width=1364&format=png&auto=webp&s=66c70f753403d9954db2b2e876242ae4bae627f3

The next step we will define our model by using Trax and combining all our layers!

&#x200B;

https://preview.redd.it/7g2c91acu0q61.png?width=1256&format=png&auto=webp&s=a496b0432aecef0e486097de82af499b441426ef

&#x200B;

https://preview.redd.it/5swtlc6du0q61.png?width=1584&format=png&auto=webp&s=5f7c79f36b8418dcfe9fac2bd771990bdefeb5ab

I have created training and evaluation loops that will help us understand how our model is performing

&#x200B;

https://preview.redd.it/p1p9ghyiu0q61.png?width=1118&format=png&auto=webp&s=5fe436a51c2d448bf2d7abd03a8cad9d387fe3c2

**Next up we will check out the models accuracy, I know its pretty low but currently a learning student looking to improve and excited on some feedback to help improve this model.** 

&#x200B;

https://preview.redd.it/9xcgv6g6v0q61.png?width=1274&format=png&auto=webp&s=3e6ab61fb41a45e051f589542bf7a7b74b3e8ed3

&#x200B;

https://preview.redd.it/g4hqesz7v0q61.png?width=1882&format=png&auto=webp&s=5ea796575a5b79cf1a384304e63f5c153db5d059

As you can see this model is performing at 67%, The following is the code that I need help with. I am trying to print the observed sentence followed by the prediction. I have tried to do it but am not getting 2 output prediction for ANGRY and HAPPY labels. 

The purpose is to help understand if a customer is either angry or happy with the bot customer experience and also identify if interference is needed from an agent. 

&#x200B;

https://preview.redd.it/kajgcwtwv0q61.png?width=1342&format=png&auto=webp&s=a3b5136cd06daa6c564256918e461c4c74bb3759

Instead of getting 2 predicting labels this is what my output looks like. 

**I would greatly appreciate any help and welcome any ML mentors for this project**.

&#x200B;

Thanks for looking everyone, and can't wait to hear from you all!",begin project help school gain experience take plenty deeplearning ai url course help build cod foundation data chat bot provide chat text angry label happy label go fee model url data preprocessing include split data 80 20 train evaluation x200b url preprocess text remove special character space stop word tokenizing text like x200b url build vocab list like x200b url create pad tensors sentence x200b url create data generator apply train evaluation x200b url start create layer forward propagation one layer next neigh relu activation function x200b url dense layer x200b url next step define model use trax combine layer x200b url create train evaluation loop help us understand model perform x200b url check model accuracy know pretty low currently learn student look improve excite feedback help improve model x200b url see model perform 67 follow code need help try print observe sentence follow prediction try get nmbr output prediction angry happy label purpose help understand customer either angry happy bot customer experience also identify interference need agent x200b url get nmbr predict label output look like would greatly appreciate help welcome ml mentor project x200b thank look everyone wait hear
weifz,MachineLearning,1617781633.0,[D] How to define Pearl's causality in time series ?,"Hi all, recently I'm studying the Granger causality and Pearl's causality, and I wonder if there is a way to define Pearl's causality in time series, and what is the relationship between Granger's and Pearl's ?(not differences). Thanks a lot !",hi recently study granger causality pearl causality wonder way define pearl causality time series relationship granger pearl differences thank lot
PytonRzeczny,MachineLearning,1616604232.0,[D] Wasserstein GAN math,"Hi.

Could You recommend me some resources to learn about math included in WGAN paper, because when i read it i don't think i understand more than the title of this paper.

I want to implement this, but without knowing what's going on with loss function it's quite useless to me.",hi could recommend resources learn math include wgan paper read think understand title paper want implement without know go loss function quite useless
AerysSk,MachineLearning,1619271507.0,"[D] ML researchers of Reddit, what qualifications do you seek in a research applicant that you have not met?","I am a third year undergraduate CS student. Recently, I apply to a research training role (similar to Google Brain AI Residency) in a company that they require ""strong math, programming skill and adequate English communication skill"" (my country's major language is not English). To my surprise, my interviewer is a Postdoc researcher at a top US University (which I will not disclose), and it does not seem like he/she is working for the company. The 1 hour interview is scheduled next week, and we have not met in real life.

Dear ML researchers, what qualifications do you seek in these applicants? What questions do you ask and what do you expect the answers to be?",third year undergraduate cs student recently apply research train role similar google brain ai residency company require strong math program skill adequate english communication skill country major language english surprise interviewer postdoc researcher top us university disclose seem like work company nmbr hour interview schedule next week meet real life dear ml researchers qualifications seek applicants question ask expect answer
bendee983,MachineLearning,1620405956.0,[R] AttendSeg: super-compact NN for semantic segmentation for edge devices,"A new neural network developed by researchers at DarwinAI and the University of Waterloo makes it possible to run semantic segmentation on resource-constrained edge devices.

Key highlights:

\- the model has 1.19 million params (in comparison, RefineNet has 85M params). Precision has been reduced to 8 bits without significant compromise in accuracy, which reduces the size of the model to 1.19MB (RefineNet is \~340MB)

\- AttendSeg uses attention condensers to provide compact self-attention

\- The model was generated using ""Generative Synthesis,"" an ML technique that explores the constraint space provided by the engineers (accuracy, model size, etc.) and creates the best model

AttendSeg will be presented at CVPR in June.

Read story with interview with U of Waterloo professor and co-author Alex Wong:

[https://bdtechtalks.com/2021/05/07/attendseg-deep-learning-edge-semantic-segmentation/](https://bdtechtalks.com/2021/05/07/attendseg-deep-learning-edge-semantic-segmentation/)

Full paper here:

[https://arxiv.org/abs/2104.14623](https://arxiv.org/abs/2104.14623)",new neural network develop researchers darwinai university waterloo make possible run semantic segmentation resource constrain edge devices key highlight model nmbr million params comparison refinenet 85m params precision reduce nmbr bits without significant compromise accuracy reduce size model 1 19mb refinenet 340mb attendseg use attention condensers provide compact self attention model generate use generative synthesis ml technique explore constraint space provide engineer accuracy model size etc create best modelattendseg present cvpr june read story interview u waterloo professor co author alex wong url paper url
AleksanderPet,MachineLearning,1616496675.0,[R] MDMMT: Multidomain Multimodal Transformer for Video Retrieval,"The research team from Lomonosov Moscow State University in cooperation with Intelligent systems and data science lab of Moscow Huawei R&D Center developed a novel multi-modal multi-domain text2video system that outperforms current state-of-the-art results (e.g. from Google, Facebook) on widely used public datasets (MSR-VTT and LSMDC) for the video retrieval task.

Paper: [https://arxiv.org/abs/2103.10699](https://arxiv.org/abs/2103.10699)

Comparison on paperwithcode with others: [MSR-VTT](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt), [MSR-VTT \[1kA split\]](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka), [LSMDC](https://paperswithcode.com/sota/video-retrieval-on-lsmdc).

**Update** on 17th fo April, 2021: we shared the model, the testing scripts (providing the numbers in the paper) and the refined lists (removed train/test intersection) for the scientific community. Please refer to our [github](https://github.com/papermsucode/mdmmt).",research team lomonosov moscow state university cooperation intelligent systems data science lab moscow huawei r center develop novel multi modal multi domain text2video system outperform current state art result e g google facebook widely use public datasets msr vtt lsmdc video retrieval task paper url paperwithcode others msr vtt url msr vtt 1ka split url lsmdc url 17th fo april 2021 share model test script provide number paper refine list remove train test intersection scientific community please refer github url
vector_machines,MachineLearning,1617491149.0,[D] Resources for crowdsourcing MCQ based dataset,"I want to crowdsource a dataset which is in MCQ format. Given a sentence, users are required to pick the right option out of 4. I explored mechanical turk and it seems they don't have a template for that, all NLP ones are generic and require hardcoded labels. This means I will have to create a custom one. From last Emnlp, I recall crowdAQ, which is a bit closed form. I wanted to learn more about cost efficient crowdsourcing options if you know any, don't want to spend a lot of time reading docs about APIs. Any suggestions, advice, experience will be appreciated.",want crowdsource dataset mcq format give sentence users require pick right option 4 explore mechanical turk seem template nlp ones generic require hardcoded label mean create custom one last emnlp recall crowdaq bite close form want learn cost efficient crowdsourcing options know want spend lot time read docs apis suggestions advice experience appreciate
argh_usernametaken,MachineLearning,1618393674.0,[Discussion] which NN architecture is best suitable for analysing the structural data of biomolecules,I'm trying to extract meaning out of structural data of biomolecules like protein. Which ML method can be applied on this type of data?,try extract mean structural data biomolecules like protein ml method apply type data
WigglyHypersurface,MachineLearning,1620230052.0,[D] Making sense of this autoencoder archetecture,"I'm looking to implement the architecture from [this paper](https://arxiv.org/pdf/1806.02382.pdf) for imputing some missing data. I'm confused on a couple of points about the choices made. 

Rather than an encoder/decoder setup, this autoencoder adds an extra generator network. I'm having some trouble parsing why this is necessary or desirable though. Also, I'm confused about what the input to the generator is. Is the input just noise and the mask? Also, how the inputs to the encoder are handled when they are missing, i.e. do they just replace the NAs with zero?

If I'm understanding correctly, in the missing data imputation case the mask is just used by the loss function to zero out the log-likelihood values for the data that are missing. Is that correct?

It seems like it would be simpler to have a standard encoder/decoder setup, and just initialize by filling in the missing data with random values, then resample a prediction for each missing value at the end of each epoch and keep training, with the log-likelihood for the missing values masked out. That way you wouldn't need the mask as an additional input to the encoder/generator.",look implement architecture paper url impute miss data confuse couple point choices make rather encoder decoder setup autoencoder add extra generator network trouble parse necessary desirable though also confuse input generator input noise mask also input encoder handle miss e replace nas zero understand correctly miss data imputation case mask use loss function zero log likelihood value data miss correct seem like would simpler standard encoder decoder setup initialize fill miss data random value resample prediction miss value end epoch keep train log likelihood miss value mask way need mask additional input encoder generator
techsucker,MachineLearning,1617727196.0,[N] Facebook AI Introduces A New Self-Supervised Learning Framework For Model Selection And Hyperparameter Tuning For Large-Scale Forecasting,"Researchers at Facebook AI have recently released a new self-supervised learning framework for model selection (SSL-MS) and hyperparameter tuning (SSL-HPT), which provides accurate forecasts with less computational time and resources. The SSL-HPT algorithm estimates hyperparameters 6-20x faster when compared with baseline (search-based) algorithms, producing accurate forecasting results in numerous applications.

At present, Forecasting is one of the significant data science and machine learning tasks performed. Therefore, it is crucial to have fast, reliable, and accurate forecasting results with large amounts of time series data for managing various businesses. 

Time series analysis is used to find trends and forecast future values. A slight difference in hyperparameters in this type of analysis could lead to very different forecast results for a given model and have serious consequences. Therefore, it’s essential to select optimal hyperparameter values. 

Summary: [https://www.marktechpost.com/2021/04/06/facebook-ai-introduces-a-new-self-supervised-learning-framework-for-model-selection-and-hyperparameter-tuning-for-large-scale-forecasting/](https://www.marktechpost.com/2021/04/06/facebook-ai-introduces-a-new-self-supervised-learning-framework-for-model-selection-and-hyperparameter-tuning-for-large-scale-forecasting/) 

Paper: [https://arxiv.org/abs/2102.05740](https://arxiv.org/abs/2102.05740)?

Facebook Source: [https://ai.facebook.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/](https://ai.facebook.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/)",researchers facebook ai recently release new self supervise learn framework model selection ssl ms hyperparameter tune ssl hpt provide accurate forecast less computational time resources ssl hpt algorithm estimate hyperparameters 6 20x faster compare baseline search base algorithms produce accurate forecast result numerous applications present forecast one significant data science machine learn task perform therefore crucial fast reliable accurate forecast result large amount time series data manage various businesses time series analysis use find trend forecast future value slight difference hyperparameters type analysis could lead different forecast result give model serious consequences therefore essential select optimal hyperparameter value summary url paper url source url
Lunavahid,MachineLearning,1620639107.0,[Vault] [Discussion] Is Human in the Loop the New Thing?,"  

&#x200B;

Recent discussion around AI and ethics goes all the way to one concept. Do we need humans in the AI pipeline loop or not? Even some CEOs declared they are worried about the fact that AI can push people perception, judgment and eventually make them biased. 

So the ultimate question is doing we need humans in the loop or not. Do we need them to surprise algorithms, analyses outcomes and results, check for hidden biases and find potential pitfalls? (and even to find those patterns we might use another AI system). 

New frontiers in AI need more data to perform better. More data and better algorithms are the recipe of success of several new achievements. 

Let's look at one application that we are familiar with. Annotation and tagging. 

Image Annotation has never been this accessible. There are billions of images, texts, audio and videos. Annotation and tagging provide a solid learning background for AIA systems to learn. 20 year ago the challenge was to recognize an apple, or the number 8 or the letter ""Z"". The new challenge is to identify a Spanish house on a sunny day next to a lake. 

The more sophistication needed form users means more sophisticated annotation and tagging for learning purposes. More sophistication means more labels and annotation and for that reason the need to the human in the loop is increasing.",x200b recent discussion around ai ethics go way one concept need humans ai pipeline loop even ceos declare worry fact ai push people perception judgment eventually make bias ultimate question need humans loop need surprise algorithms analyse outcomes result check hide bias find potential pitfalls even find pattern might use another ai system new frontiers ai need data perform better data better algorithms recipe success several new achievements let look one application familiar annotation tag image annotation never accessible billions image texts audio videos annotation tag provide solid learn background aia systems learn nmbr year ago challenge recognize apple number nmbr letter z new challenge identify spanish house sunny day next lake sophistication need form users mean sophisticate annotation tag learn purpose sophistication mean label annotation reason need human loop increase
BrettNMartensen,MachineLearning,1618610104.0,[D] Recency versus Frequency in Prediction,"A neural net can be built to just keep track of the most recent next input symbol in a sequence and use that to predict what the next symbol might be given a familiar sequence. That is, expect it to be what happened the last time.

Or it could be built to keep track of how frequently all of the next symbols that have occurred. That is build up a probability distribution of next stimuli based on a count of them being experienced. Then use the highest probable next stimulus as the prediction.

Does anyone know of any papers that have been published that compare the success rate of these two approaches i.e. Recency versus Frequency for prediction purposes?",neural net build keep track recent next input symbol sequence use predict next symbol might give familiar sequence expect happen last time could build keep track frequently next symbols occur build probability distribution next stimuli base count experience use highest probable next stimulus prediction anyone know paper publish compare success rate two approach e recency versus frequency prediction purpose
janimezzz,MachineLearning,1617183219.0,[N] Deep learning method for generating proteins will speed up drug development,"Researchers at Chalmers University of Technology, Sweden, present a way to generate synthetic proteins using generative deep learning. The new approach has huge potential for developing efficient industrial enzymes as well as new protein-based medicine, such as antibodies and vaccines.

Read more here: [https://news.cision.com/chalmers/r/unique-ai-method-for-generating-proteins-will-speed-up-drug-development,c3316116](https://news.cision.com/chalmers/r/unique-ai-method-for-generating-proteins-will-speed-up-drug-development,c3316116)

Read the article [**“Expanding functional protein sequence spaces using generative adversarial networks""​**](https://doi.org/10.1038/s42256-021-00310-5) in Nature Machine Intelligence. ",researchers chalmers university technology sweden present way generate synthetic proteins use generative deep learn new approach huge potential develop efficient industrial enzymes well new protein base medicine antibodies vaccines read url article expand functional protein sequence space use generative adversarial network url nature machine intelligence
Candid-Wishbone-692,MachineLearning,1619708153.0,[N] Call for Teachable NLP Challenge,"Hi all,

I found this exciting Teachable NLP Challenge and looking for people who want to participate with me!

Is there anyone who wants to participate? All levels in NLP are welcome.

\----------------------------------------------------------------------------------------------------------------------------------

Teachable NLP Challenge is free and open to everyone interested in training their own AI. All you need to be prepared for is good ideas and datasets.

* When: 05/05/2021 - 05/18/2021 11:59 EDT
* How: You just need to submit your AI model link and explanations on your AI (Good example: [https://forum.ainetwork.ai/c/ai-showcase/11](https://forum.ainetwork.ai/c/ai-showcase/11))
* Prizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)

To participate, submit your info via [https://forms.gle/XfUuNSS2heAn7JtH7](https://forms.gle/XfUuNSS2heAn7JtH7). You will receive an invitation email!

Check how Teachable NLP works: [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65)Or watch a 1-minute tutorial video: [https://youtu.be/hzujZOT1qz8](https://youtu.be/hzujZOT1qz8)",hi find excite teachable nlp challenge look people want participate anyone want participate level nlp welcome teachable nlp challenge free open everyone interest train ai need prepare good ideas datasets 05 05 2021 05 18 2021 11 59 edt need submit ai model link explanations ai good example url prize apple store gift card winners interview broadcast ai network youtube channel 1 48k subscribers participate submit info via url receive invitation email check teachable nlp work url watch 1 minute tutorial video url
fernauata,MachineLearning,1620097391.0,[R] House-GAN++: A breakthrough in automated floorplan design via Relational Refinement GAN,"House-GAN++: A breakthrough in automated floorplan design via Relational Refinement GAN w/ convolutional message passing. Joint work with Autodesk Research to appear in  [\#CVPR2021](https://www.facebook.com/hashtag/cvpr2021?__eep__=6&__cft__[0]=AZWYm7n5kJ6SP6qGoFQgTlDL5415dsNn8Z1UngESW8QHjiiKhFKKBnChx5eFUSRxJSmjIBAxsJnLpumzg7kup5su61JcohsFx2GuvAYxJSlkCqdYVNhPiVzpLqitwpcURVXCOop6KRzQusJwdxaedYvR&__tn__=*NK-R).

[The video shows our interactive browser demo. ](https://reddit.com/link/n4ejlo/video/z9e22ww0r0x61/player)

ArXiv: [https://arxiv.org/abs/2103.02574](https://arxiv.org/abs/2103.02574)

You can play around here [http://www.houseganpp.com/](http://www.houseganpp.com/?fbclid=IwAR1YnbsZDA-sSRL_AWSuof1llJH4n9uggg3g-u8S_xbv9tZEQgbq9cwA4Z4)

Project website: [https://ennauata.github.io/houseganpp/page.html](https://ennauata.github.io/houseganpp/page.html?fbclid=IwAR3s6cYsrvWxN5CGGA9T10GRLBsRybTbt6VlI7mbqA-3eXnX2x2TxaUijUA)",house gin breakthrough automate floorplan design via relational refinement gin w convolutional message pass joint work autodesk research appear cvpr2021 url video show interactive browser demo url url play around url website url
ykilcher,MachineLearning,1618408423.0,[P] Video: I built a Neural Network in Minecraft | Analog Redstone Network w/ Backprop & Optimizer (NO MODS),"[https://youtu.be/7OdhtAiPfWY](https://youtu.be/7OdhtAiPfWY)

I built an analog neural network in vanilla Minecraft without any mods or command blocks. The network uses Redstone wire power strengths to carry the signal through one hidden layer, including nonlinearities, and then do automatic backpropagation and even weight updates.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

1:50 - Redstone Components Explained

5:00 - Analog Multiplication in Redstone

7:00 - Gradient Descent for Square Root Computation

9:35 - Neural Network Demonstration

10:45 - Network Schema Explained

18:35 - The Network Learns a Datapoint

20:20 - Outro & Conclusion

&#x200B;

I built this during a series of live streams and want to thank everyone who helped me and cheered for me in the chat!

&#x200B;

World saves here: [https://github.com/yk/minecraft-neural-network](https://github.com/yk/minecraft-neural-network)

Game here: [https://www.minecraft.net](https://www.minecraft.net)

Multiplier Inspiration: [https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag](https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag)",url build analog neural network vanilla minecraft without mods command block network use redstone wire power strengths carry signal one hide layer include nonlinearities automatic backpropagation even weight update x200b outline 0 00 intro overview1 50 redstone components explained5 00 analog multiplication redstone7 00 gradient descent square root computation9 35 neural network demonstration10 45 network schema explained18 35 network learn datapoint20 20 outro conclusion x200b build series live stream want thank everyone help cheer chat x200b world save url url inspiration url
human_treadstone,MachineLearning,1616807524.0,[D] Choosing correct baseline model for Metric learning,"I am using Matching and Prototypical network to perform Single shot learning(SSD) on my own dataset. But before this I want to implement the baseline to keep track of improvement. I am thinking about 1-Nearest neighbour as baseline. Is this a good baseline? I could not find any such GitHub implementation of 1-NN for SSD, in case already such baseline implemeted please point me out to such resources.",use match prototypical network perform single shoot learn ssd dataset want implement baseline keep track improvement think 1 nearest neighbour baseline good baseline could find github implementation 1 nn ssd case already baseline implemeted please point resources
Yuqing7,MachineLearning,1619711797.0,[R] Toward a New Generation of Neuromorphic Computing: IBM & ETH Zurich's Biologically Inspired Optimizer Boosts FCNN and SNN Training,"IBM and ETH Zurich researchers make progress in reconciling neurophysiological insights with machine intelligence, proposing a novel biologically inspired optimizer for artificial (ANNs) and spiking neural networks (SNNs) that incorporates synaptic integration principles from biology. GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals) leads to improvements in the training time convergence, accuracy and scalability of ANNs and SNNs. 

Here is a quick read: [Toward a New Generation of Neuromorphic Computing: IBM & ETH Zurich's Biologically Inspired Optimizer Boosts FCNN and SNN Training.](https://syncedreview.com/2021/04/29/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-8/)

 The paper *Learning in Deep Neural Networks Using a Biologically Inspired Optimizer* is on [arXiv](https://arxiv.org/pdf/2104.11604.pdf).",ibm eth zurich researchers make progress reconcile neurophysiological insights machine intelligence propose novel biologically inspire optimizer artificial anns spike neural network snns incorporate synaptic integration principles biology grapes group responsibility adjust propagation error signal lead improvements train time convergence accuracy scalability anns snns quick read toward new generation neuromorphic compute ibm eth zurich biologically inspire optimizer boost fcnn snn train url paper learn deep neural network use biologically inspire optimizer arxiv url
SuperFX,MachineLearning,1618524699.0,[D] A100 vs. A6000 for sequence learning tasks?,"Does anyone have first-hand experience with the performance differences between Nvidia's A100 and A6000 GPUs for large sequence tasks, e.g. using Transformers? I'm aware of the benchmarks that Lambda has posted but was wondering if research groups with hands-on experience using real models have done some benchmarking? The Lambda numbers suggest something on the order of 50% speed gain, for a price differential of 2.5x or so (I think--if people have data on wholesale prices that would be useful too), which is not a great deal, but not terrible. I'm wondering if this holds up in practice as I'd like to assemble a GPU server using one of these GPUs.",anyone first hand experience performance differences nvidia a100 a6000 gpus large sequence task e g use transformers aware benchmarks lambda post wonder research group hand experience use real model benchmarking lambda number suggest something order 50 speed gain price differential 2 5x think people data wholesale price would useful great deal terrible wonder hold practice like assemble gpu server use one gpus
_sshin_,MachineLearning,1617440536.0,[D] Short blogpost about EfficientNetV2,[https://mlpaper.com/2021/04/03/review-efficientnetv2-smaller-models-and-faster-training/](https://mlpaper.com/2021/04/03/review-efficientnetv2-smaller-models-and-faster-training/),url
Transit-Strike,MachineLearning,1619093209.0,[D] Are we underestimating how important studying theory is?,"I hear a lot of discussions where people essentially say ""Employers and Universities only care if you have good projects that prove you know what you learned. Courses mean nothing.""

and ""Just go to Kaggle and find a cool project to work on.""  


While that is technically true, are we really underselling the importance of theory?  


  
I could open up a Kaggle repo, see that everyone uses LeakyReLU, Batch Norm and ADAM. So I would be tempted to look at youtube videos and kaggle submissions and just recreate the same output. But does it really hold weight? I don't think so.  


I tried something similar and managed to build a decent solution that would later be a github repo. 

&#x200B;

The logic really fails when you try building something bigger and better though. If you have a styleGAN for example, you have to understand what the optimisation process is like. You need to know what people use batch norm for etc etc. It is impossible to understand the code otherwise.

&#x200B;

And even then, with very similar projects, I found myself able to apprach it 100x times better after doing a simple [deeplearning.ai](https://deeplearning.ai) course.

&#x200B;

Sure, you could cheat your way to a project that sounds sexy to a novice that knows nothing about ML. But have you learned anything if you use tf.nn everywhere and find usable results? I am not that sure really.",hear lot discussions people essentially say employers universities care good project prove know learn course mean nothing go kaggle find cool project work technically true really undersell importance theory could open kaggle repo see everyone use leakyrelu batch norm adam would tempt look youtube videos kaggle submissions recreate output really hold weight think try something similar manage build decent solution would later github repo x200b logic really fail try build something bigger better though stylegan example understand optimisation process like need know people use batch norm etc etc impossible understand code otherwise x200b even similar project find able apprach 100x time better simple deeplearning ai url course x200b sure could cheat way project sound sexy novice know nothing ml learn anything use tf nn everywhere find usable result sure really
OverLordGoldDragon,MachineLearning,1620238337.0,[P] Fastest wavelet transforms in Python + synchrosqueezing,"[ssqueezepy 0.6.1](https://github.com/OverLordGoldDragon/ssqueezepy) released w/ benchmarks, CWT up to x75 faster than PyWavelets on CPU and x900 on GPU (and more correct). STFT also CPU- and GPU-accelerated, and both synchrosqueezed.

Also see [Kymatio](https://github.com/kymatio/kymatio) for SOTA on timeseries with limited data, fast and differentiable; [nice lecture](https://youtu.be/4eyUReyIPXg).",ssqueezepy 0 6 1 url release w benchmarks cwt x75 faster pywavelets cpu x900 gpu correct stft also cpu gpu accelerate synchrosqueezed also see kymatio url sota timeseries limit data fast differentiable nice lecture url
dcpyro,MachineLearning,1616358255.0,[D] Is my idea of a Feature Store wrong?,"Should a Feature Store be part of an enterprise data catalog?

To me, a feature store seems to be a highly niche data catalog but  missing a lot of the benefits of having an enterprise data catalog /  data discovery tool. My need is to have generated features discoverable  when searching for data.

For example, if I have dataset A and B used to generate a feature set  AB', I would want to know about that information if I search and ever  come across dataset A or B in my data catalog.

Along with that, it would be beneficial to have the code / git commit that generated the features.

Am I missing something?",feature store part enterprise data catalog feature store seem highly niche data catalog miss lot benefit enterprise data catalog data discovery tool need generate feature discoverable search data example dataset b use generate feature set ab would want know information search ever come across dataset b data catalog along would beneficial code git commit generate feature miss something
fripperML,MachineLearning,1616589984.0,"[P] New library for performing nested cross validation, optimizing, calibrating and reporting quality of binary classification models"," 

The title says everything. :)

You can check it out here:

[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)

It's my first python package, so I'm sure many things could be improved. But if you find it interesting, please let me know! I will appreciate it a lot!",title say everything check url first python package sure many things could improve find interest please let know appreciate lot
gtgski,MachineLearning,1619394734.0,[D] VAE but every neuron models a distribution,"Variational AutoEncoder model has a distinctive middle layer where neuron activations are modeled as a unit gaussian.

Is there any experiments where every neuron is modeled as a unit gaussian? So the rest of the layers are the same as the middle layer.",variational autoencoder model distinctive middle layer neuron activations model unit gaussian experiment every neuron model unit gaussian rest layer middle layer
proximauri,MachineLearning,1617872991.0,[D] Model does not learn with more classes.,"Hi everyone,I have tried to train a VGG16 model with my dataset using transfer learning. The task is face recognition (classification with softmax).

I have first tried to only train with 80 classes. Everything is fine and the model converges and accuracy is over 99%. However, when I train the model  with like 300 classes (including the first 80 identities as well) the network does not learn and the accuracy reaches to 4 percent only.

What could be the reason? Thanks.",hi everyone try train vgg16 model dataset use transfer learn task face recognition classification softmax first try train nmbr class everything fine model converge accuracy 99 however train model like nmbr class include first nmbr identities well network learn accuracy reach nmbr percent could reason thank
ML_WAYR_bot,MachineLearning,1620590405.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 112,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/kadisonsinger: [https://arxiv.org/abs/1802.05296](https://arxiv.org/abs/1802.05296)

/u/Z30G0D: [https://arxiv.org/pdf/2103.02667.pdf](https://arxiv.org/pdf/2103.02667.pdf)

/u/FakespotAnalysisBot: [Link to Fakespot Analysis](https://fakespot.com/product/make-art-with-artificial-intelligence-make-and-sell-your-art-with-ai-blockchain-and-nft-awesome-ai)

Besides that, there are no rules, have fun.",place share machine learn research paper journals article read week relate research mean elaborate give us insight otherwise could interest paper read please try provide insight understand please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent link previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 week 1 url 11 url 21 url 31 url 41 url 51 url 61 url 71 url 81 url 91 url 101 url 111 url 2 url 12 url 22 url 32 url 42 url 52 url 62 url 72 url 82 url 92 url 102 url 3 url 13 url 23 url 33 url 43 url 53 url 63 url 73 url 83 url 93 url 103 url 4 url 14 url 24 url 34 url 44 url 54 url 64 url 74 url 84 url 94 url 104 url 5 url 15 url 25 url 35 url 45 url 55 url 65 url 75 url 85 url 95 url 105 url 6 url 16 url 26 url 36 url 46 url 56 url 66 url 76 url 86 url 96 url 106 url 7 url 17 url 27 url 37 url 47 url 57 url 67 url 77 url 87 url 97 url 107 url 8 url 18 url 28 url 38 url 48 url 58 url 68 url 78 url 88 url 98 url 108 url 9 url 19 url 29 url 39 url 49 url 59 url 69 url 79 url 89 url 99 url 109 url 10 url 20 url 30 url 40 url 50 url 60 url 70 url 80 url 90 url 100 url 110 url upvoted paper two weeks ago u kadisonsinger url url link fakespot analysis url rule fun
cbsudux,MachineLearning,1617622290.0,[D] Anyone deploy DL models with AWS Lambda? Trying to estimate costs," Has anyone deployed DL models with AWS Lambda with a GPU? Looking to find out the minimum it would cost to deploy it serverless with AWS Lambda with some mid-level GPU - T4/P4 equivalent.

[https://www.fixmyphoto.ai/](https://www.fixmyphoto.ai/) has been deployed ""on a serverless React application with AWS lambda functions handling inference"" according to the [github page](https://github.com/MathiasGruber/PConv-Keras).",anyone deploy dl model aws lambda gpu look find minimum would cost deploy serverless aws lambda mid level gpu t4 p4 equivalent url deploy serverless react application aws lambda function handle inference accord github page url
WavyShapes,MachineLearning,1618930996.0,[P] Backprop Model Hub: a curated list of state-of-the-art models,"Hey everyone,

I want to share this [model hub](https://backprop.co/hub) we've been working on. It's a curated list of state-of-the-art text and vision models, including performance benchmarks on some notable datasets.

Our goal is to make this a convenient space to find a production-ready model that is fast and easy to use in real-world scenarios. We're trying to keep the selection focused on quality over quantity.

We've also got an [open-source library](https://github.com/backprop-ai/backprop) that makes using + finetuning these models possible in a few lines of code.

We want to enable more people to take advantage of the latest ML research without needing massive datasets or deep learning expertise.

If this helps you and you'd like to see some more tasks/models, let us know — we'd love to hear people's thoughts.",hey everyone want share model hub url work curated list state art text vision model include performance benchmarks notable datasets goal make convenient space find production ready model fast easy use real world scenarios try keep selection focus quality quantity also get open source library url make use finetuning model possible line code want enable people take advantage latest ml research without need massive datasets deep learn expertise help like see task model let us know love hear people thoughts
Seankala,MachineLearning,1616906465.0,[D] Not able to reproduce SoTA baseline results due to (assumed) package mismatch. How should I go about this?,"Hi. I'm a student working on a research project. There's a SoTA baseline model that I have managed to run, but unfortunately the performance is quite a bit lower than that reported in the paper. Searching around the closed issues in GitHub has led me to believe that it may probably be due to a package version mismatch.

Unfortunately the package that the original implementers used is a relatively older version, and in order to install it I believe I'd have to downgrade many of the drivers in the server I'm currently using to run my experiments.

What would your advice be to someone in my shoes? This is my first actual project so I'm not sure how to proceed. Should I just use these results with a disclaimer, or should I do whatever I can to achieve the original performances first?

Thanks.",hi student work research project sota baseline model manage run unfortunately performance quite bite lower report paper search around close issue github lead believe may probably due package version mismatch unfortunately package original implementers use relatively older version order install believe downgrade many drivers server currently use run experiment would advice someone shoe first actual project sure proceed use result disclaimer whatever achieve original performances first thank
jnforcer,MachineLearning,1620072239.0,[R] XGBoost works best for intelligent Deep Brain Stimulation,"https://www.biorxiv.org/content/10.1101/2021.04.24.441207v2

Smart brain implants will revolutionize neurotechnology for improving the quality of life in patients with brain disorders. The treatment of Parkinson's disease (PD) with neural implants for deep brain stimulation presents an avenue for developing machine-learning based individualized treatments to refine human motor control. We developed an optimized movement decoding approach to predict grip-force based on sensorimotor electrocorticography and subthalamic local field potentials in PD. We demonstrate that electrocorticography combined with Bayesian optimized extreme gradient boosted decision trees outperform other machine learning approaches. We elucidate a link between dopamine and movement coding capacity in PD, by showing negative correlations between decoding performance and motor symptom severity in the medication OFF state. Finally, we introduce an approach that leverages wholebrain connectomics to predict machine-learning based decoding performance in invasive neurophysiology. Our study provides a framework to aid development of intelligent adaptive deep brain stimulation.",url brain implant revolutionize neurotechnology improve quality life patients brain disorder treatment parkinson disease pd neural implant deep brain stimulation present avenue develop machine learn base individualize treatments refine human motor control develop optimize movement decode approach predict grip force base sensorimotor electrocorticography subthalamic local field potentials pd demonstrate electrocorticography combine bayesian optimize extreme gradient boost decision tree outperform machine learn approach elucidate link dopamine movement cod capacity pd show negative correlations decode performance motor symptom severity medication state finally introduce approach leverage wholebrain connectomics predict machine learn base decode performance invasive neurophysiology study provide framework aid development intelligent adaptive deep brain stimulation
FirstTimeResearcher,MachineLearning,1619818763.0,"[D] ICML Conference: ""we plan to reduce the number of accepted papers. Please work with your SAC to raise the bar. AC/SAC do not have to accept a paper only because there is nothing wrong in it.""","ICML Conference has decided to lower the acceptance rate by 10%: 

https://twitter.com/tomgoldsteincs/status/1388156022112624644

""According to the current Meta-Review statistics, we need to raise the acceptance bar. Please coordinate with ACs on reducing about 10% of accepted submissions."" -- https://twitter.com/ryan_p_adams/status/1388164670410866692",icml conference decide lower acceptance rate 10 url current meta review statistics need raise acceptance bar please coordinate acs reduce 10 accept submissions url
Yuqing7,MachineLearning,1616607343.0,"[N] CMU, Oxford & Facebook Cross-Lingual Vision-Language Model Achieves New SOTA in Zero-Shot Setting","A research team from CMU, Oxford and Facebook AI proposes a vision-language model that, when trained on a source language, can be applied to different languages without additional annotated training data.

Here is a quick read: [CMU, Oxford & Facebook Cross-Lingual Vision-Language Model Achieves New SOTA in Zero-Shot Setting](https://syncedreview.com/2021/03/24/cmu-oxford-facebook-cross-lingual-vision-language-model-achieves-new-sota-in-zero-shot-setting/)

The paper *Multilingual Multimodal pretraining for Zero-Shot Cross-Lingual Transfer of Vision-Language Models* is on [arXiv](https://arxiv.org/pdf/2103.08849.pdf).",research team cmu oxford facebook ai propose vision language model train source language apply different languages without additional annotate train data quick read cmu oxford facebook cross lingual vision language model achieve new sota zero shoot set url paper multilingual multimodal pretraining zero shoot cross lingual transfer vision language model arxiv url
StellaAthena,MachineLearning,1616370695.0,[P] EleutherAI releases 1.3B and 2.7B GPT-3-style models trained on the Pile,"The [GPT-Neo](https://github.com/EleutherAI/gpt-neo/) project by [EleutherAI](https://www.eleuther.ai/) has released 1.3B and 2.7B parameter GPT-3-style models. The models are trained on [the Pile](https://pile.eleuther.ai), a 800 GB curated dataset EleutherAI released in January.

The release includes:

1. The full modeling code, written in Mesh TensorFlow and designed to be run on TPUs.
2. Trained model weights.
3. Optimizer states, which allow you to continue training the model from where EAI left off.
4. A [Google Colab notebook](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) that shows you how to use the code base to train, fine-tune, and sample from a model.

Before people inevitably get confused, **this is not the same size as GPT-3**. The 1.3B model is a little smaller than GPT-2, and then 2.7B model is about twice as big. GPT-3 comes in a variety of sizes, including 1.3B and 2.7B (which is why we chose those values too). The ""full"" GPT-3 model is 175B parameters.

**Edit:** We've gotten a couple questions about this in Discord, so I wanted to share the following here as well. To the best of my knowledge this is a complete list of all announced autoregressive, non-MoE transformers. A model is considered public if anyone can download the trained model weights for free. No, it's not a typo that I say Facebook has trained a Megatron LM model, you can find the weights [here](https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b).

&amp;#x200B;

|Model|Size|Creator|Public|
|:-|:-|:-|:-|
|**GPT-Neo (small)**|**1.3B**|**EleutherAI**|**Yes**|
|**GPT-2**|**1.5B**|**OpenAI**|**Yes**|
|Meena|2.6B|Google|No|
|GPT-3 2.7B|2.7B|OpenAI|No|
|**GPT-Neo (mid)**|**2.7B**|**EleutherAI**|**Yes**|
|GPT-3 6.7B|6.7B|OpenAI|No|
|Megatron LM|8.3B|NVIDIA|No|
|**Megatron LM**|**11B**|**Facebook**|**Yes**|
|GPT-3 13B|13B|OpenAI|No|
|Turing NLG|17B|Microsoft|No|
|GPT-3 175B|175B|OpenAI|No|",gpt neo url project eleutherai url release 1 3b 2 7b parameter gpt 3 style model model train pile url nmbr gb curated dataset eleutherai release january release include 1 full model code write mesh tensorflow design run tpus 2 train model weight 3 optimizer state allow continue train model eai leave 4 google colab notebook url show use code base train fine tune sample model people inevitably get confuse size gpt 3 1 3b model little smaller gpt 2 2 7b model twice big gpt 3 come variety size include 1 3b 2 7b choose value full gpt 3 model 175b parameters edit get couple question discord want share follow well best knowledge complete list announce autoregressive non moe transformers model consider public anyone download train model weight free typo say facebook train megatron lm model find weight url small 1 3b eleutherai yes gpt 2 1 5b openai yes meena 2 6b google gpt 3 2 7b 2 7b openai gpt neo mid 2 7b eleutherai yes gpt 3 6 7b 6 7b openai megatron lm 8 3b nvidia megatron lm 11b facebook yes gpt 3 13b 13b openai turing nlg 17b microsoft gpt 3 175b 175b openai
,MachineLearning,1617032593.0,[P] NeMo Getting Started. Prototyping Conversational AI Application,"Hey all!  Hope everyone is having a great Monday, I know I am. I want to let everyone know about a NVIDIA AI platform piece I use daily, NeMo. 

[NVIDIA NeMo](https://github.com/NVIDIA/NeMo) is a toolkit for building new State-of-the-Art Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new Conversational AI model architectures.

Conversational AI architectures are typically large and require a lot of data and compute for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node mixed-precision training.

[Cool collab notebook for getting started with NeMo](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/NeMo_Getting_Started.ipynb) shows how we can construct a toy demo showing the ability to translate Russian audio files into English. That's what is so neat about NeMo is the pipeline is so diverse; we can translate these audio files into English, then if we wanted to, use BERT for the NLP task for the translated English as well. And so on and so on.   


Anyways, I hope you enjoy this collab book, and to have a poke around NeMo afterwards.",hey hope everyone great monday know want let everyone know nvidia ai platform piece use daily nemo nvidia nemo url toolkit build new state art conversational ai model nemo separate collections automatic speech recognition asr natural language process nlp text speech tts model collection consist prebuilt modules include everything need train data every module easily customize extend compose create new conversational ai model architectures conversational ai architectures typically large require lot data compute train nemo use pytorch lightning easy performant multi gpu multi node mix precision train cool collab notebook get start nemo url show construct toy demo show ability translate russian audio file english neat nemo pipeline diverse translate audio file english want use bert nlp task translate english well anyways hope enjoy collab book poke around nemo afterwards
programmerChilli,MachineLearning,1619010169.0,[R] Rotary Positional Embeddings - a new relative positional embedding for Transformers that significantly improves convergence (20-30%) and works for both regular and efficient attention,"This was originally discovered by some Chinese researchers  and had been circulating online for some time before they published their preprint: 
https://arxiv.org/abs/2104.09864

In the meantime, EleutherAI has been playing around with it and found it to be extremely effective across a diverse range of settings and sizes.

https://blog.eleuther.ai/rotary-embeddings/

None of the authors of the Arxiv paper are part of EleutherAI - Eleuther's interest is purely that ""this works really well"".

I wouldn't be surprised to see this positional embedding become the default.",originally discover chinese researchers circulate online time publish preprint url meantime eleutherai play around find extremely effective across diverse range settings size url author arxiv paper part eleutherai eleuther interest purely work really well surprise see positional embed become default
chilled_87,MachineLearning,1620064529.0,[P] Reproducible research: Machine learning for credit card fraud detection,"ML for credit card fraud detection is one of those fields where most of the published research is unfortunately not reproducible. Real-world transaction data cannot be shared for confidentiality reasons, but we also believe authors do not make enough efforts to provide their code and make their results reproducible. 

We just released the five first chapters of a book on the topic, which aims at making a first step towards improving the reproducibility of research in this domain: [https://fraud-detection-handbook.github.io/fraud-detection-handbook](https://fraud-detection-handbook.github.io/fraud-detection-handbook). 

This preliminary release is in a Jupyter book format, making all the experiments and results reproducible, under an open-source license. Published chapters cover the background, motivations, and baseline methodologies. Forthcoming chapters will address more advanced topics, such as class imbalance, feature engineering, deep learning, and interpretability. 

Feedback is welcome!",ml credit card fraud detection one field publish research unfortunately reproducible real world transaction data share confidentiality reason also believe author make enough efforts provide code make result reproducible release five first chapters book topic aim make first step towards improve reproducibility research domain url preliminary release jupyter book format make experiment result reproducible open source license publish chapters cover background motivations baseline methodologies forthcoming chapters address advance topics class imbalance feature engineer deep learn interpretability feedback welcome
ispeakdatruf,MachineLearning,1619311145.0,[D] Looking for interesting classification datasets,"I have some new techniques that I want to try on small cardinality classification problems where the SOTA still has some headroom. So, for example, MNIST is out, since SOTA is already 99.xx%. And so is ImageNet, as the number of classes is too large. The ideal dataset would have <= 10 classes, with SOTA being < 90%. 

Are there any such interesting datasets out there? Thanks in advance.",new techniques want try small cardinality classification problems sota still headroom example mnist since sota already 99 xx imagenet number class large ideal dataset would nmbr class sota 90 interest datasets thank advance
ilikepancakez,MachineLearning,1618773529.0,[D] Neural Nets (1994),"[https://www.teamten.com/lawrence/writings/plan02.html](https://www.teamten.com/lawrence/writings/plan02.html)

I was in college and interested in neural nets when this was written, and as it happens later graduated and worked with the author of this post. In academia at the time, I believe neural nets were seen as a fading trend, something that was a cute idea but wasn’t destined to work out because they didn’t work well enough and even if they did we didn’t have the compute for anything useful. Personally, I was fascinated, and I know several other people who kept hope as well, one who is now quite a well known AI / neural nets researcher. I didn’t pursue neural nets personally, but I did keep using related genetic algorithms and artificial evolution, which I think was viewed with the same mix of skepticism generally, with pockets of enthusiasts. Gates in the story was probably sharing what he’d been hearing from academics. Lawrence (author of the post) is an incredibly smart person, and I have no doubt was able to see past what many of the researchers were saying.

BTW, I recently happened across a [paper from 1967](http://pageperso.lif.univ-mrs.fr/~edouard.thiel/rech/1967-blum.pdf) (53 years ago) that mentions neural networks in passing as if it was a popular idea at the time - the original paper on the Medial Axis Transform! Scroll to the 2nd page (“364”) near the top: “Consider a continuous isotropic plane (an idealization of an active granular material or a rudimentary neural net) that has the following properties at each point”",url college interest neural net write happen later graduate work author post academia time believe neural net see fade trend something cute idea destine work work well enough even compute anything useful personally fascinate know several people keep hope well one quite well know ai neural net researcher pursue neural net personally keep use relate genetic algorithms artificial evolution think view mix skepticism generally pocket enthusiasts gate story probably share hed hear academics lawrence author post incredibly smart person doubt able see past many researchers say btw recently happen across paper 1967 url 53 years ago mention neural network pass popular idea time original paper medial axis transform scroll 2nd page 364 near top consider continuous isotropic plane idealization active granular material rudimentary neural net follow properties point
IborkedyourGPU,MachineLearning,1618497148.0,[D] MLFlow vs ClearML vs Gradient Paperspace,"The MLOps tools scene is slightly overcrowded (\s). I want to limit the comparison to the three tools in the subject. Requirements:

 - run on-prem (we've got our own hardware, don't need/want cloud)
 - easily launch experiments. 
 - experiment database: log metrics, create plots, compare different experiments visually, track model performance over time
 - easy to install and use. Setting up some experiment trackers require a level of DevOp savvy that many data science teams don’t have, so that's a no-no.

Nice-to-have:
 - deploy models as API endpoints. We can use Docker for this, so this isn't really crucial, but...it's a nice-to-have.
 - hyperparameter tuning. It's fine if hyperparameter tuning relies on some third-party plugin, as long as it's open source (for example, MLFlow + Hydra + Ax plugin should be able to handle hyperparameter tuning through Bayesian Optimization just fine).
 - open source. AFAIK, MLFlow & ClearML are open source, while Gradient is not.

How do MLFlow, ClearML and Gradient compare in these respects? I'm especially interested in the comparison between MLFlow & ClearML - from my point of view as an outsider, they seem pretty similar, but I could be wrong, ofc.
‍",mlops tool scene slightly overcrowd want limit comparison three tool subject requirements run prem get hardware need want cloud easily launch experiment experiment database log metrics create plot compare different experiment visually track model performance time easy install use set experiment trackers require level devop savvy many data science team nice deploy model api endpoints use docker really crucial nice hyperparameter tune fine hyperparameter tune rely third party plugin long open source example mlflow hydra ax plugin able handle hyperparameter tune bayesian optimization fine open source afaik mlflow clearml open source gradient mlflow clearml gradient compare respect especially interest comparison mlflow clearml point view outsider seem pretty similar could wrong ofc
darkmabler,MachineLearning,1617811607.0,[D] Models in production - what architecture do you use?,"Hi all! I'm new to the community but have been in the ML space for some time and was curious what everyone around here uses for serving their models in production.

Right now, I have implemented a few projects that utilize AWS EKS and NVIDIA Triton Inference Server. The projects I'm implementing this for need to handle 10's of thousands of concurrent requests so I chose EKS due to how scalable it is.

What are your thoughts/what have you done? Anyone ever use NVIDIA Triton Inference Server? Do you like it more than Tensorflow Serving?",hi new community ml space time curious everyone around use serve model production right implement project utilize aws eks nvidia triton inference server project implement need handle nmbr thousands concurrent request choose eks due scalable thoughts anyone ever use nvidia triton inference server like tensorflow serve
DeepML42,MachineLearning,1618953886.0,[D] Marginal Likelihood vs. Likelihood for Variational Autoencoder,"In this [discussion](https://www.reddit.com/r/MachineLearning/comments/5qm6ag/d_how_to_calculate_variational_autoencoder_log/) the computation of the likelihood for VAEs is discussed. Here they talk about likelihood, but isn‘t p(x) the marginal likelihood? Is there a difference between marginal likelihood and likelihood for VAEs? If so, how is each computed for VAEs?",discussion url computation likelihood vaes discuss talk likelihood p x marginal likelihood difference marginal likelihood likelihood vaes compute vaes
Megixist,MachineLearning,1619071095.0,[P] Implementation of MADGRAD optimization algorithm for Tensorflow,"I am pleased to present a Tensorflow implementation of the MADGRAD optimization algorithm, which was published by Facebook AI in their paper [Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization](https://arxiv.org/pdf/2101.11075v2.pdf) (Aaron Defazio and Samy Jelassi, 2021). When this algorithm was first introduced, several people requested that it be implemented in tf.keras, so I decided to do so. This implementation's main features include:

1. Simple integration into every tf.keras model: Since the MadGrad subclass derives from the      OptimizerV2 superclass, it can be used in the same way as any other tf.keras optimizer.
2. Built-in weight decay support.
3. Full Learning Rate scheduler support.
4. Complete support for sparse vector backpropagation.
5. Available on PyPi so you can install and import it directly.

Any questions or concerns about the implementation or the paper are welcome!

You can check out the repository [here](https://github.com/DarshanDeshpande/tf-madgrad) for examples and test cases. If you like the work then considering giving it a star! :)",please present tensorflow implementation madgrad optimization algorithm publish facebook ai paper adaptivity without compromise momentumized adaptive dual average gradient method stochastic optimization url aaron defazio samy jelassi 2021 algorithm first introduce several people request implement tf keras decide implementation main feature include 1 simple integration every tf keras model since madgrad subclass derive optimizerv2 superclass use way tf keras optimizer 2 build weight decay support 3 full learn rate scheduler support 4 complete support sparse vector backpropagation 5 available pypi install import directly question concern implementation paper welcome check repository url examples test case like work consider give star
xEdwin23x,MachineLearning,1619117403.0,"[D] In a scale of 1 to 10 how much importance or thought do you guys put into the SWE and quality of the code in general you're writing when doing experiments? I feel its important but at the same time there's lots of pressure to move fast, so how do you strike a balance in this aspect?","I've been reading a lot on how to write better code, not only to become a better SWE but a better ML researcher. I personally feel its REALLY important to write good, clear, modular and reusable code, but at the same time the fast moving nature of ML (in academia at least, dunno in industry) pushes you to get an MVP (minimum viable product, in this case, a model with results) as fast as possible, making it hard to think about what or how should things be done in advance. 

I see even big research groups putting up code that looks ugly or that needs quite a few tweaks to keep changing, so Im curious what are your opinions on this topic and how do you approach this problem.",read lot write better code become better swe better ml researcher personally feel really important write good clear modular reusable code time fast move nature ml academia least dunno industry push get mvp minimum viable product case model result fast possible make hard think things advance see even big research group put code look ugly need quite tweak keep change im curious opinions topic approach problem
Fun_Huckleberry_8991,MachineLearning,1619411818.0,[P] Reinforcement Learning with multiple simultaneous actions?,"Hi, I am currently working on a project related to using reinforcement learning. The agent has a large set of possible discrete actions. For example, given a graph with N nodes, the possible action can be choosing 1 of N node, then check the stopping criteria.

We actually can solve it with DQN. However, the bottle-neck is that the Agent only picks one node at each step. With a large N, it may make the runtime increase. Does anyone know any possible RL method that the agent can pick multiple nodes (more than one node) at each step? Thanks a lot.",hi currently work project relate use reinforcement learn agent large set possible discrete action example give graph n nod possible action choose nmbr n node check stop criteria actually solve dqn however bottle neck agent pick one node step large n may make runtime increase anyone know possible rl method agent pick multiple nod one node step thank lot
donjuan1337,MachineLearning,1617368020.0,[D] Modeling class errors,"I'm working on a project where I'll be post-processing a predictive system using ML. Both real-valued and ranked variables are predicted by this predictive system.

My strategy is to model the systems errors. For the real valued variable, the difference between the target and prediction is trivial but I have some problem regarding the ranked variable e.g. \[0,1,2,3\].

Each rank can be seen as a class. But I'm unsure how to model errors with class variables. Do you have any suggestion? The reason to model the error is to make the data time invariant which is good since the data set is limited.",work project post process predictive system use ml real value rank variables predict predictive system strategy model systems errors real value variable difference target prediction trivial problem regard rank variable e g 0 1 2 3 rank see class unsure model errors class variables suggestion reason model error make data time invariant good since data set limit
Equivalent-Choice-75,MachineLearning,1620009742.0,[D] Does Pytorch/TF/Jax work well with M1 GPU?,"This information is surprisingly difficult to find online. There are several conflicting sources of information. 

Just a simple question - Does any of pytorch/tf/jax work with M1 GPU as well as with Nvidia GPU? Or even comparable?",information surprisingly difficult find online several conflict source information simple question pytorch tf jax work m1 gpu well nvidia gpu even comparable
SomeParanoidAndroid,MachineLearning,1617702239.0,[D] Choosing ML + numerical computations workstation/server,"I am starting my PhD in machine learning in a newly established lab that is mainly focused on wireless communications and I have been tasked with finding an server/workstation to get for the office. My budget is around $3500-$4500. 

I of course will be utilizing the GPU to train models, but part of my work involves running numerical simulations, optimization algorithms, physical models, this sort of stuff. The rest of the (2-4) people will be dealing with the latter stuff exclusively.

Also, while I write all of my code in python using numba and tensorflow, the others will be using MATLAB. A final constraint is that we are based in Europe and my supervisor is skeptical about ordering from the US or abroad in general due to delivery times and extra complexity. So I would ideally like something that is generally available from suppliers anywhere (no LambdaLabs or System 76 unfortunately :/ ). 

Considering that we will all be probably needing the server concurrently and that our code usually takes several hours, what parts should I be prioritizing? Especially:

* I am thinking of getting an RTX 3080  (seems one of the best VFM GPUs for DL)
* i9/Ryzen or Xeon? supposedly Xeons are able to handle multiple jobs better and stay on without problems longer, but they look WAY slower. I see that they support ECC memory which I don't know if it has any relevance to our field. Also, most of our code could use a good CPU. **\[This is my most important dilemma\]** 
* Would 32GB RAM be enough? I would like to get 64GB, but it may be out of budget. Do you think I should prioritize this?
* Is liquid cooling needed? The server will be placed in our office.
* Does a dual GPU setup make sense? (e.g. 2x 3070) It seems ridiculous to spend such a huge portion of the budget when I have to account for everyone.

Plus some other more general questions/discussion points:

* ""Gaming"" pc as a workstation? If we opt for the i9, some alienware configurations hit the specs spot on. Should I avoid them? **\[I would appreciate opinions on this too\]**
* Are ""workstation solutions for deep learning"" from companies like IBM, Lenovo, HP, e.t.c. in this price range out of touch with reality? They offer some setups with 8GB ram, P1000 with 4GB VRAM (RTX 4000 is the closest to the 30 series I can find) and so on... For the same budget I could have a dream setup from Lambda Labs or something equivalent.
* Linux vs Windows? I feel way more comfortable on the former and I kind of fear that windows will just get in the way unnecessarily.
* MATLAB does support running code remotely, right?",start phd machine learn newly establish lab mainly focus wireless communications task find server workstation get office budget around 3500 4500 course utilize gpu train model part work involve run numerical simulations optimization algorithms physical model sort stuff rest 2 4 people deal latter stuff exclusively also write code python use numba tensorflow others use matlab final constraint base europe supervisor skeptical order us abroad general due delivery time extra complexity would ideally like something generally available suppliers anywhere lambdalabs system nmbr unfortunately consider probably need server concurrently code usually take several hours part prioritize especially think get rtx nmbr seem one best vfm gpus dl i9 ryzen xeon supposedly xeons able handle multiple job better stay without problems longer look way slower see support ecc memory know relevance field also code could use good cpu important dilemma would 32gb ram enough would like get 64gb may budget think prioritize liquid cool need server place office dual gpu setup make sense e g 2x 3070 seem ridiculous spend huge portion budget account everyone plus general question discussion point game pc workstation opt i9 alienware configurations hit specs spot avoid would appreciate opinions workstation solutions deep learn company like ibm lenovo hp e c price range touch reality offer setups 8gb ram p1000 4gb vram rtx nmbr closest nmbr series find budget could dream setup lambda labs something equivalent linux vs windows feel way comfortable former kind fear windows get way unnecessarily matlab support run code remotely right
bendee983,MachineLearning,1617032735.0,[D] Algorithms Are Not Enough,"""Algorithms Are Not Enough"" provides a fresh perspective on the shortcomings of current AI systems.

The main idea discussed in AANE is that current AI systems are heavily dependent on representations. A human engineer must discover a problem and simplify the solution into distinct steps, a set of input data and expected outcomes, or a set of rewards and actions. Only then can an AI algorithm be designed to solve that problem. What we're lacking is not algorithms that can solve complicated problems but algorithms that can seek out and discover new problems and develop their solutions without help from humans.

Review of the book and discussion with the author, Herbert Roitblat, here:

[https://bdtechtalks.com/2021/03/29/ai-algorithms-representations-herbert-roitblat/](https://bdtechtalks.com/2021/03/29/ai-algorithms-representations-herbert-roitblat/)",algorithms enough provide fresh perspective shortcomings current ai systems main idea discuss aane current ai systems heavily dependent representations human engineer must discover problem simplify solution distinct step set input data expect outcomes set reward action ai algorithm design solve problem lack algorithms solve complicate problems algorithms seek discover new problems develop solutions without help humans review book discussion author herbert roitblat url
jayalammar,MachineLearning,1619708819.0,[P] Explainable AI Cheat Sheet (Image + Video),"Hi r/MachineLearning!

I have created a high-level map to major categories of ML Explainability methods. 

Explainable AI cheat sheet: [https://ex.pegg.io/](https://ex.pegg.io/)

Video going over it: [https://www.youtube.com/watch?v=Yg3q5x7yDeM](https://www.youtube.com/watch?v=Yg3q5x7yDeM)

&#x200B;

It's a large field and this is a non-exhaustive list to help orient people coming into the domain. How do you think I can improve it? All feedback is appreciated!",hi r machinelearning create high level map major categories ml explainability methods explainable ai cheat sheet url go url large field non exhaustive list help orient people come domain think improve feedback appreciate
Advanced-Hedgehog-95,MachineLearning,1616961935.0,[D] Motivation to use 768 dimensional embeddings from Transformers?,"Is there a scientific reason to have most if not all transformer models have 768- dimensional embeddings?

Even wav2vec2 and mockingjay models for audio have 768 dimensional embeddings.",scientific reason transformer model 768 dimensional embeddings even wav2vec2 mockingjay model audio nmbr dimensional embeddings
HashRocketSyntax,MachineLearning,1618613481.0,Why do practitioners still use regular tensorflow? [D],"When I look at the 2.4 \`nn\` class, it has a handful of losses mixed in with the hidden layers, and it doesn't have optimizers. When I look for tensorflow optimizers and tensorflow losses it either points to tf.keras or tf.compat v1.

It's my understanding that a lot of practitioners are using tensorflow (not keras) - why? If this is the case, are they using v1 or v2? Are they able to do more low-level fancy footwork with their layers?

Not trying to be facetious. Truly seeking to understand.

---

*EDIT = here are my takeaways from comments:*
- Custom batch/ epoch operations.
- Performance.
- Legacy code.
- Embedded in devices.",look nmbr nn class handful losses mix hide layer optimizers look tensorflow optimizers tensorflow losses either point tf keras tf compat v1 understand lot practitioners use tensorflow keras case use v1 v2 able low level fancy footwork layer try facetious truly seek understand edit takeaways comment custom batch epoch operations performance legacy code embed devices
latticeprep,MachineLearning,1617339115.0,[D] How does stripe use GBT's to find edge similarity?,"I'm reading Stripe's [blog post on how it finds similar accounts to flag fraudulent activity](https://stripe.com/blog/similarity-clustering) and it just doesn't make any sense to me. I'm wondering if anyone has any idea on how GBT's can be used on adjacency lists like this.

From the post:

>Over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. By sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. We use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.  
>  
>Because of the wide variety of features we can construct from given pairs of accounts, we decided to use [gradient-boosted decision trees](https://en.wikipedia.org/wiki/Gradient_boosting) (GBDTs) to represent our similarity model. In practice, we’ve found GBDTs strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. When we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. The variant that we use, [XGBoost](https://xgboost.readthedocs.io/en/latest/), is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. You can read more about the infrastructure we use to [train machine learning models](https://stripe.com/en-ca/blog/railyard-training-models) at Stripe in a previous post.  
>  
>Now that we have a trained model, we can use it to predict fraudulent activity. Since this model operates on pairs of Stripe accounts, it’s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. Instead, we first generate a candidate set of edges to be scored. We do this by taking recently created Stripe accounts and creating edges between accounts that share certain attributes. Although this isn’t an exhaustive approach, this heuristic works well in practice to prune the set of candidate edges to a reasonable number.  
>  
>Once the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. We then compute the connected components on the resulting graph. The final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. In particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.  
>  
>This is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster’s operation are created. And the more fraud rings we detect and shutdown at Stripe, the more accurate our clustering model becomes at identifying new clusters in the future.

I found this baffling. I didn't realize GBTs could be used to effectively find cliques in a graph. Has anyone tried this or understand how it works better? I don't really understand how these edge candidates are scored against past known-fraudulent edges. I just don't have a sense of the feature space for this at all.",read stripe blog post find similar account flag fraudulent activity url make sense wonder anyone idea gbt use adjacency list like post years risk underwrite team manually compile many examples exist cluster fraudulent account investigations fraud ring use reference cluster train data learn similarity function sample edge group obtain dataset consist pair account along label pair indicate whether two account belong cluster use intra cluster edge positive train examples inter cluster edge negative train examples edge denote pair account wide variety feature construct give pair account decide use gradient boost decision tree url gbdts represent similarity model practice weve find gbdts strike right balance easy train strong predictive power robust despite variations data start project want get something door quickly effective well understand properties straightforward fine tune variant use xgboost url one best perform shelf model case structure also know tabular data well develop infrastructure train serve read infrastructure use train machine learn model url stripe previous post train model use predict fraudulent activity since model operate pair stripe account feasible fee possible pair account compute score across pair instead first generate candidate set edge score take recently create stripe account create edge account share certain attribute although exhaustive approach heuristic work well practice prune set candidate edge reasonable number candidate edge score filter edge select similarity score threshold compute connect components result graph final output set high fidelity account cluster analyze process manually inspect together unit particular fraud analyst may want examine cluster contain know fraudulent account investigate remain account cluster iterative process individual cluster grow quickly identify increase similarity fake account fraudsters operation create fraud ring detect shutdown stripe accurate cluster model become identify new cluster future find baffle realize gbts could use effectively find cliques graph anyone try understand work better really understand edge candidates score past know fraudulent edge sense feature space
fool126,MachineLearning,1616478880.0,[D] Recent (2021-03) review papers of different areas in the field,"Motivated by a recent post sharing the review paper [Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models](https://arxiv.org/abs/2103.04922), I wanted to crowdsource compile a list of review papers for other areas. This will be especially helpful for those looking to catch up to specific areas in the ever quickly expanding field.",motivate recent post share review paper deep generative model comparative review vaes gans normalize flow energy base autoregressive model url want crowdsource compile list review paper areas especially helpful look catch specific areas ever quickly expand field
Lairv,MachineLearning,1618660227.0,[D] How to handle big datasets in computer vision ?,"I'm currently trying to use the [ScanNet](http://www.scan-net.org/) dataset. This dataset contains a lot of things (3D models, depth images, semantic segmentation etc...) and weights a total of 1.3TB. But for my purpose I only need the RGB color images, which should be something like 200Gb. This is still quite huge, and for the moment I can't even download the dataset on my machine.

I have seen several projects (e.g. [Atlas](https://github.com/magicleap/Atlas)) which makes use of this dataset to train their models. How do they do it ? Is it possible to download such datasets on cloud platforms ?

Edit : there seems to be a debate on whether this really is ""big data"", at least at my scale it requires more work than simply training on MNIST, although I'm sure this is nothing compared to what they did to train GPT3",currently try use scannet url dataset dataset contain lot things 3d model depth image semantic segmentation etc weight total 1 3tb purpose need rgb color image something like 200gb still quite huge moment even download dataset machine see several project e g atlas url make use dataset train model possible download datasets cloud platforms edit seem debate whether really big data least scale require work simply train mnist although sure nothing compare train gpt3
n31415926535,MachineLearning,1619809245.0,[P] AdapterHub v2: Lightweight Transfer Learning with Transformers and Adapters,"Hi r/MachineLearning!

Yesterday, we released v2 of AdapterHub ([https://adapterhub.ml](https://adapterhub.ml)), a framework that makes adapters accessible for NLP by integrating them with HuggingFace's Transformers library.

**What are adapters?**

* lightweight alternative to full fine-tuning of pre-trained Transformer models
* on-par performance while only updating \~1% of the model's weights
* trained adapter modules (a few MB in size) can be, extracted, shared & plugged into models
* flexible composition of adapters trained on different tasks, e.g. via stacking, fusing, splitting, ...

**What is AdapterHub?**

* *adapter-transformers*: Drop-in replacement of HuggingFace Transformers with adapter support
* *Hub*: 230+ pre-trained, ready-to-use adapters for NLP tasks (similar to HuggingFace model hub)
* Minimal changes (2-3 lines of code) to switch existing Transformers scripts to adapter training
* Easy loading, composing, training, saving & sharing of adapters
* Support for various Transformer models (BERT, RoBERTa, GPT-2, BART, ...)

**GitHub:** [**https://github.com/Adapter-Hub/adapter-transformers**](https://github.com/Adapter-Hub/adapter-transformers)

**Documentation:** [**https://docs.adapterhub.ml/**](https://docs.adapterhub.ml/)

**Pre-trained adapters:** [**https://adapterhub.ml/explore/**](https://adapterhub.ml/explore/)

&#x200B;

https://preview.redd.it/fs4benm4ycw61.png?width=983&format=png&auto=webp&s=99849ba476eb05a47f7286622d92a1e2379eebd0",hi r machinelearning yesterday release v2 adapterhub url framework make adapters accessible nlp integrate huggingface transformers library adapters lightweight alternative full fine tune pre train transformer model par performance update 1 model weight train adapter modules mb size extract share plug model flexible composition adapters train different task e g via stack fuse split adapterhub adapter transformers drop replacement huggingface transformers adapter support hub 230 pre train ready use adapters nlp task similar huggingface model hub minimal change 2 3 line code switch exist transformers script adapter train easy load compose train save share adapters support various transformer model bert roberta gpt 2 bart github url url adapters url
eparlan,MachineLearning,1616950375.0,[P] Implementation of DenStream,"I would like to showcase something I worked on a while back. It's a Python implementation of the DenStream algorithm ([https://archive.siam.org/meetings/sdm06/proceedings/030caof.pdf](https://archive.siam.org/meetings/sdm06/proceedings/030caof.pdf)); the algorithm is based on DBScan but for streaming data (i.e. you don't acquire all the data at once during training but iteratively).

The implementation can be found here: [https://github.com/MrParosk/pyDenStream](https://github.com/MrParosk/pyDenStream)

If you have any feedback please let me know!",would like showcase something work back python implementation denstream algorithm url algorithm base dbscan stream data e acquire data train iteratively implementation find url feedback please let know
pianomano8,MachineLearning,1619197053.0,[R] MLC@Home and MLDS: A Dataset for Weight-Space Analysis of Neural Networks,"# Announcing MLC@Home and MLDS!

***tl;dr:***

Project website: [https://www.mlcathome.org](https://www.mlcathome.org)  
Paper: [https://arxiv.org/abs/2104.10555](https://arxiv.org/abs/2104.10555)

***MLC@Home***

MLC@Home is a volunteer-based distributed computing project dedicated to understanding and explaining machine learning models, specifically neural networks.  It's a collaborative project hosted at [UMBC](https://www.umbc.edu/).  Since July 2020, thousands of volunteers have been training neural networks on their home computers when otherwise idle (using [BOINC](https://boinc.berkeley.edu/),  the same framework as SETI@Home).  The project is less about chasing SotA, and more about understanding how networks encode the data that they do via inspection.  MLC@Home is also the first ML-focussed project to use BOINC framework.

***MLDS: Machine learning Dataset Generator***

Since July 2020, volunteers have been crunching away creating MLDS, a dataset of hundreds of thousands (on our way to ***1.1 million***) neural networks with a small selection of shapes, leaving only the weight space variable. We've [released](https://www.mlcathome.org/mlds.html) a few early versions of this dataset to the public, and commit to making the entire dataset available when complete. This past week, we posted our preliminary findings based on this dataset to [arXiv](https://arxiv.org/abs/2104.10555).  A summary of the preliminary findings in the paper:

* Given enough samples, identically shaped neural networks trained with different training data show clustering behavior in weight space
* We  can classify which networks were trained with which dataset with high accuracy
* We were able to differentiate most networks trained with backdoor/adversarial data versus those trained with clean data

We believe this is the largest publicly-available dataset dedicated to weight space analysis, by at least an order of magnitude, and we're still growing. The dataset leads to all sorts of interesting questions and possibilities to evaluate networks via direct inspection of the weights and structure of a network itself, as opposed to indirect measures of performance such as observed loss on test data.   

MLDS also captures meta information about the training process, such as loss history, what hardware the network was trained on (windows/llinux, amd64/arm64/arm32, cpu/cuda/rocm), and timing information, allowing even more opportunities for comparison. 

***Next Steps***

MLDS  continues to expand the MLDS dataset.  Currently, the dataset consists of small RNNs (200,000 parameters or less), but we're adding support for CNNs currently and have plans to support transformers and arbitrarily-sized networks.  MLDS will continue to grow and be a useful resource for the community. 

MLC@Home has even larger goals.  Leveraging volunteer computing is a capability multiplier for many lines of research.  MLC@Home can support multiple projects at once, and MLDS is just the first.  While MLC@Home will likely never outperform a tightly-coupled cluster for training a single large network, we foresee MLC@Home being useful for:

* Dataset generation (such as MLDS)
* Reproducibility / Robustness studies
* Architecture and Hyperparameter search
* Neuro-evolution

.. just to name a few.  Our community has contributions over 8000 computers, and more joining every day. 

***Interested and/or*** ***Want to help?***

MLC@Home is actively seeing collaborators!

**Individual contributors**

Have a computer? You can help  MLC@Home! Instructions for installing the client and joining are available on our website. 

**Researchers**

MLC@Home is actively looking for research collaborators! If you're interested in weight-space meta analysis and want to talk further about MLDS and how it can be improved; ***or*** if you have a new project idea that could benefit from the generosity of MLC@Home's volunteers,  please each out to us via email/twitter/discord, we'd love to collaborate!  

**Developers / Data Science Engineers**

Running a project that has to support cutting-edge and legacy hardware on multiple platforms is not easy. If you're a software developer or data science engineer and would like to help, please let us know.  Our client is open source, written in C++, and uses the PyTorch C++ API for most computation. We have specific needs for an OSX developer to port the client to Mac platforms, and many enhancements to make overall.  Our website could also use some updating. 

***Summary***

We're very excited about the possibilities of MLC@Home, and these past 9 months have made it a real, useful and growing resource. We'd love community feedback and collaborators to make it grow in new and fun ways.  As always, thanks to our volunteers who make is all possible.. without them, we never would have gotten off the ground. 

\-- MLC@Home Admins  
Website: [https://www.mlcathom.org/](https://www.mlcathom.org/)  
Forums: [https://www.mlcathome.org/mlcathome/forum\_index.php](https://www.mlcathome.org/mlcathome/forum_index.php)  
E-Mail: [mlcathome2020@gmail.com](mailto:mlcathome2020@gmail.com)  
Twitter:   [https://twitter.com/MLCHome2](https://twitter.com/MLCHome2)  
Gitlab: [https://gitlab.com/mlcathome/](https://gitlab.com/mlcathome/)",announce mlc home mlds tl dr project website url paper url volunteer base distribute compute project dedicate understand explain machine learn model specifically neural network collaborative project host umbc url since july 2020 thousands volunteer train neural network home computers otherwise idle use boinc url framework seti home project less chase sota understand network encode data via inspection mlc home also first ml focus project use boinc framework mlds machine learn dataset generator since july 2020 volunteer crunch away create mlds dataset hundreds thousands way 1 1 million neural network small selection shape leave weight space variable release url early versions dataset public commit make entire dataset available complete past week post preliminary find base dataset arxiv url summary preliminary find paper give enough sample identically shape neural network train different train data show cluster behavior weight space classify network train dataset high accuracy able differentiate network train backdoor adversarial data versus train clean datawe believe largest publicly available dataset dedicate weight space analysis least order magnitude still grow dataset lead sort interest question possibilities evaluate network via direct inspection weight structure network oppose indirect measure performance observe loss test data mlds also capture meta information train process loss history hardware network train windows llinux amd64 arm64 arm32 cpu cuda rocm time information allow even opportunities comparison next step mlds continue expand mlds dataset currently dataset consist small rnns 200 000 parameters less add support cnns currently plan support transformers arbitrarily size network mlds continue grow useful resource community mlc home even larger goals leverage volunteer compute capability multiplier many line research mlc home support multiple project mlds first mlc home likely never outperform tightly couple cluster train single large network foresee mlc home useful dataset generation mlds reproducibility robustness study architecture hyperparameter search neuro evolution name community contributions nmbr computers join every day interest want help mlc home actively see collaborators individual contributors computer help mlc home instructions instal client join available website researchers mlc home actively look research collaborators interest weight space meta analysis want talk mlds improve new project idea could benefit generosity mlc home volunteer please us via email twitter discord love collaborate developers data science engineer run project support cut edge legacy hardware multiple platforms easy software developer data science engineer would like help please let us know client open source write c use pytorch c api computation specific need osx developer port client mac platforms many enhancements make overall website could also use update summary excite possibilities mlc home past nmbr months make real useful grow resource love community feedback collaborators make grow new fun ways always thank volunteer make possible without never would get grind mlc home admins website url forums url e mail mlcathome2020 gmail com mailto mlcathome2020 gmail com twitter url gitlab url
giakou4,MachineLearning,1619505394.0,[P] Carotid plaque dataset, Recently i am working on my thesis which is application of some machine learning techniques on carotid plaques to classify them into symptomatic and asymptomatic. However the dateset i was provided consists of 150 images and i would like as more as possible for a better model to be trained. However i am not able to find any free dateset. Has anyone worked with something similar?,recently work thesis application machine learn techniques carotid plaques classify symptomatic asymptomatic however dateset provide consist nmbr image would like possible better model train however able find free dateset anyone work something similar
kvfrans,MachineLearning,1617836706.0,[R] StampCA: Growing Emoji with Conditional Neural Cellular Automata,"[Visuals](https://i.imgur.com/lOs9nkV.mp4)

Blog post: http://kvfrans.com/stampca-conditional-neural-cellular-automata/

[Twitter thread](https://twitter.com/kvfrans/status/1379925309311442944)

Basic Idea:
Neural CAs define local interactions which together grow into a global design. Instead of one system for one design, we can define a general system which grows *many* designs. This lets us condition our neural CA by giving it different design-specific ""seeds"".

StampCA models encode design-specific information in the cell state, and generic information in the network parameters. This means we can 1. grow many designs without retraining, and 2. grow all these designs in the same world.

[Many Emoji growing in one world](https://i.imgur.com/RGgxhIS.mp4) 

[Stamping emojis in a circle](https://video.twimg.com/ext_tw_video/1379927226125230081/pu/vid/512x512/T6LimIfaKlyHM89I.mp4)

We can also train GAN-based StampCAs. This model can use random values as a seed, and grows into various MNIST-like digits.

[Growing fake MNIST digits](https://video.twimg.com/ext_tw_video/1379930705396830210/pu/vid/482x182/26iemnkd9IY63zHg.mp4?tag=12)

Code for replicating the experiments and/or playing with the models is at [this Colab notebook for Emoji](https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg?usp=sharing#scrollTo=8_qZe_c1uPHf) and [this Colab notebook for MNIST](https://colab.research.google.com/drive/1kgYy6jebUl3bPBVlybWtHOlv635HrcL2).",visuals url post url thread url idea neural cas define local interactions together grow global design instead one system one design define general system grow many design let us condition neural ca give different design specific seed stampca model encode design specific information cell state generic information network parameters mean 1 grow many design without retrain 2 grow design world many emoji grow one world url stamp emojis circle url also train gin base stampcas model use random value seed grow various mnist like digits grow fake mnist digits url replicate experiment play model colab notebook emoji url colab notebook mnist url
bzlister,MachineLearning,1618695659.0,[P] GAN for text generation," I'm looking for a model that can be trained on text in a particular genre, and can produce new text/transform existing text into the style of that genre, using common words and phrases.

I'm aware that GANs have limited success when applied to text generation due to the non-differentiable nature of textual data compared to images, but I'm hoping that there's nonetheless something I can use for this.",look model train text particular genre produce new text transform exist text style genre use common word phrase aware gans limit success apply text generation due non differentiable nature textual data compare image hop nonetheless something use
Yuqing7,MachineLearning,1620313621.0,[R] Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning,"A research team from Facebook AI conducts a large-scale study on unsupervised spatiotemporal representation learning from videos. The work takes a unified perspective on four recent image-based frameworks (MoCo, SimCLR, BYOL, SwAV) and investigates a simple objective that can easily generalize unsupervised representation learning methodologies to space-time.

Here is a quick read: [Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning.](https://syncedreview.com/2021/05/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-13/)

 The paper *A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning* is on [arXiv](https://arxiv.org/pdf/2104.14558.pdf).",research team facebook ai conduct large scale study unsupervised spatiotemporal representation learn videos work take unify perspective four recent image base frameworks moco simclr byol swav investigate simple objective easily generalize unsupervised representation learn methodologies space time quick read facebook ai conduct large scale study unsupervised spatiotemporal representation learn url paper large scale study unsupervised spatiotemporal representation learn arxiv url
ZeroHour999,MachineLearning,1620499138.0,[R] Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers,[https://arxiv.org/abs/2105.00173](https://arxiv.org/abs/2105.00173),url
MushiML,MachineLearning,1620243275.0,[D] Shuffling Batch Normalization in MoCo - Self Supervised Learning Method,"Hi, 

The authors mentioned in the paper that 

> *Shuffling BN. Our encoders fq and fk both have Batch Normalization (BN) \[37\] as in the standard ResNet \[33\]. In experiments, we found that using BN prevents the model from learning good representations, as similarly reported in \[35\] (which avoids using BN). The model appears to “cheat” the pretext task and easily finds a low-loss solution. This is possibly because the intra-batch communication among samples (caused by BN) leaks information. We resolve this problem by shuffling BN. We train with multiple GPUs and perform BN on the samples independently for each GPU (as done in common practice). For the key encoder fk, we shuffle the sample order in the current mini-batch before distributing it among GPUs (and shuffle back after encoding); the sample order of the mini-batch for the query encoder fq is not altered. This ensures the batch statistics used to compute a query and its positive key come from two different subsets. This effectively tackles the cheating issue and allows training to benefit from BN. We use shuffled BN in both our method and its end-to end ablation counterpart (Figure 2a). It is irrelevant to the memory bank counterpart (Figure 2b), which does not suffer from this issue because the positive keys are from different mini-batches in the past.* 

I was unable to understand that what type of information is leaked due to intra-batch communication. Please can someone help in understanding this point or refer to some source?

&#x200B;

Thanks",hi author mention paper shuffle bn encoders fq fk batch normalization bn 37 standard resnet 33 experiment find use bn prevent model learn good representations similarly report 35 avoid use bn model appear cheat pretext task easily find low loss solution possibly intra batch communication among sample cause bn leak information resolve problem shuffle bn train multiple gpus perform bn sample independently gpu common practice key encoder fk shuffle sample order current mini batch distribute among gpus shuffle back encode sample order mini batch query encoder fq alter ensure batch statistics use compute query positive key come two different subsets effectively tackle cheat issue allow train benefit bn use shuffle bn method end end ablation counterpart figure 2a irrelevant memory bank counterpart figure 2b suffer issue positive key different mini batch past unable understand type information leak due intra batch communication please someone help understand point refer source x200b thank
ykilcher,MachineLearning,1619799059.0,[D] Paper Explained - Why AI is Harder Than We Think (Full Video Analysis),"[https://youtu.be/uwfVxckuq50](https://youtu.be/uwfVxckuq50)

The AI community has gone through regular cycles of AI Springs, where rapid progress gave rise to massive overconfidence, high funding, and overpromise, followed by these promises being unfulfilled, subsequently diving into periods of disenfranchisement and underfunding, called AI Winters. This paper examines the reasons for the repeated periods of overconfidence and identifies four fallacies that people make when they see rapid progress in AI.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:10 - AI Springs & AI Winters

5:40 - Is the current AI boom overhyped?

15:35 - Fallacy 1: Narrow Intelligence vs General Intelligence

19:40 - Fallacy 2: Hard for humans doesn't mean hard for computers

21:45 - Fallacy 3: How we call things matters

28:15 - Fallacy 4: Embodied Cognition

35:30 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2104.12871](https://arxiv.org/abs/2104.12871)",url ai community go regular cycle ai spring rapid progress give rise massive overconfidence high fund overpromise follow promise unfulfilled subsequently dive periods disenfranchisement underfunding call ai winter paper examine reason repeat periods overconfidence identify four fallacies people make see rapid progress ai x200b outline 0 00 intro overview2 10 ai spring ai winters5 40 current ai boom overhyped 15 35 fallacy 1 narrow intelligence vs general intelligence19 40 fallacy 2 hard humans mean hard computers21 45 fallacy 3 call things matters28 15 fallacy 4 embody cognition35 30 conclusion comment x200b paper url
Aurora_HF,MachineLearning,1617526327.0,How to handle 2D geo-spatial grid like data samples in ML [D],"I am looking into a problem wherein the whole geographic area is divided into number of bins/pixels so we get nxn matrix covering whole region. Now each bin/pixel has number of parameters/features associated with it e.g., number of buildings in that bin, number of people living in that bin, poverty level of that bin, crime level in that bin, etc. This whole information represents one sample of the training dataset i.e., we have this matrix like data for different geographic regions with corresponding labels. How to best handle this kind of dataset for machine learning task e.g., ML trained on some nxn grids for different areas like this will classify labels for some unseen test nxn grids. I am thinking that in term of CNNs, it may be represented in terms of channels so each channel represents associated features of a bin. Whats your suggestion?

https://i.stack.imgur.com/5IlH8.png

https://i.stack.imgur.com/8GaEJ.png",look problem wherein whole geographic area divide number bin pixels get nxn matrix cover whole region bin pixel number parameters feature associate e g number build bin number people live bin poverty level bin crime level bin etc whole information represent one sample train dataset e matrix like data different geographic regions correspond label best handle kind dataset machine learn task e g ml train nxn grids different areas like classify label unseen test nxn grids think term cnns may represent term channel channel represent associate feature bin whats suggestion url
VinayUPrabhu,MachineLearning,1617472489.0,[P] What do you mean when you say X-is-all-you-need? The landscape and the dataset of all the 80 odd ML papers that have made these claims either in their title or in the body of the paper,"Github: [https://github.com/vinayprabhu/X-is-all-you-need](https://github.com/vinayprabhu/X-is-all-you-need)  
This is part  of a paper titled: "" **SPICES: SURVEY PAPERS AS INTERACTIVE CHEATSHEET EMBEDDINGS** "" that just got accepted at the upcoming  *Rethinking ML Papers - ICLR 2021 Workshop*  


  


https://preview.redd.it/q0cwxks5yzq61.png?width=1760&format=png&auto=webp&s=90e589d2d881f35bc7d8febab0a7875b789ccd51",github url part paper title spice survey paper interactive cheatsheet embeddings get accept upcoming rethink ml paper iclr nmbr workshop url
Ingvariuss,MachineLearning,1620406896.0,[D] Data Collection Crowdsourcing - How to animate people?,"Hello,

I started working on an ambitious AI project that would advance a specific field in psychology. The thing with this project is that it needs much data and doing it myself would take me around 4 years if I'd be collecting it on a consistent basis.

The idea was for the community, which is quite alive, to help me out collect some data and I wrote thorough documentation with the project description, rules of data formating and collecting, and have even made a little website that does most of the work for them where they basically need to copy/paste stuff and check some boxes.

I gave the documentation to two of my friends to read and see if everything is sound, after getting a green light I shared the document around in various social media channels. A big chunk of people ran away because the documentation was long while others, out of fear, started attacking the idea of using AI. After some conversation with them, I've concluded that they have no idea about how AI modeling works, what it can and can't do, or what goes under the hood. Which is fine as their backgrounds are colorful.

Teaching them all about AI and how it works and why do I need the data in that format isn't an option as most of them are ignorant. 

My question is: Has anyone ran into something like this and what did you do to overcome it? How do you animate and/or find data collectors? Have in mind that people need to have some psychology knowledge in order to collect this data and be reasonably intelligent.

**TL**;**DR**  I need people to help to collect data but they're too scared of AI. How to effectively crowdsource your projects?",hello start work ambitious ai project would advance specific field psychology thing project need much data would take around nmbr years collect consistent basis idea community quite alive help collect data write thorough documentation project description rule data format collect even make little website work basically need copy paste stuff check box give documentation two friends read see everything sound get green light share document around various social media channel big chunk people run away documentation long others fear start attack idea use ai conversation conclude idea ai model work go hood fine background colorful teach ai work need data format option ignorant question anyone run something like overcome animate find data collectors mind people need psychology knowledge order collect data reasonably intelligent tl dr need people help collect data scar ai effectively crowdsource project
badge,MachineLearning,1617268644.0,[D] Generalized Additive Models… with trees?,"I’m looking at implementing Generalized Additive Models to work as speedily as possible (the entire end-to-end process), so started looking at using [C#’s ML.NET](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.treeextensions.gam).

I haven’t used C# since ~2014 so reading the code is a bit difficult, but it’s part of the FastTree library and is clearly a tree-based implementation. I tested it with a simple `y ~ sin(x)` model and it was dreadful (the FastForest regressor is much better).

Does anyone have any insight on what’s being used here, or references on the subject? I’ve used GAMs in R and Python and never seen a non-spline-based implementation before.",im look implement generalize additive model work speedily possible entire end end process start look use c ml net url use c since 2014 read code bite difficult part fasttree library clearly tree base implementation test simple sin x model dreadful fastforest regressor much better anyone insight whats use reference subject ive use gams r python never see non spline base implementation
ottawalanguages,MachineLearning,1620111466.0,[D] Inevitable Manual Work Required in Machine Learning Projects,"I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?",feel many people admit ultimately significant part many data mine project e g check data quality parse data etc still manually example example make relate supervise nlp natural language process classification suppose nmbr medical report patients contain unstructured text make doctor hospital visit give patient report contain text note doctor make patient visit nmbr 2020 report make mention patients bio data e g age gender medical history etc detail symptoms patient experience long period time e g let say report nmbr word average problem different doctor different style write nmbr report different another human read report human could figure happen patient patient serious condition let call class 1 non serious condition let call class 0 interest predict future patients base limit medical note make doctor future patients problem clear fast way know take nmbr medical report available label report class 1 class 0 example class 0 one doctor could clearly write end report medical test conduct result negative another doctor could end report say patient seriously consider change lifestyle eat healthier food benign example would someone assign label nmbr case without manually read decide information report correspond serious condition non serious condition think use something like sentiment analysis capture mood report use sentiment analysis method informally gauge tone report dark serious condition light non serious condition sure best way approach problem way without read report manually decide label end interest suppose new patient come first visit doctor make quick note e g patient male nmbr years old nmbr cm nmbr kg non smoker frequently complain chest pain high blood pressure work construction worker take daily medicine acid reflex base quick note nmbr report available note try illustrate point medical note new patient nmbr report format researcher predict supervise classification e g decision tree patient serious non serious condition ps suppose doctor detail medical encyclopedia computers medical encyclopedia use alongside nmbr medical report improve prediction result
Ingvariuss,MachineLearning,1619513497.0,[P][D] NLP question - Question Answering AI,"Hey people,

I'm working on my personal project which will be quite a challenge. One of its features is that the user can interact with an ""Open-domain question answering"" chatbot which will be trained on the data I provide it.

I want the model to resemble a specific person/group and it will be fed everything that that person/group wrote, said and etc. Have in mind that the model can answer questions in 1-4 sentences and it doesn't need to be based on pure facts. This means that the user won't ask the model questions like ""What is the capital of France?"" but more something along the lines of existential questions (""What is the meaning of Life?"").

Here are the questions I have as I didn't dabble into the NLP world of AI at all:

1. Are there any pre-trained or prebuilt models out there that I could use for this? I've found that the open-source Pavlov AI library has some interesting ones.
2. Which models would suit this task the best?
3. Are there any features I should watch out for or provide more information on?

The biggest part of the job will be to collect relevant data on the group I want the model to resemble. What would be some of the best practices when making the data as informative as it can be? Also, if I want there to be 4 groups that the model can resemble - Do I need to train 4 models or can I filter what a model learned into 4 categories?

Thanks for all replies and questions in advance. If some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great.",hey people work personal project quite challenge one feature user interact open domain question answer chatbot train data provide want model resemble specific person group feed everything person group write say etc mind model answer question 1 4 sentence need base pure facts mean user win ask model question like capital france something along line existential question mean life question dabble nlp world ai 1 pre train prebuilt model could use find open source pavlov ai library interest ones 2 model would suit task best 3 feature watch provide information biggest part job collect relevant data group want model resemble would best practice make data informative also want nmbr group model resemble need train nmbr model filter model learn nmbr categories thank reply question advance interest project feel free send dm could even collaborate part project make model great
SQL_beginner,MachineLearning,1620420157.0,"[D] has anyone ever worked on a machine learning model for ""queues""?","Has anyone ever worked on a machine learning model for ""queues""? Suppose there is a bakery: the bakery has has ""n"" people working, ""m"" people in line""  and ""q"" orders that they are currently working on. The bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer's order is ready and how long will the next customer have to wait before they can place an order. 

Has anyone ever come across a machine learning model which can predict waiting and processing times? I have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit, as well as trying different m/m/k combinations... but has anyone ever come across an instance where machine learning algorithms (e.g. random forest, neural networks) are used to predict waiting times? 

I saw something like this: https://arxiv.org/abs/2002.10788 

But there was no python or R code for this paper. Can anyone recommend some source (blog, github, website, book, YouTube lectures etc) which show and provide computer code for analyzing queues using machine learning models?

Thanks",anyone ever work machine learn model queue suppose bakery bakery n people work people line q order currently work bakery interest make machine learn model predict long customer wait customer order ready long next customer wait place order anyone ever come across machine learn model predict wait process time see examples online people try fit exponential distributions historical wait time see well fit well try different k combinations anyone ever come across instance machine learn algorithms e g random forest neural network use predict wait time saw something like url python r code paper anyone recommend source blog github website book youtube lecture etc show provide computer code analyze queue use machine learn model thank
_Arsenie_Boca_,MachineLearning,1620480658.0,[D] Multiple fine-tuning steps order,"When trying to maximize performance on a target task, transfer learning can not only be applied by fine-tuning a pretrained model on this task, but one might also want to incorporate several fine-tuning steps, that specialize the model to the target task. This is called (domain)-adaptive and behavorial fine-tuning (terminology see [here](https://ruder.io/recent-advances-lm-fine-tuning/)) depending on whether the data shares a domain or task setting with the target task. 

This is probably especially helpful in settings where the amount of labelled data for the specific target task is limited and the domain is significantly different from the pretraining data (e.g. [here](https://www.aclweb.org/anthology/2020.acl-main.740.pdf) related to adapting via language modeling).

The fine-tuning steps might contain

* domain adaptation via language modeling
* task adaptation via the same task setting as the target task from a different domain
* task adaptation via the same task setting as the target task from a similar domain
* the target task itself

Now, when applying several of those fine-tuning / adaptation steps, how would one choose their order? Empirically testing all possible orders might not be feasible and a ""wrong"" order might make the model forget valuable skills/knowledge. 

My intuition would roughly be the order listed above with the reasoning that ordering ascendingly with the similarity to the target task will lead to the best model.

Is there a rule of thumb? Any literature investigating this or at least applying and briefly discussing it?",try maximize performance target task transfer learn apply fine tune pretrained model task one might also want incorporate several fine tune step specialize model target task call domain adaptive behavorial fine tune terminology see url depend whether data share domain task set target task probably especially helpful settings amount label data specific target task limit domain significantly different pretraining data e g url relate adapt via language model fine tune step might contain domain adaptation via language model task adaptation via task set target task different domain task adaptation via task set target task similar domain target task itselfnow apply several fine tune adaptation step would one choose order empirically test possible order might feasible wrong order might make model forget valuable skills knowledge intuition would roughly order list reason order ascendingly similarity target task lead best model rule thumb literature investigate least apply briefly discuss
savoga,MachineLearning,1618414525.0,[D] Advantages of ML approaches for anomaly detection?,"What are the advantages of ML approaches for anomaly detection (isolation forests, autoencoders, DBSCAN etc.) over more traditional approaches such as finding points that are situated in the extreme part of a distribution?",advantage ml approach anomaly detection isolation forest autoencoders dbscan etc traditional approach find point situate extreme part distribution
blkpingu,MachineLearning,1616784241.0,[D] Looking for Deep Learning Workstation / Server vendor in Europe / Germany,"I want to buy a workstation and so far I only found Lambdaworks and BIZON, and both are located in the US.

Does anyone know a company that sells workstations or servers in Germany or at least the EU?",want buy workstation far find lambdaworks bizon locate us anyone know company sell workstations servers germany least eu
KaleidoscopeBest1569,MachineLearning,1620233329.0,[R] Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation,"**Project Webpage:** [https://zubair-irshad.github.io/projects/robo-vln.html](https://zubair-irshad.github.io/projects/robo-vln.html)  
**Pytorch Code and Dataset:** [https://github.com/GT-RIPL/robo-vln](https://github.com/GT-RIPL/robo-vln)  
**ArXiv paper:** [https://arxiv.org/abs/2104.1067](https://arxiv.org/abs/2104.10674) 

**Abstract:**

Deep Learning has revolutionized our ability to solve complex problems such as Vision-and-Language Navigation (VLN). This task requires the agent to navigate to a goal purely based on visual sensory inputs given natural language instructions. However, prior works formulate the problem as a navigation graph with a discrete action space. In this work, we lift the agent off the navigation graph and propose a more complex VLN setting in continuous 3D reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics the challenges of real world navigation. Robo-VLN tasks have longer trajectory lengths, continuous action spaces, and challenges such as obstacles. We provide a suite of baselines inspired by state-of-the-art works in discrete VLN and show that they are less effective at this task. We further propose that decomposing the task into specialized high- and low-level policies can more effectively tackle this task. With extensive experiments, we show that by using layered decision making, modularized training, and decoupling reasoning and imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms existing baselines in all key metrics and sets a new benchmark for Robo-VLN.",project webpage url pytorch code dataset url arxiv paper url abstract deep learn revolutionize ability solve complex problems vision language navigation vln task require agent navigate goal purely base visual sensory input give natural language instructions however prior work formulate problem navigation graph discrete action space work lift agent navigation graph propose complex vln set continuous 3d reconstruct environments propose set robo vln closely mimic challenge real world navigation robo vln task longer trajectory lengths continuous action space challenge obstacles provide suite baselines inspire state art work discrete vln show less effective task propose decompose task specialize high low level policies effectively tackle task extensive experiment show use layer decision make modularized train decouple reason imitation propose hierarchical cross modal hcm agent outperform exist baselines key metrics set new benchmark robo vln
whyhateverything,MachineLearning,1617030324.0,[P] Passing embeddings to faiss for clustering,"Hi everyone,

I am having a hard time with faiss documentation for clustering. I have embeddings created with Sentence Transformers and now I want to use faiss to do the clustering part and get the final list that shows which document belongs to which cluster. I have no idea how to do this with faiss or more precisely how to pass my embeddings to it.

I would be endlessly grateful for your help..

Thanks in advance",hi everyone hard time faiss documentation cluster embeddings create sentence transformers want use faiss cluster part get final list show document belong cluster idea faiss precisely pass embeddings would endlessly grateful help thank advance
lifeonahilltop,MachineLearning,1620348179.0,[D] How do companies typically develop ML models for different customers?,"I work in the fraud protection industry, where my company ships fraud protection solutions (basically a binary classifier wrapped in some UI) to other companies in e-commerce and healthcare sectors. As we are getting more customers these days, we have to decide whether we want to build a single ML model or custom ML models for them. From an engineering and MLOps point of view, having a single model is more scalable. However, how to combine data from multiple customers in training and how to take performance tradeoffs between different customers doesn't seem straightforward.

Can anyone inform me how this is typically done in industry? Do people use techniques like domain adaptation or are the assumptions too brittle in reality? I imagine this is a common challenge in industry, and any pointers to relevant materials, case studies, etc. would be greatly appreciated.",work fraud protection industry company ship fraud protection solutions basically binary classifier wrap ui company e commerce healthcare sectors get customers days decide whether want build single ml model custom ml model engineer mlops point view single model scalable however combine data multiple customers train take performance tradeoffs different customers seem straightforward anyone inform typically industry people use techniques like domain adaptation assumptions brittle reality imagine common challenge industry pointers relevant materials case study etc would greatly appreciate
lukasus,MachineLearning,1617900076.0,[P] Mask2Face: How We Built AI That Shows the Face Beneath the Mask,"Can you virtually remove a face mask to see what a person looks like underneath? STRV Machine Learning team proves it’s possible via an image inpainting-based ML solution. Here’s exactly how we approached the problem — from the preconditions to the implementation, results, and future improvements:   

[Mask2Face: How We Built AI That Shows the Face Beneath the Mask](https://www.strv.com/blog/mask2face-how-we-built-ai-that-shows-face-beneath-mask-engineering)

And here is the link to the project on GitHub: [https://github.com/strvcom/strv-ml-mask2face](https://github.com/strvcom/strv-ml-mask2face)",virtually remove face mask see person look like underneath strv machine learn team prove possible via image inpainting base ml solution heres exactly approach problem precondition implementation result future improvements mask2face build ai show face beneath mask url link project github url
atif_hassan,MachineLearning,1618641825.0,[P] PyImpetus - A Markov Blanket based new SOTA feature selection algorithm for python,"PyImpetus is a Markov Blanket based **feature selection algorithm** that selects a subset of features by considering their performance both individually as well as a group. This allows the algorithm to not only select the best set of features but also select the **best set of features that play well with each other**. For example, the best performing feature might not play well with others while the remaining features, when taken together could out-perform the best feature. PyImpetus takes this into account and produces the best possible combination. Thus, the algorithm provides a minimal feature subset. So, **you do not have to decide on how many features to take. PyImpetus selects the optimal set for you.**

PyImpetus has been completely revamped and now supports **binary classification, multi-class classification and regression** tasks. It has been tested on 14 datasets and outperformed state-of-the-art Markov Blanket learning algorithms on all of them along with traditional feature selection algorithms such as Forward Feature Selection, Backward Feature Elimination and Recursive Feature Elimination.

&#x200B;

**Performance Comparison:**

For classification tasks, Accuracy as a metric (higher score is better) has been used while for Regression tasks, Mean Squared Error as a metric (lower score is better) has been used. The final model used for comparison on all tasks is a decision tree with scores being reported on 5-fold cross validation.

|**Dataset**|**# of samples**|**# of features**|**Task Type**|**Score using all features**|**Score using PyImpetus**|**# of features selected**|**% of features selected**|
|:-|:-|:-|:-|:-|:-|:-|:-|
|Ionosphere|351|34|Classification|88.01%|92.86%|14|42.42%|
|Arcene|100|10000|Classification|82%|84.72%|304|3.04%|
|AlonDS2000|62|2000|Classification|80.55%|88.49%|75|3.75%|
|slice\_localization\_data|53500|384|Regression|6.54|5.69|259|67.45%|

&#x200B;

[Link to the GitHub repo](https://github.com/atif-hassan/PyImpetus)

[Link to pypi](https://pypi.org/project/PyImpetus/)

&#x200B;

PyImpetus already has over 15K+ downloads so do check it out and please let me know how it worked out for you in your Machine Learning project. Don't forget to pen down your feedback and doubts in the comment section.

And of course, don't forget to star the GitHub repo!!

Thank you and have an amazing day!",pyimpetus markov blanket base feature selection algorithm select subset feature consider performance individually well group allow algorithm select best set feature also select best set feature play well example best perform feature might play well others remain feature take together could perform best feature pyimpetus take account produce best possible combination thus algorithm provide minimal feature subset decide many feature take pyimpetus select optimal set pyimpetus completely revamp support binary classification multi class classification regression task test nmbr datasets outperform state art markov blanket learn algorithms along traditional feature selection algorithms forward feature selection backward feature elimination recursive feature elimination x200b performance comparison classification task accuracy metric higher score better use regression task mean square error metric lower score better use final model use comparison task decision tree score report 5 fold cross validation dataset sample feature task type score use feature score use pyimpetus feature select feature select ionosphere 351 34 classification 88 01 92 86 14 42 42 arcene 100 10000 classification 82 84 72 304 3 04 alonds2000 62 2000 classification 80 55 88 49 75 3 75 slice _localization _data 53500 384 regression 6 54 5 69 259 67 45 x200b link github repo url pypi url already 15k download check please let know work machine learn project forget pen feedback doubt comment section course forget star github repo thank amaze day
Quantum_Stat,MachineLearning,1619531012.0,"The NLP Index: 3,000+ code repos for hackers and researchers. [Project]","Want to introduce “The NLP Index”, a new asset in NLP code discovery. It's free and open to the public.

It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.

https://index.quantumstat.com/",want introduce nlp index new asset nlp code discovery free open public house 3 000 code repositories one search include side bar important topics nlp today engine search type typo tolerant crazy fast index include arxiv research paper pdf connectedpapers link github repo url
NeoDio_02,MachineLearning,1619551730.0,[D] Question on ROUGE scores for evaluating summaries,"I am doing a project on text summarization, and I want to run a ROUGE  benchmark test on my summarization model. One thing I am confused about  is the large differences in scores I am seeing in different sources. For  example, here:[https://github.com/andersjo/pyrouge](https://github.com/andersjo/pyrouge) They get a ROUGE-1 f-score of \~0.7, while in this paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9321308](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9321308) they show ROUGE-1 f-scores like 41. Can someone explain the discrepancy and how ROUGE scores work? Thanks!",project text summarization want run rouge benchmark test summarization model one thing confuse large differences score see different source example url get rouge 1 f score 0 7 paper url show rouge 1 f score like 41 someone explain discrepancy rouge score work thank
Yuqing7,MachineLearning,1617673931.0,[N] Yann LeCun Team Uses Dictionary Learning To Peek Into Transformers' Black Boxes,"A Yann LeCun team proposes dictionary learning to provide detailed visualizations of transformer representations and insights into semantic structures such as word-level disambiguation, sentence-level pattern formation, and long-range dependency captured by transformers.

Here is a quick read: [Yann LeCun Team Uses Dictionary Learning To Peek Into Transformers' Black Boxes](https://syncedreview.com/2021/04/05/yann-lecun-team-uses-dictionary-learning-to-peek-into-transformers-black-boxes/)

The paper *Transformer Visualization via Dictionary Learning: Contextualized Embedding as a Linear Superposition of Transformer Factors* is on [arXiv](https://arxiv.org/pdf/2103.15949.pdf).",yann lecun team propose dictionary learn provide detail visualizations transformer representations insights semantic structure word level disambiguation sentence level pattern formation long range dependency capture transformers quick read yann lecun team use dictionary learn peek transformers black box url paper transformer visualization via dictionary learn contextualized embed linear superposition transformer factor arxiv url
SZenne_,MachineLearning,1617887636.0,What cloud computing setup should I get for my Deepfakes [R]," Hi!

For my graduation project I am doing research on the creation of Deepfakes. Now I have a small budget of $200 and I would want to spend this on 2 weeks op cloud computing because my own computer doesn't have the right requirements to run Deepfake software. I just want to do some 10 second basic deepfakes to test out the softwares.

So does anybody have any recommendations on which online cloud computing servers I could use and what I should look out for?",hi graduation project research creation deepfakes small budget 200 would want spend nmbr weeks op cloud compute computer right requirements run deepfake software want nmbr second basic deepfakes test softwares anybody recommendations online cloud compute servers could use look
chasep255,MachineLearning,1620298405.0,[D] Learning a discrete encoding for raw audio.,"I have been working on various ways to generate music using neural nets.  I started with a WGAN using 1D convs which produced not the best results. I would really like to use an RNN to generate audio just like how you would generate text by sampling the output and feeding that sample back to the input on the next time step.  Since the audio I am using is sampled at 22050hz this method is not practical.  So what I decided to do is first use an auto encoder with a gumbel softmax in-between.   This way I could compress the audio data and learn a discrete encoding for it.  Once I have this encoder I plan to train an RNN on the encoding just like you would on text, then I can use the decoder to convert this into sound.  Currently the best architecture I have is as follows...

The encoder compresses the audio down 256x to a discrete representation with 4096 different values.  
Hard gumbel softmax between encoder and expander.  
The expander  expands the discrete representation back out to the length of the original raw audio.  
The decoder uses an RNN to predict the next sample based on the prior sample and input from the expander. (I tried a wavenet instead of RNN and so far the RNN produces a better result, I care more about quality than speed.)

\--Encoder--

\#256 Mu-Law Encoding  
input\_audio = keras.Input((None,), dtype = 'int32')

x = layers.Embedding(256, 8)(input\_audio)    

x = layers.Conv1D(128, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(256, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(4096, 1, padding = 'same')(x)

\--Expander--

 input\_data = keras.Input((None, 4096))

e = layers.Conv1D(32, 1, padding = 'same', kernel\_initializer = 'random\_normal', use\_bias = False)(input\_data)

x = layers.Conv1D(512, 5, padding = 'same')(e)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(e)  
x = layers.LeakyReLU()(x)

x = layers.Conv1DTranspose(256, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1DTranspose(128, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

\--Decoder--  
prior\_audio\_input = keras.Input((None,), batch\_size = 1 if stateful else None, dtype = 'int32')  
expander\_input = keras.Input((None, 128), batch\_size = 1 if stateful else None)  
x = layers.Embedding(256, 8)(prior\_audio\_input)  
x = layers.Concatenate(axis = -1)((expander\_input, x))  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.GRU(1024, return\_sequences = True, stateful = stateful)(x)  
x = layers.GRU(1024, return\_sequences = True, stateful = stateful)(x)  
\#Add expander\_input in again to help with gradients  
x = layers.Concatenate(axis = -1)((expander\_input, x))  
x = layers.Dropout(0.5)(x)  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.TimeDistributed(layers.Dense(256))(x)

\--Training--

\#r is mu-law encoded audio samples  
r\_prior = r\[..., :-1\]  
r = r\[..., 1:\]

e = encoder(r, training = training)  
e\_sm = tf.nn.softmax(e)  
enc\_loss = tf.reduce\_mean(e\_sm \* (tf.math.log(e\_sm + 1e-20) - tf.math.log(1.0 / tf.cast(tf.shape(e\_sm)\[-1\], tf.float32))))

e = model.gumbel\_softmax(e, 1.0)  
\#Convert e to a one hot representation  
e = tf.stop\_gradient(model.argmax2onehot(e) - e) + e

e = expander(e, training = training)  
...  
\#take a random chunk of the expander output if the sequence is too long  
\#also add noise to r\_prior  
g = decoder((r\_prior, e), training = training)  
sound\_loss = tf.reduce\_mean(tf.keras.losses.sparse\_categorical\_crossentropy(r, g, from\_logits = True))  
total\_loss = sound\_loss + enc\_loss  


So I would like to ask a few things...

1) Do you think the way I am using the gumbel softmax makes sense?

2) Since I am using the ""hard"" version of the gumbel softmax with the stop\_graidents do I still need to use the temperature parameter and anneal it?

3) Does my loss function make sense?  I added the enc\_loss term to the output of the encoder which adds a penalty for it deviating from a flat distribution.  I figure this is like the second term in the ELBO loss function used in a VAE.  I found it helps the model to learn to use more of the encoding.

4) Should I be using the gumbel softmax at all?  Maybe it makes more sense to just use a normal softmax and always take the argmax.  I figure the randomness adds helps it explore the possible encodings.

5) Any other thoughts?",work various ways generate music use neural net start wgan use 1d convs produce best result would really like use rnn generate audio like would generate text sample output feed sample back input next time step since audio use sample 22050hz method practical decide first use auto encoder gumbel softmax way could compress audio data learn discrete encode encoder plan train rnn encode like would text use decoder convert sound currently best architecture follow encoder compress audio 256x discrete representation nmbr different value hard gumbel softmax encoder expander expander expand discrete representation back length original raw audio decoder use rnn predict next sample base prior sample input expander try wavenet instead rnn far rnn produce better result care quality speed encoder 256 mu law encode input _audio keras input none dtype int32 x layer embed 256 8 input _audio x layer conv1d 128 32 stride 16 pad ame x x layer leakyrelu x x layer conv1d 256 32 stride 16 pad ame x x layer leakyrelu x x layer conv1d 512 5 pad ame x x layer leakyrelu x x layer conv1d 512 5 pad ame x x layer leakyrelu x x layer conv1d 4096 1 pad ame x expander input _data keras input none 4096 e layer conv1d 32 1 pad ame kernel _initializer random _normal use _bias false input _data x layer conv1d 512 5 pad ame e x layer leakyrelu x x layer conv1d 512 5 pad ame e x layer leakyrelu x x layer conv1dtranspose 256 32 stride 16 pad ame x x layer leakyrelu x x layer conv1dtranspose 128 32 stride 16 pad ame x x layer leakyrelu x decoder prior _audio _input keras input none batch _size nmbr stateful else none dtype int32 expander _input keras input none 128 batch _size nmbr stateful else none x layer embed 256 8 prior _audio _input x layer concatenate axis 1 expander _input x x layer timedistributed layer dense 512 x x layer leakyrelu x x layer gru 1024 return _sequences true stateful stateful x x layer gru 1024 return _sequences true stateful stateful x add expander _input help gradients x layer concatenate axis 1 expander _input x x layer dropout 0 5 x x layer timedistributed layer dense 512 x x layer leakyrelu x x layer timedistributed layer dense 512 x x layer leakyrelu x x layer timedistributed layer dense 256 x train r mu law encode audio sample r _prior r 1 r r 1 e encoder r train train e _sm tf nn softmax e enc _loss tf reduce _mean e _sm tf math log e _sm 1e 20 tf math log 1 0 tf cast tf shape e _sm 1 tf float32 e model gumbel _softmax e 1 0 convert e one hot representation e tf stop _gradient model argmax2onehot e e ee expander e train train take random chunk expander output sequence long also add noise r _prior g decoder r _prior e train train sound _loss tf reduce _mean tf keras losses sparse _categorical _crossentropy r g _logits true total _loss sound _loss enc _loss would like ask things 1 think way use gumbel softmax make sense 2 since use hard version gumbel softmax stop _graidents still need use temperature parameter anneal 3 loss function make sense add enc _loss term output encoder add penalty deviate flat distribution figure like second term elbo loss function use vae find help model learn use encode 4 use gumbel softmax maybe make sense use normal softmax always take argmax figure randomness add help explore possible encode 5 thoughts
glampiggy,MachineLearning,1620164503.0,Is it common for Transfer Learning to decrease the accuracy of a model? [Project],"I trained PSPNet and DeepLab from scratch and also using pre-trained backbones on a very specific urban scene dataset. The pre-trained backbone I used was ResNet, with weights downloaded from the ImageNet dataset. I then trained both models without freezing any layers. My accuracy for the models from scratch proved to be higher than the models with pre-trained backbones. My dataset is relatively small; only 1000 images.

Could this have happened because the ImageNet dataset is too general when compared to the specific dataset I am working on, and has thus limited its learning ability? Or have I most likely done something wrong?",train pspnet deeplab scratch also use pre train backbones specific urban scene dataset pre train backbone use resnet weight download imagenet dataset train model without freeze layer accuracy model scratch prove higher model pre train backbones dataset relatively small nmbr image could happen imagenet dataset general compare specific dataset work thus limit learn ability likely something wrong
deadmanscurve,MachineLearning,1617728458.0,[Research] How do you train on and evaluate GLUE STS-B?,"Hello!


I'm experimenting with multi-task learning, and I have read some of the literature around it. I'm trying to benchmark all of the GLUE tasks, but I can't figure out how people train and evaluate [STS-B](https://huggingface.co/datasets/viewer/?dataset=glue). The labels are similarity scores between two sentences, ranging from 0 to 5.


Spearman's correlation is used for evaluation. If the labels were discrete, I could train a classification model and compute this, but they are not- they are floats. I have read that some train a regression model with sigmoid activation, after normalizing the labels to range \[0, 1\]. If this is done, is an L2 loss used? How is the Spearman's correlation then computed exactly?


This information has been surprisingly difficult to find for a standard benchmark dataset!


Any info is gladly appreciated :D",hello experiment multi task learn read literature around try benchmark glue task figure people train evaluate sts b url label similarity score two sentence range nmbr 5 spearman correlation use evaluation label discrete could train classification model compute float read train regression model sigmoid activation normalize label range 0 1 l2 loss use spearman correlation compute exactly information surprisingly difficult find standard benchmark dataset info gladly appreciate
miladink,MachineLearning,1618988935.0,[D]Is im2latex considered solved?,I hope you know the im2latex dataset by OpenAI in which you need to infer the equation that generated a latex image from the image. I found it a little surprising that actually little work is done there. Is reading equations from the images considered a solved problem?,hope know im2latex dataset openai need infer equation generate latex image image find little surprise actually little work read equations image consider solve problem
minimaxir,MachineLearning,1620230683.0,[N] Wired: It Began As an AI-Fueled Dungeon Game. It Got Much Darker (AI Dungeon + GPT-3),"https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/

If you haven't been following the drama around AI Dungeon, this is a good summary and a good discussion on filter/algo difficulty.",url follow drama around ai dungeon good summary good discussion filter algo difficulty
IglooAustralia88,MachineLearning,1617224051.0,[P] How Bad is a Bad Classifier: Is there any signal here?,"I have a text classification problem where I am classifying song lyrics by the performing artist.  Right now I have a small dataset of 3000 lyrics (two lines each) each from 9 different artists.  All of my train, dev, and test sets are balanced by artist.

As a baseline I encoded each lyrics observation using Bert and performed a Logistic Regression.  I got a 0.35 top-1 and a 0.65 top-3 on a held out test set.

I'm not sure how to phrase this, but is this classifier doing anything, and is it worth trying to improve on (my original idea was comparing word-level vs. subword-level NN classifiers for the task)?  Like obviously with 9 artists a random classifier would be doing \~0.1111, and this is 3x that, so seemingly it has found some signal.  However, it's hard to look at a 0.35 score and think there's anything there.

I guess my question is, can I hope to find some signal in this small amount of data for this text classification text, or is this baseline implementation trying to tell me that the data is very noisy and thus the experiment will be hard to get significant results from?

Thanks!",text classification problem classify song lyric perform artist right small dataset nmbr lyric two line nmbr different artists train dev test set balance artist baseline encode lyric observation use bert perform logistic regression get nmbr top 1 nmbr top 3 hold test set sure phrase classifier anything worth try improve original idea compare word level vs subword level nn classifiers task like obviously nmbr artists random classifier would 0 1111 3x seemingly find signal however hard look nmbr score think anything guess question hope find signal small amount data text classification text baseline implementation try tell data noisy thus experiment hard get significant result thank
Separate_Run2806,MachineLearning,1620376282.0,[D] Tool to help ML Engineering follow code best practices.,"Hello, my data science team was involved in an increasing amount of ML engineering tasks but had small knowledge on code quality best practices so we made this VS Code Extension to help python developers build unit tests quickly and efficiently. Our AI makes suggestions of input and then generate the test file for you. Would love to hear your feedback if you are willing to give a try to it. It's available here [www.ponicode.com](https://www.ponicode.com)

Are you doing unit tests at all when implementing your models? Are you ever testing your code at the pre processing or evaluation stage?",hello data science team involve increase amount ml engineer task small knowledge code quality best practice make vs code extension help python developers build unit test quickly efficiently ai make suggestions input generate test file would love hear feedback give try available url unit test implement model ever test code pre process evaluation stage
idg101,MachineLearning,1617314266.0,[D] Hyper-parameter tuning takes the majority of my time!," I  am a 4th year PhD student with a few papers published already.  As I  come to the end of my PhD, I find I spend the majority of my time  hyper-parameter tuning to get the best performance possible.  This takes  \*forever\* to do on my dual GPU machine.  Does anyone have any shortcuts  on how to do this?  I have tried just tuning over the best 1-5 epochs  but I find that this isn't a good proxy for long-term validation loss  convergence.

The lifecycle of  research seems to be come up with an idea, and then tune the network to  show that your performance is better than some benchmark.  After doing  this a few times, it seems like a complete waste of research cycles  spent.  There has to be a better way.",4th year phd student paper publish already come end phd find spend majority time hyper parameter tune get best performance possible take forever dual gpu machine anyone shortcuts try tune best 1 5 epochs find good proxy long term validation loss convergence lifecycle research seem come idea tune network show performance better benchmark time seem like complete waste research cycle spend better way
Superb-Drawer5214,MachineLearning,1617108593.0,"[D] If the number of machine learning PhD graduate is increasing rapidly, wouldn't it get exponentially harder to be hired at machine learning related jobs without PhD?",It seems everyone wants to do machine learning these days and those who did PhD in machine learning is increasing rapidly. Wouldn't it get harder and harder to be employed in machine learning related jobs without PhD?,seem everyone want machine learn days phd machine learn increase rapidly get harder harder employ machine learn relate job without phd
Yuqing7,MachineLearning,1620141876.0,[R] Huawei & Tsinghua U Method Boosts Task-Agnostic BERT Distillation Efficiency by Reusing Teacher Model Parameters,"A research team from Huawei Noah’s Ark Lab and Tsinghua University proposes Extract Then Distill (ETD), a generic and flexible strategy for reusing teacher model parameters for efficient and effective task-agnostic distillation that can be applied to student models of any size. 

Here is a quick read: [Huawei & Tsinghua U Method Boosts Task-Agnostic BERT Distillation Efficiency by Reusing Teacher Model Parameters.](https://syncedreview.com/2021/05/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-11/)

 The paper *Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation* is on [arXiv](https://arxiv.org/pdf/2104.11928.pdf).",research team huawei noahs ark lab tsinghua university propose extract distill etd generic flexible strategy reuse teacher model parameters efficient effective task agnostic distillation apply student model size quick read huawei tsinghua u method boost task agnostic bert distillation efficiency reuse teacher model parameters url paper extract distill efficient effective task agnostic bert distillation arxiv url
opensourcecolumbus,MachineLearning,1617938253.0,[P] BERT-based Financial Question Answering System,"Built this production-ready project using Jina, PyTorch, and Hugging Face transformers. Adapted a passage reranking approach by first retrieving the top-50 candidate answers, then reranking the candidate answers using FinBERT-QA, a BERT-based model fine-tuned on the FiQA dataset that achieved the state-of-the-art results.

### [GitHub Repo](https://github.com/yuanbit/jina-financial-qa-search)

Looking for your feedback and questions. What would you need me to do to make it more useful and easy to use for your own use case?",build production ready project use jina pytorch hug face transformers adapt passage reranking approach first retrieve top 50 candidate answer reranking candidate answer use finbert qa bert base model fine tune fiqa dataset achieve state art result github repo url feedback question would need make useful easy use use case
mridal,MachineLearning,1620289195.0,[D] ACL-IJCNLP 2021 Acceptances are out,"How did everyone do? Mine was accepted in ACL findings with scores of 3, 4 and 3.",everyone mine accept acl find score 3 nmbr 3
minidiable,MachineLearning,1619767355.0,[D] Object detection - useful resources,"Hi all, I am working for a new company. I am a drone expert with some knowledge in Computer Vision but my main expertise is about control, state estimation, and robotics. One of the main macro tasks for the new job will revolve around the topic of object detection from onboard an aircraft (small, medium, large size).

I know that we could divide the field of object detection into two main streams:

\- the data-driven approaches (e.g., machine learning, deep learning-based)

\- the classical approaches (using classic computer vision)

However, I have never implemented an object detector and I would really like to know more about the history of it and the main tools I can use to achieve this. Could you point out useful resources like books, online courses (best if on Coursera), links, milestone papers, youtube videos, and channels, etc.?

Thanks!",hi work new company drone expert knowledge computer vision main expertise control state estimation robotics one main macro task new job revolve around topic object detection onboard aircraft small medium large size know could divide field object detection two main stream data drive approach e g machine learn deep learn base classical approach use classic computer vision however never implement object detector would really like know history main tool use achieve could point useful resources like book online course best coursera link milestone paper youtube videos channel etc thank
bendee983,MachineLearning,1618849898.0,[D] The challenges of applied machine learning,"From setting up the data infrastructure (breaking down the silos, creating data lakes, etc.) to collecting and cleaning training data, to setting up a scalable networking and compute clusters, dealing with business objectives and ethical issues, applying machine learning to real-world applications poses challenges that are absent in academic and research settings. 

Four key challenges in applied machine learning include the following:

* Defining the problem: Are you solving the right problem? Does it align with your business objectives? What is the desired accuracy from the model? Are you addressing a special problem or has anyone else solved it before? Can it be solved with a pre-trained model or do you need to develop your own models?
* Collecting training data: Are public datasets (ImageNet, MSCOCO, etc.) enough to train your models (most probably not)? Can you buy the data, do you already have it, or do you have to collect it? If you already have the data, is it in a consolidated store or is it siloed? Do you need to clean and label the data?
* Maintaining models: How much will your model be affected by decay? How frequently do you need to retrain your models? Do you have a pipeline for continuously collecting and maintaining your data in the future?
* Gathering the right team: Do you have the talent in-house? Do you have subject matter experts who can weigh in on the model's performance? Do you have a product manager who can define and assess business objectives for your machine learning strategy? Do you have a process to get input and feedback from the end-users of your system? Do you have software engineers who can integrate your models into existing products and IT staff who can provide reliable and scalable infrastructure for training and inference?

These are some of the challenges discussed in my latest article on the challenges of applied machine learning:

[https://bdtechtalks.com/2021/04/19/applied-machine-learning-challenges/](https://bdtechtalks.com/2021/04/19/applied-machine-learning-challenges/)

You'll find a lot more in  [*Real World AI: A Practical Guide for Responsible Machine learning*](https://appen.com/real-world-ai/) by Alyssa Simpson Rochwerger and Wilson Pang",set data infrastructure break silos create data lakes etc collect clean train data set scalable network compute cluster deal business objectives ethical issue apply machine learn real world applications pose challenge absent academic research settings four key challenge apply machine learn include follow define problem solve right problem align business objectives desire accuracy model address special problem anyone else solve solve pre train model need develop model collect train data public datasets imagenet mscoco etc enough train model probably buy data already collect already data consolidate store siloed need clean label data maintain model much model affect decay frequently need retrain model pipeline continuously collect maintain data future gather right team talent house subject matter experts weigh model performance product manager define assess business objectives machine learn strategy process get input feedback end users system software engineer integrate model exist products staff provide reliable scalable infrastructure train inference challenge discuss latest article challenge apply machine learn url find lot real world ai practical guide responsible machine learn url alyssa simpson rochwerger wilson pang
Bitmore11,MachineLearning,1617281029.0,[D][P] Raven Protocol,"I am looking for any input on the Raven Protocol project. Raven is a decentralized network of compute nodes that utilize idle compute power for AI training where speed is the key. Is this a protocol you could benefit from? Besides adding to their library, is there anything that this protocol is missing for the solution to the problem they are trying to solve? I am not experienced in machine learning and would like input from those of you that stand to benefit the most from Raven Protocol.

[Website](https://www.ravenprotocol.com/)

[Github](https://github.com/raven protocol)",look input raven protocol project raven decentralize network compute nod utilize idle compute power ai train speed key protocol could benefit besides add library anything protocol miss solution problem try solve experience machine learn would like input stand benefit raven protocol website url protocol
Yuqing7,MachineLearning,1617249392.0,[N] Google Research’s Novel High Efficient Neural Volumetric Representation Enables Real-Time View Synthesis,"A Google Research team accelerates Neural Radiance Fields’ rendering procedure for view-synthesis tasks, enabling it to work in real-time while retaining its ability to represent fine geometric details and convincing view-dependent effects.

Here is a quick read: [Google Research’s Novel High Efficient Neural Volumetric Representation Enables Real-Time View Synthesis](https://syncedreview.com/2021/03/31/google-researchs-novel-high-efficient-neural-volumetric-representation-enables-real-time-view-synthesis/)

The paper *Baking Neural Radiance Fields for Real-Time View Synthesis* is on [arXiv](https://arxiv.org/pdf/2103.14645.pdf).",google research team accelerate neural radiance field render procedure view synthesis task enable work real time retain ability represent fine geometric detail convince view dependent effect quick read google research novel high efficient neural volumetric representation enable real time view synthesis url paper bake neural radiance field real time view synthesis arxiv url
montebicyclelo,MachineLearning,1620460319.0,[P] Patchless MLP-Mixer,"https://github.com/sradc/patchless_mlp_mixer

This is a preliminary exploration of an even simpler MLP-Mixer style architecture.",url preliminary exploration even simpler mlp mixer style architecture
Tolstoyevskiy,MachineLearning,1617215508.0,[D] Comparison of experiment tracking tools,"Hey r/MachineLearning! 

A while back I published a post [comparing data versioning tools](https://dagshub.com/blog/data-version-control-tools/), and people seemed to find it useful. So I wrote another one comparing  experiment tracking tools – there are so many options to choose, and it make sense to consider the pros and cons of each option. 

[https://dagshub.com/blog/how-to-compare-ml-experiment-tracking-tools-to-fit-your-data-science-workflow/](https://dagshub.com/blog/how-to-compare-ml-experiment-tracking-tools-to-fit-your-data-science-workflow/)

My criteria were mainly around: 

1. What do you want to track
2.  Where is the data saved
3. What visualization capabilities are there
4. How easy is it to set up
5. How stable it is
6. Does it support large scale experimentation

As usual, here is a summary in table form, though the article of course contains more details:

[Tabular comparison of ML experiment tracking tools](https://preview.redd.it/b3rlshkcqdq61.png?width=1600&format=png&auto=webp&s=c55613adee339bee7f606b81f2a4427cb670ab32)",hey r machinelearning back publish post compare data versioning tool url people seem find useful write another one compare experiment track tool many options choose make sense consider pros con option url criteria mainly around 1 want track2 data saved3 visualization capabilities there4 easy set up5 stable is6 support large scale experimentationas usual summary table form though article course contain detail tabular comparison ml experiment track tool url
stupidMZ,MachineLearning,1617940296.0,"[R]Group-Free 3D Object Detector: New SOTA on ScanNet V2(69.1 mAP@0.25, 52.8@mAP@0.5) and SUN RGB-D(62.8 mAP@0.25 and 42.3 mAP@0.5)🔥","**Group-Free 3D Object Detection via Transformers(**another work from MSRA Visual Computing Group)

Paper link:   [\[2104.00678\] Group-Free 3D Object Detection via Transformers (arxiv.org)](https://arxiv.org/abs/2104.00678)

Code link:  [zeliu98/Group-Free-3D: Group-Free 3D Object Detection via Transformers (github.com)](https://github.com/zeliu98/Group-Free-3D)

👉 What?

A simple yet effective method for directly detecting 3D objects from the 3D point cloud without the handcraft and heuristic point grouping mechanism.

❓Why?

The mainstream point-based object detectors based on the heuristic point grouping step, but the complexity and diversity of objects in the real scene may lead to wrong point assignments and degrade the 3D object detection performance, shown in Fig. 1

&#x200B;

[Fig 1.  With the heuristic point grouping step, all points in the blue box of RoI-Pooling or blue ball of Voting are assigned and aggregated to derive the object features, resulting in wrong assignments. Our group-free based approach automatically learns the contribution of all points to each object, which has the ability to alleviate the drawbacks of the hand-crafted grouping. ](https://preview.redd.it/m6rbt1f6l2s61.png?width=524&format=png&auto=webp&s=37c77b021b7a7f14d44c52210ad4e2fc3607a7b1)

&#x200B;

🥊 The main ideas

1. Using the transformer decoder to model the relationship between the points and objects. All the points are used to form the feature of objects and the weighting of each point is automatically learned during training.
2. A multi-stage iterative box prediction framework is adopted, and the spatial encoding is refined iteratively in each stage.
3. A free-lunch multi-stage ensemble mechanism is used for improving the performance.

⚙️ Overall Architecture

PointNet is used as the backbone, the detection head is the multi-stage transformer decoder, shown in Fig. 2.

&#x200B;

[Fig 2.  This figure illustrates the simple architecture of our approach, including three major components: a backbone network to extract feature representations for each point in the point cloud, a sampling method to generate initial object candidates, and stacked attention modules to extract and refine object representations from all points. ](https://preview.redd.it/85qadvidl2s61.png?width=1107&format=png&auto=webp&s=42a4a31b10ccc824304d233df236b2d75fbdc94c)

🦾 Results

Very high performance was achieved on ScanNet V2 and SUN RGB-D

[Table. 1 Results on ScanNet V2](https://preview.redd.it/gahnurngl2s61.png?width=708&format=png&auto=webp&s=7080af89923d1933097dfef85c2a6664558f2d09)

&#x200B;

[Table. 2 Results on SUN RGB-D](https://preview.redd.it/ls35kckil2s61.png?width=827&format=png&auto=webp&s=6957ab3066b81a4bfaf93efdd67579ef657402e2)",group free 3d object detection via transformers another work msra visual compute group paper link 2104 00678 group free 3d object detection via transformers arxiv org url link zeliu98 group free 3d group free 3d object detection via transformers github com url simple yet effective method directly detect 3d object 3d point cloud without handcraft heuristic point group mechanism mainstream point base object detectors base heuristic point group step complexity diversity object real scene may lead wrong point assignments degrade 3d object detection performance show fig 1 x200b fig 1 heuristic point group step point blue box roi pool blue ball vote assign aggregate derive object feature result wrong assignments group free base approach automatically learn contribution point object ability alleviate drawbacks hand craft group url main ideas1 use transformer decoder model relationship point object point use form feature object weight point automatically learn train 2 multi stage iterative box prediction framework adopt spatial encode refine iteratively stage 3 free lunch multi stage ensemble mechanism use improve performance overall architecturepointnet use backbone detection head multi stage transformer decoder show fig 2 x200b fig 2 figure illustrate simple architecture approach include three major components backbone network extract feature representations point point cloud sample method generate initial object candidates stack attention modules extract refine object representations point url resultsvery high performance achieve scannet v2 sun rgb table nmbr result scannet v2 url nmbr result sun rgb url
giangblackk,MachineLearning,1618393121.0,[R] Time Series Forecasting Survey,"I can't find any thorough survey about time series forecasting methods with benchmarks?

Does anyone have a suggestion?",find thorough survey time series forecast methods benchmarks anyone suggestion
sgevorg,MachineLearning,1616600557.0,[N] Aim 2.2.0 is out! Hugging Face integration and advanced params table management ...,"Hi r/MachineLearning community!

Excited to launch [Aim](https://aimstack.io) v2.2.0  🎉🎉🎉

We are building an open-source self-hosted tool for AI training run comparison. It can handle 1000s of experiments at once and has a simple, straightforward API - super-easy to get started with.

Thanks for the incredible support - helping us democratize AI dev tools.

Check out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjpmYWxzZSwicnVuSGFzaCI6bnVsbCwibWV0cmljTmFtZSI6bnVsbCwidHJhY2VDb250ZXh0IjpudWxsLCJzdGVwIjpudWxsfX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCBpbiAodGVzdCwgdmFsKSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImFnZ3JlZ2F0ZWQiOnRydWUsImFnZ3JlZ2F0ZWRBcmVhIjoibWluX21heCIsImFnZ3JlZ2F0ZWRMaW5lIjoibWF4Iiwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=)

Below are a few highlights of this release. Check out the [full release post here](https://medium.com/aimstack/aim-v2-2-0-hugging-face-integration-57efa2eec104).

**1. HuggingFace Integration**

[Aim and Hugging Face](https://preview.redd.it/wna6uq6svzo61.png?width=2240&format=png&auto=webp&s=b06c286cde98511ddba8ecb9caa29e05208a1312)

**2. Metric Visibility Control: hide metrics while they are still in search**

[Hide individual metrics as well as collectively](https://i.redd.it/2nxdnewiwzo61.gif)

**3. Column Resize: control your screen real estate for super-long params**

[Drag column edges back-and-forth to resize](https://i.redd.it/xwnkk9ikwzo61.gif)

If you haven't yet, drop us a star for support! 🙌

Come say hi at the [Aim Slack community](https://slack.aimstack.io/).

Check out this version and let us know how we can improve it further 🙏",hi r machinelearning community excite launch aim url v2 2 0 build open source self host tool ai train run comparison handle 1000s experiment simple straightforward api super easy get start thank incredible support help us democratize ai dev tool check new feature play aimstack io url highlight release check full release post url huggingface integration aim hug face url metric visibility control hide metrics still search hide individual metrics well collectively url column resize control screen real estate super long params drag column edge back forth resize url yet drop us star support come say hi aim slack community url version let us know improve
doyougitme,MachineLearning,1618477849.0,"""[Discussion]"" Should I be using DVC (Data Version Control) in my day-to-day work?","I've been following [dvc.org](https://dvc.org) for a while now but am yet to be fully sold on when/if I should be using it in my everyday work. Dev work in ML does seem unduly clumsy (because of the experimental nature of the work I guess) but I'm not sure whether data versioning is the problem and whether DVC is the solution. I've been freelancing for a while now, so I'm not sure what the stack for ML devs looks like these days.

I'm curious to know whether DVC forms a popular part of the data and ML engineering stack both for working individually and in a team. If so, what are the unique advantages it provides and how has it made your life better? If you don't use it, why not? Or what are the shortcomings that made you give it up?",follow dvc org url yet fully sell use everyday work dev work ml seem unduly clumsy experimental nature work guess sure whether data versioning problem whether dvc solution freelance sure stack ml devs look like days curious know whether dvc form popular part data ml engineer stack work individually team unique advantage provide make life better use shortcomings make give
questions2067,MachineLearning,1619731841.0,[D] Does anyone here have a career in machine learning that they applied to the medical field?,"I’m really looking to just ask what you career is like/to learn more about it. I’m currently in undergrad and not sure what I want to do, but this topic interests me.",im really look ask career like learn im currently undergrad sure want topic interest
htahir1,MachineLearning,1617206594.0,[D] Why ML should be written as pipelines from the get-go,"Thought id share my ideas on why a multi-step fragmented approach to productionalization of ML is flawed. Rather, we should be creating abstractions geared towards data scientists to incentivize them to write more production-ready code from day 0. Happy  to hear thoughts from the community.  


[https://towardsdatascience.com/why-ml-should-be-written-as-pipelines-from-the-get-go-b2d95003f998](https://towardsdatascience.com/why-ml-should-be-written-as-pipelines-from-the-get-go-b2d95003f998)",think id share ideas multi step fragment approach productionalization ml flaw rather create abstractions gear towards data scientists incentivize write production ready code day 0 happy hear thoughts community url
DNA1987,MachineLearning,1619196786.0,[D] 3D CNN how to deal with empty space,"Hello, 

I am working on a 3D CNN, I am using voxel to represent protein with one channel for each amino acid, so total 20 channel, 

There is lot of empty space in my voxel ~90% and as each channel just encode one type of amino acid, so each channel is even more empty. It is not like an RGB image where each channel has a integer gradient on all positions of x * y.   

My current network only work for a couple of samples, after that it needs too much training. Somehow I am thinking it could be like an unbalance class problem, but the 3D space convolution thing is confusing me.",hello work 3d cnn use voxel represent protein one channel amino acid total nmbr channel lot empty space voxel 90 channel encode one type amino acid channel even empty like rgb image channel integer gradient position x current network work couple sample need much train somehow think could like unbalance class problem 3d space convolution thing confuse
Inferrd_F,MachineLearning,1618153944.0,[P] How to make your Models available ?,"I've created a tool to make your models available for other people: [Inferrd.com](https://Inferrd.com)   


**What is it?** Create an API for your models in seconds.

**What can you do with it?**   
  •  Put your models to use right when they're trained so you don't waste time creating/monitoring an infrastructure.   
  •  Create engaging demos to show users what your models can do in real conditions.

**Why is it interesting ?**

* You can drag-and-drop to deploy all your models, it's that simple ! Afterwards you can call them from your jupyter notebook or any IDE. The whole process from registration to deployment won't take you more than 2 minutes.
* You know what you pay for, no hidden fees. We charge a little bit more than the price of the RAM. We want to have a 256MB RAM box for 1$ per month.

I am an Infrastructure Engineer. I built this because I wanted everyone to be able to make the most out of their models. What I usually see is models sleeping on the shelf somewhere, forever waiting to be deployed by someone else, so this is their time to shine.  


Lastly why use [inferrd.com](https://inferrd.com) when you can use AWS/GCP? I think AWS/GCP/Azure become too expensive once you're captive. So I wanted to build a tool that allows you to migrate easily and won't make you dependant of it.   


AMA!",create tool make model available people inferrd com url create api model second put model use right train waste time create monitor infrastructure create engage demo show users model real condition interest drag drop deploy model simple afterwards call jupyter notebook ide whole process registration deployment win take nmbr minutes know pay hide fee charge little bite price ram want 256mb ram box 1 per month infrastructure engineer build want everyone able make model usually see model sleep shelf somewhere forever wait deploy someone else time shine lastly use inferrd com url use aws gcp think aws gcp azure become expensive captive want build tool allow migrate easily win make dependant ama
Grid_AI,MachineLearning,1617751199.0,[N] Latest Innovations with Grid.ai and PyTorch Lightning,"Join us on April 13th at 11 am ET to hear about Grid.ai and PyTorch Lightning's latest innovations with our CEO and Founder William Falcon and Thomas Chaton, Research Engineering Manager. This discussion is excellent for AI researchers, machine learning engineers, and data scientists looking for new ways to accelerate and improve their current AI model training process. Leave with tangible strategies, new tools, and great ideas.

Register now and submit any questions you have for William and Thomas!

[https://zoom.us/webinar/register/1016176774118/WN\_yW66h71HSz-MXWNGAu6OOg](https://zoom.us/webinar/register/1016176774118/WN_yW66h71HSz-MXWNGAu6OOg)",join us april 13th nmbr et hear grid ai pytorch lightning latest innovations ceo founder william falcon thomas chaton research engineer manager discussion excellent ai researchers machine learn engineer data scientists look new ways accelerate improve current ai model train process leave tangible strategies new tool great ideas register submit question william thomas url
Evening-Use-7142,MachineLearning,1617874115.0,[D] Pytorch (geometric) over neo4j,"Hi \*,

&#x200B;

Are there any good guidelines/best practices/ cookbooks/ example code for running pytorch GNN over large cloud-based database such as neo4j?

&#x200B;

My data is going to be stored at AWS using neo4j. Currently its a small db but we expect it to grow. Are there any good guidelines to working with data that way?

&#x200B;

I know neo4j has a DS library which allows for running algorithms directly over the data but we wish to design our own.

&#x200B;

&#x200B;

The goals are currently standard ones such as link-prediction and node classifications.

&#x200B;

&#x200B;

Thank you all",hi x200b good guidelines best practice cookbooks example code run pytorch gnn large cloud base database neo4j x200b data go store aws use neo4j currently small db expect grow good guidelines work data way x200b know neo4j ds library allow run algorithms directly data wish design x200b x200b goals currently standard ones link prediction node classifications x200b x200b thank
soulslicer0,MachineLearning,1616883606.0,[D] PSA: IEEE PDF Express stores passwords in Plaintext,"For CVPR 2021, I had to use the IEEE PDF Express PDF Checking service. It asks that we create an account and forces password rules (Capital letter etc.). So I foolishly decided to use my personal bank/email password.

I then get an email saying that ""You have created your account etc."" and it prints the password right there in my email in plain text.

Absolute bullshit",cvpr 2021 use ieee pdf express pdf check service ask create account force password rule capital letter etc foolishly decide use personal bank email password get email say create account etc print password right email plain text absolute bullshit
radjeep,MachineLearning,1620015489.0,[Discussion] A very rudimentary solution to the XOR problem with a single layer (excluding output) neural network.,"With two inputs x^(1) and x^(2) that can have binary values (0 or 1).We have two weights for each of those w^(1) and w^(2), and a bias unit b.

Our output neuron z hence will have the equation z = w^(1)x^(1) \+ w^(2)x^(2) \+ b.

Let's say we set our bias unit to 0, and w^(1) to 1 and w^(2) to -1.In this way we will have output 0 for input combinations (0, 0) and (1, 1) - which is required.

And we will have output 1 when input is (1, 0) and the only exceptional case would be that of output being -1 when input is (0, -1). But we can mitigate this by using and additional condition that the result is 0 only when the output is 0 and it is 1 when the output is any other value than 0.

Thoughts?",two input x 1 x 2 binary value 0 1 two weight w 1 w 2 bias unit b output neuron z hence equation z w 1 x 1 w 2 x 2 b let say set bias unit 0 w 1 nmbr w 2 1 way output nmbr input combinations 0 0 1 1 require output nmbr input 1 0 exceptional case would output 1 input 0 1 mitigate use additional condition result nmbr output nmbr nmbr output value 0 thoughts
keepthepace,MachineLearning,1619525246.0,[P] I am writing a copyleft license for machine learning models. I'd love some comments and criticism,"Hello,

I am a software developer in robotics with a lot of love for open source. Recent events have made me ponder how one could release a trained model in order to keep it really open/free and I think (but would love to be proved otherwise) that there are no licenses applicable right now, as trained weights are a very different beast from source code or executable binaries. So I tried to write one by modifying the Affero GPL license (something the FSF explicitly allows in their FAQ): [I posted it here](https://github.com/yquemener/MLMPL) 

This is actually an old subject. I saw a discussion on the [debian-legal mailing list from 2009](https://lists.debian.org/debian-legal/2009/05/msg00028.html) about it. It also [crops up](https://lists.debian.org/debian-devel/2018/07/msg00175.html) occasionally on [debian-devel](https://lists.debian.org/debian-devel/2019/05/msg00380.html), was discussed at the [2012 Debconf](https://saimei.ftp.acc.umu.se/pub/debian-meetings/2012/debconf12/high/888_Machine_learning_threats_and_opportunities_for_Debian_and_Free_Software.ogv) and was also discussed by the [ffmpeg team](https://ffmpeg.org/pipermail/ffmpeg-devel/2018-July/231834.html).

Having read that and many other cases and opinions, I think it is necessary to have a new license to ""open source"" trained weights but it is not necessarily a difficult task. I think (thanks to the previous free software efforts) it is already 95% done, we just need to adapt an existing license but be a bit more explicit about what a model is and not try to shoehorn a license designed for compiled software into the machine learning world.

I am not formally trained in copyright law, so I'd really appreciate someone with such a background to poke holes at my proposition, but everybody's constructive criticism is welcomed!",hello software developer robotics lot love open source recent events make ponder one could release train model order keep really open free think would love prove otherwise license applicable right train weight different beast source code executable binaries try write one modify affero gpl license something fsf explicitly allow faq post url actually old subject saw discussion debian legal mail list 2009 url also crop url occasionally debian devel url discuss 2012 debconf url also discuss ffmpeg team url read many case opinions think necessary new license open source train weight necessarily difficult task think thank previous free software efforts already 95 need adapt exist license bite explicit model try shoehorn license design compile software machine learn world formally train copyright law really appreciate someone background poke hole proposition everybody constructive criticism welcome
P4TR10T_TR41T0R,MachineLearning,1619775478.0,[P] A review of recent research on transformers in vision,"Hey folks,

I recently ~~published~~ released a review on transformers in vision, focused on the last months of research. I remember a few discussions around Vision Transformers in particular, but I would like to highlight a fact: while they're often seen as an intellectual curiosity that only Google can train, recent derivative models enjoy significant efficiency, outperforming all but EfficientNetV2 (if you allow for DeiT-like hard-label distillation) or all but EfficientNetV2 and Normalizer Free Networks (if you do not allow for hard-label distillation) on ImageNet.

Transformers-based models are also advantaged in large data regimes (e.g. ImageNet-21k or Google's internal JFT-300M datasets) due to their reduced inductive bias.

The post discusses discusses this, includes interactive performance visualizations and more, so consider checking it out: [https://iaml-it.github.io/posts/2021-04-28-transformers-in-vision/](https://iaml-it.github.io/posts/2021-04-28-transformers-in-vision/)

One last thing: it's the first time I write a blog post, so any feedback (be it about the format or the content itself) is deeply appreciated.",hey folks recently publish release review transformers vision focus last months research remember discussions around vision transformers particular would like highlight fact often see intellectual curiosity google train recent derivative model enjoy significant efficiency outperform efficientnetv2 allow deit like hard label distillation efficientnetv2 normalizer free network allow hard label distillation imagenet transformers base model also advantage large data regimes e g imagenet 21k google internal jft 300m datasets due reduce inductive bias post discuss discuss include interactive performance visualizations consider check url last thing first time write blog post feedback format content deeply appreciate
DAL59,MachineLearning,1619555922.0,[D] Whats the point of CLIP opposed to Dalle?,"Clip generates very weird images with artifacts and massive distortions, and often tiles the screen with requested objects.  Dalle on the other hand creates completely normal looking pictures most of the time.  So why would anyone use CLIP?",clip generate weird image artifacts massive distortions often tile screen request object dalle hand create completely normal look picture time would anyone use clip
cosapocha,MachineLearning,1619473464.0,[P] How would you measure uncertainty in a classification task?,"Hello everybody, I've been using dropout, deep ensembles, evidence, and bayes by backprop to generate samples and then measure uncertainty.

With the samples I can compute the mean and variance per class and see if the bars overlap with each other, or sum all the variances and see if it is a big number, or see if there is two or more means too high and then say the model is not reliable there. I mean, I can think of a lot of methods, but I was wondering if there is a standard, or if any of you guys have a good idea of how to measure the uncertainty of a model in classification. 

Thank you very much!",hello everybody use dropout deep ensembles evidence bay backprop generate sample measure uncertainty sample compute mean variance per class see bar overlap sum variances see big number see two mean high say model reliable mean think lot methods wonder standard guy good idea measure uncertainty model classification thank much
KirillTheMunchKing,MachineLearning,1617126474.0,[D] Are there any other GAN based image editing projects with an encoder-generator architecture that actually work in real time?,"I mostly see GAN image editing projects rely on Pix2Pix distillation to work in realtime, but the authors of ""Using latent space regression to analyze and leverage compositionality in GANS"" claim their encoder -> generator setup works in realtime. I tried the demo from github, and it does work pretty fast for small edits, kinda strange that it hangs for larger edits. 

In case you are not familiar with the paper, and want to learn about it, I explained the main ideas in my  [telegram channel](https://t.me/casual_gan/17)",mostly see gin image edit project rely pix2pix distillation work realtime author use latent space regression analyze leverage compositionality gans claim encoder generator setup work realtime try demo github work pretty fast small edit kinda strange hang larger edit case familiar paper want learn explain main ideas telegram channel url
bjourne-ml,MachineLearning,1616320233.0,[D] Thoughts on unlikelihood training/antitraining?,"What is the sub's thoughts on unlikelihood training/antitraining?

The idea is dirt-simple; if the model does what you want you
reward it (likelihood) and if it doesn't you slap it
(unlikelihood). Say you have a language model that predicts the next
word. Such models tend to, should we say, ""cheat"" when confused by
guessing on words it has already seen.

For example, suppose the sentence is:

    in particular standard likelihood training and decoding

The model is given all words except for the last and is asked to
predict the next word. Likely, it will assign a high probability to
some or all of the preceding words (a well-known problem), leading to
continuations like:

    in particular standard likelihood training and in
    in particular standard likelihood training and particular
    in particular standard likelihood training and standard
    in particular standard likelihood training and training
    ...

This leads to repetitive text. Unlikelihood training/antitraining
attempts to solve this by penalizing the model for assigning
probability mass to preceding incorrect words. Suppose part of the
probability distribution produced by the model looks like:

    p(in|in particular standard likelihood training and) = 0.2
    p(particular|in particular standard likelihood training and) = 0.02
    p(standard|in particular standard likelihood training and) = 0.05
    p(standard|in particular standard likelihood training and) = 0.02

Then we'd calculate the unlikelihood (penalty) as

    -alpha*(log(1-0.2) + log(1-0.02) + log(1-0.05)+(1-0.02)),

where alpha is a weighting hyper-parameter. Backprop and the next time
the model makes the same prediction, it will assign less probability
mass to the preceding words.

Increasing diversity/reducing repetition is just one use case for
unlikelihood. Suppose you have the sentence

    recent large-scale language models ...

The target word is ""are"" but if the model predicts ""is"" you penalize
it because ""is"" is grammatically incorrect! Same for ""Perplexity and
accuracy ..."" - reward the model for ""were"" but penalize it for ""was"".

Here are papers that explain the concept much better than I can. The authors claim to have achieved
some very impressive results:

* [Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)
* [Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training](https://arxiv.org/abs/1911.03860)",sub thoughts unlikelihood train antitraining idea dirt simple model want youreward likelihood slap unlikelihood say language model predict nextword model tend say cheat confuse byguessing word already see example suppose sentence particular standard likelihood train decodingthe model give word except last ask topredict next word likely assign high probability tosome precede word well know problem lead tocontinuations like particular standard likelihood train particular standard likelihood train particular particular standard likelihood train standard particular standard likelihood train train lead repetitive text unlikelihood train antitrainingattempts solve penalize model assigningprobability mass precede incorrect word suppose part theprobability distribution produce model look like p particular standard likelihood train nmbr p particular particular standard likelihood train nmbr p standard particular standard likelihood train nmbr p standard particular standard likelihood train 0 02then calculate unlikelihood penalty alpha log 1 0 2 log 1 0 02 log 1 0 05 1 0 02 alpha weight hyper parameter backprop next timethe model make prediction assign less probabilitymass precede word increase diversity reduce repetition one use case forunlikelihood suppose sentence recent large scale language model target word model predict penalizeit grammatically incorrect perplexity andaccuracy reward model penalize paper explain concept much better author claim achievedsome impressive result neural text generation unlikelihood train url say make inconsistent dialogue unlikely unlikelihood train url
Present-Percentage88,MachineLearning,1619541186.0,[R] Online study: predicting COVID-19 misinformation in Twitter data using AI,"Hi everyone,

I am conducting a user study for my master's thesis regarding explainable AI (XAI) solutions that can predict COVID-19 misinformation on Twitter. Misinformation has been an important topic during the pandemic. Explainable AI solutions can help us detect misinformation and simultaneously explain their prediction which is key to understanding how they work.

Your participation will be beneficial towards the battle against misinformation and the pursuit towards transparent and responsible AI. During the online study, you will label COVID-19 related tweets and answer a set of questionnaires.

Requirements for participation:

* At least 18 years old
* Run the study on a laptop/desktop computer

You can participate autonomously using the following link: 

[https://userstudy-thesis.herokuapp.com/en/group1](https://userstudy-thesis.herokuapp.com/en/group1)

Thank you in advance and stay safe.",hi everyone conduct user study master thesis regard explainable ai xai solutions predict covid 19 misinformation twitter misinformation important topic pandemic explainable ai solutions help us detect misinformation simultaneously explain prediction key understand work participation beneficial towards battle misinformation pursuit towards transparent responsible ai online study label covid 19 relate tweet answer set questionnaires requirements participation least nmbr years old run study laptop desktop computeryou participate autonomously use follow link url advance stay safe
fedetask,MachineLearning,1618071318.0,[R] Applications of Graph Neural Networks to Reinforcement Learning,Could you point me to some papers applying Graph Neural Networks to Reinforcement Learning tasks?,could point paper apply graph neural network reinforcement learn task
luisgasco,MachineLearning,1619167986.0,"[R] - Call For Participants MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents","**\*\*\* CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) \*\*\***

[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) 

**MESINESP2 Awards by BSC-Plan TL \[2,700€\]**

**Test sets and additional data are now available**

There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.

Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):

**MESINESP-L – Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).

**MESINESP-T – Clinical trials**: for automatic labelling of clinical trials summaries.

**MESINESP-P – Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.

**Key information**

**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) 

**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)

**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)

MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.

A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (> 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*. 

Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. 

**Important dates**

* April 19: Updated Train, Validation and Test sets release
* April 19: Additional datasets release (Medical entities present in documents)
* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline
* May, 7: Start of the evaluation period
* May, 17: End of the evaluation period
* May,28 :Submission of Participant Papers at CLEF2021
* July, 2: Camera ready paper submission.
* Sep 21-24: CLEF 2021 Conference

**Publications and BioASQ/CLEF2021 workshop**

Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.

**Main Track organizers**

* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.
* **Luis Gascó**, Barcelona Supercomputing Center (BSC), Spain.
* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.
* **Elena Primo-Peña,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.
* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.
* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.
* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.
* **Renato Murasaki,** BIREME – Organización Panamericana de la Salud (WHO), Brasil.

**Scientific Committee**

* **Tristan Naumann,** Microsoft Research (USA)
* **Prof. Xavier Tannier,** Sorbonne Université and LIMICS (France)
* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)
* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Parminder Batia,** Amazon Health AI (USA)
* **Prof. Irena Spasic,** School of Computer Science & Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)
* **Jose Luis Redondo García,** Amazon Alexa, Amazon (UK)
* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Allan Hanbury,**  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)
* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)
* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)
* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)
* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)
* **Prof. Henning Müller,** University of Applied Sciences Western Switzerland – Valais (Switzerland)
* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)
* **Georg Rehm,** Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)
* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)
* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)
* **Prof. Jesús Tramullas,** Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)",cfp2 mesinesp2 track medical semantic index bioasq clef 2021 url mesinesp2 award bsc plan tl 2 700 test set additional data available press need advance multilingual semantic search strategies health relate content like literature patent clinical trials cross genre use semantic search techniques combination structure vocabularies critical sophisticate search content analysis need healthcare professionals researchers pharmaceutical industry patient group private citizens follow impact past bioasq track benchmarking study e g biobert organization initiatives like biocreative iberlef propose three semantic label subtracks use widely use decs vocabulary similar mesh term mesinesp l scientific literature automatic label medical literature abstract spanish include recent covid 19 literature mesinesp clinical trials automatic label clinical trials summaries mesinesp p patent automatic label health relate patent spanish improve patent intelligence key information web url registration url bioasq task nmbr mesinesp data url organize close collaboration widely use multilingual medical literature databases bireme isciii spain express direct need advance technologies accelerate manual index efforts content spanish speak globally nmbr million people face challenge keep increase number publish medical paper use purely manual index large manually index collection train document provide document already automatically annotate nmbr million entity mention medical entities diseases medical procedures drug symptoms facilitate use complementary strategies like multi label classification multilingual transformers graph match text similarity advance term match name entity recognition components participate systems directly useful ongoing medical literature index efforts thus improve competitive intelligence prior art search enable complex search query need evidence base medicine clinical decision make elaboration clinical practice guidelines serve base future task semantic index medical record content languages important date april 19 update train validation test set release april 19 additional datasets release medical entities present document april 30 bioasq9 lab u clef nmbr registration deadline may 7 start evaluation period may 17 end evaluation period may 28 submission participant paper clef2021 july 2 camera ready paper submission sep 21 24 clef nmbr conference publications bioasq clef2021 workshop team participate mesinesp2 invite contribute systems description paper bioasq clef 2021 work note proceed short presentation approach bioasq nmbr workshop main track organizers martin krallinger barcelona supercomputing center bsc spain luis gascó barcelona supercomputing center bsc spain anastasios nentidis national center scientific research demokritos greece elena primo peña biblioteca nacional de ciencias de salud instituto de salud carlos iii spain cristina bojo canal biblioteca nacional de ciencias de la salud instituto de salud carlos iii spain george paliouras national center scientific research demokritos greece anastasia krithara national center scientific research demokritos greece renato murasaki bireme organización panamericana de la salud brasil scientific committee tristan naumann microsoft research usa prof xavier tannier sorbonne université limics france lucy lu wang allen institute ai ai2 usa prof david camacho apply intelligence data analysis research group universidad politécnica de madrid spain prof oscar corcho ontology engineer group universidad politécnica de madrid spain parminder batia amazon health ai usa prof irena spasic school computer science informatics co director data innovation research institute cardiff university uk jose luis redondo garcía amazon alexa amazon uk carlos badenes olmedo ontology engineer group universidad politécnica de madrid spain prof allan hanbury e commerce research unit faculty informatics tu wien austria prof alfonso valencia barcelona supercomputing center spain prof stefan j darmoni department biomedical informatics rouen university hospital france limics france rezarta islamaj national center biotechnology information usa prof rafael berlanga llavori universidad jaume spain prof henning müller university apply sciences western switzerland valais switzerland prof gareth j f jones school compute dublin city university ireland georg rehm deutsches forschungszentrum für künstliche intelligenz germany petr knoth research studios austria forschungsgesellschaft mbh austria natalia manola ceo openaire amke greece prof jesús tramullas departamento de ciencias de la documentación e historia de la ciencia universidad de zaragoza spain
Intelligent-Fun-5311,MachineLearning,1619703113.0,[P] Real-time Object Detection on Jetson Nano,Hi everyone. I need help in real-time object detection on Jetson Nano. I trained a yolov3 model a while back and it is pretty accurate but gives a very low FPS of 0.223 on nano. I tried converting the model to tflite but that gives an even lower fps of 0.07. Please tell me how I can achieve real-time inference without any loss in accuracy?? Thanks in advance.,hi everyone need help real time object detection jetson nano train yolov3 model back pretty accurate give low fps nmbr nano try convert model tflite give even lower fps 0 07 please tell achieve real time inference without loss accuracy thank advance
lkncy,MachineLearning,1619314378.0,[P] ESL Solution,"I found a really good website containing solutions to most exercises in ESL (The Elements of Statistical Learning)

https://yuhangzhou88.github.io/ESL_Solution/",find really good website contain solutions exercise esl elements statistical learn url
ML_WAYR_bot,MachineLearning,1619380804.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 111,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/evanatyourservice: [ASAM](https://arxiv.org/abs/2102.11600)

/u/awesomeai: [MAKE ART with Artificial Intelligence](https://www.amazon.com/dp/B091J3T4HM)

Besides that, there are no rules, have fun.",place share machine learn research paper journals article read week relate research mean elaborate give us insight otherwise could interest paper read please try provide insight understand please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent link previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 week 1 url 11 url 21 url 31 url 41 url 51 url 61 url 71 url 81 url 91 url 101 url 2 url 12 url 22 url 32 url 42 url 52 url 62 url 72 url 82 url 92 url 102 url 3 url 13 url 23 url 33 url 43 url 53 url 63 url 73 url 83 url 93 url 103 url 4 url 14 url 24 url 34 url 44 url 54 url 64 url 74 url 84 url 94 url 104 url 5 url 15 url 25 url 35 url 45 url 55 url 65 url 75 url 85 url 95 url 105 url 6 url 16 url 26 url 36 url 46 url 56 url 66 url 76 url 86 url 96 url 106 url 7 url 17 url 27 url 37 url 47 url 57 url 67 url 77 url 87 url 97 url 107 url 8 url 18 url 28 url 38 url 48 url 58 url 68 url 78 url 88 url 98 url 108 url 9 url 19 url 29 url 39 url 49 url 59 url 69 url 79 url 89 url 99 url 109 url 10 url 20 url 30 url 40 url 50 url 60 url 70 url 80 url 90 url 100 url 110 url upvoted paper two weeks ago u evanatyourservice asam url make art artificial intelligence url rule fun
ML_WAYR_bot,MachineLearning,1616961605.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 109,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)||

Most upvoted papers two weeks ago:

/u/boy_named_su: https://arxiv.org/pdf/1609.02943.pdf

/u/Vinay_Kumar20: [https://acuvate.com/blog/machine-learning-in-supply-chain/](https://acuvate.com/blog/machine-learning-in-supply-chain/)

Besides that, there are no rules, have fun.",place share machine learn research paper journals article read week relate research mean elaborate give us insight otherwise could interest paper read please try provide insight understand please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent link previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 week 1 url 11 url 21 url 31 url 41 url 51 url 61 url 71 url 81 url 91 url 101 url 2 url 12 url 22 url 32 url 42 url 52 url 62 url 72 url 82 url 92 url 102 url 3 url 13 url 23 url 33 url 43 url 53 url 63 url 73 url 83 url 93 url 103 url 4 url 14 url 24 url 34 url 44 url 54 url 64 url 74 url 84 url 94 url 104 url 5 url 15 url 25 url 35 url 45 url 55 url 65 url 75 url 85 url 95 url 105 url 6 url 16 url 26 url 36 url 46 url 56 url 66 url 76 url 86 url 96 url 106 url 7 url 17 url 27 url 37 url 47 url 57 url 67 url 77 url 87 url 97 url 107 url 8 url 18 url 28 url 38 url 48 url 58 url 68 url 78 url 88 url 98 url 108 url 9 url 19 url 29 url 39 url 49 url 59 url 69 url 79 url 89 url 99 url 10 url 20 url 30 url 40 url 50 url 60 url 70 url 80 url 90 url 100 url upvoted paper two weeks ago u boy_named_su url url rule fun
Programmierer,MachineLearning,1616598438.0,[R] Mastering Real-Time Strategy Games with Deep Reinforcement Learning: Mere Mortal Edition,"By employing an array of techniques that includes a novel form of automatic domain randomization, curricula, canonicalization of spatial features, an omniscient value function, and a network architecture designed to encode task-specific invariants, we can train deep reinforcement learning agents for the CodeCraft real-time strategy game within hours on a single GPU.

&#x200B;

Blog post: [https://clemenswinter.com/2021/03/24/mastering-real-time-strategy-games-with-deep-reinforcement-learning-mere-mortal-edition/](https://clemenswinter.com/2021/03/24/mastering-real-time-strategy-games-with-deep-reinforcement-learning-mere-mortal-edition/)

Code: [https://github.com/cswinter/DeepCodeCraft](https://github.com/cswinter/DeepCodeCraft)",employ array techniques include novel form automatic domain randomization curricula canonicalization spatial feature omniscient value function network architecture design encode task specific invariants train deep reinforcement learn agents codecraft real time strategy game within hours single gpu x200b blog post url url
kul_xjia,MachineLearning,1616341170.0,[R] Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images,[https://arxiv.org/pdf/2101.02824.pdf](https://arxiv.org/pdf/2101.02824.pdf),url
KirillTheMunchKing,MachineLearning,1617985631.0,[R] ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement - Explained,"[ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://t.me/casual_gan/24) 

A great idea to improve StyleGAN inversion for complex real images that builds on top of the recent e4e and pSp papers.

The authors propose a fast iterative method of image inversion into the latent space of a pretrained StyleGAN generator that acheives SOTA quality at a lower inference time. The core idea is to start from the average latent vector in W+ and predict an offset that would make the generated image look more like the target, then repeat this step with the new image and latent vector as the starting point. With the proposed approach a good inversion can be obtained in about 10 steps. More details [here](https://t.me/casual_gan/24)

[The inversions are awesome!](https://preview.redd.it/sa63gu0dc6s61.png?width=1106&format=png&auto=webp&s=56a647abbb5b1bb0a44f6c75e5fd78bb81ec5858)

P.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/24):",restyle residual base stylegan encoder via iterative refinement url great idea improve stylegan inversion complex real image build top recent e4e psp paper author propose fast iterative method image inversion latent space pretrained stylegan generator acheives sota quality lower inference time core idea start average latent vector w predict offset would make generate image look like target repeat step new image latent vector start point propose approach good inversion obtain nmbr step detail url inversions awesome url case familiar paper check url
fripperML,MachineLearning,1617172367.0,"[D] What’s the simplest, most lightweight but complete and 100% open source MLOps toolkit? -> MY OWN CONCLUSIONS","Although I have posted this summary in the [thread](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/), most people won't find it, so to make it more visible I post it as another thread.

First of all, I have to thank the reddit ML community in general and each of you in particular for the detailed, insightful and interesting answers I have received in the past few days. I have learnt a lot and the picture in my head is now clearer. Now, I am posting a summary with the things that, for me, make more sense (it's my opinion and will serve as our guideline for making the decision, so it's not just a bare summary).

**General advice**

We should start with a reduced set of tools, the most useful ones, in order to have the flexibility to change or adapt our projects to a new infrastructure a provider could offer us. This is something that could happen.

**End-to-end solutions**

There are mainly two solutions that are 100% open source and free to install and use, and that may solve most of the requirements of ML practitioners: [Hopsworks](https://hopsworks.readthedocs.io/en/stable/) and [ClearML](https://allegro.ai/clearml/docs/). Among this two, if I had to chose one right now, it will be ClearML. Hopsworks might be much more complete, but ClearML seems to have a bigger community behind it and to be easier to install and use. So ClearML will be something to take a look at in case we go for an all-in-one package. I also like the idea of having a platform with an UI with all our projects.

**Python Programming**

[Flake8](https://flake8.pycqa.org/en/latest/) (including flake8-docstrings), [MyPy](http://mypy-lang.org/) and [Black](https://black.readthedocs.io/en/stable/) are hugely recommended. [Google style guide](https://google.github.io/styleguide/pyguide.html) is something to take a look at too.

This morning I have found this [guide](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/) that might be worth it, as it covers many good practices. Also this [article](https://martinheinz.dev/blog/14).

Regarding the IDE, VSCode is not the same as Visual Studio, the most recommended one is VSCode.

[Poetry](https://python-poetry.org/) is also something to consider. But also one should be careful with it: its current development state is not very promising and maybe pip is more secure, as it is the official way.

**CI and Deployment**

Jenkins is a good tool, although maybe not the easiest one (Gitlab, Drone, and Circle are all easier to use). Docker might not be totally needed, but is hugely recommended as it is becoming a standard, and even many of the libraries rely on it (for example, ClearML does). In addition, it works very well with Jenkins.

We should switch from SVN to git (strongly recommended). [Gitlab](https://about.gitlab.com/) is a good option.

**Project Scaffolding**

[CookieCutter](https://cookiecutter.readthedocs.io/en/1.7.2/) or [Kedro](https://kedro.readthedocs.io/en/stable/) are the winners. I still think we will stick to Kedro template, because it offers extra functionality, and I like to think of each project as a set of pipelines to be run. Anyway, some cookiecutter templates are very good, like this [one](https://github.com/TezRomacH/python-package-template). In case we use both Kedro and ClearML, we'll have to figure out how to integrate its pipelines with ClearML tasks. But in the slack channel of ClearML there are other teams doing the same, so at least it's possible.

**Documentation**

[Sphinx](https://www.sphinx-doc.org/en/master/index.html) for the documentation is totally recommended (Google style docstrings). [Napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html) can be very useful for helping with that. This covers documentation of the actual code. For documenting the business objective and other project related stuff, we could use jupyter notebooks in order to have everything inside the repo.

**Project registry**

ClearML if we finally chose it. Otherwise, we migth use an internal wiki or just the repository with a clear documentation.

**Data Exploration and Preparation**

We should use PySpark when things go ""big"", and Pandas when things fit in memory.

**Tests**

I expected Great Expectations library to be recommended, but nobody told anything. Instead, unit testing and/or smoke tests using [pytest](https://docs.pytest.org/en/stable/). And checking them with Jenkins. Anyway, if Kedro ends up being our project template, I'll keep an eye on the [plugin](https://github.com/tamsanh/kedro-great) with [Great Expectations](https://github.com/great-expectations/great_expectations).

**Feature Store, Data Versioning**

Maybe not so important in the beginning. [DVC](https://dvc.org/doc) looks good, but it's not easy to use.

**Workflow engine or orchestrator**

In our case, we have one, but otherwise it is an important piece. Prefect is maybe the option I like the most for its simplicity, but Luigi is also a tool that I like.

Kedro, also related with this, because it is a tool for defining pipelines, does not care about how to run the pipelines and you can deploy them in several engines like Luigi, Prefect, Airflow or Kubeflow.

**Model registry**

Its importance depends on several considerations:

* If you have too many models in production.
* If models are frecuently retrained.
* If lots of models are trained and or tested in parallel.
* If some models make real-time predictions, and their performance is critical.

If any of the previous point happens to be true, a model registry can be a very important piece of the MLOps solution. Otherwise, you can consider it not essential.

**Experimenting**

It's an important piece. If we use ClearML, this will be solved. Otherwise, we might try [MLFlow](https://www.mlflow.org/docs/latest/index.html) using Kedro-MLFlow or [PipelineX](https://pipelinex.readthedocs.io/en/latest/).

[Hydra](https://hydra.cc/docs/intro/) can be an interesting addition to define configurations, although Kedro does have a nice way too.

**Training**

Apart from the ""classical"" libraries, in case of DL for simplicity [PyTorch Lighting](https://www.pytorchlightning.ai/) will be our first option. Anyway, hardware limitations could be an issue (when models don't fit into memory, when training must be distributed... so that problems should be at least foreseen... both TensorFlow and PyTorch have ways of dealing with it).

**Model serving**

[FastAPI](https://fastapi.tiangolo.com/). Or even simpler: [DL4J](https://deeplearning4j.org/), to be used in Java when we need to communicate with the rest of the applications in real time.

Other interesting solutions are [BentoML](https://github.com/bentoml/BentoML) and [Cortex](https://www.cortex.dev/), we should take a look at it too.

When high availability is important, we should take into account having redundant nodes and a resilient infraestructure (Kubernetes could be a solution).

**Visualization**

We should take a look at [voila](https://voila.readthedocs.io/en/stable/using.html) and [streamlit](https://streamlit.io/).

**Model monitoring**

We could use Jenkins pipelines or ad-hoc scheduled processed. We don't need a tool for that.",although post summary thread url people win find make visible post another thread first thank reddit ml community general particular detail insightful interest answer receive past days learn lot picture head clearer post summary things make sense opinion serve guideline make decision bare summary general advice start reduce set tool useful ones order flexibility change adapt project new infrastructure provider could offer us something could happen end end solutions mainly two solutions 100 open source free install use may solve requirements ml practitioners hopsworks url clearml url among two choose one right clearml hopsworks might much complete clearml seem bigger community behind easier install use clearml something take look case go one package also like idea platform ui project python program flake8 url include flake8 docstrings mypy url black url hugely recommend google style guide url something take look morning find guide url might worth cover many good practice also article url ide vscode visual studio recommend one vscode poetry url also something consider also one careful current development state promise maybe pip secure official way ci deployment jenkins good tool although maybe easiest one gitlab drone circle easier use docker might totally need hugely recommend become standard even many libraries rely example clearml addition work well jenkins switch svn git strongly recommend gitlab url good option project scaffold cookiecutter url kedro url winners still think stick kedro template offer extra functionality like think project set pipelines run anyway cookiecutter templates good like one url case use kedro clearml figure integrate pipelines clearml task slack channel clearml team least possible documentation sphinx url documentation totally recommend google style docstrings napoleon url useful help cover documentation actual code document business objective project relate stuff could use jupyter notebooks order everything inside repo project registry clearml finally choose otherwise migth use internal wiki repository clear documentation data exploration preparation use pyspark things go big pandas things fit memory test expect great expectations library recommend nobody tell anything instead unit test smoke test use pytest url check jenkins anyway kedro end project template keep eye plugin url great expectations url store data versioning maybe important begin dvc url look good easy use workflow engine orchestrator case one otherwise important piece prefect maybe option like simplicity luigi also tool like kedro also relate tool define pipelines care run pipelines deploy several engines like luigi prefect airflow kubeflow model registry importance depend several considerations many model production model frecuently retrain lot model train test parallel model make real time predictions performance critical previous point happen true model registry important piece mlops solution otherwise consider essential experiment important piece use clearml solve otherwise might try mlflow url use kedro mlflow pipelinex url interest addition define configurations although kedro nice way train apart classical libraries case dl simplicity pytorch light url first option anyway hardware limitations could issue model fit memory train must distribute problems least foresee tensorflow pytorch ways deal model serve fastapi url even simpler dl4j url use java need communicate rest applications real time interest solutions bentoml url cortex url take look high availability important take account redundant nod resilient infraestructure kubernetes could solution visualization take look voila url streamlit url monitor could use jenkins pipelines ad hoc schedule process need tool
KirillTheMunchKing,MachineLearning,1617381315.0,[R] StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery - SOTA StyleGAN image editing,"[StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://t.me/casual_gan/18)

This idea is so elegant, yet powerful:  
The authors use the recent CLIP model in a loss function to train a mapping network that takes text descriptions of image edits (e.g. ""a man with long hair"", ""Beyonce"", ""A woman without makeup"") and an image encoded in the latent space of a pretrained StyleGAN generator and predicts an offset vector that transforms the input image according to the text description of the edit.

&#x200B;

https://preview.redd.it/kemg73bcfsq61.png?width=1438&format=png&auto=webp&s=0c3a259abe37ef32bafa022195ca4ce3f3ab320b

P.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/18):",styleclip text drive manipulation stylegan imagery url idea elegant yet powerful author use recent clip model loss function train map network take text descriptions image edit e g man long hair beyonce woman without makeup image encode latent space pretrained stylegan generator predict offset vector transform input image accord text description edit x200b url case familiar paper check url
meldiwin,MachineLearning,1620241072.0,"[N] Joscha Bach ""cognitive Architectures""","&#x200B;

Hello Everyone,

We (IEEE Soft Robotics Podcast) are going to have Joscha Bach on the podcast, if you have any questions or arguments to Joscha, please send them here:

[https://docs.google.com/forms/d/e/1FAIpQLSegi1wwNNrYaxvkfVvRe3pB5fk6HUuSbIsL1N8b9r41EB2NEg/viewform?vc=0&c=0&w=1&flr=0&gxids=7628](https://docs.google.com/forms/d/e/1FAIpQLSegi1wwNNrYaxvkfVvRe3pB5fk6HUuSbIsL1N8b9r41EB2NEg/viewform?vc=0&c=0&w=1&flr=0&gxids=7628)

Thanks,

https://preview.redd.it/uxjjln65mcx61.png?width=2586&format=png&auto=webp&s=d093db04455bf0b4909ee1818e67055d2ce0aa6b",x200b hello everyone ieee soft robotics podcast go joscha bach podcast question arguments joscha please send url
HashRocketSyntax,MachineLearning,1616324158.0,[P] AIQC (deep learning framework) is now seeking collaborators.,"AIQC is now open to contributors! [Link to low hanging fruit GitHub issues](https://github.com/aiqc/aiqc/issues).

AIQC is a framework for rapid and reproducible deep learning that aims to drive adoption of deep learning in scientific research.

\> It does so by providing an object oriented Python API (similar to ORM) that serves as bumper rails for both advanced and entry-level deep learning workflows. The high level API allows you to perform best practice machine learning by simply calling [\`aiqc.Pipeline\` and \`aiqc.Experiment\`](https://aiqc.readthedocs.io/en/latest/notebooks/keras_multi-label_classification.html)

https://preview.redd.it/8bxqk6g03do61.png?width=1784&format=png&auto=webp&s=3e6addbfb5bc309d01b186ce4c077067e6d6b044",aiqc open contributors link low hang fruit github issue url framework rapid reproducible deep learn aim drive adoption deep learn scientific research provide object orient python api similar orm serve bumper rail advance entry level deep learn workflows high level api allow perform best practice machine learn simply call aiqc pipeline aiqc experiment url
aminnikanjam,MachineLearning,1617047762.0,"[R] A survey on ""Design Smells in Deep Learning Programs"""," Our research group (SWAT Lab., Polytechnique Montréal under supervision of Prof. Foutse Khomh) is conducting a survey on “Design Smells in Deep Learning Programs”. We have prepared an online survey that takes around 5-10 minutes to complete asking about relevance and severity of observed design issues in DL programs. 

We are looking for participants who have a strong background and experience in research/ development of Deep Learning programs (specially convolutional networks-CNNs). Please feel free to participate if you find yourself eligible. Moreover, you could kindly share this survey with colleagues/friends who you consider eligible to participate.

The results of this survey will be publicly accessible through arXiv.org in anonymized form. At no point in the survey will we ask you for your name, and we will not be logging your IP address to allow anonymity. If you would like to know more about this study, feel free to contact us with your questions.

Link: [https://forms.gle/Yedpq3Dx8tAoxYkL8](https://forms.gle/Yedpq3Dx8tAoxYkL8)

We really appreciate your time and support!

Best regards,

Amin Nikanjam (amin.nikanjam@polymtl.ca),

Foutse Khomh

SWAT Lab., Polytechnique Montréal, Montréal, Canada

[http://swat.polymtl.ca/](http://swat.polymtl.ca/)",research group swat lab polytechnique montréal supervision prof foutse khomh conduct survey design smell deep learn program prepare online survey take around 5 10 minutes complete ask relevance severity observe design issue dl program look participants strong background experience research development deep learn program specially convolutional network cnns please feel free participate find eligible moreover could kindly share survey colleagues friends consider eligible participate result survey publicly accessible arxiv org anonymized form point survey ask name log ip address allow anonymity would like know study feel free contact us question link url really appreciate time support best regard amin nikanjam amin nikanjam polymtl ca foutse khomhswat lab polytechnique montréal montréal canada url
skwaaaaat,MachineLearning,1617210746.0,[Discussion] Methods for interpreting the meta knowledge learned by meta-learning methods?,"Hi, I've been trying to draw some insights from the meta-learned model, e.g., the task relationship, summarization of the shared task structure, etc.

To be more specific, for example, when learning from certain chemistry datasets, a good meta-learning method should implicitly learn about the shared chemistry principles. And my question is, how could we visualize/interpret such learned meta knowledge? 

I couldn't find any relevant literature on this topic. Could someone point me in the right direction? Thanks a lot!",hi try draw insights meta learn model e g task relationship summarization share task structure etc specific example learn certain chemistry datasets good meta learn method implicitly learn share chemistry principles question could visualize interpret learn meta knowledge find relevant literature topic could someone point right direction thank lot
Seankala,MachineLearning,1619520030.0,[D] Is there any work highlighting the effectiveness of using bilinear transformations for certain tasks?,"I recently read a paper in computer vision titled [_Learning Deep Bilinear Transformation for Fine-grained Image Representation (Zheng et al., 2019)_](https://arxiv.org/abs/1911.03621) about a particular type of bilinear transformation (coined the ""group bilinear"") and they claim that bilinear transformations work well for fine-grained image recognition. Some works in natural language processing (in particular relation extraction) also claim that bilinear classifier layers work well.

I'm curious if there's any work out there that details _why_ this may be the case? Most of the material I read claim that the transformation ""learn semantic grouping"" or ""learn pairwise factorization,"" but it usually ends at about that.

Any recommendations or opinions are appreciated. Thanks.",recently read paper computer vision title _learning deep bilinear transformation fine grain image representation zheng et al 2019 _ url particular type bilinear transformation coin group bilinear claim bilinear transformations work well fine grain image recognition work natural language process particular relation extraction also claim bilinear classifier layer work well curious work detail _why_ may case material read claim transformation learn semantic group learn pairwise factorization usually end recommendations opinions appreciate thank
ydennisy,MachineLearning,1620044509.0,[D] Many Logistic Regression heads.,"I am interested in building / finding a NN architecture which would consist of a network, which can be viewed as an embedding layer, with many LogReg heads on top each one predicting a certain class. The purpose is to learn a dense representation of features, which would be good at predicting the various classes, hence the loss from the LR heads needs to be propagated down into the network.

The interest I have here is would this architecture work well for new unseen classes from a similar domain / problem space.

Has anyone come across such a model architecture implementation?",interest build find nn architecture would consist network view embed layer many logreg head top one predict certain class purpose learn dense representation feature would good predict various class hence loss lr head need propagate network interest would architecture work well new unseen class similar domain problem space anyone come across model architecture implementation
mLalush,MachineLearning,1618474344.0,[D] Why have the standard data formats in object detection remained as COCO/PASCAL VOC/YOLO as opposed to switching over to a nested columnar format?,"In image classification, early standards generally had users divide image files into different training and validation folders, as well as (sometimes) requiring separate folders for each class label. While researchers seem to have held on to these standards, the APIs of deep learning libraries eventually evolved to allow more flexible usage which didn't necessarily require users to shuffle around image files in folders should they want to adjust their training and validation setups.

Keeping training and validation files in different folders probably has certain benefits for the reproducibility of research, namely in making it *abundantly* clear how the train/val split was performed. However, this convention -- when enforced -- does introduce something of a ""barrier"" for beginners just looking to train a model on their existing labeled data with minimal data wrangling.

Thus, existing library APIs for image classification seem to have converged to loading references to filepaths, either as a list or in a tabular/columnar format, while encouraging users to perform their own train/val splits on these references to the data. This allows for maximum flexibility, as users can have their data organized however they wish, and easily perform whatever augmentations they wish to perform on the splits.

Most approachable tutorials for adapting a model to custom datasets now follow the standard of having users define their own dataloaders. This new standard, however, does not seem to have proliferated to *user facing* object detection and semantic/instance segmentation libraries.

Here, some of you might interject and point out that

1. The data is hierarchical in nature. Several bounding boxes, polygon coordinates or RLEs may exist for each image. COCO/PASCAL VOC/YOLO are the natural way to store such data.
2. Storing hierarchical data in a (nested) columnar format will introduce memory overhead.
3. Data formats such as .csv, .tsv don't work well with nested data.

None of these, in my opinion, are convincing arguments for why so many object detection libraries need to break with the conventions that have evolved for image classification. Hierarchical data can easily be stored as lists nested in cells of a dataframe. We don't necessarily have to repeat a row for however many objects exist in a given image.  The arrow ([https://arrow.apache.org/install/](https://arrow.apache.org/install/)) library exists for storing and loading nested columnar data.

Am I insane in thinking that object detection library APIs will eventually -- but inevitably -- converge to the standard of image classification APIs? Why haven't they already? Are the established standards too entrenched? Torchvision already seems to be heading there: [https://pytorch.org/tutorials/intermediate/torchvision\_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). I can't say the same for most other libraries, where the standard remains ""Please organize your data after these very specific instructions and execute this script with 20 poorly documented (optional) args"".",image classification early standards generally users divide image file different train validation folders well sometimes require separate folders class label researchers seem hold standards apis deep learn libraries eventually evolve allow flexible usage necessarily require users shuffle around image file folders want adjust train validation setups keep train validation file different folders probably certain benefit reproducibility research namely make abundantly clear train val split perform however convention enforce introduce something barrier beginners look train model exist label data minimal data wrangle thus exist library apis image classification seem converge load reference filepaths either list tabular columnar format encourage users perform train val split reference data allow maximum flexibility users data organize however wish easily perform whatever augmentations wish perform split approachable tutorials adapt model custom datasets follow standard users define dataloaders new standard however seem proliferate user face object detection semantic instance segmentation libraries might interject point that1 data hierarchical nature several bound box polygon coordinate rles may exist image coco pascal voc yolo natural way store data 2 store hierarchical data nest columnar format introduce memory overhead 3 data format csv tsv work well nest data none opinion convince arguments many object detection libraries need break conventions evolve image classification hierarchical data easily store list nest cells dataframe necessarily repeat row however many object exist give image arrow url library exist store load nest columnar data insane think object detection library apis eventually inevitably converge standard image classification apis already establish standards entrench torchvision already seem head url say libraries standard remain please organize data specific instructions execute script nmbr poorly document optional args
Science_Squid,MachineLearning,1618565067.0,[D] AutoML MOOC,"In a collaboration of groups working on AutoML we developed a ***free*** ***MOOC*** on this topic. If you'd like to learn more about AutoML the MOOC is live at [https://learn.ki-campus.org/courses/automl-luh2021](https://learn.ki-campus.org/courses/automl-luh2021).

In 65 videos with overall \~19h, we cover Hyperparameter Optimization (HPO), Neural Architecture Search (NAS), Bayesian Optimization (BO), Evolutionary Algorithms (EAs) and meta-learning for AutoML. The course includes quizzes and coding exercises (in python and R) to allow you to deepen your expertise.

If you're interested check out the trailer for the course [https://youtu.be/9wzS7tGwI9g](https://youtu.be/9wzS7tGwI9g)

Edit: Tried to make more apparent that the course is free.",collaboration group work automl develop free mooc topic like learn automl mooc live url nmbr videos overall 19h cover hyperparameter optimization hpo neural architecture search nas bayesian optimization bo evolutionary algorithms eas meta learn automl course include quiz cod exercise python r allow deepen expertise interest check trailer course url try make apparent course free
akirp001,MachineLearning,1617202841.0,[D] Are there any practical reasons for learning about the Boltzmann machines and how they work?,"About three years ago, I used a replicated softmax for some text data that required its own special embedding. I was finally able to apply something to a topic I had spent a ton of time trying to understand.

And yet today, even in that use case, I would probably go to hugging faces. 

It seems to me, there just isn't any notable papers or breakthroughs using it.

Part of my machine learning journey was going through the last chapter on generative modeling in the MIT book. It's certainly a slog and you get a lot out of it but now I wonder if it's even worth spending anytime on the boltzman sections.",three years ago use replicate softmax text data require special embed finally able apply something topic spend ton time try understand yet today even use case would probably go hug face seem notable paper breakthroughs use part machine learn journey go last chapter generative model mit book certainly slog get lot wonder even worth spend anytime boltzman section
vonum,MachineLearning,1619692257.0,[D] Audio processing on mobile devices,"Hello, has anyone been doing audio processing on mobile devices? I am having trouble finding tooling for audio processing. What tools did you use and which technologies?

  


I am using tensorflow js, so loading and making predictions is not a problem, however transforming the audio input from the microphone is a problem due to not being able to find libraries.",hello anyone audio process mobile devices trouble find tool audio process tool use technologies use tensorflow js load make predictions problem however transform audio input microphone problem due able find libraries
yusuf-bengio,MachineLearning,1617269822.0,[D] Keras: Killed by Google,"First of all, this is not a rant about Tensorflow (it actually is but more on that later). Disclaimer: I have been working on research projects with Teano, JAX, PT, TF 1 &2, and of course the original Keras.

The **original Keras** was just a high-level API specification for machine learning, which was really nice when collaborating with people who have less engineering background. The API was framework agnostic and the main implementation supported multiple backends (Teano, Tensorflow, and MS-CNTK)

Essentially, the API design resembled the abstractions of modern high-level frameworks such as PyTorch-Lightning and fast.ai, with slightly different *design* *flavors* (e.g., a Keras model combines the network with the metrics and training code in a single object, whereas other frameworks usually separate the network from the learner object).

The huge advantage of keras was that it was available and the API stable **back in 2016, 2017.** I think this is something remarkable in a field that moves so fast.

But then, you know the story, Google announced its plans to incorporated it into Tensorflow 2. This wouldn't have been a problem on its own, but it slowly killed keras for 3 reasons:

1. During the time-span of this merge, the keras API was effectively ""frozen"", making it lag behind alternatives in terms of features
2. The release of TF2 came too late. On top of that, the first versions were buggy and even now are lacking some basic features.
3. Instead of making a hard cut between TF 1 and 2, Google decided that it's better to carried over a lot of baggage and crap from TF1, making the framework extremely bloated. When something does not work, you get overwhelmed by long cryptic error messages and stacktraces longer than your screen can visualize.

So, this post is really intended as a **funeral for the keras API**.

Looking forward to know your thoughts.

EDIT: I have nothing personal against Google. Far from it, I really like their impressive contributions to ML (Colab, TPU, JAX, ...), but the story with keras and TF2 is really frustrating for me who liked working with it in the past.",first rant tensorflow actually later disclaimer work research project teano jax pt tf nmbr 2 course original keras original keras high level api specification machine learn really nice collaborate people less engineer background api framework agnostic main implementation support multiple backends teano tensorflow ms cntk essentially api design resemble abstractions modern high level frameworks pytorch lightning fast ai slightly different design flavor e g keras model combine network metrics train code single object whereas frameworks usually separate network learner object huge advantage keras available api stable back 2016 2017 think something remarkable field move fast know story google announce plan incorporate tensorflow 2 problem slowly kill keras nmbr reason 1 time span merge keras api effectively freeze make lag behind alternatives term features2 release tf2 come late top first versions buggy even lack basic feature 3 instead make hard cut tf nmbr 2 google decide better carry lot baggage crap tf1 make framework extremely bloat something work get overwhelm long cryptic error message stacktraces longer screen visualize post really intend funeral keras api look forward know thoughts edit nothing personal google far really like impressive contributions ml colab tpu jax story keras tf2 really frustrate like work past
hyunwoongko,MachineLearning,1616882340.0,[P] Openchat 1.1 is released! (support 30+ conversational model),"&#x200B;

https://preview.redd.it/laki77rl6np61.png?width=2412&format=png&auto=webp&s=bdb626be79abfa4c4e3720e6203655d00a71548b

Hello, I'm hyunwoongko who made openchat, an artificial intelligence conversation framework. openchat is an open-source framework that allows you to communicate with artificial intelligence with **just one line of code.** 

&#x200B;

Today, openchat has been updated to version 1.1 and I am writing this article. Unlike before, most engines have been changed to parlai and we can support 30+ conversational models. If you want to talk with artificial intelligence, try installing an openchat!

&#x200B;

For more information, plz visit [https://github.com/hyunwoongko/openchat](https://github.com/hyunwoongko/openchat) . Thank you!",x200b url hyunwoongko make openchat artificial intelligence conversation framework openchat open source framework allow communicate artificial intelligence one line code x200b today openchat update version nmbr write article unlike engines change parlai support 30 conversational model want talk artificial intelligence try instal openchat x200b information plz visit url thank
Pestocalypse,MachineLearning,1620144671.0,[N] Transformer and Capsule co-inventors launch new API-based NLP startup,"Announcement here: [https://twitter.com/AidanNGomez/status/1389574000796479489](https://twitter.com/AidanNGomez/status/1389574000796479489)

Article: [https://www.theglobeandmail.com/business/article-toronto-startup-backed-by-ai-experts-aims-to-bring-google-quality/](https://www.theglobeandmail.com/business/article-toronto-startup-backed-by-ai-experts-aims-to-bring-google-quality/)

>Some of the world’s leading artificial intelligence experts are backing a Toronto startup co-founded by protégés of AI luminaries Geoffrey Hinton and Jeff Dean that is attempting to make it easier for humans to talk to machines.  
>  
>Cohere Inc. is officially launching Tuesday, offering to plug companies into its machine-learning software over the internet by uploading three lines of code into their systems that provide access to their [technology](https://archive.is/o/dFsXl/https://www.theglobeandmail.com/topics/technology/). Cohere claims its software will provide a richer understanding of human language, including semantics, sentiments and tone.  
>  
>Early investors include Prof. Hinton, the University of Toronto professor and Google engineering fellow known as the “godfather” of deep learning; Ian Goodfellow, Apple’s head of AI; Raquel Urtasun, chief scientist and head of Uber’s ATG research and development division; Nvidia AI director Sanja Fidler and AI pioneers Fei-Fei Li and Pieter Abbeel. Toronto AI financier Radical Ventures wrote the first cheque to Cohere.",announcement url url worlds lead artificial intelligence experts back toronto startup co found protégés ai luminaries geoffrey hinton jeff dean attempt make easier humans talk machine cohere inc officially launch tuesday offer plug company machine learn software internet upload three line code systems provide access technology url cohere claim software provide richer understand human language include semantics sentiments tone early investors include prof hinton university toronto professor google engineer fellow know godfather deep learn ian goodfellow apples head ai raquel urtasun chief scientist head ubers atg research development division nvidia ai director sanja fidler ai pioneer fei fei li pieter abbeel toronto ai financier radical venture write first cheque cohere
HybridRxN,MachineLearning,1618034652.0,Machine learning is getting easier software engineering still hard [D],"Hi, this is my first discussion post here. I'm currently an ML graduate student at university.

What do you think of [this](https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046) article?
Will those seeking or currently in machine learning roles be replaced by sophisticated frameworks in a few years driving down jobs? Or will jobs remain stable with more complex demands and advanced questions enabled by these tools ? Should Machine Learning enthusiasts and students interested in industry jobs focus more on things that are more difficult to automate like data engineering, data augmentation, devops and reliable interpretability (or something else?) than Kaggle-style architecture stacking?  

There may be some evidence for this at least in NLP.  With Huggingface [AutoNLP product](https://huggingface.co/autonlp) and [GPT-3 enabled applications ](https://openai.com/blog/gpt-3-apps/) .
I am really curious to hear the thoughts of the community.

Edit: article may be behind a paywall because of Medium so you can use incognito or outline site

Edit2: This is what GPT3 thinks (from one sample): ""I think the job market will continue to grow. The tools are getting easier, but there is still a lot of work to do. I think the ML community is going to have to figure out how to make the tools easier to use, but also how to make them more powerful. I think we will see a lot of ML engineers who are experts at using these tools, but not necessarily experts in the underlying algorithms.

I think there will always be a need for people who can design and implement new algorithms. I think there will also always be a need for people who can understand the limitations of the tools, and can design experiments to test out new ways of using the tools""",hi first discussion post currently ml graduate student university think url article seek currently machine learn roles replace sophisticate frameworks years drive job job remain stable complex demand advance question enable tool machine learn enthusiasts students interest industry job focus things difficult automate like data engineer data augmentation devops reliable interpretability something else kaggle style architecture stack may evidence least nlp huggingface autonlp product url gpt 3 enable applications url really curious hear thoughts community edit article may behind paywall medium use incognito outline siteedit2 gpt3 think one sample think job market continue grow tool get easier still lot work think ml community go figure make tool easier use also make powerful think see lot ml engineer experts use tool necessarily experts underlie algorithms think always need people design implement new algorithms think also always need people understand limitations tool design experiment test new ways use tool
Combination-Fun,MachineLearning,1620065927.0,[R] Video explaining Swin Transformers,"In the paper series of videos we present the latest ""Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"" paper this week. Hope its useful for understanding how transformers are increasingly getting used for vision tasks: [https://youtu.be/tFYxJZBAbE8](https://youtu.be/tFYxJZBAbE8)",paper series videos present latest swin transformer hierarchical vision transformer use shift windows paper week hope useful understand transformers increasingly get use vision task url
_conquistador,MachineLearning,1620076306.0,[P] Hi r/machinelearning! We created an AI-assisted video annotation tool that speeds up labelling time by 17x - looking for BETA testers to help us refine web application,"As part of our ML postgrad, we developed a tool for our lab that speeds up video labelling times by 15-20x using few-shot classification / human-in-the-loop input (e.g., a person labels a few frames and the algorithm handles the rest).

We’re looking for beta testers to help refine the platform to additional real-world use-cases. Besides early access to the platform, we’ll give any early testers free lifetime access once we launch.

Please fill out this form to get early access: [https://forms.gle/CGWd29xNv24Kwm1Y6](https://forms.gle/CGWd29xNv24Kwm1Y6)",part ml postgrad develop tool lab speed video label time 15 20x use shoot classification human loop input e g person label frame algorithm handle rest look beta testers help refine platform additional real world use case besides early access platform well give early testers free lifetime access launch please fill form get early access url
SherdyRavers,MachineLearning,1616770387.0,[R] I'm currently doing research about Gaussain Processes and I'm trying to develop a deeper understanding. My main concern is GP prior sampling,"I'm having a hard time understanding Gaussian Processes prior sampling. Here is the article from medium I'm using to learn about Gaussian Processes \[Article link\]\[[https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804](https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804)\].

&#x200B;

I'm going to start off by explaining my understanding of the partconfuses me. So we get the probability of 2 functions using this PDF equation. I'll call it equation\_1

&#x200B;

https://preview.redd.it/4ctmv5soydp61.png?width=671&format=png&auto=webp&s=2b154b7e075a8307051321bb486cd2fa112fec81

https://preview.redd.it/xmhxv4soydp61.png?width=790&format=png&auto=webp&s=4eb78b6195a0df8b384234ac2261afa23e4dcba2

&#x200B;

Then we plot each function and get the probability of each function using this pdf graph. I'll call it graph\_1 because I'm going to ask questions about it later on

&#x200B;

&#x200B;

https://preview.redd.it/6h5o4edqydp61.png?width=914&format=png&auto=webp&s=48b0143ba36230692fdace4529ed45f28400c9b9

After it is assumed that the prior is sampled using 600 data points and 50 functions are obtained. I've put the graph obtained below. I'll call it graph\_2

https://preview.redd.it/b4bwlfdqydp61.png?width=754&format=png&auto=webp&s=7d571dfc66db12c0ab4fce660ed6c21a8ee9e019

P.S: If you don't understand my questions because I missed out a few details, you can refer to thsi Article for missing details. \[Article link\]\[1\]

\*\*My Questions\*\*

1. When training the GP prior, is X and X\* two data points from the raining data?
2. For graph\_1, where does the Gaussian distribution come from?, also where do the positions of the functions on the x-axis come from? are they the standard deviation postions of the functions results?
3. For graph\_2, whats on the y axis? Is it the results axis standardised i.e instead of showing the actual result, the standardised result is shown?",hard time understand gaussian process prior sample article medium use learn gaussian process article link url go start explain understand partconfuses get probability nmbr function use pdf equation call equation _1 x200b url plot function get probability function use pdf graph call graph _1 go ask question later x200b x200b url assume prior sample use nmbr data point nmbr function obtain put graph obtain call graph _2 url understand question miss detail refer thsi article miss detail article link 1 question 1 train gp prior x x two data point rain data 2 graph _1 gaussian distribution come also position function x axis come standard deviation postions function result 3 graph _2 whats axis result axis standardise e instead show actual result standardise result show
King-Little,MachineLearning,1619775635.0,[D] M1 MacBooks versus Google Colab for deep learning,"I am just starting getting into deep learning with `tf.keras`. I am at the point where I have to decide where I want to develop. The thesis project will be timeseries prediction. My options are PyCharm on Macbook Air M1 2020 or a 2013 4th Gen Intel i5 Linux desktop and Google Colab (please let me know if there are others).

So the question I have now is which one is faster/better suited for my puropses. M1 got [hyped](https://machinelearning.apple.com/updates/ml-compute-training-on-mac) a lot so I thought the M1 would savage my desktop (and acutally the hype biased my purchase decision), but well its only slightly better (like 1.2-1.5x faster in my cifar10 benchmark) and I wonder if its worth the effective 1-2 GB of RAM left on MacOS vs the \~14 GB on my Linux machine. Further there is Colab and I can't really tell which one will win the race, since Colab limits resources by demand but also allows distributed fit on cloud TPUs, which would introduce some extra coding efforts. Then again I have to say: so does ML on Apple Silicon, which comes with [a handful of limitations](https://github.com/apple/tensorflow_macos#additional-information), a [peculiar MiniConda setup](https://github.com/apple/tensorflow_macos/issues/153), a [lot of issues](https://github.com/apple/tensorflow_macos/issues) (also severe ones, like training errors etc., problems which I would not even recognize) which are actually not really being worked on.

Is from the perspective of professional data scientists, which I hope to find here, a clear indication on which I should choose?",start get deep learn tf keras point decide want develop thesis project timeseries prediction options pycharm macbook air m1 nmbr nmbr 4th gen intel i5 linux desktop google colab please let know others question one faster better suit puropses m1 get hype url lot think m1 would savage desktop acutally hype bias purchase decision well slightly better like 1 2 1 5x faster cifar10 benchmark wonder worth effective 1 2 gb ram leave macos vs 14 gb linux machine colab really tell one win race since colab limit resources demand also allow distribute fit cloud tpus would introduce extra cod efforts say ml apple silicon come handful limitations url peculiar miniconda setup url lot issue url also severe ones like train errors etc problems would even recognize actually really work perspective professional data scientists hope find clear indication choose
LynnHoHZL,MachineLearning,1619491098.0,[R] EigenGAN: Layer-Wise Eigen-Learning for GANs,"We post the paper and code of our new work EigenGAN which unsupervisedly learns hierarchical interpretable dimensions for GANs. Welcome to discuss.

Paper: [https://arxiv.org/pdf/2104.12476.pdf](https://arxiv.org/pdf/2104.12476.pdf)

Code: [https://github.com/LynnHo/EigenGAN-Tensorflow](https://github.com/LynnHo/EigenGAN-Tensorflow)

&#x200B;

[Gender](https://i.redd.it/81csinpunmv61.gif)

[Pose \(Yaw\)](https://i.redd.it/kf84ko6comv61.gif)

&#x200B;

[Painting Style](https://i.redd.it/zncv9a3lomv61.gif)

[Hue](https://i.redd.it/dl13laanomv61.gif)",post paper code new work eigengan unsupervisedly learn hierarchical interpretable dimension gans welcome discuss paper url url yaw url style url
jj4646,MachineLearning,1619378764.0,[D] why are neural networks better than polynomial approximation?,Has anyone ever come across a formal mathematical explanation as to why neural networks  are more powerful than polynomial approximation?  Have some results (e.g. papers) been proven that conclusively show neural networks have certain advantages over polynomial approximation?,anyone ever come across formal mathematical explanation neural network powerful polynomial approximation result e g paper prove conclusively show neural network certain advantage polynomial approximation
Yuqing7,MachineLearning,1616720149.0,[N] Tsinghua & MIT’s P-Tuning Boosts Performance on NLU Benchmarks,"Tsinghua & MIT researchers break the stereotype that GPTs can generate but not understand language, showing that GPTs can compete with BERT models on natural language understanding tasks using a novel P-tuning method that can also improve BERT performance in both few-shot and supervised settings.

Here is a quick read: [GPT Understands, Too! Tsinghua & MIT’s P-Tuning Boosts Performance on NLU Benchmarks](https://syncedreview.com/2021/03/25/gpt-understands-too-tsinghua-mits-p-tuning-boosts-performance-on-nlu-benchmarks/)

The paper *GPT Understands, Too* is on [arXiv](https://arxiv.org/pdf/2103.10385.pdf).",tsinghua mit researchers break stereotype gpts generate understand language show gpts compete bert model natural language understand task use novel p tune method also improve bert performance shoot supervise settings quick read gpt understand tsinghua mits p tune boost performance nlu benchmarks url paper gpt understand arxiv url
austingwalters,MachineLearning,1619503940.0,[P] Data Profiler | What's in your data?,"Hello /r/MachineLearning

I thought the community might be interested in a project I have been apart of.

Our team has been working on a python library called the [DataProfiler](https://github.com/capitalone/DataProfiler). 

The project had two objectives:

1. Quickly and accurate (cheaply) identify sensitive data (PII/NPI) in datasets.
2. Generate data profiles which can be utilized in downstream (ML) applications

Regarding sensitive data detection, we published a workshop paper on the model within the library: [Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions](https://aaai-kdf.github.io/kdf2021/assets/pdfs/KDF_21_paper_10.pdf)

In addition to sensitive data detection, the library also calculates statistical features and general characteristics of a dataset. This has helped our team quickly evaluate datasets, but also enabled the profiles use in downstream applications.

Some nifty features the community may be interested in:  


* [Can load most files into a DataFrame with a single command](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/data_readers.html) (identifies headers, formats, etc)
* [Extending the current entity detection model with transfer learning](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/data_labeling.html#extending-a-data-labeler-with-transfer-learning) is easy and takes only a few lines of code (or retrain from scratch).
* The model works on both structured (CSV, TSV, JSON, etc) and unstructured data (text)
* It's possible (though a tad rough) to [add a new custom model for entity detection](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/add_new_model_to_data_labeler.html)
* [Profiles can be saved, loaded and merged](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/profiler.html)

Generally, we are looking for feedback and curious what the community thinks of the project?",hello r machinelearningi think community might interest project apart team work python library call dataprofiler url project two objectives 1 quickly accurate cheaply identify sensitive data pii npi datasets 2 generate data profile utilize downstream ml applicationsregarding sensitive data detection publish workshop paper model within library sensitive data detection high throughput neural network model financial institutions url addition sensitive data detection library also calculate statistical feature general characteristics dataset help team quickly evaluate datasets also enable profile use downstream applications nifty feature community may interest load file dataframe single command url identify headers format etc extend current entity detection model transfer learn url easy take line code retrain scratch model work structure csv tsv json etc unstructured data text possible though tad rough add new custom model entity detection url profile save load merge url look feedback curious community think project
regalalgorithm,MachineLearning,1618964072.0,[D] New Tag for Self Promotion Content?,"As this sub has grown, it has become normal for podcast / YouTube / blog creators to post links to their episodes /  videos / posts as [D] text posts on here, which are little more than links to the content. There was a [discussion ](https://www.reddit.com/r/MachineLearning/comments/j1s3yw/d_recent_increase_in_self_promotion_content/) specifically about this hald a year ago. I've done this myself quite a bit, as co-runner of two publications and a podcast about AI. I've stopped a while ago since it felt potentially obnoxious / spammy (actually I got called out for my newsletter posts, lol) , but still see it a lot. 

Maybe I'm just salty, but I do wonder if these are in the spirit of this sub. So, wanted to share an idea: perhaps there should a tag [SP] specifically for self promotion? This would differentiate it from [D] posts that are actually for discussion. I am not sure how much this community cares about this, and gramted it is pretty easy to differentiate self promotion posts from normal posts based on the title. But still, felt like floating the idea as someone who was not sure what is cool to do and what is obnoxious.",sub grow become normal podcast youtube blog creators post link episodes videos post text post little link content discussion url specifically hald year ago quite bite co runner two publications podcast ai stop ago since felt potentially obnoxious spammy actually get call newsletter post lol still see lot maybe salty wonder spirit sub want share idea perhaps tag sp specifically self promotion would differentiate post actually discussion sure much community care gramted pretty easy differentiate self promotion post normal post base title still felt like float idea someone sure cool obnoxious
throwaway_secondtime,MachineLearning,1618380124.0,"[D] Sam Altman, Founder of OpenAI, proposes a ""Wealth For All"" plan for dealing with AI disruption",https://moores.samaltman.com/,url
ScienTecht,MachineLearning,1619845553.0,[P] I created a way to learn machine learning through Jupyter,"Hey all,

I’ve been working on a new way to help people practice machine learning concepts. 

Since most professionals in data science use Jupyter notebooks, I thought it’d be really cool for people to learn through interactive Jupyter notebooks as well. Here I’ve written an exercise that guides you through building a K-Nearest Neighbors classifier from scratch. As far as I know, I haven’t seen this done elsewhere. 

Please [**check it out**](https://www.confetti.ai/questions/1-3?utm_source=reddit&utm_medium=web&utm_campaign=jupyterhub-knn) and let me know what you think!",hey ive work new way help people practice machine learn concepts since professionals data science use jupyter notebooks think itd really cool people learn interactive jupyter notebooks well ive write exercise guide build k nearest neighbor classifier scratch far know see elsewhere please check url let know think
a_computer_pun,MachineLearning,1617902342.0,[Research] Companies for compiling training data,"I was wondering if anyone had any useful companies that they've used for data retrieval. I need to retrieve data for Machine Learning training using sample data from event sites for training of a web scraper. So far I've looked into using Fiverr for some of the data but the results are pretty hit or miss. Here's a list of some I've looked into so far but have no idea about whether they are useful or not:

Lionbridge AI

Amazon Mechanical Turk

Clickworker

Appen

Globalme

Google Labeling Service

BasicAI",wonder anyone useful company use data retrieval need retrieve data machine learn train use sample data event sit train web scraper far look use fiverr data result pretty hit miss list look far idea whether useful lionbridge aiamazon mechanical turkclickworkerappenglobalmegoogle label servicebasicai
this_username_is_tkn,MachineLearning,1620337867.0,[Research] Seeing use of ML for ordinary work puts a smile on my face.,"The objective of this paper is to find an alternative to conventional method of concrete mix design. For finding the alternative, 4 machine learning algorithms viz. multi-variable linear regression, Support Vector Regression, Decision Tree Regression and Artificial Neural Network for designing concrete mix of desired properties. 

[original Article ](https://dx.doi.org/10.22115/scce.2021.248779.1257)",objective paper find alternative conventional method concrete mix design find alternative nmbr machine learn algorithms viz multi variable linear regression support vector regression decision tree regression artificial neural network design concrete mix desire properties original article url
aledinuso,MachineLearning,1618959406.0,[D] When do you start optimizing hyperparameters when trying out a new idea?,"When implementing a new idea, I find it hard to decide how much time I should spend on getting it to work before moving on to the next one. So I would like to know how other people do it, do you optimize hyperparameters always before abandoning a new approach?  Or is it more like the last thing to do if you already had some level of success?",implement new idea find hard decide much time spend get work move next one would like know people optimize hyperparameters always abandon new approach like last thing already level success
tdls_to,MachineLearning,1618660921.0,[N] Spotify Confidence - open source for analyzing a/b test data,"what do I all think about this library Spotify open sourced? sounds pretty useful but wondering if anyone has tried this or similar ones and if you have any recommendations?

https://github.com/spotify/confidence",think library spotify open source sound pretty useful wonder anyone try similar ones recommendations url
chasep255,MachineLearning,1617278214.0,[D] Using activity regularization instead of batch norm.,"Is there any reason I can't accomplish the same goal of batch normalization using an activity regularizer? Basically I would add a penalty to the loss function for a layer who's activation does not have an mean of zero and variance of one. There are two reasons I might prefer this method...

I am trying to train GANs and when using batch norm it behaves differently at inference then during training. This causes the discriminator to be able to have something like 90% accuracy while the generator also thinks it has 90% accuracy. I can solve this by using a really fast momentum of 0.5 but does not seem like the best solution. Ideally I want something that behaves the same during training as inference.

Secondly, I only have 8GB of vram on my RTX 3070 (really wish I went for the 3090 now). This poses a tight constraint on my batch size depending on the size of the model. As I understand batch norm works best with large batches of at least size 32. I usually am training with smaller batches.

I can't really think of a reason not to try this. Any thoughts? Also I am unsure what would be the best term to add to the loss function. I was thinking something like the following but there may be a better way.

`def normal_reg(x):`  
`return tf.square(tf.reduce_mean(x)) + tf.square(tf.math.reduce_variance(x) - 1)`",reason accomplish goal batch normalization use activity regularizer basically would add penalty loss function layer activation mean zero variance one two reason might prefer method try train gans use batch norm behave differently inference train cause discriminator able something like 90 accuracy generator also think 90 accuracy solve use really fast momentum nmbr seem like best solution ideally want something behave train inference secondly 8gb vram rtx nmbr really wish go nmbr pose tight constraint batch size depend size model understand batch norm work best large batch least size 32 usually train smaller batch really think reason try thoughts also unsure would best term add loss function think something like follow may better way def normal_reg x return tf square tf reduce_mean x tf square tf math reduce_variance x 1
Purple-Ad-3492,MachineLearning,1619042230.0,[D] Thoughts on using VADER for sentiment analysis on texts other than social media?,"Given that the package is designed for sentiment analysis tuned to social media (Twitter, NYTo, Amazon Movie Reviews). I like this tool for its sentiment rating on a -4 to 4 scale, but it’s limitations seem constrained as far as its classification system, i.e. words and phrases are pre-set and unlike other SA tools, words can’t be added to lists that would simply classify positive/neutral/negative on a -1 to 1 scale. 

(VADER tool was constructed and scores were given in these ranges based on an evaluation of 9,000 token features by 10 independent humans. )

I’ve seen the Harry Potter Book project subsected by chapter using VADER. Therefore my first question pertains to thoughts on reliability of these results? 

https://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-sentiment-analysis-1b474b13651d


I’m also looking into non-English text translations. i.e. texts that are originally written in another language and using the VADER sentiment analysis on them. I’d rather find some sort of work-around other than translating the texts themselves, perhaps translating the lexicons in the .txt file and in the code example negate, booster and special_case words lists.

...Although, I couldn’t say the word translation would have the same effective score as its English version. So maybe the preventive hiccup method would be to translate first .... 
or use another SA tool like Dostoyevsky (for Russian).",give package design sentiment analysis tune social media twitter nyto amazon movie review like tool sentiment rat 4 nmbr scale limitations seem constrain far classification system e word phrase pre set unlike sa tool word cant add list would simply classify positive neutral negative 1 nmbr scale vader tool construct score give range base evaluation 9 000 token feature nmbr independent humans ive see harry potter book project subsected chapter use vader therefore first question pertain thoughts reliability result url also look non english text translations e texts originally write another language use vader sentiment analysis id rather find sort work around translate texts perhaps translate lexicons txt file code example negate booster special_case word list although say word translation would effective score english version maybe preventive hiccup method would translate first use another sa tool like dostoyevsky russian
rahulkumar1210,MachineLearning,1618165092.0,[P] Footprint recognition using feature extraction algorithm,"If someone can help me with this project and implementation. Please reply.

I am implementing the flow chart (attached image) in python. Got stuck in some stages (coding error).

Feature Extraction algorithm using is Principal Component Analysis (PCA), Independent Component Analysis (ICA), Linear Discriminant Analysis (LDA), VGG16, VGG19, InceptionV3, and ResNet50.

I want to use the Combination classifier (KNN or SVM) or one at a time. I want to conclude and compare the result of each of these algorithms. and declare the best.

[Process](https://preview.redd.it/tlqwjkz4fls61.png?width=561&format=png&auto=webp&s=f1e16cdbd5f32472071a0a6d91cf1eadc25d42c8)",someone help project implementation please reply implement flow chart attach image python get stick stag cod error feature extraction algorithm use principal component analysis pca independent component analysis ica linear discriminant analysis lda vgg16 vgg19 inceptionv3 resnet50 want use combination classifier knn svm one time want conclude compare result algorithms declare best process url
sensetime,MachineLearning,1616860563.0,[D] Jürgen Schmidhuber's work on fast weights from 1991 is similar to linearized variants of Transformers,"I saw that Schmidhuber [tweeted](https://twitter.com/SchmidhuberAI/status/1375345693758521345) a new blog post:

https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html

and in the post he discussed (in the Schmidhuber style) some of the works he did from the 1990's, in particular the use of ""fast weights"" which in principle would allow neural nets to learn to ""program"" other neural nets. He mentions that the methods proposed enabled ""fast weight changes through additive outer products of self-invented activation patterns"" which are similar to today's self-attention mechanism used in Transformers. Recently there has been several variants of Transformers that uses linear approximation for efficiency purposes, and such works demonstrate similar performance as the version with softmax, which he claims to be similar to fast-weights.

Apart from this blog post, Schmidhuber's lab also published an article recently on this topic, “Linear Transformers Are Secretly Fast Weight Memory Systems” (https://arxiv.org/abs/2102.11174). In this paper, they also propose better ways to linearize transformers inspired by some techniques from the fast-weight days, and show improvements compared to other linear variants of transformers, so I think this topic / discussion would be of interest to this forum.",saw schmidhuber tweet url new blog post url post discuss schmidhuber style work 1990 particular use fast weight principle would allow neural net learn program neural net mention methods propose enable fast weight change additive outer products self invent activation pattern similar today self attention mechanism use transformers recently several variants transformers use linear approximation efficiency purpose work demonstrate similar performance version softmax claim similar fast weight apart blog post schmidhuber lab also publish article recently topic linear transformers secretly fast weight memory systems url paper also propose better ways linearize transformers inspire techniques fast weight days show improvements compare linear variants transformers think topic discussion would interest forum
VDevAGI,MachineLearning,1616668296.0,[D] Few-shot learning in practice.,"Although tons of few shot learning approaches have come up in recent years, they're only tested on Omniglot, miniImageNet and the likes.

I was wondering if these methods work for practical use cases. For instance can the latest CVPR'21 (say) SOTA few-shot paper be used to industrial problems with some engineering?

Or well-known baseline approaches like RFs, SVMs, kNN beat the SOTA in practice and are more robust.",although tons shoot learn approach come recent years test omniglot miniimagenet like wonder methods work practical use case instance latest cvpr 21 say sota shoot paper use industrial problems engineer well know baseline approach like rfs svms knn beat sota practice robust
begooboi,MachineLearning,1620577957.0,[D] How do we define a discriminative model?,Generative models are those which learns the distribution of the data. If generative models approximates the distribution of data then what does a discriminative model learns?,generative model learn distribution data generative model approximate distribution data discriminative model learn
Equivalent-Choice-75,MachineLearning,1618590893.0,[D] ML PhD at top 5-10 ranked school vs RE at FAANG (Applied teams),"Hi,

I'm trying to decide between ML PhD at 5-10 CS school (in USA) and MLE at FAANG (It is one of the applied teams and not pure research teams like FAIR/Google Brain)

If going to industry is eventual goal, which of these two options is preferable?

ML PhD - Highly reputed but takes 5 years to finish. Not sure if the effort + wait is worth the reward.MLE - Might not be cutting edge work. Focus is still on the product, but get to do decent amount of engineering + research for the product. Title of the role is Research Engineer though!

Eventually, I'd love to work in places like DeepMind, AI for drug discovery, AI for Climate Change, etc.

Towards this, is it beneficial to do a 5 year PhD or start from MLE/RE and graduate my way there? Really confused as to how to make this decision.

Thanks!",hi try decide ml phd 5 10 cs school usa mle faang one apply team pure research team like fair google brain go industry eventual goal two options preferable ml phd highly repute take nmbr years finish sure effort wait worth reward mle might cut edge work focus still product get decent amount engineer research product title role research engineer though eventually love work place like deepmind ai drug discovery ai climate change etc towards beneficial nmbr year phd start mle graduate way really confuse make decision thank
hellohihello__,MachineLearning,1616586780.0,[D] [P] Evaluation Metrics for Pre-Trained Faster R-CNN,"Hi everyone

Recently I've been working on creating a social distancing detection model for which I used detectron2 for Faster R-CNN pre-trained weights.

I am using video datasets.

But the issue now is that I am unable to perform an evaluation for the model.  I don't know how to evaluate precision, recall, mAP etc since it is pre-trained.

Is there any way to perform the evaluation for a pre-trained faster R-CNN?

I do not have ground truth values for the datasets.",hi everyonerecently work create social distance detection model use detectron2 faster r cnn pre train weight use video datasets issue unable perform evaluation model know evaluate precision recall map etc since pre train way perform evaluation pre train faster r cnn grind truth value datasets
ProbablyCloseEnough,MachineLearning,1620535166.0,[R] Slurm Interface Investigation Report,"This is a followup on my previous threads [\[R\] Slurm interface survey (2 minutes)](https://www.reddit.com/r/MachineLearning/comments/lfn7d9/r_slurm_interface_survey_2_minutes/?utm_source=share&utm_medium=web2x&context=3) and [\[R\] Slurm Interface Prototype Evaluation Survey (2 minutes)](https://www.reddit.com/r/MachineLearning/comments/mf847y/r_slurm_interface_prototype_evaluation_survey_2/?utm_source=share&utm_medium=web2x&context=3). Your responses to these surveys helped me complete coursework in the human-computer interaction course I was taking. Thank you.

The first survey was an instance of **needfinding**, which is where I investigate the users of an interface with an aim to find their demographics, the things they are trying to do with the interface, the context of these tasks, and their impressions of the interface(s) they use to do a certain **target task**, which in this case is scheduling a computing job on shared computing resources using Slurm. The following is copied from my paper.

>There are a few takeaways that I can get from the survey results. Most respondents use a command line interface, presumably the one that Slurm provides, and they are not satisfied and not unsatisfied with it. Most respondents use Slurm fewer than 10 times per week, which is expected because we expect computing jobs to have high enough complexity to be worth scheduling with Slurm. Other insights might be compromised by bias, as explained below.  
>  
>I had intended to take certain steps to control for biases, but I had not anticipated the limited time and responses that I would have to gather responses. Therefore, some of the mechanisms that I had intended to use to reduce bias are not implemented. One such mechanism was providing the survey in Mandarin in addition to English. I only provided it in English because I did not have time to verify a Mandarin translation. Thus, English-literate users are overrepresented. Another mechanism was distributing the survey in different communities. Because I was limited to 25 responses, I had to stop gathering responses before I could send it to communities other than students taking CS 6750 at Georgia Tech and [r/machinelearning](https://www.reddit.com/r/machinelearning) on Reddit, which is frequented by machine learning researchers. Thus, academic users and researchers are overrepresented. I was able to implement mechanisms that were inherent to the survey design, such as not showing questions before participants start the survey and asking questions so that they do not encourage certain responses over others.

I executed one other needfinding activity, in which I observed 3 participants doing the target task, giving them the option to explain their actions. I also performed a heuristic evaluation of the Slurm interface for doing the target task. Based on these activities, I defined performance goals for proposed prototype interfaces.

>A new interface should maintain functionality for the most common tasks that the users do, which includes the target task, as well as determining available partitions, charge IDs, and hardware configurations that are available on the computing clusters. Criteria for evaluation is whether a user can perform those tasks. At the same time, it should be at least as accurate and efficient compared to the command line interface. Evaluation criteria are time and keystrokes or clicks. It should require the same or less time or the same or fewer keystrokes or clicks to perform the same tasks and doing them predictably and reliably.  
>  
>At the same time, it should be more learnable such that a new user should be able to learn to use it from within the interface, and perform at least the target task without script errors caused by forgetting key words or syntax. The criterion for evaluation is whether a new user can do so.  
>  
>In terms of accessibility, if appropriate files are in place, performing at least the target task must require only mouse clicks or finger taps, with a keyboard required only for first-time authentication. The criterion for evaluation is whether a user can do so.  
>  
>The interface must be compatible with client devices running Linux, and preferably also Windows and macOS. The interface must be compatible with Slurm servers running Linux and comply with their terms of use. The criterion for evaluation is whether it can be installed and used on Linux, connecting to a Slurm server running Linux.  
>  
>The development of the interface must cost no more than 20 hours of development time for a single researcher who has limited experience in graphical user interface development.

At the time, I was not sure whether I had to implement the interface. Upon learning that I did not, I decreased the importance of cost.

I then proceeded to brainstorm 20 prototype interfaces. I used my gut feeling to evaluate the anticipated performance of each idea in terms of functionality, accuracy, efficiency, learnability, accessibility, compatibility, compliance, and cost. Weights were 1, 1, 0.8, 1, 0.8, 1, 1, -0.2 respectively. (The weight of cost is negative because lower is better.) I found the top three ideas were

1. A simple executable with a form
2. A DAG for organizing jobs
3. A run configuration available in an IDE

The third one was presented in my second survey. [This figure](https://imgur.com/0m9onoh) was presented with the following text.

>Consider an interface that is an extension that can be installed on an integrated development environment (IDE), that enables scheduling jobs in the same way that a user would run them locally through a run configuration. The configuration settings panel (shown in the figure above) enables changing common settings and enforces a valid configuration. Upon clicking the Run button, the interface attempts to schedule the job.

[This figure](https://imgur.com/7AH5zCX) summarizes the responses I got from the survey. Because respondents need not perceive the intervals between response options to be constant, I can't do any statistical tests, but I can still eyeball the results to inform a subsequent round of needfinding.

>The main takeaways are that the prototype might not meet efficiency requirements. As expected, the greatest advantages the proposed interface has over the command line interface (which I determined users typically use) seem to be that it is more accurate, more learnable, more memorable, and that it fits better into users' workflows. I was surprised that many respondents expected to perform the target task somewhat slower. None expected to do it much more quickly, and 2 expected to do it much more slowly. An interpretation would be that many respondents are so experienced at using the command line interface that it would be difficult to outperform them.  
>  
>Changes that are suggested from the feedback are that the next iteration of this prototype, if it will be subject to further development, should try to increase efficiency. However, the demographic may find that the sacrifice in efficiency is worth the advantages in other areas. I did not collect information about the relative importance of these aspects.

At this point, the course assignments changed such that I had to investigate a different interface. I've become very busy with work, and since I won't be paid by anyone for continuing this investigation, and because my grade is no longer being held hostage, I'm going to just leave this as is.",followup previous thread r slurm interface survey 2 minutes url r slurm interface prototype evaluation survey 2 minutes url responses survey help complete coursework human computer interaction course take thank first survey instance needfinding investigate users interface aim find demographics things try interface context task impressions interface use certain target task case schedule compute job share compute resources use slurm follow copy paper takeaways get survey result respondents use command line interface presumably one slurm provide satisfy unsatisfied respondents use slurm fewer nmbr time per week expect expect compute job high enough complexity worth schedule slurm insights might compromise bias explain intend take certain step control bias anticipate limit time responses would gather responses therefore mechanisms intend use reduce bias implement one mechanism provide survey mandarin addition english provide english time verify mandarin translation thus english literate users overrepresented another mechanism distribute survey different communities limit nmbr responses stop gather responses could send communities students take cs nmbr georgia tech r machinelearning url reddit frequent machine learn researchers thus academic users researchers overrepresented able implement mechanisms inherent survey design show question participants start survey ask question encourage certain responses others execute one needfinding activity observe nmbr participants target task give option explain action also perform heuristic evaluation slurm interface target task base activities define performance goals propose prototype interfaces new interface maintain functionality common task users include target task well determine available partition charge ids hardware configurations available compute cluster criteria evaluation whether user perform task time least accurate efficient compare command line interface evaluation criteria time keystrokes click require less time fewer keystrokes click perform task predictably reliably time learnable new user able learn use within interface perform least target task without script errors cause forget key word syntax criterion evaluation whether new user term accessibility appropriate file place perform least target task must require mouse click finger tap keyboard require first time authentication criterion evaluation whether user interface must compatible client devices run linux preferably also windows macos interface must compatible slurm servers run linux comply term use criterion evaluation whether instal use linux connect slurm server run linux development interface must cost nmbr hours development time single researcher limit experience graphical user interface development time sure whether implement interface upon learn decrease importance cost proceed brainstorm nmbr prototype interfaces use gut feel evaluate anticipate performance idea term functionality accuracy efficiency learnability accessibility compatibility compliance cost weight 1 1 0 8 1 0 8 1 1 0 2 respectively weight cost negative lower better find top three ideas were1 simple executable form2 dag organize jobs3 run configuration available idethe third one present second survey figure url present follow text consider interface extension instal integrate development environment ide enable schedule job way user would run locally run configuration configuration settings panel show figure enable change common settings enforce valid configuration upon click run button interface attempt schedule job figure url summarize responses get survey respondents need perceive intervals response options constant statistical test still eyeball result inform subsequent round needfinding main takeaways prototype might meet efficiency requirements expect greatest advantage propose interface command line interface determine users typically use seem accurate learnable memorable fit better users workflows surprise many respondents expect perform target task somewhat slower none expect much quickly nmbr expect much slowly interpretation would many respondents experience use command line interface would difficult outperform change suggest feedback next iteration prototype subject development try increase efficiency however demographic may find sacrifice efficiency worth advantage areas collect information relative importance aspects point course assignments change investigate different interface become busy work since win pay anyone continue investigation grade longer hold hostage go leave
sarmientoj24,MachineLearning,1619179833.0,"[P] Is it possible to create a benchmark OSes performance in terms of ML training, prediction?","I am thinking of a possible topic for Advanced OS (for a small research paper):  


My goal is to try and check the performance evaluation of different Operating Systems such as Ubuntu, Debian, Fedora, Mint, Windows, CentOS, etc. in terms of training and prediction from simple ML models to CNN and RNNs.

Is it possible to do some benchmarks **focusing on Operating Systems** for this? If so, what kind of tools can I use to do some benchmarks? For example, check CPU usage, RAM usage, etc. 

I am planning to do the following:  
\- Create a Docker container of the specified OS  
\- Start tool benchmark/measurement   
\- Start Training #1  
\- Start Prediction #1  
\- Start Training #2  
...  
and so on...  


My questions would be:  
\- is it possible? I haven't seen papers tackling OS effect on Machine Learning training  
\- Is there a measurement tool?  
\- Can I do these given the outline of my preferred methodology",think possible topic advance os small research paper goal try check performance evaluation different operate systems ubuntu debian fedora mint windows centos etc term train prediction simple ml model cnn rnns possible benchmarks focus operate systems kind tool use benchmarks example check cpu usage ram usage etc plan follow create docker container specify os start tool benchmark measurement start train 1 start prediction 1 start train 2 question would possible see paper tackle os effect machine learn train measurement tool give outline prefer methodology
meldiwin,MachineLearning,1617817881.0,"[N] Dieter Fox "" The Next Generation Of Robotics"" New Episode","Hello Guys,

\*\* please feel free to remove if it is not relevant\*\*

We   (IEEE Soft Robotics Podcast) recently Interviewed Prof.Dieter Fox, and we would like  to have your feedback about the episode, any comments would be helpful for future guests :)

You can find the episode here

Audio: [https://soundcloud.com/ieeeras-softrobotics/dieter-fox-the-next-generation-of-robotics](https://soundcloud.com/ieeeras-softrobotics/dieter-fox-the-next-generation-of-robotics)

Video: [https://youtu.be/ssTFlcoAdsc](https://youtu.be/ssTFlcoAdsc)",hello guy please feel free remove relevant ieee soft robotics podcast recently interview prof dieter fox would like feedback episode comment would helpful future guests find episode hereaudio url url
ilovemouchou,MachineLearning,1619365371.0,[D] Need help figuring out job offers,"I just got an offer from Amazon for a Research Scientist position in one of the AWS customer facing teams. I really like the ideas of working as a ML consultant on super diverse projects but I'm a bit scared of the potential pressure and hard deadlines that might come with it. On the other hand, I got an offer from Google for a Data Scientist position in one of their Trust and Safety team. In terms of work life balance and overall employee well being Google is an amazing company (I interned there and loved it) but I'm afraid being a Data Scientist in Trust and Safety could be more of a niche. I was wondering how easy it would be to move to another team within Google after a couple of years.

I don't care too much about the comp, I come from academia so any tech job offer feels like I've won the lottery compared to my postdoc salary :) but more about growth and opportunities within these companies, or how these positions would be perceived by recruiters in the future. I also care about flexibility with remote work : my family is in Europe and my partner in the US so it would be great to be able to spend extended periods of time on both continents.

Another thing : if I decline the Google offer to go to Amazon, will it be harder for me to get into Google later?",get offer amazon research scientist position one aws customer face team really like ideas work ml consultant super diverse project bite scar potential pressure hard deadlines might come hand get offer google data scientist position one trust safety team term work life balance overall employee well google amaze company intern love afraid data scientist trust safety could niche wonder easy would move another team within google couple years care much comp come academia tech job offer feel like win lottery compare postdoc salary growth opportunities within company position would perceive recruiters future also care flexibility remote work family europe partner us would great able spend extend periods time continents another thing decline google offer go amazon harder get google later
MohamedRashad,MachineLearning,1620348992.0,[D] Is there an idea similar to SPP but for Convolutions ?,"I need a way to keep the output of the last convolution layer fixed (32x24 for example) whatever the input size is.

Variable Input Size -> Fixed feature map size, Do anyone have an idea on how to do something like this?",need way keep output last convolution layer fix 32x24 example whatever input size variable input size fix feature map size anyone idea something like
svantana,MachineLearning,1620286725.0,"[D] Is the concept of an 'epoch' being phased out, or even harmful?","I've noticed a trend that more papers are reporting the number of training ""steps"" rather than epochs.It kinda makes sense since datasets now vary in size from the dozens to the billions. This got me thinking: is the concept of an epoch potentially harmful, as it enforces the idea of the data as something finite? Ideally, minibatch training approximates IID sampling from an infinite dataset (the true distribution). What is the argument against IID sampling of the training data?

I recently saw someone who was scatterplotting batch losses during training. This is more informative than the standard ""loss per epoch"" plot. It could be combined with a moving average of the latest K batches, which is \~gaussian, so confidence intervals are easily added. And with IID sampling there won't be any leftover batches with odd sizes.

EDIT: The reason this came up for me is because I have a model trained on two datasets with different losses. I was struggling with how to define an epoch when it dawned on me, why even use epochs at all? Just sample the data for each batch. Works fine. And one less nested loop is good for your health :)",notice trend paper report number train step rather epochs kinda make sense since datasets vary size dozens billions get think concept epoch potentially harmful enforce idea data something finite ideally minibatch train approximate iid sample infinite dataset true distribution argument iid sample train data recently saw someone scatterplotting batch losses train informative standard loss per epoch plot could combine move average latest k batch gaussian confidence intervals easily add iid sample win leftover batch odd size edit reason come model train two datasets different losses struggle define epoch dawn even use epochs sample data batch work fine one less nest loop good health
fasttosmile,MachineLearning,1618593591.0,[R] Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little,"
https://arxiv.org/abs/2104.06644",url
adcamuto,MachineLearning,1620562568.0,[R] Explicit Regularisation in Gaussian Noise Injections,"This work studies the regularisation induced in neural networks by Gaussian noise injections (GNIs). Though such injections have been extensively studied when applied to data, there have been few studies on understanding the regularising effect they induce when applied to network activations.

Key findings:

\-  The work derives the explicit regulariser of such injections, which is a positive term added to the loss function obtained marginalising out the injected noise. 

\- This regulariser penalises networks that learn functions with high-frequency content in the Fourier domain and most heavily regularises neural network layers that are closer to the output (see Figure below for an illustration of this). 

arxiv: [https://arxiv.org/abs/2007.07368](https://arxiv.org/abs/2007.07368)

code: [https://github.com/alexander-camuto/exp\_reg\_GNIs](https://github.com/alexander-camuto/exp_reg_GNIs)

&#x200B;

[Figure:   We illustrate the effect of GNIs injected throughout a network’s activations.  Each coloured dot represents a neuron’s activations. We add GNIs, represented as circles, to each layer’s activations bar the output layer. GNIs induce a network for which each layer learns a progressively lower frequency function, represented as a sinusoid matching in colour to its corresponding layer](https://preview.redd.it/yk00zwag63y61.png?width=1308&format=png&auto=webp&s=f66628ab9cc84ca7d801c0d34781a898eba2993a)",work study regularisation induce neural network gaussian noise injections gnis though injections extensively study apply data study understand regularise effect induce apply network activations key find work derive explicit regulariser injections positive term add loss function obtain marginalise inject noise regulariser penalise network learn function high frequency content fourier domain heavily regularise neural network layer closer output see figure illustration arxiv url url illustrate effect gnis inject throughout network activations colour dot represent neurons activations add gnis represent circle layer activations bar output layer gnis induce network layer learn progressively lower frequency function represent sinusoid match colour correspond layer url
SQL_beginner,MachineLearning,1617811685.0,[D] Feature Selection for Large Datasets,"To begin my question, I would like to quote a paper (by Ishawaran et al) on ""random forests for survival analysis data"", in which the authors (very concisely) outline the difficulties of feature selection (i.e. which variables to include in a statistical model) in classical regression models and how this problem is somewhat alleviated with more advanced models :

""Further, because these methods (i.e. classical regression models, e.g. cox ph regression - even though it's semi-parametric) are often parametric, nonlinear effects of variables must be modeled by transformations or expanding the design matrix to include specialized basis functions. Often ad hoc approaches, such as stepwise regression, are used to determine if nonlinear effects exist. Identifying interactions, especially those involving multiple variables, is also problematic. This must be done by brute force (examining all two-way and threeway interactions, e.g.), or must rely on subjective knowledge to narrow the search.

In contrast, these difficulties are handled automatically using forests. We illustrate the ease with which RSF can uncover complex data structures through an in-depth case study of the prognostic implications of being underweight, overweight, or obese and having severe, but stable coronary artery disease.

Investigators have noted complex patterns surrounding possible reverse causation in underweight individuals, interactions with smoking, and an unclear inflection point at which point increasing body mass confers increased risk Some have identified a possible obesity paradox among patients with established heart disease in which increased body mass predicts better survival. To clarify these issues, we analyzed a large cohort of patients with coronary artery disease undergoing isolated coronary artery bypass surgery. Using RSF, (random survival forest) we identified a complex relationship between long-term survival, body mass, renal (kidney) function, smoking, and number of internal coronary artery bypass grafts. We believe our novel findings help explain some of the apparent contradictions previously reported.""

Source: https://arxiv.org/pdf/0811.1645.pdf

Essentially, the authors claim that traditional regression models struggle with feature selection and the newer models (e.g. bagging, random forest) are able to better deal with feature selection. I do remember from an intro stats class, the somewhat tedious process of determining which variables to include in a multiple linear regression model. As the authors described, I remember there was something called ""CP Mallow's Criteria"" in which potential variables were repeatedly included and excluded in the regression model and the value of CP Mallow's Criteria was monitored - a final selection of variables for the model was decided on the basis of this criteria. However, this selection process becomes inefficient for large datasets (if I understand correctly, this means you would have to refit the model for many different combinations of variables, resulting in a ""combinatorics explosion"" for a large number of variables). Like the authors mention, you can also ""manually hard code"" interaction terms in the model (e.g. log(var1), var1var2, var1/(var2var3), var1/(var2+var3), etc.) - and there an infinite such number of potential interactions. Improper feature selection can also result in unwanted effects such as multicollinearity. The last point I would like to bring up - although my knowledge of mathematics is not strong enough to fully substantiate it - is that classical regression models are said to have a tendency to overfit (I don't know why - I have seen visual demonstrations of this, but I don't know if there is a mathematical explanation behind this, or if it's just an empirical observation) and poorly generalize to new data (again, I don't know why); and that classical regression models are only able to ""recognize linearly separable patterns in the data"" (intuitively I can understand this, e.g. draw a circle of red points and a smaller circle of blue points that fits in the red circle, a single line can not separate the two colors - but I don't know if there is a mathematical explanation behind this).

This brings me to my question about feature selection for large datasets. With the advent of technology, data is becoming bigger and bigger everyday - convolution neural networks are the ""go to method"" for analyzing pictures (a standard black and white picture is said to have 786 variables), whereas DNA is said to have even more. In such instances, it surely must be impossible to address feature selection as done in conventional statistical modelling. Please excuse my poor understanding of math - but my understanding is that newer statistical models have ""built in"" methods of handling the feature selection problem. For instance, random forest ""randomly"" chooses different combinations of variables and sees which combinations result in better model performance, the exact randomizing mechanism (uncorrelated trees) is said to also prevent against multicollinearity (I ahve heard that the creator of the random forest algorithm Leo Breiman claims through theoretical statistics that random forest by definition can not ""over fit"" and has some desirable error bounds and convergence properties - is this true?). Meanwhile, I have read on data science blogs (I'm not going to lie) that deep neural networks are able to ""automatically"" learn and consider ""useful"" combinations of features for approximating the target function (am I correct?).

All in all, what I want to ask here : for large datasets, where sometimes the features don't have any immediate meanings (e.g. a patient's blood pressure vs the information contained in the 231st pixel of a photograph) - is there any ""real"" way to handle feature selection? Or is this usually taken care of by the statistical model itself (e.g. random forest and neural networks)? I have seen examples online where people attempted to write a massive FOR LOOP in which they train the same model with thousands of variable combinations ... but I am not sure how feasible this is.

Can someone please provide a comment on this?

Thanks",begin question would like quote paper ishawaran et al random forest survival analysis data author concisely outline difficulties feature selection e variables include statistical model classical regression model problem somewhat alleviate advance model methods e classical regression model e g cox ph regression even though semi parametric often parametric nonlinear effect variables must model transformations expand design matrix include specialize basis function often ad hoc approach stepwise regression use determine nonlinear effect exist identify interactions especially involve multiple variables also problematic must brute force examine two way threeway interactions e g must rely subjective knowledge narrow search contrast difficulties handle automatically use forest illustrate ease rsf uncover complex data structure depth case study prognostic implications underweight overweight obese severe stable coronary artery disease investigators note complex pattern surround possible reverse causation underweight individuals interactions smoke unclear inflection point point increase body mass confer increase risk identify possible obesity paradox among patients establish heart disease increase body mass predict better survival clarify issue analyze large cohort patients coronary artery disease undergo isolate coronary artery bypass surgery use rsf random survival forest identify complex relationship long term survival body mass renal kidney function smoke number internal coronary artery bypass graft believe novel find help explain apparent contradictions previously report source url author claim traditional regression model struggle feature selection newer model e g bag random forest able better deal feature selection remember intro stats class somewhat tedious process determine variables include multiple linear regression model author describe remember something call cp mallow criteria potential variables repeatedly include exclude regression model value cp mallow criteria monitor final selection variables model decide basis criteria however selection process become inefficient large datasets understand correctly mean would refit model many different combinations variables result combinatorics explosion large number variables like author mention also manually hard code interaction term model e g log var1 var1var2 var1 var2var3 var1 var2 var3 etc infinite number potential interactions improper feature selection also result unwanted effect multicollinearity last point would like bring although knowledge mathematics strong enough fully substantiate classical regression model say tendency overfit know see visual demonstrations know mathematical explanation behind empirical observation poorly generalize new data know classical regression model able recognize linearly separable pattern data intuitively understand e g draw circle red point smaller circle blue point fit red circle single line separate two color know mathematical explanation behind bring question feature selection large datasets advent technology data become bigger bigger everyday convolution neural network go method analyze picture standard black white picture say nmbr variables whereas dna say even instance surely must impossible address feature selection conventional statistical model please excuse poor understand math understand newer statistical model build methods handle feature selection problem instance random forest randomly choose different combinations variables see combinations result better model performance exact randomize mechanism uncorrelated tree say also prevent multicollinearity ahve hear creator random forest algorithm leo breiman claim theoretical statistics random forest definition fit desirable error bound convergence properties true meanwhile read data science blog go lie deep neural network able automatically learn consider useful combinations feature approximate target function correct want ask large datasets sometimes feature immediate mean e g patient blood pressure vs information contain 231st pixel photograph real way handle feature selection usually take care statistical model e g random forest neural network see examples online people attempt write massive loop train model thousands variable combinations sure feasible someone please provide comment thank
mikegartrell,MachineLearning,1619447583.0,[N] Deadline extended: Call for papers: KDD 2021 Workshop on Bayesian Causal Inference for Real-World Interactive Systems,"[https://bcirwis2021.github.io](https://bcirwis2021.github.io/)

August 14 - 18, 2021 (final workshop date TBD)

&#x200B;

\---

Submission deadline (**extended**): May 20, 2021, anywhere on Earth

Format: 3 page extended abstract + references + appendices, ACM Proceeding Template

Submission website: [https://cmt3.research.microsoft.com/BCIRWIS2021](https://cmt3.research.microsoft.com/BCIRWIS2021)

\---

&#x200B;

Increasingly we use machine learning to build interactive systems that learn from past actions and the reward obtained. Theory suggests several possible approaches, such as contextual bandits, reinforcement learning, the do-calculus, or plain old Bayesian decision theory. What are the most theoretically appropriate and practical approaches to doing causal inference for interactive systems?

We are particularly interested in case studies of applying machine learning methods to interactive systems that *did* or *did* *not* use Bayesian or *likelihood* based methods, with a discussion about why this choice was made in terms of practical or theoretical arguments. We also welcome submissions in the following areas:

* Offline evaluation of recommender and interactive systems.
* Comparison of Bayesian, off-policy and other heuristic approaches for offline metrics.
* Probabilistic approaches applied to contextual bandits and reinforcement learning approaches.
* Probabilistic approaches to incrementality and attribution.
* Non-Bayesian approaches and trade-offs with Bayesian/Likelihood approaches.
* Bayesian methods in a production environment.

&#x200B;

Organizers

* Nicholas Chopin (ENSAE)
* Mike Gartrell (Criteo AI Lab)
* Dawen Liang (Netflix)
* Alberto Lumbreras (Criteo AI Lab)
* David Rohde (Criteo AI Lab)
* Yixin Wang (UC Berkeley)",url nmbr 18 nmbr final workshop date tbd x200b submission deadline extend may 20 2021 anywhere earthformat nmbr page extend abstract reference appendices acm proceed templatesubmission website url use machine learn build interactive systems learn past action reward obtain theory suggest several possible approach contextual bandits reinforcement learn calculus plain old bayesian decision theory theoretically appropriate practical approach causal inference interactive systems particularly interest case study apply machine learn methods interactive systems use bayesian likelihood base methods discussion choice make term practical theoretical arguments also welcome submissions follow areas offline evaluation recommender interactive systems comparison bayesian policy heuristic approach offline metrics probabilistic approach apply contextual bandits reinforcement learn approach probabilistic approach incrementality attribution non bayesian approach trade bayesian likelihood approach bayesian methods production environment x200b organizers nicholas chopin ensae mike gartrell criteo ai lab dawen liang netflix alberto lumbreras criteo ai lab david rohde criteo ai lab yixin wang uc berkeley
xdtolm,MachineLearning,1618772636.0,[P] VkFFT now supports OpenCL,"Hello, I am the creator of the VkFFT - GPU Fast Fourier Transform library. In the latest update, I have added OpenCL as a backend option (in addition to Vulkan, CUDA and HIP) so if some of you are interested in OpenCL FFT - feel free to check it out and ask any questions! The performance of it is on the same level as other backends.

GitHub link: [https://github.com/DTolm/VkFFT](https://github.com/DTolm/VkFFT)",hello creator vkfft gpu fast fourier transform library latest update add opencl backend option addition vulkan cuda hip interest opencl fft feel free check ask question performance level backends github link url
zy415,MachineLearning,1616513517.0,[D] IJCAI 2021 Paper Reviews,IJCAI 2021 paper reviews are supposed to be released soon (tomorrow). Creating a discussion thread for this year's reviews.,ijcai nmbr paper review suppose release soon tomorrow create discussion thread year review
gokuresearch,MachineLearning,1617167679.0,[D] Is it worth buying a GPU workstation as a graduate student?,"I'm graduate student pursuing masters with thesis. I'm also hoping to pursue PhD in the future. Right now, I've a laptop with GTX 1070 and it helped me a lot for learning various algorithms in ML/DL, which also indirectly helped me publish few research papers in well known conferences during my undergraduate.

But with the growing computation requirements for DL, I was wondering that is it worth investing in a proper workstation having 2x RTX 3080 or other similar high configs for a graduate student (who is already a part of the well funded research lab and have access to GPU clusters for lab's research work)??

I love to do personal experiments and exploring latest research whenever possible. And because of the current trend, it requires more and more computations to just explore most of the things. Apart from that, global chip shortage is also a concern as it's leading to increase in GPU prices overall.

Therefore, I would like to receive any kind of suggestions and thoughts.



Note to admin: Please let me know if this post is out of scope here.",graduate student pursue master thesis also hop pursue phd future right laptop gtx nmbr help lot learn various algorithms ml dl also indirectly help publish research paper well know conferences undergraduate grow computation requirements dl wonder worth invest proper workstation 2x rtx nmbr similar high configs graduate student already part well fund research lab access gpu cluster lab research work love personal experiment explore latest research whenever possible current trend require computations explore things apart global chip shortage also concern lead increase gpu price overall therefore would like receive kind suggestions thoughts note admin please let know post scope
thisisdhruvagarwal,MachineLearning,1619340081.0,Bullet Physics vs Pandas3D [D],"Since you know that Mujoco is not free.. Hence, I was looking for some alternatives and got Bullet Physics and Pandas3d... But I am not sure which one to use... Which do you think is more easy to learn and which one would you prefer?",since know mujoco free hence look alternatives get bullet physics pandas3d sure one use think easy learn one would prefer
techsucker,MachineLearning,1616957686.0,[R] Researchers at Lawrence Livermore National Laboratory (LLNL) Developed a Novel Deep Learning Framework for Symbolic Regression,"At the Lawrence Livermore National Laboratory (LLNL), scientists have developed a novel framework and an accompanying visualization tool that utilizes deep reinforcement learning for symbolic regression problems, outperforming baseline methods on benchmark problems.

Their paper was recently accepted as an oral presentation at the International Conference on Learning Representations (ICLR 2021). In their paper, the researchers describe applying deep reinforcement learning to discrete optimization. Discrete optimization focuses on problems that deal with discrete “building blocks” that must be combined in a particular order or configuration to optimize the desired property. They focused on a type of discrete optimization called symbolic regression. Symbolic regression finds short mathematical expressions that fit data gathered from an experiment. It aims to discover the underlying equations or dynamics of a physical process.

Summary: [https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/](https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/) 

Paper: [https://openreview.net/forum?id=m5Qsh0kBQG](https://openreview.net/forum?id=m5Qsh0kBQG)",lawrence livermore national laboratory llnl scientists develop novel framework accompany visualization tool utilize deep reinforcement learn symbolic regression problems outperform baseline methods benchmark problems paper recently accept oral presentation international conference learn representations iclr 2021 paper researchers describe apply deep reinforcement learn discrete optimization discrete optimization focus problems deal discrete build block must combine particular order configuration optimize desire property focus type discrete optimization call symbolic regression symbolic regression find short mathematical expressions fit data gather experiment aim discover underlie equations dynamics physical process summary url paper url
user692646,MachineLearning,1620387919.0,[D] Element-wise multiplication instead of Convolution,"I'm considering using element-wise multiplication as the fundamental operation in a net, and I wanted to know if this has been done before in the literature.

The operation for a single layer would work as follows:

1. Given an M x N input image, multiply it element-wise by an M x N weight matrix.
2. Add an M x N matrix of biases to the output of step 1.
3. Compute a non-linearity (e.g. ReLU) element-wise on the output of step 2.
4. Use a K x K average pooling kernel with stride K to down-sample the output of step 3.

The reason I am considering using this operation instead of a conv layer is because I think the conv layer is a bit too regularized for my problem. By that, I mean if the convolution operation were treated as a matrix-vector multiplication, such that the input to the conv layer is an image re-shaped to be a column vector, then the matrix would be too sparse.

If needed, I could enforce sparsity later with L1 regularization, for example.

Has this sort of operation been considered before in the literature?",consider use element wise multiplication fundamental operation net want know literature operation single layer would work follow 1 give x n input image multiply element wise x n weight matrix 2 add x n matrix bias output step 1 3 compute non linearity e g relu element wise output step 2 4 use k x k average pool kernel stride k sample output step 3 reason consider use operation instead conv layer think conv layer bite regularize problem mean convolution operation treat matrix vector multiplication input conv layer image shape column vector matrix would sparse need could enforce sparsity later l1 regularization example sort operation consider literature
regularized,MachineLearning,1620406026.0,[D] Have you heard of MBZUAI (Mohamed bin Zayed University of Artificial Intelligence)?,"When I look at their faculty, I see very prominent ML researchers such as Eric Xing and Le Song.

[https://mbzuai.ac.ae/study#faculty-sec](https://mbzuai.ac.ae/study#faculty-sec)

Have you heard of this university? What do you think? 

Are these professors really based in Abu Dhabi? Maybe someone who is from CMU (Eric Xing's former university) and from GaTech (Le Song's former university) can comment.",look faculty see prominent ml researchers eric xing le song url hear university think professors really base abu dhabi maybe someone cmu eric xing former university gatech le song former university comment
davex32,MachineLearning,1620599656.0,[P] Latest TensorFlow 2.5.0rc3 optimized wheels with CUDA 11 and Python3.9,"I  built some wheels for the new Tensorflow 2.5.0rc3 with CUDA 11 and cuDNN 8  in case anyone finds them useful. This includes SSE4.X,AVX2,FMA  instructions: I usually build these for skylake march  (and in this case with glibc 2.33)  or other  architectures on request, depending on my availability.

Why is this useful? For when you install the official binaries and see a warning like this:  
`Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2`

[https://github.com/davidenunes/tensorflow-wheels](https://github.com/davidenunes/tensorflow-wheels)

in case anyone finding these useful, contribute to my coffee addiction 🤣☕ and support these builds and related projects here: [https://github.com/sponsors/davidenunes](https://github.com/sponsors/davidenunes) or [https://ko-fi.com/davidenunes](https://ko-fi.com/davidenunes)

or just say hi [@davidelnunes](https://twitter.com/davidelnunes) on Twitter.",build wheel new tensorflow 2 5 0rc3 cuda nmbr cudnn nmbr case anyone find useful include sse4 x avx2 fma instructions usually build skylake march case glibc 2 33 architectures request depend availability useful install official binaries see warn like cpu support instructions tensorflow binary compile use avx avx2 url case anyone find useful contribute coffee addiction support build relate project url url say hi davidelnunes url twitter
post_hazanko,MachineLearning,1618592038.0,[D] Filling in missing data for bad video on client end,"I think that would be an interesting application, I've started to see it where you can make a blurry image become super clear.

So I'm wondering if there would be a thing at some point where they integrate a ""model"" on the client side think Web/JS for video transmission... usually what you see is the local/source is crisp and the recipient is a little worse, makes sense.

Concern is how accurate the fill is.",think would interest application start see make blurry image become super clear wonder would thing point integrate model client side think web js video transmission usually see local source crisp recipient little worse make sense concern accurate fill
thunder_jaxx,MachineLearning,1617344522.0,"[D] For Anyone Who Has Clocked More Than 50+ Days Of DL Model Training Time, Do You Use Anything Other Than Adam or AdamW?","For almost All ML projects which had DL, I used AdamW and it just worked. So fucking well. So a few questions to fellow Redditors who might be training models frequently :

Do you use a different optimizer? Why?  

Do you tune the Beta values? 

Have you consciously ever chosen not to use Adam? Why? 

I have seen some recent fancy optimizers like [PCGrad](https://arxiv.org/pdf/2001.06782.pdfhttps://arxiv.org/pdf/2001.06782.pdf) but never found the need to use it. When did you use them if you had to?",almost ml project dl use adamw work fuck well question fellow redditors might train model frequently use different optimizer tune beta value consciously ever choose use adam see recent fancy optimizers like pcgrad url never find need use use
brandonrussell757,MachineLearning,1619633167.0,[D] TP/FP Object Detection Question," Hey everyone, so I am in the middle of implementing the mAP metric from scratch so I can get details on all the data associated with the calculation of the mAP itself (ie. AP, Precision, Recall, TP, FP, FN).

I understand the logic behind calculating TP/FP in such that:

* TP = class detections with IOU >= IOU\_THRESH
* FP = class detections with IOU < IOU\_THRESH

My question is how would you calculate a situation where a **SINGLE** ***predicted*** bounding box overlaps **TWO** or **MORE** ***ground truth*** bounding boxes with an **IOU** that meets the **IOU\_THRESH,** for each of the ground truth boxes? My initial thought would be you would just rule the one with the highest **IOU** as the TP and all others as FP. Am I right with this assumption?",hey everyone middle implement map metric scratch get detail data associate calculation map ie ap precision recall tp fp fn understand logic behind calculate tp fp tp class detections iou iou _thresh fp class detections iou iou _threshmy question would calculate situation single predict bound box overlap two grind truth bound box iou meet iou _thresh grind truth box initial think would would rule one highest iou tp others fp right assumption
SubstantialRange,MachineLearning,1616789302.0,[D] What are the CASP competition equivalents for scientific fields other than protein folding?,"The Critical Assessment of protein Structure Prediction is a bi-annual contest that measures the progress of computational methods in the domain of protein folding. It was famously won last year by DeepMind's Alphafold.

What are some similar benchmarks in other scientific fields that most people may not know about?",critical assessment protein structure prediction bi annual contest measure progress computational methods domain protein fold famously win last year deepmind alphafold similar benchmarks scientific field people may know
Firehead1971,MachineLearning,1617264663.0,[D] Collecting ideas and hot topics for possible PhD thesis,"Hi,  I am an Argentinian ML researcher who is at the beginning of his  dissertation (i.e. choosing an suitable and interesting topic). I think  it would not be bad if we could collect the hot topics here and thus  have a simple overview of them. For example, what are the edging topics  where the world is right now? Which topics are still in the shadows and  might come up soon? So I will just start and write what comes to my  mind:

\- Transformer Models (still a hot topic)

\- Neural Network Optimization Techniques (more on a mathematical and comparison basis)

\- ML Integration in different areas of life (health care, road traffic,...)

\- MLops for enterprises or production environments (how does it look in reality?)

\- Make ML more understandable (can you use ml to teach ml?)

\- Computer Vision (certainly there are lot of possibilities there but also already lot of things have been done here)

\-  Doing some fancy audio ML classification stuff to recognize approaching  killer insects/animals (cross field between ml and biology)

\- You might continue this list with your ideas...",hi argentinian ml researcher begin dissertation e choose suitable interest topic think would bad could collect hot topics thus simple overview example edge topics world right topics still shadow might come soon start write come mind transformer model still hot topic neural network optimization techniques mathematical comparison basis ml integration different areas life health care road traffic mlops enterprises production environments look reality make ml understandable use ml teach ml computer vision certainly lot possibilities also already lot things fancy audio ml classification stuff recognize approach killer insects animals cross field ml biology might continue list ideas
Puzzleheaded-Drop297,MachineLearning,1617041579.0,"[D] EMNIST dataset down, network unreachable","I'm using torchvision to download the EMNIST dataset. But it is down at this moment. Can someone send me a copy? Thanks!

[http://www.itl.nist.gov/iaui/vip/cs\_links/EMNIST/gzip.zip](http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip)

Perhaps we need better ways to host datasets like this.

\------------

The dataset just went back online.",use torchvision download emnist dataset moment someone send copy thank url need better ways host datasets like dataset go back online
dsmlthrowaway,MachineLearning,1617121603.0,[D] Looking for advice: hiring data practitioners,"I work for a large company focused on supply chain. Medium cost of living city. We are starting to incorporate machine learning techniques to improve a lot of our processes. We've had good success and the goal of ""cushioning the blow of investment"" has been met.

So now we're looking to invest in scaling out this team (hiring maybe 3-5 additional people). The rub of the matter is: I'm not really sure how to attract the right talent for the job. We do not need cutting edge stuff. For most business problems, the value is immense if we can get something ""good enough"" out of the door. Pragmatic, but in between 'hacker' and 'enterprise.'

I come from a comp sci/programming background, moved into data, and now am basically using the tools without much in depth knowledge, but have the business domain knowledge to know where to successfully apply them. Ideally, I would love to hire ""problem solvers"" who care more about the business and generating value than how they accomplish it. I think these types of positions will start popping up a lot more as the barrier to entry decreases, but I'm having the damndest time on attracting the right people. It's not really a data science role, or a data engineering role, or a ML role. It's kind of a 'jack of all trades."" A data practitioner is what I call it, but I don't see any similar postings on indeed (if anyone knows of any I could cheat off of, that'd be great).

So now the questions:

1. Do you guys have any advice on what job title we should be posting?
2. What would you expect a salary range to be like for this type of position? Midlevel data folks vs mid level programming seems to pay less, but I don't want to drive away anyone.
3. What types of phrases can I use to make it clear that this isn't necessarily a ""data science"" role or a ""ml"" role. It's a problem solving role around data. I'd hate for anyone to be disappointed in the work.",work large company focus supply chain medium cost live city start incorporate machine learn techniques improve lot process good success goal cushion blow investment meet look invest scale team hire maybe 3 5 additional people rub matter really sure attract right talent job need cut edge stuff business problems value immense get something good enough door pragmatic hacker enterprise come comp sci program background move data basically use tool without much depth knowledge business domain knowledge know successfully apply ideally would love hire problem solvers care business generate value accomplish think type position start pop lot barrier entry decrease damndest time attract right people really data science role data engineer role ml role kind jack trade data practitioner call see similar post indeed anyone know could cheat great question 1 guy advice job title post 2 would expect salary range like type position midlevel data folks vs mid level program seem pay less want drive away anyone 3 type phrase use make clear necessarily data science role ml role problem solve role around data hate anyone disappoint work
Yuqing7,MachineLearning,1618591151.0,[N] ETH Zurich Leverages Spiking Neural Networks To Build Ultra-Low-Power Neuromorphic Processors,"A research team from ETH Zurich leverages existing spike-based learning circuits to propose a biologically plausible architecture that is highly successful in classifying distinct and complex spatio-temporal spike patterns. The work contributes to the design of ultra-low-power mixed-signal neuromorphic processing systems capable of distinguishing spatio-temporal patterns in spiking activity.

Here is a quick read: [An Error-Propagation Spiking Neural Network Compatible With Neuromorphic Processors](https://syncedreview.com/2021/04/16/eth-zurich-leverages-spiking-neural-networks-to-build-ultra-low-power-neuromorphic-processors/).

The paper *An Error-Propagation Spiking Neural Network Compatible With Neuromorphic Processors* is on [arXiv](https://arxiv.org/pdf/2104.05241.pdf).",research team eth zurich leverage exist spike base learn circuit propose biologically plausible architecture highly successful classify distinct complex spatio temporal spike pattern work contribute design ultra low power mix signal neuromorphic process systems capable distinguish spatio temporal pattern spike activity quick read error propagation spike neural network compatible neuromorphic processors url paper error propagation spike neural network compatible neuromorphic processors arxiv url
ArulVendhan,MachineLearning,1617187489.0,[P] Hawking Date Time Parser is Open-Source Now,"It's a great pleasure to announce our Natural Language Date Time Parser using Stanford CoreNLP in the backend is open-source now. Do Check and Let us Know the Feedback.  
Github: [https://github.com/zoho/hawking](https://github.com/zoho/hawking)

Blog : [https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html](https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html)  
Tweet from [Stanford University](https://twitter.com/stanfordnlp) : [https://twitter.com/stanfordnlp/status/1376914683127492614?s=20](https://twitter.com/stanfordnlp/status/1376914683127492614?s=20)

\#nlp #stanfordnlp #datetimeparser",great pleasure announce natural language date time parser use stanford corenlp backend open source check let us know feedback github url url tweet stanford university url url stanfordnlp datetimeparser
TheElementsOf,MachineLearning,1620638064.0,[D] Trustworthiness in current AI applications,"I am currently doing a research on trustworthiness in current applications of AI and would like to discuss this issue, as I believe that it is very important aspect if the AI should be use on daily basis by everyone. My main interest is in automotive industry, i.e., trust in self-driving cars and / or weaker decision support systems for driving. I believe that this discussion will be beneficial for everyone, especially if many people from different fields will contribute, as a subject such as trust or ethics should never be done by one person / organisation. Please include your ideas of current and future problems and maybe also some references.

&#x200B;

I see the main problem of trustworthiness of AI applications, which are not interpretable (black-box models). However, even if researchers would somehow understand the decision process of a model and why it arrived to the decision it did, how can we ensure consumer that it is safe to use the machine with such setting, if we can not process all possible situations and outcomes anyway? On the other hand, if the model is interpretable, how can we ensure consumer that their data are still private? Is an interpretability of a model problem in privacy preserving? And how to explain pros and cons to the wide population? ",currently research trustworthiness current applications ai would like discuss issue believe important aspect ai use daily basis everyone main interest automotive industry e trust self drive cars weaker decision support systems drive believe discussion beneficial everyone especially many people different field contribute subject trust ethics never one person organisation please include ideas current future problems maybe also reference x200b see main problem trustworthiness ai applications interpretable black box model however even researchers would somehow understand decision process model arrive decision ensure consumer safe use machine set process possible situations outcomes anyway hand model interpretable ensure consumer data still private interpretability model problem privacy preserve explain pros con wide population
strngelet,MachineLearning,1620054250.0,[P] Python library to boost T5 models speed up to 5x & reduce the model size by 3x,"Edit :

>**T5** \- (Text to Text Transfer Transformer) is a large seq2seq **transformer** model ( it has both encoder and decoder). it is pre-trained on the C4 (Colossal Clean Crawled Corpus) dataset and is flexible for fine-tuning on a variety of downstream tasks. It achieves state-of-the-art results on many NLP benchmarks. T5 models can be used for several **NLP** tasks such as Summarization, translation, Q&A, text generation etc... for more info on the model refer to [this](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) article by Google.

I wanted to share this new library I've been working on and that I open-sourced!.

here are some links to the library:

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

&#x200B;

[logo](https://preview.redd.it/ez5ghvoc6xw61.png?width=1280&format=png&auto=webp&s=72a11a53b71e1954d386247b7339e2d46e8d7610)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5`. This code snippet from the repository's README gives a concise overview:

&#x200B;

[usage](https://preview.redd.it/85wkze0l6xw61.png?width=1496&format=png&auto=webp&s=f4acb68e3e0bb10b6ce57d79f1aa6704bf020ec5)

The fastT5 library exports the T5 model to onnx with `past_key_values`, then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x).",edit t5 text text transfer transformer large seq2seq transformer model encoder decoder pre train c4 colossal clean crawl corpus dataset flexible fine tune variety downstream task achieve state art result many nlp benchmarks t5 model use several nlp task summarization translation q text generation etc info model refer url article google want share new library work open source link library github repository url pypi project url title suggest increase inference speed pretrained t5 model also decrease model size single line code library instal pip install fastt5 code snippet repository readme give concise overview x200b usage url fastt5 library export t5 model onnx past_key_values quantize run onnxruntime export onnx model support generate method huggingface transformers inferencing information project refer repository url
OnlyProggingForFun,MachineLearning,1620476125.0,[R] Learning to Relight Portraits based on the Background,"A novel per-pixel lighting representation in a deep learning framework, which explicitly models the diffuse and the specular components of appearance, producing relit portraits with convincingly rendered effects like specular highlights. This might be a great extension for more realistic online (Zoom) calls with a background!

[Read the article](https://www.louisbouchard.ai/backgrounds-with-lighting/) or [watch the video](https://youtu.be/rVP2tcF_yRI), whatever you prefer!

**References**  
Pandey et al., 2021, Total Relighting: Learning to Relight Portraits for Background Replacement, doi: 10.1145/3450626.3459872

https://preview.redd.it/eno436ppwvx61.png?width=1280&format=png&auto=webp&s=751d751548bba92dba7139d497524347d4a7ef8d",novel per pixel light representation deep learn framework explicitly model diffuse specular components appearance produce relit portraits convincingly render effect like specular highlight might great extension realistic online zoom call background read article url watch video url whatever prefer reference pandey et al 2021 total relighting learn relight portraits background replacement doi 10 1145 3450626 3459872 url
mamrollahi,MachineLearning,1616615847.0,"[D] Compare my word embedding models (Count based, PMI, SPPMI) #","I am going to build some models over wiki-dump dataset and then try to compare the results to WS353 (for word similarity). So, I need to check whether my understanding is correct or not. Firstly, I need to read the text from wiki file and tokenize it, so that I am able to build the co-occurrence matrix. I am going to build the co-occurrence matrix in 3 ways : based on count, based on PMI and based on SPPMI. Then, I am going to build the embedding matrix using SVD. So, after that, I can have the word embedding matrix and I can compare the results to WS353.

So, the way is correct ? Thanks",go build model wiki dump dataset try compare result ws353 word similarity need check whether understand correct firstly need read text wiki file tokenize able build co occurrence matrix go build co occurrence matrix nmbr ways base count base pmi base sppmi go build embed matrix use svd word embed matrix compare result ws353 way correct thank
New-Psychology-1148,MachineLearning,1620287525.0,[D] Why git is not enough for data science,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

I wrote [a blog post](https://dagshub.com/blog/how-to-use-git-for-data-science/) about why Git is good but not enough for our day-to-day job as data scientists.

I think there is a lot of potential in using Git, combined with other tools, to track our code and take advantage of its features to also help us track our data and models.

What do you think about this Git+ML flow? Does anyone know any other workflow?

[https://dagshub.com/blog/how-to-use-git-for-data-science/](https://dagshub.com/blog/how-to-use-git-for-data-science/)

**TL;DR** Git is used in almost every software development project to track code and file changes. Based on this ability to track every change, there has also been a tremendous increase in Gits adoption for Data science projects. In this post we discuss;

1. Benefits of Git for data science
2. The gaps and limitations of Git
3. Best practices for using Git for data science projects",hi r machinelearning url write blog post url git good enough day day job data scientists think lot potential use git combine tool track code take advantage feature also help us track data model think git ml flow anyone know workflow url git use almost every software development project track code file change base ability track every change also tremendous increase gits adoption data science project post discuss 1 benefit git data science2 gap limitations git3 best practice use git data science project
PaganPasta,MachineLearning,1616769689.0,[D]Doubt in Bayes by Backprop,"I am trying to go through this work: [https://arxiv.org/pdf/1505.05424.pdf](https://arxiv.org/pdf/1505.05424.pdf) by Blundell et al. 

I am not very familiar with Bayesian side of things for DNNs and will try to summarize some of the stuff which I understood below for others much like me. Also, I have highlighted the things which I failed to understand. It'll be helpful if someone can clarify these.

1.   Point estimates based on MLE or MAP give you a (sort of) solution for P(w|D). 
2. P(w|D) written in bayesian form is intractable to compute. The alternative is to find a proxy for P(w|D) called as variational posterior q(w|θ) which minimizes L(θ, w) = KL(q(w|θ)||P(w|D))
3. Eq-(1) breaks it further into the known ELBO form.
4. Proposition 1. is introduced as the generalisation for the gaussian re-parameterisation trick. I can understand the intention behind it somewhat but didn't get the proof and result part. Where is the first term in the right-hand expression coming from?\[**HELP**\]
5.  L(θ, w) is estimated empirically through monte-carlo sampling. eq(2)
6. The gaussian variational posterior estimate part now applies the Proposition 1 to practice . However eq(3) and (4) stem from proposition 1. Hence, not clear to me. 
7. For the prior on w, P(w), authors suggest sampling from mixture of 2 gaussians. Also it appears that params for the priors remain constant in training. 
8. Lastly a weighting scheme for the contribution of each of the Loss params(Likelihood vs complexity). 

I have gone through couple of blog posts but proposition 1 is still unclear to me :/ Any help in appreciated. Sorry in advance if its very basic and/or common knowledge.",try go work url blundell et al familiar bayesian side things dnns try summarize stuff understand others much like also highlight things fail understand helpful someone clarify 1 point estimate base mle map give sort solution p w 2 p w write bayesian form intractable compute alternative find proxy p w call variational posterior q w θ minimize l θ w kl q w θ p w 3 eq 1 break know elbo form 4 proposition 1 introduce generalisation gaussian parameterisation trick understand intention behind somewhat get proof result part first term right hand expression come help 5 l θ w estimate empirically monte carlo sample eq 2 6 gaussian variational posterior estimate part apply proposition nmbr practice however eq 3 4 stem proposition 1 hence clear 7 prior w p w author suggest sample mixture nmbr gaussians also appear params priors remain constant train 8 lastly weight scheme contribution loss params likelihood vs complexity go couple blog post proposition nmbr still unclear help appreciate sorry advance basic common knowledge
PsychologicalDemand0,MachineLearning,1617212778.0,[R] Explainability Guided Multi-Site COVID-19 CT Classification,"Happy to share with you our recent paper on COVID-19 detection from CT images.

Our method achieves state-of-the art results over multiple datasets with a sizable margin.

[Explainability Guided Multi-Site COVID-19 CT Classification](https://arxiv.org/abs/2103.13677)

Abstract :

Radiologist examination of chest CT is an effective way for screening COVID-19 cases. In this work, we overcome three challenges in the automation of this process: (i) the limited number of supervised positive cases, (ii) the lack of region-based supervision, and (iii) the variability across acquisition sites. These challenges are met by incorporating a recent augmentation solution called SnapMix, by a new patch embedding technique, and by performing a test-time stability analysis. The three techniques are complementary and are all based on utilizing the heatmaps produced by the Class Activation Mapping (CAM) explainability method. Compared to the current state of the art, we obtain an increase of five percent in the F1 score on a site with a relatively high number of cases, and a gap twice as large for a site with much fewer training images.

joint work with : Tal Shaharabany , Lior Wolf",happy share recent paper covid 19 detection ct image method achieve state art result multiple datasets sizable margin explainability guide multi site covid 19 ct classification url radiologist examination chest ct effective way screen covid 19 case work overcome three challenge automation process limit number supervise positive case ii lack region base supervision iii variability across acquisition sit challenge meet incorporate recent augmentation solution call snapmix new patch embed technique perform test time stability analysis three techniques complementary base utilize heatmaps produce class activation map cam explainability method compare current state art obtain increase five percent f1 score site relatively high number case gap twice large site much fewer train image joint work tal shaharabany lior wolf
ai_researcherr,MachineLearning,1617050577.0,[Discussion] AI and Memory Wall,"A brief blogpost analyzing the overhead of training recent SOTA models, especially Transformers, arguing that \*memory\* will soon become the main bottleneck, and not training \*FLOPs\*. It would be great to have the community's feedback about this, and whether you agree/disagree with this conclusion:

[https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8](https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8)

TLDR:

\- The computational cost of training recent SOTA Transformer based models in NLP has been scaling at a rate of 750x/2yrs, and the model parameter count is scaling at 240x/2yrs.

\- In contrast, the GPU/TPU DRAM capacity has been only scaling at a rate of 2x/2yrs. In the meantime, the peak hardware FLOPS has been scaling at a rate of 3.1x/2yrs. 

\- To put these numbers into perspective, peak hardware FLOPS has increased by 90,000x, while DRAM/Interconnect bandwidth has only scaled by a factor of 30x over the past 20 years.

\- ""No exponential can continue forever"", and delaying an exponential with the rate of 240x/2yrs will not be feasible for long. We need to rethink the training, deployment, and design of AI models and hardware to deal with this increasingly challenging memory wall.",brief blogpost analyze overhead train recent sota model especially transformers argue memory soon become main bottleneck train flop would great community feedback whether agree disagree conclusion url computational cost train recent sota transformer base model nlp scale rate 750x 2yrs model parameter count scale 240x 2yrs contrast gpu tpu dram capacity scale rate 2x 2yrs meantime peak hardware flop scale rate 3 1x 2yrs put number perspective peak hardware flop increase 90 000x dram interconnect bandwidth scale factor 30x past nmbr years exponential continue forever delay exponential rate 240x 2yrs feasible long need rethink train deployment design ai model hardware deal increasingly challenge memory wall
kaleb7589,MachineLearning,1617994609.0,[N] GTC 2021 Free Registration,"[FREE GTC 2021 Registration ](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH)


Sign up folks, it’s FREE, amazing talks and a key note you won’t want to miss!",free gtc nmbr registration url folks free amaze talk key note want miss
bendee983,MachineLearning,1619457034.0,[R] ThreeDWorld Transport Challenge -- Embodied AI,"New challenge by researchers at IBM, MIT, and Stanford aims to provide a realistic simulated environment to train and test RL models on task and motion planning (TAMP) problems. 

The challenge takes place in the ThreeDWorld environment, which is a visually, audibly, and physically realistic simulation. The agent is placed inside a multi-roomed house and must locate and carry objects to the specified destination within a specified number of steps. The agent is a two-armed robot. It can only carry two items simultaneously. But the environment also has containers that the agent can use to carry multiple items at once. The agent must find the right balance between exploration, carrying tasks, using containers, etc.

According to the researchers, pure end-to-end RL approaches perform poorly on the challenge. On the other hand, a hybrid approach where the RL agent is controlled by a rule-based high-level planner improves the performance (though the problem is still far from solved).

The challenge makes use of some simplifications in terms of computer vision and action/state complexity:

\- the agent uses magnetic hands and doesn't need to handle objects with fingers

\- the environment is viewed in first person, though the agent is provided with RGB, depth, and segmentation maps

\- movements and rotation are limited to specific increments (0.25m movements, 15-degree rotations)

The challenge is still open for submissions and will be presented at CVPR embedded AI workshop in June. It will be interesting to see what new innovations the challenge will usher.

Full story with comments from lead researcher:

[https://bdtechtalks.com/2021/04/26/reinforcement-learning-embodied-ai/](https://bdtechtalks.com/2021/04/26/reinforcement-learning-embodied-ai/)

Challenge website:

[http://tdw-transport.csail.mit.edu/](http://tdw-transport.csail.mit.edu/)

Gym code for TDW environment:

[https://github.com/chuangg/tdw-transport-challenge-starter-code](https://github.com/chuangg/tdw-transport-challenge-starter-code)

Arxiv paper:

[https://arxiv.org/abs/2103.14025](https://arxiv.org/abs/2103.14025)",new challenge researchers ibm mit stanford aim provide realistic simulate environment train test rl model task motion plan tamp problems challenge take place threedworld environment visually audibly physically realistic simulation agent place inside multi room house must locate carry object specify destination within specify number step agent two arm robot carry two items simultaneously environment also containers agent use carry multiple items agent must find right balance exploration carry task use containers etc accord researchers pure end end rl approach perform poorly challenge hand hybrid approach rl agent control rule base high level planner improve performance though problem still far solve challenge make use simplifications term computer vision action state complexity agent use magnetic hand need handle object finger environment view first person though agent provide rgb depth segmentation map movements rotation limit specific increments 0 25m movements 15 degree rotations challenge still open submissions present cvpr embed ai workshop june interest see new innovations challenge usher full story comment lead researcher url website url code tdw environment url paper url
SomeParanoidAndroid,MachineLearning,1617579186.0,[D] Practical tips for Active Learning (my approach does not outperform random sampling),"Hello fellow practitioners,

I have a dataset for classification where the labeling process is the output of a very computationally expensive physical simulation (that I have to run for every datapoint). So I proposed to incorporate the active learning framework to limit the amount of data needed.

I am implementing the approach(es) from Yarin Gal's et al. [Deep Bayesian Active Learning with Image Data](https://arxiv.org/abs/1703.02910). In that they use their *MC Dropout* Bayesian neural network and they take advantage of its ability to quantify uncertainties in the predictions.  This allows them to apply the acquisition functions on the pool/unlabelled data and include the `argmax` datapoint into the training set at every iteration.

My problem is that all those functions are no better than randomly choosing training instances every time in my dataset.

The parameter settings that correspond to the figure I am showing are the following (should be self explanatory I believe):

    # Specific parameters for the MC Dropout CNN 
    dropout_p                     = 0.3   # Higher values make the network less certain
    reg                           = 10e-5 # Higher values make the network less certain
    MC_samples                    = 30    # The higher the number, the more accurate the predictions will be
    
    # Parameters for the Active Learning iterations
    initial_dataset_size          = 0.05 # 5% of the training dataset (that is, 220 datapoints)
    training_set_increments       = 2    # How many new datapoints to add from the pool at every iteration
    training_epochs_per_iteration = 1    # For reference, training on the full dataset converges after about 35 epochs

I have been experimenting with different values but running a comparison takes about half a day, so I would be very grateful to anyone that has dealt with such a situation before and has any pratical tips so as to choose appropriate values. Guestimates from anyone are welcome as well.

Briefly, I found out that increasing the `initial_dataset_size` to 15% or `training_epochs_per_iteration` to 5 does not provide any advantage to random sampling. Meddling with the regularization value however does help, but the results I am showing are the best I can get so far.

[Comparison of performance of different acquisition functions as a function of the size of the dataset](https://preview.redd.it/5bzraj6pr8r61.png?width=958&format=png&auto=webp&s=66a7cff96ac48cbd7cccc7cec40ee41b9b3b7cb2)",hello fellow practitioners dataset classification label process output computationally expensive physical simulation run every datapoint propose incorporate active learn framework limit amount data need implement approach es yarin gal et al deep bayesian active learn image data url use mc dropout bayesian neural network take advantage ability quantify uncertainties predictions allow apply acquisition function pool unlabelled data include argmax datapoint train set every iteration problem function better randomly choose train instance every time dataset parameter settings correspond figure show follow self explanatory believe specific parameters mc dropout cnn dropout_p nmbr higher value make network less certain reg 10e 5 higher value make network less certain mc_samples nmbr higher number accurate predictions parameters active learn iterations initial_dataset_size nmbr 5 train dataset nmbr datapoints training_set_increments nmbr many new datapoints add pool every iteration training_epochs_per_iteration nmbr reference train full dataset converge nmbr epochsi experiment different value run comparison take half day would grateful anyone deal situation pratical tip choose appropriate value guestimates anyone welcome well briefly find increase initial_dataset_size 15 training_epochs_per_iteration nmbr provide advantage random sample meddle regularization value however help result show best get far comparison performance different acquisition function function size dataset url
Competitive-Rub-1958,MachineLearning,1620563385.0,[D] Are ViT's good enough for moderate/low data conditions?,"The OG `ViT` was pretty data heavy, and there were some improvements with Data efficient flavours.

But has there been any significant advances in places like CIFAR-100 without huge pre-training dataset? I would hardly classify IMAGENET21K as something that is used in the real-world.

I have about 200k images over 50 classes (Million Songs Dataset) for pre-training, and a 20K subset (MagNaTune) for fine-tuning. Should I use any of the latest Visual Transformers or standard CNN's?

Thanks for taking the time out to address my query!",og vit pretty data heavy improvements data efficient flavour significant advance place like cifar 100 without huge pre train dataset would hardly classify imagenet21k something use real world 200k image nmbr class million songs dataset pre train 20k subset magnatune fine tune use latest visual transformers standard cnn thank take time address query
