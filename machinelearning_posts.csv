Author,Subreddit,Date,Title,Post
SisyphusGuy,MachineLearning,1617898792.0,[D] Are there any reliable open-source out-of-the-box Face Anti-Spoofing detectors?,"I am looking through the [Face Anti-Spoofing section](https://paperswithcode.com/task/face-anti-spoofing/latest) of Papers With Code and I can't find any reliable implementation of an open-source Face Anti-Spoofing detector for RGB images. 

Some implementations that I found at Papers With Code require depth images, others don't make the pre-trained weights available and others even make the wrong pre-trained weights available. The ones that I can actually use as an out-of-the-box model don't present very good generalization.

For my project, ideally, I would like to have a reliable passive Face Anti-Spoofing detector (i.e., a detector that doesn't actively challenges the user)."
dadadidi,MachineLearning,1616942192.0,"[P] Guide: Finetune GPT2-XL (1.5 Billion Parameters, the biggest model) on a single 16 GB VRAM V100 Google Cloud instance with Huggingface Transformers using DeepSpeed","I needed to finetune the GPT2 1.5 Billion parameter model for a project, but the model didn't fit on my gpu. So i figured out how to run it with deepspeed and gradient checkpointing, which reduces the required GPU memory. Now it can fit on just one GPU.

Here i explain the setup and commands to get it running: [https://github.com/Xirider/finetune-gpt2xl](https://github.com/Xirider/finetune-gpt2xl)

I was also able to fit the currently largest GPT-NEO model (2.7 B parameters) on one 16 GB VRAM gpu for finetuning, but i think there might be some issues with Huggingface's implementation.

I hope this helps some people, who also want to finetune GPT2, but don't want to set up distributed training."
mroc_lak,MachineLearning,1616579417.0,[D] How does one test for shortcut learning of deep neural networks in computer vision?,"Hi all, 

  
I’ve come across reviews of Covid-19 diagnosis based on Chest-X-rays and many studies were flawed. Some learned shortcuts (e.g. using hospital specific tokens in an image). How does one systematically test that the neural net isn’t using any shortcuts?"
jafioti,MachineLearning,1617299891.0,[D] Adaptive Computation Time Uses?,"I read the paper for Adaptive Computation Time ([https://arxiv.org/abs/1603.08983](https://arxiv.org/abs/1603.08983)) by Alex Graves a while ago, and I've only played around with it, but my basic question is: why has this not been used more? The only mainstream project I've seen it in was ALBERT, where one version used adaptive computation time to determine the number of copies of layers to run.

It seems like this would be hugely important as it allows for iterative refinement of a speculative output, like many networks, but for a dynamic number of times, decided by the network. Originally it was implemented for RNNs, but I think its pretty trivial to implement it for most other architectures. I would surely think this could be implemented with a Mixture of Experts model where the model can rerun the same layers and choose a different expert each time, which could allow for big parameter space without having to be constrained to using one or two experts each forward pass. Maybe I'm overhyping ACT and its really not very useful, but is there any reason that it hasn't seen more widespread adoption?"
goktugkt,MachineLearning,1618608106.0,[P] Minimal PyTorch Library for Natural Evolution Strategies,"https://github.com/goktug97/nes-torch

My main goal with this project was to test my new configuration system library `pipcs`
https://github.com/goktug97/pipcs and see how practical to write a library with it. Also, it can act as an example for pipcs. 

It also supports mpi4py. Without changing anything you can run the same script with `mpirun` to train in parallel."
Yuqing7,MachineLearning,1618587259.0,[N] ETH Zurich & UC Berkeley Method Automates Deep Reward-Learning by Simulating the Past,"A research team from ETH and UC Berkeley proposes a Deep Reward Learning by Simulating the Past (Deep RLSP) algorithm that represents rewards directly as a linear combination of features learned through self-supervised representation learning and enables agents to simulate human actions backwards in time to infer what they must have done.

Here is a quick read: [ETH Zurich & UC Berkeley Method Automates Deep Reward-Learning by Simulating the Past](https://syncedreview.com/2021/04/14/eth-zurich-uc-berkeley-method-automates-deep-reward-learning-by-simulating-the-past/).

The paper Learning What To Do by Simulating the Past is on [arXiv](https://arxiv.org/pdf/2104.03946.pdf)."
BRadoslaw,MachineLearning,1620071706.0,[D] Transformer Positional Embeddings for the nth time - are the representations unique?,"Hi all,

I'm trying to get the intuition behind positional embeddings described in the famous paper [*""Attention is all you need""*](https://arxiv.org/abs/1706.03762). It makes total sense for me but I notice one caveat.

I make a case where a particular tokenized word at position x might result in the same vector as another word at another position.

Let me make up a case for it:

    representation_1 = tokenize(""learning"") + positional_embedding(position=1)
    representation_2 = tokenize(""other"") + positional_embedding(position=20)
    representation_1 == representation_2

It would means that word representations are not unique. If so, why does the model learn the sequential nature of input? Is my case valid?

&#x200B;

[https:\/\/kazemnejad.com\/blog\/transformer\_architecture\_positional\_encoding\/](https://preview.redd.it/tjr3ziednyw61.png?width=1528&format=png&auto=webp&s=17740eb65a883a74d9565bad0122d707f26612d0)"
limarg,MachineLearning,1618568522.0,[D] Marginal Likelihood Estimation based on VAE,"Why is the Marginal Likelihood Estimator proposed in VAE not based on importance sampling?

I would expect the straightforward way to estimate the marginal likelihood to be based on importance sampling:

*𝑝*(*𝑥*)=∫*𝑧𝑝*(*𝑧*)*𝑝*(*𝑥*|*𝑧*)*𝑞*(*𝑧*|*𝑥*)*𝑞*(*𝑧*|*𝑥*)*𝑑𝑧*=𝔼*𝑞*(*𝑧*|*𝑥*)\[*𝑝*(*𝑥*|*𝑧*)*𝑝*(*𝑧*)*𝑞*(*𝑧*|*𝑥*)\]

However,

1. in the original VAE paper, the authors suggest a different method in appendix D that is *not* based on importance sampling. Is there a good reason for this?
2. The authors furthermore suggest to estimate *𝑞*(*𝑧*|*𝑥*) with a density estimation after drawing samples from it. Why is this necessary if we know *𝑞*(*𝑧*|*𝑥*) explicitly?"
mistermysterioyster,MachineLearning,1617448235.0,[D] Paper Reading Group #016 - Tackling climate change with machine learning. (Link to full slides in comments!),
yaxu,MachineLearning,1617265002.0,[D] Non-automated machine learning?,"Machine  learning can be a bit unfathomable for most people. What machine  learning/AI algorithms are there that can be reasonably worked out on  paper? Is there a way to understand an ANN by getting 10 people together  exchanging numbers and adjusting state? Is there a way to do machine  learning without a machine?

I'm  guessing that a lot of systems are too complex/only work at scale.. But  it would be interesting at least to de-automate at least part of a  machine learning system. Kids learn sorting algorithms like bubblesort  at school by standing in a line and following an algorithm. Is anything  like this possible with ML algorithms?"
Tuba202,MachineLearning,1617219025.0,My fork of RameenAbdal's StyleFlow! [P],"For those of you who don't know, StyleFlow is a [SUPER COOL AI](https://youtu.be/Lt4Z5oOAeEY) that edits facial parameters like age and gender.

The only issue is that it wouldn't run! After a week of on-and-off work, I finally got it working, and decided to share my work in the form of a fork of the project.

[My StyleFlow Fork](https://github.com/Tuba202/StyleFlow-Made-Easy)

As long as you have a windows computer with a Cuda compatible GPU, please take some time and check it out!

[An example of what it can do!](https://preview.redd.it/3lhtf2eq0fq61.png?width=1919&format=png&auto=webp&s=b5d3ecdc844a5dfad2796fe9b91db4a2e648fe4b)"
Vegetable_Ganache_37,MachineLearning,1616679356.0,[R] New Pre-Print: Bio-Inspired Robustness: A Review,"Hello everyone,

We recently added a new pre-print on how human visual system-inspired components can help with adversarial robustness. We study recent attempts in the area and analyze their properties and evaluation criteria for robustness. Please let us know what you think of the paper and any feedback is highly appreciated!!! :)

P.S Please forgive the word format TT TT, first and last time I do this in my life. Else it's Latex all the way.

Title: 'Bio-Inspired Robustness: A Review '

Arxiv link: [https://arxiv.org/abs/2103.09265](https://t.co/m1lZbQqhEW?amp=1)

Abstract: Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision-inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision."
Seankala,MachineLearning,1618615742.0,[D] If I can't reproduce the exact reported scores of a paper (1-2% difference) then is it okay to report the scores I obtained or should I copy the originally reported scores?,"Sorry if the title is a little confusing. I'm sure many people have experienced this, but what I'm referring to is that situation where you take the code released by the authors of a particular model, set up your virtual environment to match theirs, and still can't reproduce the exact reported scores. By ""exact"" I'm referring to a difference of maybe 0.1-0.2%. A particular SoTA baseline I'm using shows this behavior and I'm getting a 1-2% difference.

Is it fine to use those scores or should I copy and paste the originally reported scores? Thanks."
Shoulder_Feeling,MachineLearning,1619417114.0,[Project] DataTap provides droplets ( containers for datasets) to make working on popular deep learning datasets easy.,"Excited to share [DataTap](https://www.datatap.dev), An open-source dataset management tool that makes it easy to ""containerize"" datasets to let you  focus on machine learning not data ops.  DataTap lets you build data set droplets ( think of a droplet as a docker container for data ).  A droplet encapsulates a dataset that can then easily be used , imported, shared across different teams and projects.

Each Data Droplet consists for 2 items

* **Droplet Template**, similar to a docker file this specifies the dataset schema
* **Dataset annotations, metadata and media** (this is typically images / videos / rich media )

Learn more about how you can start using this here [https://github.com/zensors/datatap-python](https://github.com/zensors/datatap-python)

Many machine learning projects use proprietary data formats that require tools and utilities to be re-written from scratch to accommodate them. Not only does this slow down development substantially, but it also increases the probability that developers introduce bugs in the very code that validates models’ performance.

As part of dataTap’s efforts to allow machine learning engineers to focus only on the machine learning itself, we introduced an open-source data interchange format called Droplet. The data container format, called “annotation,” provides a standardized way to describe what is in an image. DataTap is designed to be the data platform for Software 2.0. Machine learning on reach media like images , audio or video needs a special data pipeline to version and manage data much like there are MLOps tools to version and manage models

Currently the project has common datasets available that you can download or stream with 3 lines of code.

* coco
* Open-Imagees
* AI food Dataset
* Large Person Dataset
* Combined Vehicles Dataset

See the full list 

[https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e](https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e)

Request Your own to be added in or use the open source tools to import data into the droplet format using this example

[https://zensors.typeform.com/to/WXo3ZlSN](https://zensors.typeform.com/to/WXo3ZlSN)"
s-lilo,MachineLearning,1620223692.0,[N] Call for Participation in a Shared Task about occupations detection in clinical texts,"Hi, everyone!

I'm a researcher from the Text Mining Unit at the Barcelona Supercomputing Center, and I wanted to share with you some information about MEDDOPROF, a Shared Task that we are currently organizing focused on the detection and normalization of professions and employment status in clinical texts in Spanish.

Even if these types of entities might seem really niche, every day we learn more and more about their importance. Just think about how someone's occupation can have a radical impact in their physical and mental health, habits, lifestyle choices, ... There is even an entire medical specialty, occupational medicine, that is centered around this topic. In the context of the current pandemic, many people with specific occupations have been specially affected (for instance, health professionals and other essential workers). The detection of these terms will help researchers to better characterize health risks of specific occupations.

Outside medicine, we foresee that the systems resulting from MEDDOPROF may be used in fields such as social care, human resources, legal NLP and even gender studies. Personally, I think one of the main contributions of the task is the inclusion of employment status in a broad sense. We have annotated unemployed and retired people, family caregivers, people who are homeless, people who depend on government subsidies, etc, which strengthens the social side of this project.

Additionally, each mention in the corpus (which includes 2000 documents from over 20 different medical specialties) has been normalized to either the European Skills, Competences, Qualifications and Occupations classification (ESCO) or SNOMED-CT. These are multilingual vocabularies, which we hope might inspire similar tasks in other languages (to the best of our knowledge, there haven't been any similar tasks yet).

We released the training set some weeks ago, and on June 1st we will release the test set. If you are interested in the task, want to see some annotated examples, the data or the annotation guidelines, ... please check out the task's website:[ https://temu.bsc.es/meddoprof/](https://temu.bsc.es/meddoprof/)

Thank you if you have read up to here, I hope to see at least some of you at the task!"
jj4646,MachineLearning,1619072260.0,"[D] is this the ""unanswered question"" of machine learning?",""" Generalization performance of classifiers in deep learning has recently become a subject of intense study . Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this “overfitting”, they perform well on test data, a phenomenon not yet fully understood.  ""

Source: [http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf](http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf) 

What is the consensus about this question in the statistics community? Is this the equivalent of ""the big bang"" or ""how did the dinosaurs go extinct"" questions for the statistics/machine learning community? Is it fundamentally impossible to answer this question - or just extremely difficult?

My very naïve answer to this question (i.e. why do deep learning models generalize well to unseen data?) is that :

A) ""unseen"" data can apparently (on some level) be well represented by complex (e.g. non-linear) combinations of ""seen"" data : deep learning models are very good at recognizing and figuring out these complex combinations

B) ""show me your 5 closest friends and I will tell you who you are"" : on some level, data probably displays the ""nearest neighbor principle""  - in big and complex data sets, there are small ""pockets"" of homogenous data  that the model uses as stepping stones to generalize.

This of course does not explain the mathematics of why deep learning models are able to generalize to unseen data - but is this basically the essence of the matter? Deep learning models are able to generalize to unseen data because on some level, unseen data is similar to seen data. A silly and extreme example, it is unlikely that even the ""strongest"" recurrent neural networks when provided data about a certain stock from 1980-1995 would be able to predict today's weather - this is because the data too fundamentally different to allow even the best model to generalize. Or, suppose you want to use a regular statistical model to predict information about a normally distributed variable with a certain mean and standard deviation - if you drastically change the mean and standard deviation of this variable and ask the previously trained model to continue making predictions (assume the model doesn't know that anything has changed), it's natural to expect that the predictions will be less accurate. Statistical models (no matter how ""great"" they are) are able to generalize to unseen data, so long as this unseen data comes from the same ballpark as the seen data.

Am I understanding this correctly?"
RandomTensor,MachineLearning,1618938639.0,[N] Workshop on the Theory of Overparameterized Machine Learning **Going on right now!!**,"Just FYI the Workshop on the Theory of Overparameterized Machine Learning (TOPML) is happening right now! Lots of big names with hard hitting talks on this fascinating phenomenon. Registration is free! Although I'm not sure if its still open.  


[http://topml.rice.edu/](http://topml.rice.edu/)"
universome,MachineLearning,1618499655.0,[P] Aligning Latent and Image Spaces to Connect the Unconnectable,"Hi! Wanted to share our latest project on infinite image generation: [http://universome.github.io/alis](http://universome.github.io/alis)

[The method works without any conditioning and learns from a dataset of unrelated square images](https://reddit.com/link/mrgrdn/video/53apkx1asct61/player)

Basically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.

Our generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), that have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.

&#x200B;

[𝛿 is the value of the coordinates shift. Pixel values inside circles are equal for different generations \(up to numerical precision\)](https://preview.redd.it/4wbquwfgsct61.jpg?width=1389&format=pjpg&auto=webp&s=b8e75ffec468a79a12cdaba1b97f2280a9ebf71d)

A surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very ""difficult"" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.

&#x200B;

[Results on LSUN bedroom](https://preview.redd.it/ugyrknposct61.jpg?width=2000&format=pjpg&auto=webp&s=398e0c2b9083027c2f89645a3d5762afc6a3ac1c)

Besides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.

**Drawbacks of the approach:**

* Generating patches completely independently limits the generation quality (by \\\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).
* Not all the datasets are ""connectable"". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the ""study"" on this in tables 3 & 4 in Appendix C)

Project page: [https://universome.github.io/alis](https://universome.github.io/alis)

Code: [https://github.com/universome/alis](https://github.com/universome/alis)

Paper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)"
pmp-dash1,MachineLearning,1619732343.0,[R] Improving ETA Prediction Accuracy for Long-tail Events - Doordash ML Blogpost,"If you are interested in improving the accuracy of long-tail events for your ML models, check out this blog article I wrote about making DoorDash's Delivery ETA 10% more accurate. Some of our orders were taking longer than we expected to arrive, so we added some key historical and real-time features into the mix and iterated on custom loss functions to improve the accuracy of our predictions and overall customer experience. Check out the technical details or just learn all about long-tail prediction problems here ([link](https://doordash.engineering/2021/04/28/improving-eta-prediction-accuracy-for-long-tail-events/))

[#MachineLearning](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#ETA](https://www.linkedin.com/feed/hashtag/?keywords=eta&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#mapping](https://www.linkedin.com/feed/hashtag/?keywords=mapping&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432) [#logistics](https://www.linkedin.com/feed/hashtag/?keywords=logistics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6793614489205010432)"
amasterblaster,MachineLearning,1619128441.0,[Discussion] Any new / good hyperparam tuning approaches?," 

I'm looking to tune a very slow function (one call takes 20 mins \~ 1hr). How do you guys handle optimizing something this slow? Have there been any good advancements / new libraries for this problem?

I feel like the techniques I use suck, and am looking to see what everyone is up to here."
grid_world,MachineLearning,1617182279.0,[R] Dataset for research paper,"I  am in process for publishing a paper in ""Deep Learning compression"" by  comparing a model's original size and performance vs. compressed size  and performance on some dataset. Majority of the research papers either  focus on CIFAR-10 and/or ImageNet.

ImageNet  becomes an infrastructure challenge since the dataset size is upward of  150 GB. The problem with CIFAR-10 is that you have a smaller dataset  (60K images) which doesn't scale well if your model size grows ->  think ResNet-50 and bigger.

Therefore,  can you all suggest some other dataset which sits somewhere in between  and whose results will be accepted by journals, conferences, etc. (from  the academic point of view)?"
regalalgorithm,MachineLearning,1619187929.0,[D] Your Favorite AI Podcasts / Blogs / Newsletters / YouTube Channels?,"Hi there, I want to write a little blog post summarizing different ways of keeping up with AI by way of Podcasts / Blogs / Newsletters / YouTube Channels. Yeah there are a million of these, but most are not so well curated, miss a lot of stuff, and are not up to date. Criteria: still active, focused primarily on AI, high quality.

Here's what I have so far, would appreciate if you can suggest any additions!

* **Podcasts**
   * [**Machine Learning Street Talk**](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ)
   * **Lex Fridman (mainly first \~150 eps)**
   * **Gigaom Voices in AI**
   * **Data Skeptic**
   * **Eye on AI**
   * **Gradient Dissent**
   * **Robot Brains**
   * **RE Work podcast**
   * **AI Today Podcast**
   * **Chat Time Data Science**
   * **Let’s Talk AI**
   * **In Machines We Trust**
* **Publications**
   * **The Gradient**
   * **Towards Data Science**
   * **Analytics Vidhya**
   * **Distill**
* **Personal Blogs**
   * [**Lil’Log**](https://lilianweng.github.io/lil-log/)
   * **Gwern**
   * **Sebastian Ruder**
   * **Alex Irpan**
   * **Chris Olah**
   * **Democratizing Automation**
   * **Approximately Correct**
   * **Off the Convex Path**
   * **Arg min blog**
   * **I’m a bandit**
* **Academic Blogs**
   * **SAIL Blog**
   * **Berkeley AI Blog**
   * **Machine Learning at Berkeley Blog**
   * **CMU ML Blog**
   * **ML MIT**
   * **ML Georgia Tech**
   * **Google / Facebook / Salesforce / Microsoft / Baidu / OpenAI /  DeepMind** 
* **Journalists**
   * **Karen Hao** 
   * **Cade Metz**
   * **Will Knight**
   * **Khari Johnson**
* **Newsletters**
   * **Last Week in AI**
   * **Batch.AI**
   * **Sebasting Ruder**
   * **Artificial Intelligence Weekly News**
   * **Wired AI newsletter**
   * **Papers with Code**
   * **The Algorithm**
   * **AI Weekly**
   * **Weekly Robotics**
   * **Import AI**
   * **Deep Learning Weekly**
   * **H+ Weekly**
   * **ChinAI Newsletter**
   * **THe EuropeanAI Newsletter**

**Youtube Channels**

* **Talks**
   * [**Amii Intelligence**](https://www.youtube.com/channel/UCxxisInVr7upxv1yUhSgdBA)
   * [**CMU AI Seminar**](https://www.youtube.com/channel/UCLh3OUmBGe4wPyVZiI771ng)
   * [**Robotics Institute Seminar Series**](https://www.youtube.com/playlist?list=PLCFD85BC79FE703DF)
   * [**Machine Learning Center at Georgia Tech**](https://www.youtube.com/channel/UCugI4c0S6-yVi9KfdkDU0aw/videos)
   * [**Robotics Today**](https://www.youtube.com/channel/UCtfiXX2nJ5Qz-ZxGEwDCy5A)
   * [**Stanford MLSys Seminars**](https://www.youtube.com/channel/UCzz6ructab1U44QPI3HpZEQ)
   * [**MIT Embodied Intelligence**](https://www.youtube.com/channel/UCnXGbvgu9071i3koFooncAw)
* **Interviews**
   * **See podcasts**
* **Paper Summaries** 
   * [**AI Coffee Break with Letitia**](https://www.youtube.com/c/AICoffeeBreak/featured)
   * [**Henry AI Labs**](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw)
   * [**Yannic Kilcher**](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)
   * **Arxiv Insights**
* **Lessons**
   * [**3Blue1Brown**](https://www.youtube.com/c/3blue1brown/featured)
   * [**Jordan Harrod**](https://www.youtube.com/channel/UC1H1NWNTG2Xi3pt85ykVSHA)
   * [**vcubingx**](https://www.youtube.com/channel/UCv0nF8zWevEsSVcmz6mlw6A)
   * [**Leo Isikdogan**](https://www.youtube.com/channel/UC-YAxUbpa1hvRyfJBKFNcJA)
* **Demos**
   * [**bycloud**](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng)
   * [**Two Minute Papers**](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
   * [**Code Bullet**](https://www.youtube.com/channel/UC0e3QhIYukixgh5VVpKHH9Q)
   * [**What's AI**](https://www.youtube.com/c/WhatsAI/videos)"
New_Date5540,MachineLearning,1618921283.0,[P] I have created a script to convert video into slides (ppt) for StatQuest,"I was wondering if I can get those slides as the video contents are awesome so I have decided to create a script to extract the slides from the video using OpenCV and generated ppt from the slides.
You can check the code. Any feedback would be highly appreciable.
Repo: https://github.com/ninjakx/youtube-video2ppt"
VerySecretCactus,MachineLearning,1618782264.0,"[D] Is the ""Super Harsh Guide to ML"" reddit post out of date yet? (2021 version)","Last time this was posted was over a year ago. Wondering if anything's changed.

Original post: https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/

Last year's update: https://www.reddit.com/r/MachineLearning/comments/emmxp6/d_is_the_super_harsh_guide_to_ml_reddit_post_out/"
L-MK,MachineLearning,1620290611.0,[R] Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet,"TL;DR: Got scooped by MLP-Mixer, so I'm releasing my writeup/code/models. I hope someone finds them interesting/useful.

Lately I've been trying a couple variants of simple vision transformers to better understand what makes them perform well. About a month ago, I found that you could replace the attention layers with feed-forward layers and get quite good results. Last week I started a short writeup of the experiment (just a few pages, as I didn't see it as a full paper).

Today Google put out a paper (MLP-Mixer) that proposes exactly the same architecture.

When I saw the paper earlier today I considered scrapping what I had done, but now I figure that I might as well just put it out there.

For those who are interested, here's a [GitHub repo](https://github.com/lukemelas/do-you-even-need-attention) with pretrained models, a [W&B log](https://wandb.ai/lukemelas2/deit-experiments/reports/Do-You-Even-Need-Attention---Vmlldzo2NjUxMzI?accessToken=8kebvweue0gd1s6qiav2orco97v85glogsi8i83576j42bb1g39e59px56lkk4zu) of the experiments, and a 3-page [writeup](https://github.com/lukemelas/do-you-even-need-attention/blob/main/Do-You-Even-Need-Attention.pdf).

Also, if anyone has stories about getting scooped, feel free to share -- I'd imagine people have some crazy stories.

Edit: Wow, thank you all for the support! I really didn't expect this. Based on your suggestions, I've also uploaded a version of the report to arXiv: [https://arxiv.org/abs/2105.02723](https://arxiv.org/abs/2105.02723) "
patrickkidger,MachineLearning,1617795134.0,"[P] torchtyping -- documentation + runtime type checking of tensor shapes (and dtypes, ...)","Hello everyone. I'm excited to announce [torchtyping](https://github.com/patrick-kidger/torchtyping), as a way to document -- and check -- that PyTorch tensors have the correct shape (dtype, names, layout, ...).

Turn this:

    def batch_outer_product(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        # x has shape (batch, x_channels)
        # y has shape (batch, y_channels)
        # return has shape (batch, x_channels, y_channels)
    
        return x.unsqueeze(-1) * y.unsqueeze(-2)

into this:

    def batch_outer_product(x:   TensorType[""batch"", ""x_channels""],
                            y:   TensorType[""batch"", ""y_channels""]
                            ) -> TensorType[""batch"", ""x_channels"", ""y_channels""]:

        return x.unsqueeze(-1) * y.unsqueeze(-2)

**with runtime checking that the sizes of these channels line up and are consistent.**

Bye-bye bugs! Say hello to enforced, clear documentation of your code.

---

Personally I find that I leave comments on the shape of tensors all over my code, just to keep track of what each function expects. `torchtyping` is designed to fix this.

Check out the documentation [on GitHub](https://github.com/patrick-kidger/torchtyping) for usage, examples, and ways to extend `torchtyping`.

Finally if you're curious, then have a look at the `torchtyping` FAQ: there's a few other libraries out there doing similar things (e.g. for JAX) so if `torchtyping` isn't quite what you're after, then you have a few other options too. :)"
Transit-Strike,MachineLearning,1616942574.0,[D] Does Dataset balance matter for a Style GAN?," When we look at classifiers, if Class 1 is dominant over Class 0. (A lot more samples) that really hurts the accuracy of our model since there is a strong bias towards one of the classes. Our model can now blindly assume that all samples are from Class 1 and it would be right very often.

With a Style GAN I am looking at, I fear I have a similar issue. I need to convert photos into fake paintings and I would say it is easy, but I have a lot more photos than painting.

Would something like that have an affect on Binary Cross Entropy? I think it would since one class is better represented. But at the same time, since the fake images are generated based on real images, we could just ensure that we generate the same number of fake images as the number of real images in the same class.

(If we have 100 paintings, generate 100 fake paintings). The number of Photos then may or may not matter. I am not sure which.

But in such a case, would a Wasserstein Loss over BCE help mitigate any issues? Since it cares about distributions and not class labels."
jj4646,MachineLearning,1619072295.0,[D] why did kernel methods become less popular than neural networks?,"I was reading today that in earlier neural networks, a popular choice of the the activation function in neural networks was the ""radial basis function"" (RBF). This is apparently why earlier neural networks were called ""kernel approximators"". This was a bit surprising to me, seeing as now most people right away assume that the popular choice of activation functions in neural networks is ReLu. It seems to me that the transition away from RBF activation functions happened around the same time that neural networks overtook SVM's in terms of popularity.

Does anyone know why this happened? I read that neural networks with the RBF function also (along with standard neural networks) have the ""universal approximation property"" - i.e., theoretically, they are able to approximate any target function to any level of accuracy (how efficiently, this is another question).  

Is there a reason that RBF's lost their popularity? Does anyone know why ReLu is the go-to activation function for neural networks these days? 

Just a guess: perhaps a neural network with ReLu activations is able to ""better"" (consistency, convergence) approximate the same target function compared to RBF, given the same ""resources"" (e.g. number of neurons and layers)?

Thanks"
zhangboknight,MachineLearning,1617946769.0,[P] Colorizing the legacy videos with attention mechanism,"We recently released the code for our paper ""Deep Exemplar-based Video Colorization"". The code along with the **Colab demo** is available at: [https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization](https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization). Welcome to have a try.

https://preview.redd.it/qsx2amq253s61.png?width=2466&format=png&auto=webp&s=96eee6ce9db190c57f2701cf0b517f4def866d6f

Youtube demo is available at:

[https://youtube.com/watch?v=HXWR5h5vVYI&feature=share](https://youtube.com/watch?v=HXWR5h5vVYI&feature=share)"
rafcy,MachineLearning,1617542304.0,[P] Object Detection and Tracking,"Sharing here this project on my github since this version is being published on CodeOcean as well. Noted that you can change the Darknet's YOLO trained weights with your own YOLO detector and use this repository as a detector-tracker for the objects of your choice.

Language: Python

Project refers to this paper : [https://arxiv.org/abs/2007.03227](https://arxiv.org/abs/2007.03227) (also an IEEE publication can be found in the repo description)

Github Repository : [https://github.com/rafcy/HarpyTM](https://github.com/rafcy/HarpyTM)

Brief description :

>The main purpose of the application is to extract traffic data from vehicles on roads using aerial footage taken from static UAVs. To process the footage, deep neural network detector is used (YOLO) alongside with the OpenCV library in ordered to be executed in python. Furthermore, multiple algorithms are used, such as Kalman, Hungarian, in order to match the detections between sequential frames and extract the vehicles and their trajectories. Hence, the velocities and the moving direction of the vehicles are also calculated for each vehicle for every frame.

You are free to discuss about it or suggest any improvments/features."
Yungpastorphillswift,MachineLearning,1616744476.0,[D] Is it possible to combine the weights of multiple CNN Models?,"First of all, I want to thank all of you guys for the help you've given me since I first took the dive into Data Science and Machine Learning. I don't post often at all, but I have gone through almost all of the material and resources in the Data Science Wiki, and I'm lurking around in here almost everyday, picking up little jewels of information. I have now clocked in about 800 or so dedicated hours to learning, and I am super excited about all the wild and interesting things I still don't know about. The deeper I go, the more I realize that this rabbit hole never ends. So for real and honestly, thank you guys!

Now on to my question; I am currently working on a semi large-scale Facial Recognition project with a small group. We have scraped, compiled and labeled a huge dataset of image files (aprox. 2mil images). We have them stored in a cloud database, and will train our model in batches by downloading and unzipping directly from the cloud to our notebook. I had the idea that it would be extremely time efficient if everyone in our group pulled a different batch of images, trained models on our own respective machines, and then combined our saved weights into a final model. In theory, the idea sounded good, however, after searching around google for a bit, it's not clear if this is possible, or if it's even a beneficial idea.

Looking for any input, direction towards resources, or helpful advice in general. Thanks in advance!"
optimized-adam,MachineLearning,1620252116.0,[D] Sub-pixel convolutions vs. transposed convolutions,"I am trying to understand the different types of convolutions used for upsampling. In particular, the difference between sub-pixel convolutions and transposed convolutions (or lack thereof). My current understanding is that they are equivalent operations (and from my understanding the authors of the sub-pixel convolution have shown this equivalency in the original paper [https://arxiv.org/abs/1609.05158](https://arxiv.org/abs/1609.05158)). However the difference is that the sub-pixel convolution can be implemented more efficiently.

Is this understanding correct? If so, why are some people (e.g. [https://github.com/atriumlts/subpixel](https://github.com/atriumlts/subpixel)) strongly recommending sub-pixel convolutions over transposed convolutions for what seem to be reasons other than just performance?"
natalieberlin,MachineLearning,1618937360.0,[P] Applied our research (ML on dynamic knowledge graphs) to files and documents,"We have built our first ""smart"" feature into our app. Now, we can show when ""similar"" content likely needs changes. Say, you have 20 files (invoices/contracts/job postings) that all include an address, a company description/name, an IBAN/routing number, legal clauses. You open one of them and edit that clause/change the address/IBAN/company description. Our engine will say ""there are 4 files that probably require changes, too"". 

We use LSH & MinHashing for the file similarities and run our ML for dynamic knowledge graphs to determine which of those 20 ""similar files"" are still active (papers below)

A quick graphic: 

https://preview.redd.it/imrsumx8xcu61.png?width=2856&format=png&auto=webp&s=926e1d90dbba93457917d6cbf09720e415069a86

Papers here:  

[https://dl.acm.org/doi/abs/10.1145/3038912.3052672](https://dl.acm.org/doi/abs/10.1145/3038912.3052672)

[https://arxiv.org/abs/1905.05305](https://arxiv.org/abs/1905.05305)

[http://proceedings.mlr.press/v124/tabibian20a](http://proceedings.mlr.press/v124/tabibian20a)

[https://www.pnas.org/content/116/10/3988.short](https://www.pnas.org/content/116/10/3988.short)

[https://dl.acm.org/doi/abs/10.1145/3018661.3018685](https://dl.acm.org/doi/abs/10.1145/3018661.3018685)

[https://dl.acm.org/doi/abs/10.1145/2939672.2939875](https://dl.acm.org/doi/abs/10.1145/2939672.2939875)

[https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)

&#x200B;

(for good measure: we're just building a little community here [r/reasonal](https://www.reddit.com/r/reasonal/), or you can get to the survey to get access to our closed beta here [reason.al](https://reason.al/?utm_source=reddit.com&utm_medium=referral&utm_campaign=comm-rd-21-010-mling))"
svij137,MachineLearning,1620442366.0,[D] Number of businesses that actually spend money on training their own AI models?,"I have been looking for this data but seems most reports are heavily skewed towards larger companies like amazon and google.

Or i get a generic answer that companies spent over $100B last year

But that’s not the answer i am looking for as 90% of that spend could be just from the top 100 companies

What’s the best way to get around this?"
designer1one,MachineLearning,1618667070.0,[P] *Semantic* Video Search with OpenAI’s CLIP Neural Network,"I made a simple tool that lets you search a video \*semantically\* with AI. 🎞️🔍

✨ Live web app: [http://whichframe.com](http://whichframe.com/) ✨

Example: Which video frame has a person with sunglasses and earphones?

The querying is powered by OpenAI’s CLIP neural network for performing ""zero-shot"" image classification and the interface was built with Streamlit.

Try searching with text, image, or text + image and please share your discoveries!

👇 More examples  
[https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)"
ykilcher,MachineLearning,1618150557.0,"[D] Paper Explained - DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning (Full Video Analysis)","[https://youtu.be/qtu0aSTDE2I](https://youtu.be/qtu0aSTDE2I)

Classic Machine Learning struggles with few-shot generalization for tasks where humans can easily generalize from just a handful of examples, for example sorting a list of numbers. Humans do this by coming up with a short program, or algorithm, that explains the few data points in a compact way. DreamCoder emulates this by using neural guided search over a language of primitives, a library, that it builds up over time. By doing this, it can iteratively construct more and more complex programs by building on its own abstractions and therefore solve more and more difficult tasks in a few-shot manner by generating very short programs that solve the few given datapoints. The resulting system can not only generalize quickly but also delivers an explainable solution to its problems in form of a modular and hierarchical learned library. Combining this with classic Deep Learning for low-level perception is a very promising future direction.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:55 - DreamCoder System Architecture

9:00 - Wake Phase: Neural Guided Search

19:15 - Abstraction Phase: Extending the Internal Library

24:30 - Dreaming Phase: Training Neural Search on Fictional Programs and Replays

30:55 - Abstraction by Compressing Program Refactorings

32:40 - Experimental Results on LOGO Drawings

39:00 - Ablation Studies

39:50 - Re-Discovering Physical Laws

42:25 - Discovering Recursive Programming Algorithms

44:20 - Conclusions & Discussion

&#x200B;

Paper: [https://arxiv.org/abs/2006.08381](https://arxiv.org/abs/2006.08381)

Code: [https://github.com/ellisk42/ec](https://github.com/ellisk42/ec)"
RyanAI100,MachineLearning,1618164984.0,[D] NER for Social Media Texts with Semantic Augmentation | Research Papers Summary 013,
eatpasta_runfastah,MachineLearning,1619618374.0,[D] Masking gradients before the update,"Hello  


I was reading this paper [Learning explanations that are hard to vary](https://arxiv.org/abs/2009.00329) and found the relative [github repo](https://github.com/gibipara92/learning-explanations-hard-to-vary/blob/main/notebooks/linear_regression_ilc.ipynb). To keep it short, before updating the parameters  `theta = theta - lr * final_grads` pytorch (cuda) computes by default the arithmetic mean of the gradients, whereas I want to compute the geometric mean or to apply a mask as shown in the code. 

 Is there a way to do this leveraging pytorch autograd + cuda without the need to write a custom training loop?"
prestodigitarium,MachineLearning,1619288479.0,"[P] Gourdian Free Dataset Download: OpenStreetMap Points of Interest (Restaurants, Bars, Grocery Stores, Transit, Shops, Swingers Clubs, Hospitals, etc)","Hi there!

A friend and I are working on something to help people search for, filter, and download subsets of datasets.

We're excited to share that we've just incorporated all(?) of the points of interest from OpenStreetMap, broken down by group (from their ontologies here https://wiki.openstreetmap.org/wiki/Key:amenity and here https://wiki.openstreetmap.org/wiki/Key:shop )

The groups are below, with the tags that went into each. 

What do people think these might be useful for? Maybe making your own version of WalkScore? Perhaps cross referencing with real estate listings to find a house that's within walking distance of a bakery, library, cafe, and pyrotechnics shop? LoveHotelMapper.com? The possibilities are endless!

**Restaurants and Bars**: https://gourdian.net/g/eric/osm_points_of_interest.restaurants_and_bars

Amenities points of interest labeled with bar, biergarten, cafe, fast_food, food_court, ice_cream, pub, or restaurant.

**Education Services**: https://gourdian.net/g/eric/osm_points_of_interest.education_services

Amenities points of interest labeled with college, driving_school, kindergarten, language_school, library, toy_library, music_school, school, or university.

**Transportation Related**: https://gourdian.net/g/eric/osm_points_of_interest.transportation_related

Amenities points of interest labeled with bicycle_parking, bicycle_repair_station, bicycle_rental, boat_rental, boat_sharing, bus_station, car_rental, car_sharing, car_wash, vehicle_inspection, charging_station, ferry_terminal, fuel, grit_bin, motorcycle_parking, parking, parking_entrance, parking_space, or taxi.

**Financial**: https://gourdian.net/g/eric/osm_points_of_interest.financial

Amenities points of interest labeled with atm, bank, or bureau_de_change.

**Healthcare Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.healthcare_facilities

Amenities points of interest labeled with baby_hatch, clinic, dentist, doctors, hospital, nursing_home, pharmacy, social_facility, or veterinary.

**Entertainment**: https://gourdian.net/g/eric/osm_points_of_interest.entertainment

Amenities points of interest labeled with arts_centre, brothel, casino, cinema, community_centre, conference_centre, events_venue, fountain, gambling, love_hotel, nightclub, planetarium, public_bookcase, social_centre, stripclub, studio, swingerclub, or theatre.

**Public Services**: https://gourdian.net/g/eric/osm_points_of_interest.public_services

Amenities points of interest labeled with courthouse, embassy, fire_station, police, post_box, post_depot, post_office, prison, ranger_station, or townhall.

**Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.facilities

Amenities points of interest labeled with bbq, bench, dog_toilet, drinking_water, give_box, shelter, shower, telephone, toilets, water_point, or watering_place.

**Waste Management**: https://gourdian.net/g/eric/osm_points_of_interest.waste_management

Amenities points of interest labeled with sanitary_dump_station, recycling, waste_basket, waste_disposal, or waste_transfer_station.

**Other Amenities**: https://gourdian.net/g/eric/osm_points_of_interest.other_amenities

Amenities points of interest labeled with animal_boarding, animal_breeding, animal_shelter, baking_oven, childcare, clock, crematorium, dive_centre, funeral_hall, grave_yard, gym, hunting_stand, internet_cafe, kitchen, kneipp_water_cure, lounger, marketplace, monastery, photo_booth, place_of_mourning, place_of_worship, public_bath, public_building, refugee_site, or vending_machine.

**Food Shops**: https://gourdian.net/g/eric/osm_points_of_interest.food_shops

Shops points of interest labeled with alcohol, bakery, beverages, brewing_supplies, butcher, cheese, chocolate, coffee, confectionery, convenience, deli, dairy, farm, frozen_food, greengrocer, health_food, ice_cream, organic, pasta, pastry, seafood, spices, tea, wine, or water.

**General Shops**: https://gourdian.net/g/eric/osm_points_of_interest.general_shops

Shops points of interest labeled with department_store, general, kiosk, mall, supermarket, or wholesale.

**Clothing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.clothing_shops

Shops points of interest labeled with baby_goods, bag, boutique, clothes, fabric, fashion, fashion_accessories, jewelry, leather, sewing, shoes, tailor, watches, or wool.

**Second Hand Shops**: https://gourdian.net/g/eric/osm_points_of_interest.second_hand_shops

Shops points of interest labeled with charity, second_hand, or variety_store.

**Health and Beauty Shops**: https://gourdian.net/g/eric/osm_points_of_interest.health_and_beauty_shops

Shops points of interest labeled with beauty, chemist, cosmetics, drugstore, erotic, hairdresser, hairdresser_supply, hearing_aids, herbalist, massage, medical_supply, nutrition_supplements, optician, perfumery, or tattoo.

**Hardware Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hardware_shops

Shops points of interest labeled with agrarian, appliance, bathroom_furnishing, doityourself, electrical, energy, fireplace, florist, garden_centre, garden_furniture, gas, glaziery, groundskeeping, hardware, houseware, locksmith, paint, security, trade, or windows.

**Furnishing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.furnishing_shops

Shops points of interest labeled with antiques, bed, candles, carpet, curtain, doors, flooring, furniture, household_linen, interior_decoration, kitchen, lamps, lighting, tiles, or window_blind.

**Electronics Shops**: https://gourdian.net/g/eric/osm_points_of_interest.electronics_shops

Shops points of interest labeled with computer, electronics, hifi, mobile_phone, radiotechnics, or vacuum_cleaner.

**Vehicle and Outdoor Shops**: https://gourdian.net/g/eric/osm_points_of_interest.vehicle_and_outdoor_shops

Shops points of interest labeled with atv, bicycle, boat, car, car_repair, car_parts, caravan, fuel, fishing, golf, hunting, jetski, military_surplus, motorcycle, outdoor, scuba_diving, ski, snowmobile, sports, swimming_pool, trailer, or tyres.

**Hobby Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hobby_shops

Shops points of interest labeled with art, collector, craft, frame, games, model, music, musical_instrument, photo, camera, trophy, video, or video_games.

**Stationary and Gift Shops**: https://gourdian.net/g/eric/osm_points_of_interest.stationary_and_gift_shops

Shops points of interest labeled with anime, books, gift, lottery, newsagent, 
stationery, or ticket.

**Other Shops**: https://gourdian.net/g/eric/osm_points_of_interest.other_shops

Shops points of interest labeled with bookmaker, cannabis, copyshop, dry_cleaning, e-cigarette, funeral_directors, laundry, money_lender, party, pawnbroker, pet, pet_grooming, pest_control, pyrotechnics, religion, storage_rental, tobacco, toys, travel_agency, vacant, weapons, outpost, or user defined.


A bit about what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Feedback welcome! If there are any datasets that you'd like to see added, let us know!

Also, if you'd like updates as we put up more datasets, we just got on the Twitters as [@GourdianData](https://twitter.com/GourdianData) ."
spot4992,MachineLearning,1619196341.0,[D] Going From 4 Core/8 Thread CPU To 32 Core/64 Thread CPU," What kind of speed up could be expected going from a 4c/8t CPU to a 32c/64t CPU for a training that runs in parallel on all threads? The specific method being used is the Catboost CPU training. I'm considering upgrading to a CPU with significantly more threads than my current CPU, but want to make sure it's worth it and I can't find anything on Google."
everybody_wants_some,MachineLearning,1618599546.0,[P] Is such a project/task doable using machine learning?,"Hello everyone. I hope that this is the right subreddit for this kind of question. If not, then I want to apologise in advance. 

So my problem is that I was offered to solve a problem with machine learning which I am not sure that it actually is possible to do so. I could not find any literature on it. 

**So here is the background story:**

I was offered a research internship topic at my university. It would be 9 weeks full-time. From the expected level it is between a bachelor’s thesis and a master’s thesis. My supervisors do not have much knowledge about machine learning, since this is not the main focus of their work and can therefore not help me much. I have undergrad knowledge of machine learning. My main knowledge is in classical ML algorithms (SVM, kNN etc.) and DL (mainly CNNs).

However, I am not sure if this topic can be solved with machine learning and if yes then how. 

I am going to describe the goal of the internship and what they expect of me as good as possible. 

The goal is to get accurate wind velocity values for small areas/regions using machine learning. 

**Those are the things I have:**

So I have the MERA dataset which has hourly weather data, like wind velocity, pressure etc. for the entire world.The resolution of the data is 50 km times 50 km. I am going to refer to such 2500 km\^2 squares as big squares. So now one can divide Europe into such big squares. However, since the squares are very big, the accuracy of the wind velocity is not good. Therefore, I have a second dataset the Wind Atlas dataset which has a higher resolution. The squares are 3 km times 3 km. I am going to name this squares as small squares. 

So each big square can be divided into many small squares. Approximately 280 small squares fit into one big square. However, this squares do not have hourly data. They only have one wind velocity value per year. 

Additionally, I will have topological data, e.g. heights, where forests, mountains etc. lie.

**This is what I am supposed to do:**

I am somehow supposed to predict (with the help of ML) for each of this small squares which wind speed it had during some time in the past. For example, in some small square what was the wind velocity of it in May 10 2010 (optionally between 3 - 4 PM).

As a “test set“ I will only have some values from a few weather stations. Those are of course very accurate but only for a very small region. But the number of weather stations will be very small. I will have like maybe 10 weather stations for the entire Germany. Meanwhile, Germany consist of approximately 140 big squares and 40 000 small squares.  

**So my question is:**

I do not have much knowledge regarding this kind of problem. Can this actually be done with machine learning? My gut instinct is no, since it seems too utopian for me. The task seems too difficult and there is hardly any useful test data. But I would like to verify that by some of you who are way more experienced than me. 

Is this doable in general with only machine learning without any other mathematical methods? 

Can someone with my knowledge do something like that on my own in just 9 weeks? 

And if it doable, what methods, algorithms are out there? 

&#x200B;

P.S. I am sorry if the formatting is not good. I am not someone who usually posts on Reddit.

But I really need some help with that. Also I am sorry for any grammar errors since I am not a native speaker."
SQL_beginner,MachineLearning,1619582066.0,"[D] understanding the ""bottleneck"" principle in machine learning","https://openreview.net/forum?id=ry_WPG-A-

Can someone please try to explain in simple terms, what is the bottleneck principle in machine learning and why its important?

Thanks"
Massive-Marzipan,MachineLearning,1617927599.0,[P] Feedback requested on nlp project related to news story chains changing over time,"This sub has been very helpful in the past so I am hoping I can get some feedback. For my project, I am essentially trying to find a way to detect changes over time to a news narrative. At this stage I have applied an algorithm to successfully group together the stories that follow the development of the same event. So now I need to find a way to track, analyze, and maybe quantify how the events (and their coverage changes) . My current approach is using topic modeling to find important keywords in each of the articles. Then, I use those key words to map how the stories change overtime. So in the most basic of terms, I am identifying key words in the first article in each narrative chain and then comparing how those keywords change and are different from key words identified in subsequent articles about the same event. Does this approach sound reasonable? Is there anything else I should be trying instead? Thanks everyone!"
kanxx030,MachineLearning,1616756076.0,[D] How to get into prestigious research labs,"Hi all. I have completed a Master's and would like to explore PhD opportunities. However, it's been hard to get in touch with anyone from the research groups that I'm interested in. Not to mention I've recently moved countries (currently in the UK). Would love to get advice from the community on how to get myself noticed by top research labs/ ways to start a conversation about researching with them besides cold emailing. Thank you in advance :)"
Arioxel_,MachineLearning,1620586861.0,[P] How do you cope with very little data ?,"I am currently working on a project of machine learning interpolation (more or less, but what's to know is it's not classification and my output is a vector of floats) and my issue is I have very little data.

I have a unique set of data of around 50 items to train and test my model with. That's all and nothing more. Fortunately, those are quite simple (not pictures whatsoever).

How do you think I could cope with this issue and especially how to divide the set between training and testing ? I thought that maybe I could build new data out of thin air by, for example, averaging two data."
trackerFF,MachineLearning,1616484384.0,[D] What's the most comprehensive book on mathematical theory behind Deep Learning?,"Hi, 

I'm looking for a book on the math behind current Deep Learning topics - and a lot of the papers I read simply reference the Ian Goodfellow et.al book, when it comes to mathematical proofs. 

Has there been released any comprehensive book that focuses on the mathematical rigor behind Deep Learning - anything the past 2-4 years that's worth checking out?"
QueasyArm8328,MachineLearning,1619987846.0,[Discussion] Graphics in Python,"Hello!  I've been wondering what libraries are used for graphics in Python by professionals in ML; I've just been using Pygame for my pet projects but something tells me this isn't the industry standard.

Thanks!"
Yuqing7,MachineLearning,1617841746.0,"[N] DeepMind, Microsoft, Allen AI & UW Researchers Convert Pretrained Transformers into RNNs, Lowering Memory Cost While Retaining High Accuracy","A research team from University of Washington, Microsoft, DeepMind and Allen Institute for AI develop a method to convert pretrained transformers into efficient RNNs. The Transformer-to-RNN (T2R) approach speeds up generation and reduces memory cost.

Here is a quick read: [DeepMind, Microsoft, Allen AI & UW Researchers Convert Pretrained Transformers into RNNs, Lowering Memory Cost While Retaining High Accuracy](https://syncedreview.com/2021/04/07/deepmind-microsoft-allen-ai-uw-researchers-convert-pretrained-transformers-into-rnns-lowering-memory-cost-while-retaining-high-accuracy/)

The paper *Finetuning Pretrained Transformers into RNNs* is on [arXiv](https://arxiv.org/pdf/2103.13076.pdf)."
study_ai,MachineLearning,1616922070.0,[D][R] Best way to pick up functional analysis for kernel methods' research,"I am looking into applying for PhD in ML. It is quite possible that my research area would be kernel methods.

The problem is - I never had a course in functional analysis. My background: CS Master's in ML, self-learned real analysis at the level of baby Rudin chapters 1-5, abstract algebra, statistics/probability from stats department, and currently reading Casella & Berger.

I am looking a concise book in functional analysis. 2 options I found: Kreiszig's ""Introductory functional analysis with applications"" and Axler's ""Measure, Integration and Real Analysis"".

The problem is; I am not sure how much efforts these books are. I would like some rigorous material BUT with simple exercises. Something way simpler than baby Rudin in terms of exercises.

Would you recommend some book?

Or maybe i approach the problem incorrectly? How did YOU learn kernel methods' prerequisites?"
kaiser_17,MachineLearning,1617537298.0,[D] How is the current research in Long tailed classification?,"I have been going through a lot of literature on long tailed distribution based classification recently. It seems if you were cluster those papers ,they will most likely belong to one of these 3 types:

1) Sampling Based

2) Class weighted losses

3) Meta learning based(which is the new trend)

My question is ,are there any papers which go beyond these 3 categories or has the research been limited to only these 3?"
yourpaljon,MachineLearning,1619119971.0,[D] Why isn't quantile regression used more in neural networks?,"Isn't quantile regression a good solution for estimating uncertainty in neural networks? I haven't seen much use of it, any reason why?"
craffel,MachineLearning,1620323975.0,"[R] ICLR Workshop on Enormous Language Models - May 7th, 2021 (livestream)","The  ICLR workshop on Enormous Language Models will take place on May 7th,  2021, virtually. The workshop will include talks and panels by experts  on training large LMs. The goal is to answer questions like: Will  scaling lead to models that outperform humans on all text-based tasks,  or are there limits to the scalability of these models? Should we focus  on simply scaling these models, or should we design more sophisticated  architectures and training schemes? Do our current benchmark effectively  test capabilities that humans can master but large language models  lack? How can we address the legal and ethical issues that arise from  using unstructured web crawls for training language models? What can we  learn from the fields of cognition, linguistics, and philosophy as we  attempt to measure the ""intelligence"" of machines?

Full information is available here: [https://welmworkshop.github.io/](https://welmworkshop.github.io/) and a livestream will appear at [https://welmworkshop.github.io/livestream/](https://welmworkshop.github.io/livestream/)."
GiuPaolo,MachineLearning,1617648036.0,[R] Call for Papers: Evolutionary Reinforcement Learning workshop @ GECCO 2021,"Time is passing fast! Only 1 week to go before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)

In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.

Recent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.

Nevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.

The goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.

The topics at the heart of the workshop include:

* Evolutionary reinforcement learning
* Evolution strategies
* Population-based methods for policy search
* Neuroevolution
* Hard exploration and sparse reward problems
* Deceptive reward
* Novelty and diversity search methods
* Divergent search
* Sample-efficient direct policy search
* Intrinsic motivation, curiosity
* Building or designing behaviour characterizations
* Meta-learning, hierarchical learning
* Evolutionary AutoML
* Open-ended learning

Autors are invited to submit **new original work**, or **new perspectives on recently published work**  on those topics. Top submissions will be selected for oral presentation and be presented alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). 

&#x200B;

Important dates

* Submission deadline: **April 12, 2021**
* Notification: **April 26, 2021**
* Camera-ready: **May 3, 2021**

**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**"
Hydra1721,MachineLearning,1620342579.0,"Has There Been Any Follow Up Research Papers for the Anti-FRS AI Called ""Fawkes"" ""[Discussion]""","Last year a research paper was published that described a AI that could alter a imagine in a certain manner that prevented FRS from correctly identifying a individual's face without changing the appearance of the photo to the viewer:

[https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes](https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes)

It was stated at the time that Fawkes was VERY computationally expensive and therefore was unable to perform these modifications in real time. Since the publication, I have not read about any other research papers derived from their original work. Has there been any developments in this field of research since then and if so has anyone managed to develop a network that runs in real-time?"
kaia_1527,MachineLearning,1617626377.0,[P] Basic Floor Plan Image Recognition,"Hi everyone, first-time poster. This one should be easy: is there a model that, given an image, recognizes whether the image is floor plan (typically of a residential property)? Other than a boolean, don't need anything else. Should be quick to train one, but wanted to check whether there's a generally accepted model out there. Thanks!"
ottawalanguages,MachineLearning,1620432416.0,[D] Reinforcement Learning with R software,"Reinforcement Learning seems to be becoming a popular topic these days. Does anyone know if people are working on Reinforcement Learning problems using R? Does anyone have any links/tutorials that show Reinforcement Learning and Game Theory material being done using R software?

Thanks"
JosiahWGibbs,MachineLearning,1619932666.0,"[D] How do you try out architecture changes, etc. when a model takes days to train?","I am currently training a model that takes an extremely long time to converge. The accuracty is good but not excellent, so I've been trying to introduce some modifications to the architecture, tune its hyperparameters, etc.

Since the model takes so long to train (couple of days, close to a week) it is very hard to get immediate feedback on wether the changes I'm making mean anything at all or if they are just keeping things the same, or worsening them.

I notice that many SOTA models also take an extremely long time to train, so this is probably an issue many people are used to dealing with.

How do researchers typically deal with this problem? Do you have any recomendations? I first thought of extracting a subset of my dataset and training there, and hoping that the results would extrapolate to the whole thing, but it seems to me that the results don't extrapolate very well at all. Sometimes an idea that seems to work very well on the subset just doesn't introduce any  changes on the full dataset, and the reverse also happens (usually for the worse)."
DeMorrr,MachineLearning,1619215359.0,[P] TorchPQ: Efficient Nearest Neighbor Search and Clustering on GPUs,"Hi everyone!

I’m happy to introduce an open source project that I have been working for a while:

[TorchPQ](https://github.com/DeMoriarty/TorchPQ) is a python library for approximate nearest neighbor search on GPUs. It has efficient implementations of [IVFPQ](https://hal.inria.fr/inria-00514462v2/document) algorithm as well as some of its variants (e.g IVFPQ+R). The project is written mostly in python using pytorch library, with some custom CUDA kernels to accelerate clustering, searching and indexing.

TorchPQ allows you to search with tens of thousands of queries in millions of vectors within a second. In some settings where high recall rate is prioritized, TorchPQ outperforms the implementation of the same algorithm in [faiss](https://github.com/facebookresearch/faiss). For benchmark results see the bottom part of the README page.

I’ve also spent a lot of time optimizing the k-means clustering algorithm for gpu, as a result it’s ultra fast and memory efficient. I recommend you to give it a try even if you have no interest in nearest neighbor search.

The project is still in an early stage, there could be bugs or performance issues, feel free to create an issue if you encounter any of those.

Contributions are welcomed!"
Kal217,MachineLearning,1617403979.0,[P] Intuitive StarGAN Implemented in Tensorflow 2.3,"Lately, a lot of my work has been involving StarGAN, so I wanted to create an easy to read implementation of the architecture that functions, can be freely used, and most importantly that explains what's going on!  \[StarGAN\]([https://arxiv.org/abs/1711.09020](https://arxiv.org/abs/1711.09020)) is a class translation model that uses a single generator to translate freely between some number of classes.  It is an advanced GAN, and although outdated, shares some important ideas.  My implementation documentation is aimed at those who have a familiarity with machine learning.  Please let me know any feedback you might have, this is my first time sharing a project!  


[https://github.com/Kal213/StarGAN-Tutorial-Tensorflow-2.3](https://github.com/Kal213/StarGAN-Tutorial-Tensorflow-2.3)"
putinwhat,MachineLearning,1618758422.0,[D] MLOps Stack,I’ve been researching the different libraries and tools available to use for experiment reproducibility and I recently implemented a few open-source tools like MLFlow for tracking and model storage as well as DVC for data versioning and pipeline generation. I was curious to know what tools have been tried by others and how they’ve worked out?
born_in_cyberspace,MachineLearning,1618471311.0,[R][D] On the Impossibility of Supersized Machines,"Found an excellent satire about the typical arguments against artificial general intelligence, by Garfinkel et al (2017):

 [On the Impossibility of Supersized Machines](https://arxiv.org/abs/1703.10987)

**Abstract**

In recent years, a number of prominent computer scientists, along with academics in fields such as philosophy and physics, have lent credence to the notion that machines may one day become as large as humans. Many have further argued that machines could even come to exceed human size by a significant margin. However, there are at least seven distinct arguments that preclude this outcome. We show that it is not only implausible that machines will ever exceed human size, but in fact impossible."
divergentdata,MachineLearning,1620228956.0,[D] Leveraging Dropout for Uncertainty Quantification / Adversary Rejection,"Hey all, I use uncertainty quantification a lot at work for some of our production models and risk mitigation. I made a write-up of some of the underlying ideas [here](https://www.rossidata.com/DropoutTensorFlowUncertaintyErrorMNIST) for some of the background and put a demo notebook [here](https://github.com/NicholasARossi/UQ_methods/blob/master/notebooks/05_Neural_Network_Uncertainty_Quantification_with_Dropout.ipynb). Would love some feedback as i'm the only ML engineer at my company.  Thanks!"
Caffeinated-Scholar,MachineLearning,1617713834.0,[R] Facebook AI: An Empirical Study of Training Self-Supervised Visual Transformers,
bendee983,MachineLearning,1617629350.0,[R] Data poisoning circumvents certified adversarial training methods,"A paper accepted at CVPR introduces a new data poisoning method that undermines ""randomized smoothing"" training techniques that make machine learning models against adversarial attacks.

Called “Poisoning Against Certified Defenses” (PACD), the method generates poisoned data that have been optimized for the target robustness techniques. The result is a dataset that reduces the average certified radius (ACR), the distance within which a trained machine learning model remains robust against adversarial perturbations. The technique was tested on GA, MACER, and SmoothAdv, three popular randomized smoothing techniques.

This is a grey-box attack: The attacker needs some knowledge of the target ML model architecture and the training method used. But they don't need access to model weights. PACD data generated for one randomized smoothing technique also transfers to other methods, though not to optimal state.

The main takeaway of the paper is that data security is an underrated aspect of adversarial attacks. While many defense techniques are focused on making model weights robust against adversarial perturbation, there's need for more efforts on detecting adversarial perturbations in training data. We also need more measures to certify the provenance of training data and protect machine learning development and deployment pipelines to prevent the compromise of training data.

Read the coverage of the paper and interview with lead author here:

[https://bdtechtalks.com/2021/04/05/machine-learning-data-poisoning-2/](https://bdtechtalks.com/2021/04/05/machine-learning-data-poisoning-2/)

Full paper here:

[https://arxiv.org/abs/2012.01274](https://arxiv.org/abs/2012.01274)

Implementation here:

[https://github.com/akshaymehra24/poisoning\_certified\_defenses](https://github.com/akshaymehra24/poisoning_certified_defenses)"
thunder_jaxx,MachineLearning,1616434209.0,[2103.06326] S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning,
TheCockatoo,MachineLearning,1618916138.0,[D] Arguments for supervised approach when an unsupervised one already exists?,"Assume a niche topic that has received little attention in terms of machine / deep learning papers. There's only one, but it does *unsupervised* deep learning. I'd like to propose a supervised approach, but how can I argue for it in the presence of an unsupervised one? Doesn't ""works without labels"" trump anything supervised?"
pcaversaccio,MachineLearning,1617645967.0,[R] Facebook AI: Self-supervised learning for fast and scalable time series hyper-parameter tuning,
mcbal31,MachineLearning,1620119944.0,[P] Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms,"In this project, we model attention in terms of the collective response of a statistical-mechanical system. We consider a vector generalization of an Ising-like spin system and treat incoming data as applied magnetic fields and outputs of attention modules as spin expectation values in order to rephrase attention as an (inner-loop) fixed-point optimization.

We introduce a slow, explicit attention module which implements adaptive Thouless–Anderson–Palmer mean-field theory and a fast, neural one which parametrizes the so-called Onsager self-correction term. The latter module looks a lot like a transformer module. By approximating/constraining the mean-field equations, we show how a simplified update step appears which mirrors the vanilla transformer architecture, explaining the origin of feed-forward layers and the importance of residual connections.

**Blog:** https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/

**Code:** https://github.com/mcbal/deep-implicit-attention

**TL;DR:**A project on modeling attention as the collective response of a statistical-mechanical system. We use deep equilibrium models to solve a set of self-consistent equations and provide a mean-field theory perspective on transformers. The gist of the post is a combination of a physics-y mean-field interpretation of the DEQ Transformer introduced in [Deep Equilibrium Models](https://arxiv.org/abs/1909.01377) and a bunch of papers on Boltzmann machines from the 90s. Comments welcome."
juliensalinas,MachineLearning,1616595392.0,[D] Production-Ready Machine Learning NLP API with FastAPI and spaCy,"Hey,

FastAPI has been a nice addition to the Python ecosystem. In my opinion it makes API creation easier, and less error-prone. It also comes with great performances that make it perfectly suited for machine learning APIs.

The [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&utm_campaign=l80a8332-aaaf-11eb-bcbc-0242ac130002) API has been developed using FastAPI, so I thought it would be interesting to write a concrete article about how to set up an NLP API with FastAPI that is serving spaCy models for NER:

[https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/](https://juliensalinas.com/en/machine-learning-nlp-api-production-fastapi-nlpcloud/)

I'd love to have your feedback on this guys. Are you also FastAPI users? Did you notice caveats I'm not aware of? Or can you think of better tools for machine learning APIs?

Thanks!"
yusuf-bengio,MachineLearning,1616429243.0,[D] Jürgen Schmidhuber - But what has he published recently (Ep. 2),"Because so many people requested it last time, here it is:

**Jürgen Schmidhuber** has worked on some truly revolutionary ideas and algorithms back in the early 90s. Among other things, he proposed the artificial curiosity (AC) framework, an early prototype of what is now known as generative adversarial networks (GANs).

In 1997 Together with his student Hochreiter he proposed the infamous LSTM architecture, which is still considered the de-facto standard RNN architecture today. 

But since then, he only sporadically published a few interesting papers.

When comparing his high status and generous funding with other machine learning groups led by famous researchers, his research output is relatively low.

So my questions is, **what has he published recently?**"
CvikliHaMar,MachineLearning,1618568912.0,[D] Pushforward vs Pullback algorithms,"Hello guys,

For a long time I skipped pushforward but I just read it here: [https://juliadiff.org/ChainRulesCore.jl/dev/](https://juliadiff.org/ChainRulesCore.jl/dev/) and it was very clear description. The terms here are frule, rrule, I tried to google in the internet but I didn't get why don't we use pushforward for gradient computation.

Why are we using pullback, is it faster? What are the downside of using pushforward to compute gradients does anyone know?"
flippy98026,MachineLearning,1616699703.0,[N] Common Application Framework (CAF) for Synthetic Data Generation by Rendered.ai at GTC21,"**CAF supports containerized simulation applications** with all the tools needed to produce, analyze, and integrate synthetic data into a computer vision project. This includes tools for scenario generation, compute management, collaboration, analysis, data management, GUI and API interfaces, chain-able agent factories, and standard and custom scene modifiers. The Framework (CAF) hosts both Rendered.ai and **3rd party simulation and synthetic data engines** (NIR/WMIR/Thermal), RADA,SAR, Satellite-EO and Xray sensors. #Nathan Kundtz , #[rendered.ai](https://rendered.ai) \#GTC21

[Synthetic Data](https://preview.redd.it/bnbc7hvd48p61.png?width=155&format=png&auto=webp&s=1e20ccd5711e16f6766aaa1cd0e9a9aa27dbcab0)"
mgalarny,MachineLearning,1619030679.0,[P] Attention Nets and More with RLlib’s Trajectory View API,"Hey Everyone,

I wanted to share two new features now stable in [RLlib](https://docs.ray.io/en/master/rllib.html): Support for Attention networks as custom models, and the “trajectory view API” ([RLlib](https://docs.ray.io/en/master/rllib.html) is a popular reinforcement learning library that is part of the open-source [Ray project](https://github.com/ray-project/ray)).

There’s [a blog post](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65) with code snippets about what this is, how it works, and all that, but I want to share the motivation/importance of this here as well.

# Motivation

The goal of any RL algorithm is to train the neural network, such that its action choices become optimal with respect to a reward signal, which is also provided by the environment. We often refer to our neural network function as the *policy* or π: `action = π(observation) (Eq. 1)`

In the common case above (Eq. 1), observation is the current “frame” seen by the agent, but more and more often we’re seeing RLlib users try out models where this isn’t enough. For example:

* In “frame stacking”, the model sees the last n observations to account for the fact that a single time frame does not capture the entire state of the environment (think of a ball seen in a screenshot of a game and we wouldn’t know whether it’s flying to the left or right).

https://preview.redd.it/p3kr4huknku61.png?width=770&format=png&auto=webp&s=913e0486c74bf903117a81f6043310ddc51e6627

`action = π(observations[t, t-1, t-2, ..]) (Eq. 2)`

* In recurrent neural networks (RNN), the model sees the last observation, but also a tracked hidden state or memory vector that has previously been produced by that model itself and is altered over time:  
`action, memory[t] = π(observation[t], memory[t-1]). (Eq. 3)`
* Furthermore, in attention nets (e.g. transformer models), the model sees the last observation and also the last N tracked memory vectors:  
`action, memory[t] = π(observation[t], memory[t-n:t]) (Eq. 4)`

RLlib’s **new trajectory view API** that makes these complex policy models possible (and fast). Building on that functionality, we’ll show how this enables [efficient attention net support in RLlib.](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)

Let me know if you have any thoughts or questions!"
mediaml,MachineLearning,1619099948.0,[D] What are good places to advertise PhD and post-doc positions in ML?,"I  am principal investigator in a European research group focused on applied machine learning. I am interested in experiences of what are good places to advertise PhD and post-doc positions. Specifically,

* PhD students, where do you go to look for machine learning PhD and post-doc positions?
* Faculty, what have you found are good venues to advertise PhD and post-doc positions?

In your experience, is it worth investing money for ads in a portal like [findaphd.com](https://findaphd.com/), or is advertising on group webpage + social media promotion the way to go?

I  would in particular like to reach female candidates and applicants from other underrepresented groups in AI with adequate qualifications.  Any insights on how to advertise to reach those groups are also very  welcome!

EDIT: Some commenters seem to have misconstrued that I am looking to exclusively hire female candidates. This is of course not the case. I want to increase the pool of female applicants and people from other underrepresented groups to increase the probability of finding excellent candidates from those groups. This is partially because I believe that building a diverse research group leads to a more interesting and healthier lab life, and partially because I strongly believe the research concerning [gender bias](http://curt-rice.com/2017/09/23/wheres-the-evidence-a-little-science-about-bias-and-gender-equality/) and [affirmative action](https://www.gse.harvard.edu/news/uk/18/07/case-affirmative-action)."
Andy_Reds,MachineLearning,1619795835.0,[2104.14421] What Are Bayesian Neural Network Posteriors Really Like?,
OnlyProggingForFun,MachineLearning,1618588775.0,"[News] Create 3D Models from Images! AI and Game Development, Design... GANverse3D & NVIDIA Omniverse","Omniverse, NVIDIA, (2021): [https://www.nvidia.com/en-us/omniverse/](https://www.nvidia.com/en-us/omniverse/)

Zhang et al., (2020), ""IMAGE GANS MEET DIFFERENTIABLE RENDERING FOR INVERSE GRAPHICS AND INTERPRETABLE 3D NEURAL RENDERING"": [https://arxiv.org/pdf/2010.09125.pdf](https://arxiv.org/pdf/2010.09125.pdf)

GANverse3D official NVIDIA video: [https://youtu.be/0PQnrnUIBlU](https://youtu.be/0PQnrnUIBlU)

NVIDIA'S GANverse 3D blog article: [https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/](https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/)

Watch the video demo [https://youtu.be/dvjwRBZ3Hnw](https://youtu.be/dvjwRBZ3Hnw)"
davidbun,MachineLearning,1620139610.0,[N] Access Google Objectron (~1.92 TBs) in less than 5 seconds with Activeloop Hub,"&#x200B;

https://i.redd.it/y52s5u8594x61.gif

Hi r/machinelearning,

My team at [Activeloop](https://activeloop.ai/?utm_source=social&utm_medium=reddit&utm_campaign=objectron) partnered with Google to make Google Objectron available in under ±5 seconds (per dataset category). Google Objectron is one of Google’s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4M annotated images).

All you need to do to get started is:

Install [Hub, the open-source package](https://github.com/activeloopai/Hub) that converts computer vision datasets into cloud-native NumPy-like arrays and enables a few nifty features like streaming to PyTorch and TensorFlow, dataset version-control, collaboration, etc.

`pip install hub`

And then Load the data for the bike category.

`import hub`

`bikes = hub.Dataset(""google/bike"")`

In ±5 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). The whole dataset ( \~1.92 TBs +metadata) would take about 33 seconds to access.

Thanks to Hub, you can visualize Google Objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2Fshoe&utm_source=social&utm_medium=reddit&utm_campaign=objectron)).

More details on using [Objectron with Hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&utm_medium=reddit&utm_campaign=objectron).

\*Please make sure that you are using latest update for hub.

https://i.redd.it/e1179xa394x61.gif

We’re working to get more datasets on the platform and improve [github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) as a tool. Let us know if you have any feedback - we’d like to deliver maximum value to the community.

Thanks,

DavitBun"
Tea_Pearce,MachineLearning,1618352493.0,[R][P] Counter-Strike from Pixels with Behavioural Cloning,"https://reddit.com/link/mqd1ho/video/l2o09485n0t61/player

A deep neural network that plays CSGO deathmatch from pixels. It's trained on a dataset of 70 hours (4 million frames) of human play, using behavioural cloning.

ArXiv paper: [https://arxiv.org/abs/2104.04258](https://arxiv.org/abs/2104.04258)

Gameplay examples: [https://youtu.be/p01vWk7uMvM](https://youtu.be/p01vWk7uMvM)

""Counter-strike Deatmatch with Large-Scale Behavioural Cloning""

Tim Pearce (twitter [https://twitter.com/Tea\_Pearce](https://twitter.com/Tea_Pearce)), Jun Zhu

Tsinghua Unviersity | University of Cambridge"
windbreaker14,MachineLearning,1616513372.0,[D] Disappointed with the reviews in ICML21.,"I submitted a paper in ICML21. I received reviews 3 days ago.

I was supposed to receive 4 reviews from reviewers 5,6,7,8.

However, I only received only 2 reviews from reviewers 5,8 and two are missing. 

&#x200B;

Even though, I sent a message for this to the contact e-mail [""icml2021chairs@gmail.com](mailto:""icml2021chairs@gmail.com)"", I have not received any responses.  

&#x200B;

&#x200B;

Also, the quality of 1 review is very poor. The reviewer weakly rejected my paper with the reason that the paper is not well organized (only three sentences). There were no other critical issues. I already submitted in ICLR2021 and my paper was praised as being well-written and well-organized.

&#x200B;

To sum up, I was expected to receive 4 reviews but two of them are missing and one of them is very poor. Therefore, I was quite disappointed."
proof_required,MachineLearning,1616753332.0,[D] How Facebook got addicted to spreading misinformation,"Behind paywall:

With new machine-learning models coming online daily, the company created a new system to track their impact and maximize user engagement. The process is still the same today. Teams train up a new machine-learning model on FBLearner, whether to change the ranking order of posts or to better catch content that violates Facebook’s community standards (its rules on what is and isn’t allowed on the platform). Then they test the new model on a small subset of Facebook’s users to measure how it changes engagement metrics, such as the number of likes, comments, and shares, says Krishna Gade, who served as the engineering manager for news feed from 2016 to 2018.

If a model reduces engagement too much, it’s discarded. Otherwise, it’s deployed and continually monitored. On Twitter, Gade explained that his engineers would get notifications every few days when metrics such as likes or comments were down. Then they’d decipher what had caused the problem and whether any models needed retraining.

But this approach soon caused issues. The models that maximize engagement also favor controversy, misinformation, and extremism: put simply, people just like outrageous stuff. Sometimes this inflames existing political tensions. The most devastating example to date is the case of Myanmar, where viral fake news and hate speech about the Rohingya Muslim minority escalated the country’s religious conflict into a full-blown genocide. Facebook admitted in 2018, after years of downplaying its role, that it had not done enough “to help prevent our platform from being used to foment division and incite offline violence.”

While Facebook may have been oblivious to these consequences in the beginning, it was studying them by 2016. In an internal presentation from that year, reviewed by the Wall Street Journal, a company researcher, Monica Lee, found that Facebook was not only hosting a large number of extremist groups but also promoting them to its users: “64% of all extremist group joins are due to our recommendation tools,” the presentation said, predominantly thanks to the models behind the “Groups You Should Join” and “Discover” features.

https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/"
LSTMeow,MachineLearning,1620131082.0,[N] ClearML release: v1.0 (open source MLOPs solution),"Hi r/ml,

It has been almost two years since I first posted here about our open-source solution, and here we are again (thanks for the amazing Github star bump btw, you guys rocked!).

Today, after what seems like over 9000 person-years of working, and tinkering, long into the night, I am pleased to announce ClearML has hit version 1.0.

Following quickly after the release of ClearML 0.17.5, we added the last remaining features we felt 1.0 needed. Namely multi-model support, as well as improved batch operations.

It has been a long and sometimes bumpy path to get us to version 1.0. There have been good times and some mis-steps. Every journey is different, and we have been helped along the way by having one of the nicest slack communities you could ever wish for. 

This feels truly like a milestone. A birthday. I hope you will help us in celebrating it.

More details about the 1.0 release here:

[https://t.clear.ml/horrayv1](https://t.clear.ml/horrayv1)

Our  community roadmap is here:

[https://t.clear.ml/roadmap1](https://t.clear.ml/roadmap1)

Formal changelog:

[https://github.com/allegroai/clearml/releases/tag/1.0.0](https://github.com/allegroai/clearml/releases/tag/1.0.0)

Special Episode of my show (featuring a tiger):

[https://youtu.be/r2BMMDzfyA0](https://youtu.be/r2BMMDzfyA0)

Celebratory meme:

[https://twitter.com/clearmlapp/status/1389201845957054466](https://twitter.com/clearmlapp/status/1389201845957054466)

&#x200B;

PS: I still owe this community the so-called ""comparison matrix"" that most like to glance at. This was a hard thing to do, especially since our MLOps tooling ecosystem is constantly growing and changing. I'm happy to say that some form of comparison will be here this week, and I will edit this post when it arrives ;)"
hardmaru,MachineLearning,1618215799.0,"[R] A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes",
RaivoK,MachineLearning,1616590712.0,Fair Model Comparison [R],"Have not published before and need some advice. I am comparing the accuracy of different video classification models, by training each one myself in the same training environment with the same training tricks. I would like to make the comparison as fair as possible. There are a few differences between the models:

1. Each model uses a certain number of input frames. Some might use 16, some 32, and some 64.
2. Each model uses a certain spatial size. Some 114x114, others 224x224.

Now, I am not completely sure how to standardize this in my experiment. Do I train each model with the exact choices they made in their paper, or do I use the same spatial size and frame number for all models?

A reviewer might say that forcing all models to use 224x224 and 16 input frames is not fair because these values were not what some of the models were designed for.

On the other hand, if I train every model with the values that they use in their paper, some might get an advantage just from using **way** more compute.

I would like to use the same spatial size and number of frames for all of them and then compare how fast vs how accurate they are on this. Will a reviewer criticize this if I do it like this?"
ElCobo,MachineLearning,1618694910.0,[P] Wiggle-GAN: Stereoscopic camera simulation using generative adversarial neural networks,"Hello everyone,

I wanted to post my grad thesis here, in which I tried to simulate the use of a stereoscopic camera utilizing a monocular image as input. The main goal was to give a new alternative when someone wanted to create a [wigglegram](https://www.reddit.com/r/wigglegrams/), because all the ways you could achieve the effect have limitations. For example, you could use a normal camera and take many pictures with it but if the scene is moving, the effect would mimic a stop motion more than a wigglegram. On the other hand, you could use an array of cameras but that's expensive or you could use a stereoscopic camera with 3 of 4 lenses but these kinds of cameras are analogic so it takes more time to do the effect. That's the reason why we took an approach by using Neural Networks where you don't have any of these limitations.  

In this project I used a WGAN with Improved Consistency Regularization and L1 loss comparing the output and real image, you can see some the result of adjusting this values in the [github](https://github.com/RoCoBo44/Wiggle-GAN) (and the code). Also, you can see how I trained multiple some multiple solutions:

* An autoencoder (AE) which is only one net
* The Wiggle-GAN solution (two nets fighting)
* The Wiggle-GAN solution but extracting some metrics to see what it does (Wiggle-GAN no CR)

The input of the Wiggle-GAN are the images you want to move, their depth maps and the direction (left or right) and the outputs are the new images moved and their depth map estimations, so you could iterate multiple times.  If you want to try it out I made a [google colab](https://colab.research.google.com/drive/1N5HJ1geVM1ymLoE5C2jkLs3F_tQn-s-r?usp=sharing), but you have to download the checkpoints. 

The limitation of the net is that all are trained with images of 128x128 pixels except the last one in the colab that is 256x256. The code automatically changes the scale of your image to the size it needs so you don't have to change it manually.

Sometimes it doesn't generate the wiggle expected and that may be due to the depth map estimation. If that happens you could download the depth map image in the folder (/Image/Input - Test/) with the name ""{number}\_d.png"", then with an editing software you could change it and uploaded it with the same name in the same place.

If you want to try it and you have some problems with that, feel free to ask me for anything.

Lots of love"
TuSharma,MachineLearning,1616862277.0,[R] Suggestions welcome: Code smell detection using deep learning,"Recently, our paper ""Code smell detection by deep direct-learning and transfer-learning"" was accepted in [JSS](https://www.sciencedirect.com/science/article/abs/pii/S0164121221000339). You may find preprint [here](https://tusharma.in/preprints/JSS21_Code-Smell-Detection-Using-Deep-Learning_Preprint.pdf).

The key contribution of the paper is that 1) we can detect smells without explicit feature engg (e.g., metrics) esp. impl smells, 2) autoencoder works best among other options (CNN, RNN), and 3) transfer learning works in this context.

[GitHub reposiotory](https://github.com/tushartushar/DeepLearningSmells) and [autoencoder implementation article on Medium](https://tusharma.medium.com/autoencoders-for-source-code-analysis-a-use-case-4600da86b718) provide implementation details.

Would you like to provide any opinion/suggestion/feedback on this work?"
chasep255,MachineLearning,1617966989.0,[D] Loss function for audio auto encoder.,"Trying to build an auto encoder for raw audio data (time domain.)  I did this once before a while back and never got a good result.  I tried using mean squared error loss and the output really only captured a muted version of the low frequency signals.  I also tried computing the mean squared error of the fft of the output but that did not do much better.  

Now I am trying to combine WGAN-GP with some sort of other loss function.  I did this same approach last week with images where I combined a WGAN and VGG19 perceptual loss.  This produced really sharp images.  However I don't have any sort of pretrained models for raw audio (I supposed I could make one.)  So I am trying to combine the FFT loss with WGAN.  I just kicked off training and have not seen the result yet, however, I am worried that the FFT loss will prevent the model from learning the higher frequency signals since there will be a larger penalty for guessing something that sounds realistic but is say 180 degrees out of phase.  Does anyone know of a better loss function I could try for sound data?  Right now my fft loss is defined as ...

fft\_loss = tf.reduce\_mean(tf.square(tf.abs(tf.signal.stft(g, 64, 32) - tf.signal.stft(r, 64, 32))))"
Jump2Fly,MachineLearning,1618672265.0,[P][Code Release] A neural network implementation to detect whether a given video clip is in-game or not using transfer learning (can be applied to all sorts of games),
wasabi_toast,MachineLearning,1616635709.0,[Discussion] PlaidML and Intel's Iris Xe GPU support...?,"This is probably just wishful thinking, but since [PlaidML](https://github.com/plaidml/plaidml) is part of the Intel AI project and with the upcoming release of both the discrete and integrated Iris Xe units, will we soon be able to use these chips as GPUs for machine learning?

Yes, I know that these new Iris Xe units are not all that powerful at all compared to Nvidia and AMD’s offerings, but it does not look like the end of this cryptomining craze or GPU shortage is coming soon and I would really like to be able to test out some DL codes on something for work, so any sort of GPU would help tremendously."
__Julia,MachineLearning,1616598885.0,"[D] Research in the Global South: Academic conference should build bridges, not barriers","Hello,

In the last few months, I have seen ""the academic community on twitter and on r/MachineLearning "" striving to promote diversity, [defend researchers](https://www.wired.com/story/second-ai-researcher-says-fired-google/), and question some companies' approaches to silence controversy over its Ethical AI. However, I stand in awe and think of how the same community is gate-keeping research  for other groups of the same community. The same community (who organizes **International** academic conferences) asks ""high"" fees based on a **national life standards** to researchers from the Global South. ""high""  is relative to the life standards of some countries.

It is hard to forge a path without being included. It is hard for students and professors to learn, and share because they need to pay ""tax of being included"". it is hard to see the same people defending a group of this community and ignore another group. [It is hard to see most International academic conferences in  places (that bans several nationalities).](https://twitter.com/andrewyng/status/825745002831585280?lang=eu)

Today, in order for a student from the global south to make it to a good university in US/Canada/EU for PhD program, the students need at least a paper in top tier conferences. Obviously, this is not doable for most people.

There are some initiatives such as [this one](http://www.datascienceafrica.org/dsa2019addis/), but I think we -- as a community -- can do better.

If this message sounds like a rant, it's not, or maybe it is .. but I have been thinking about this after stumbling upon this tweet.

&#x200B;

>A student in Africa who wishes to start a career in Research does their best, self-learn, submit to a workshop, get accepted.     Now online conferences ask them to pay +1 month of their house rent to present their work in a zoom call.   Why are we gatekeeping research?

[https://twitter.com/hadyelsahar/status/1374699909451030542](https://twitter.com/hadyelsahar/status/1374699909451030542) 

&#x200B;

If you **review papers** for a conference, or if you organize a conference, ... you can make a difference by defending democratizing science. 

We can do better, you can do better to lower the bar for those who live in the other part of the world."
KingHultan,MachineLearning,1617799367.0,[D] Metric for computational efficiency?," I'm trying to find a metric for measuring the amount of predictive power of a model in relation to the complexity of the model. Similar to the fuel efficiency of a car.

I have my own metric, which is penalty = loss \* flops  
. The objective is to minimize the penalty of a model to find the most predictive efficient model configuration (quite useful in scenarios where computational power is severely limited, like a phone or an autonomous vehicle).

I have been trying to find a similar metric in the literature but to no avail. Do any of you know of a metric to measure the computational efficiency of a model?"
dummy-gummy,MachineLearning,1619449303.0,"[P] Pytorch reimplementation of Encode-Attend-Navigate, a RL-based TSP solver"," [https://github.com/astariul/encode-attend-navigate-pytorch](https://github.com/astariul/encode-attend-navigate-pytorch)

I recently re-implemented [encode-attend-navigate](https://github.com/MichelDeudon/encode-attend-navigate), a TSP solver based on RL. The official repo was using tensorflow 1.x, so I decided to re-implement it in Pytorch. I wanted to share it here to get some opinion :)

*You can train the model using a free GPU from Google Colab, a Colab notebook is provided in the README !*"
CireNeikual,MachineLearning,1619882590.0,"[P][R] AOgmaNeo ""Imagination"" after reinforcement learning",
sensetime,MachineLearning,1619065013.0,[R] Carbon Emissions and Large Neural Network Training (Google Brain and Berkeley paper with Jeff Dean as the last author),
nicolas-gervais,MachineLearning,1620053920.0,"[D] Half of my team knows Tensorflow, the other half PyTorch. How can we decide on which to use?","Sadly, the title says it all. We don't know what to use as we need to start collaborating together. All I can find online is ""Pytorch is more often used in research"" which is probably not even true anymore, since everyone says that because that's the only thing people are saying."
SmittyMcSmitherson,MachineLearning,1617948856.0,[D] MLCommons & inference benchmarking,Why isn’t MLCommons (MLPerf) constantly adding new results? Is there a better resource for benchmark results of various inferencing ASICs?
mistermysterioyster,MachineLearning,1616923147.0,[D] Paper Reading Group #015 - DERAIL: Diagnostic Environments for Reward And Imitation Learning. (Link to full slides in comments!),
PetarVelickovic,MachineLearning,1619684755.0,"[R] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics and Gauges (""proto-book"" + blog + talk)","Hi everyone,

I am proud to share with you the first version of a project on a geometric unification of deep learning that has kept us busy throughout COVID times (having started in February 2020).

We release our 150-page ""proto-book"" on geometric deep learning (with Michael Bronstein, Joan Bruna and Taco Cohen)! We have currently released the arXiv preprint and a companion blog post at:

[https://geometricdeeplearning.com/](https://geometricdeeplearning.com/)

Through the lens of symmetries, invariances and group theory, we attempt to distill ""all you need to build the neural architectures that are all you need"". All the 'usual suspects' such as CNNs, GNNs, Transformers and LSTMs are covered, while also including recent exciting developments such as Spherical CNNs, SO(3)-Transformers and Gauge Equivariant Mesh CNNs.

Hence, we believe that our work can be a useful way to navigate the increasingly challenging landscape of deep learning architectures. We hope you will find it a worthwhile perspective!

I also recently gave a virtual talk at FAU Erlangen-Nuremberg (the birthplace of Felix Klein's ""Erlangen Program"", which was one of our key guiding principles!) where I attempt to distill the key concepts of the text within a \~1 hour slot:

[https://www.youtube.com/watch?v=9cxhvQK9ALQ](https://www.youtube.com/watch?v=9cxhvQK9ALQ)

More goodies, blogs and talks coming soon! If you are attending ICLR'21, keep an eye out for Michael's keynote talk :)

Our work is very much a work-in-progress, and we welcome any and all feedback!"
prestodigitarium,MachineLearning,1617931758.0,"[P] Gourdian Free Dataset Download: EPA Air Quality System Daily CO, NO2, O3, SO2 Concentrations since 1980","Howdy!

We've just added another few free dataset downloads to our project (Gourdian), this time from the EPA Air Quality System, daily levels of the following pollutants from measuring stations throughout the US, going back to 1980:

* Carbon Monoxide (CO): https://gourdian.net/g/eric/epa_aqs.co_daily_summary
* Nitrogen Dioxide (NO2): https://gourdian.net/g/eric/epa_aqs.no2_daily_summary
* Ozone (O3): https://gourdian.net/g/eric/epa_aqs.ozone_daily_summary
* Sulfur Dioxide (SO2): https://gourdian.net/g/eric/epa_aqs.so2_daily_summary

This dataset is only updated twice per year.

Why should you care about these things? Well, they're pretty nasty pollutants, which frequently affect human health, especially over long exposure periods. Perhaps concentrations should affect where we build homes more than they do, for example? They're also useful proxies for various types of human activity.

**Carbon monoxide** can be deadly in high concentrations, as it binds to hemoglobin, and makes it so that one's blood can't carry as much oxygen. Created via combustion of fossil fuels.

**Nitrogen dioxide** is one of the results of road traffic and other fossil fuel combustion, is one of the precursors for other harmful pollutants such as ozone and particulates, and plays a role in the formation of acid rain (it becomes nitric acid). It's also generally correlated with exposure to other byproducts of road traffic, many of which have been linked to respiratory illnesses and systemic inflammation, which can lead to a whole host of other health issues.

**Ozone** is a powerful oxidant, which makes it so that it can cause damage to respiratory tissues. It also attacks various polymers like rubber, eventually causing it to crack. It's formed primarily from photochemical reactions between nitrogen oxides (like NO2) and volatile organic compounds (VOCs). High concentrations aren't limited to urban areas, however, and it can travel hundreds of miles downwind from the source.

Finally, **sulfur dioxide**. Formed largely by the combustion of fossil fuels with high levels of sulfur, as well as volcanic activity. It's a precursor to acid rain (it becomes sulfuric acid). Sulfur dioxide is known to be at least mildly toxic, and can be hazardous in high concentrations. Long term exposure to low concentrations can also be problematic. The amount of sulfur in fossil fuels vary widely by type - automotive gasoline has much less than bunker fuel commonly used by container ships, for example.

If you'd like to download any part of these, you can filter down to the parts you care part via the lat/long filter (button on the map to select area) or by year, via sliders. As you do this, the download size noted on the download button in the upper right should change. When you've got the part you want, click download, and you should get a CSV.

A bit about our goals and what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Feedback welcome! If there are any datasets that you'd like to see added, let us know!"
weifz,MachineLearning,1620356445.0,[D]A question about causal discovery and causal inference,"Hi there,

I know causality consists of causal discovery and causal inference, and I wonder if there exists some order between these two components, e.g., should we konw the causal structure(causal discovery) before causal inference?"
Electronic-Lie4077,MachineLearning,1617117959.0,[P] OpenAI CLIP: Connecting Text and Images Gradio web demo,"&#x200B;

https://reddit.com/link/mgiimg/video/qstepjl4o6q61/player

web demo for open ai clip for visual classification, try it out here: [https://gradio.app/hub/AK391/CLIP](https://gradio.app/hub/AK391/CLIP)"
prakhar21,MachineLearning,1620325388.0,[D] graph2vec: Learning Distributed Representations of Graphs | ML with Graphs (Paper Walkthrough),"Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. graph2vec proposes a technique to embed entire graph in high dimension vector space. It is inspired from doc2vec learning approach over graphs and rooted subgraphs.

Paper Walkthrough: https://youtu.be/h400_OMWNLo"
TartarQ,MachineLearning,1620576418.0,[Discussion] Defining optimal false-positives and false-negatives balance with a cost function,"\#roc\_curve  

https://preview.redd.it/80m5ee9yb4y61.png?width=1382&format=png&auto=webp&s=79aa05882369f6e1e2449679e062ae3c26634aed

&#x200B;

https://preview.redd.it/sheqqo2zb4y61.png?width=1548&format=png&auto=webp&s=e9904a82673b9d340b9db6a176c42501ea831279

&#x200B;

https://preview.redd.it/qifab1rzb4y61.png?width=932&format=png&auto=webp&s=4c2ae6f5f80a014ac60d0def347e6d333618de80"
False-Grape7566,MachineLearning,1620578000.0,[P] Keytotext Convert Keywords to Large texts,"Hello, Presenting Keytotext: Keytotext is an NLP model that can convert keywords to sentences and larger texts. It is built using the T5 model. Keytotext has a PyPI installation and on-demand inference API too. It also features a UI built using streamlit and a GPU-enabled colab notebook for easy usage! Please do check it out on GitHub: [https://github.com/gagan3012/keytotext](https://github.com/gagan3012/keytotext) Please to star 📷 if you liked the work!"
CC_sciguy,MachineLearning,1618532149.0,[D] Anyone have experience running pytorch with AMD GPUs,"In March pytorch mainstreamed ROCm support meaning that AMD GPUs could be viable for DL workflows. I have seen some compelling benchmarks for AMD mi100 GPUs, but nothing that does end-to-end tests on real deep learning workflows.

Does anyone have experience running pytorch with AMD gpus? Are the speeds really better than top of the line nvidia gpus? And more importantly, is the support as good as nvidia at this point, or is the SW side still maturing?"
Philipp,MachineLearning,1618835247.0,[P] [D] Using GPT-3 to write short stories,"Hopfully this is of interest to some, I created an ongoing [series](https://aiwrotethis.substack.com/) of short stories on Substack which were co-written wih the GPT-3 AI. Here's [a video](https://www.youtube.com/watch?v=8fWK0k7aSRs) showing some approaches on how to move a story in a certain direction.

Would also love to discuss this topic of AI fiction writing. I'm utterly fascinated by the use of GTP-3 as a creative tool."
PresentCompanyExcl,MachineLearning,1618111444.0,[mocov3] An Empirical Study of Training Self-Supervised Visual Transformers,
setzRFD,MachineLearning,1617646105.0,[Discussion] What metaheuristics are interesting as an exercise to expand my knowledge of AI/ML?,"I'm interested in metaheuristic searches, specifically how they can be interwoven with neural networks with some interesting results. I've worked with particle swarms and genetic algorithms to train a neural network and I want to try some others since it's a fun exercise.

I've looked into ant colony and bee colony optimization metaheuristics and they seem to only work on problems that can be reduced to graph traversal. Are there other metaheuristics you can think of that are more suited to optimizing error functions?"
Ziinxx,MachineLearning,1618173390.0,[P] Using StyleGAN2-Ada and Pixel2Style2Pixel to Become a Professional Artist,
xdtolm,MachineLearning,1617563498.0,[P] Nvidia A100 and AMD MI100 benchmarks - join VkFFT panel on Nvidia GTC 2021,"Hello! I am the creator of [VkFFT](https://github.com/dtolm/VkFFT) \- Vulkan/CUDA/HIP Fast Fourier Transform library. I would like to invite you to the [GTC 2021](https://gtc21.event.nvidia.com/) panel of VkFFT, which will happen on April 13th at 4 PM CEST in the Higher Education and Research category. It will be focused on implemented optimizations and how to create cross-platform code that can scale from Raspberry Pi 4 to HPC GPUs like Nvidia A100. The session will also compare Vulkan, CUDA and HIP compute platforms on Nvidia A100 and AMD MI100 GPUs.

In this post, I would like to give you a sneak peek at a part of the talk regarding VkFFT/cuFFT/rocFFT performance comparison in single precision in 1D batched FFT test of all systems from 2 to 4096, representable as an arbitrary multiplication of 2s, 3s, 5s, 7s, 11s and 13s. The bandwidth is calculated as total memory transferred (2x system size) divided by the time taken, so the higher - the better.

[Nvidia A100 results](https://preview.redd.it/jook8yvtf7r61.png?width=10000&format=png&auto=webp&s=2ead6c986ae71e506fc0c717e8ec005e4768577b)

[AMD MI100 results](https://preview.redd.it/8okf9jevf7r61.png?width=10000&format=png&auto=webp&s=e70ca0b6abd5d39a70f760c483c8192d56bd1fab)

The talk will also cover double-precision, multidimensional tests and their analysis, so I would really appreciate it if you can check it out!"
lfolle,MachineLearning,1620324878.0,[R] Deep learning methods allow fully automated segmentation of metacarpal bones to quantify volumetric bone mineral density,Check out our latest research on applying deep learning-based segmentation networks in the field of rheumatology: https://www.nature.com/articles/s41598-021-89111-9 [Open access]
seagullonthetop,MachineLearning,1619320683.0,Machine learning for a theoretical physicist [D],"Hi,

My background is in theoretical (high energy) physics and cosmology. I'm comfortable with Mathematica, and have used MATLAB and Python in the past, though not for large-scale projects. I'm interested in learning machine learning, in a way that is suited for someone with my background experience. Has anyone trod a similar path? What are some useful resources, books, online courses and already-done projects/examples (preferably by someone in physics) that I can run through myself to get a hands-on, practical experience with ML?

Thanks"
chimp73,MachineLearning,1620077927.0,[D] An RL agent based on a large NN that one-shot learns in single SGD updates. Recall and planning through generalization from one-shot learned predictions and policy updates,"[Yesterday](https://www.reddit.com/r/MachineLearning/comments/n2ts0l/d_how_far_can_we_get_with_oneshot_learning/), I explored the consequences if the scaling hypothesis were true, in particular what if upscaling gives us one-shot learning in single SDG updates as well as (super)human-level inference, prediction and generalization abilities.

Today, I've been thinking how a concrete implementation of this approach could look like. Ideas are cheap, but here it goes:

The model I've come up with is simply a fully-connected, wide VAE that at each step performs one inference and then produces two Gaussian samples and two predictions based on the samples. The first sample is used to predict the future, and the second sample is used to predict its own prediction (based on the first sample). As a consequence, the model would one-shot learn both the thought and sensory experience to have occurred.

Let x\_t be an N x T tensor containing T time steps (say 2 seconds sampled at about 10 Hz) of N = S + P + 1 features, where S is the length of the sensor vector s, P is the number of motor neurons p (muscle contractions between 0 and 1, i.e. sigmoidal) and one extra dimension for the experienced reward r. Let the first prediction be x'\_t = VAE(concat(x\_{t-1}, x''\_{t-1})) and the second prediction x''_t corresponding to the second sample from the VAE produced in the same way. Then, minimize the loss by SGD: (x_t - x'_t)^2 + (x'_t - x''_t)^2 + KLD(z') + KLD(z'') + p'_t(p'_t - α∙r_t)^2 + p''_t(p''_t - α∙r''_t)^2 + λ||p'_t||_1, where KLD is the KL regularizer for each Gaussian sample, α is a scaling constant for the reward such that strong absolute reward is > 1 and ||p'_t||_1 is a sparsity prior on the policy to encourage competition between actions. The two RL losses simply punish/reinforce actions that coincide with reward (though a slight temporal delay would likely help). The second loss acts on the imagined policy and imagined reward.

I'm unsure the thoughts can actually become goal-directed this way and I'd be extremely surprised if this actually works, but it is fun to think about."
Superb-Drawer5214,MachineLearning,1618760402.0,[D]What universities are considered top10 and top20 for ML/CV/NLP?,"It seems top4 are Berkeley, MIT, Stanford, CMU, but I am not sure what universities are considered top 10 and top 20 for ML/CV/NLP... Whenever some person mentions that he or she has applied to the top 20 schools for ML, what schools is he or she referring to?"
grid_world,MachineLearning,1617303683.0,[D] Quantization in Deep Learning,"For Deep Learning model compression, the standard steps appear to be: pruning, clustering and quantization. I have experimented and implemented the first two steps in detail. Now, I am interested in learning about quantization techniques applied to deep learning for their compression. Can you point me to a nice resource (research paper, blog, tutorial, video, etc.) as a starting point?

Thanks!"
m1900kang2,MachineLearning,1617071083.0,[R] Predicting Multiple Sclerosis from Gait Dynamics Using an Instrumented Treadmill – A Machine Learning Approach,"This paper by researchers from University of Illinois at Urbana-Champaign looks into ML being able to spot gait problems in individuals with multiple sclerosis.

\[[1-Min Presentation Video](https://crossminds.ai/video/predicting-multiple-sclerosis-from-gait-dynamics-using-an-instrumented-treadmill-a-machine-learning-approach-60626a18eb4e66fab2491414/)\] \[[Paper Link](https://ieeexplore.ieee.org/document/9311191)\] 

**Abstract:** Multiple Sclerosis (MS) is one of the most common neurological conditions worldwide whose prevalence is now greatest among people 50-60 years of age. While clinical presentations of MS are highly heterogeneous, mobility limitations is one of the most frequent symptoms. The aims of this study were to examine MS and disability related changes in spatiotemporal and kinetic gait features after normalization; and evaluate the effectiveness of a gait data-based machine learning (ML) framework for MS prediction (GML4MS). Methods: In this study, gait data during self-paced walking on an instrumented treadmill from 20 persons with MS and 20 age, weight, height and gender-matched healthy older adults (HOA) were obtained. We explored two normalization strategies, namely size-N (standard body size-based normalization) and regress-N (regression-based normalization using scaling factors derived by regressing gait features on multiple subject demographics) to minimize the dependency of derived gait features on the subject demographics; and proposed GML4MS, a ML based methodology to classify individual strides of older persons with MS (PwMS) from healthy controls, so as to generalize across different walking tasks and subjects after gait normalization. Results: We observed that regress-N improved the accuracy of identifying pathological gait using ML when compared to size-N. When generalizing from comfortable walking to walking while talking, gradient boosting machine achieved the optimal subject classification accuracy and AUC of 94.3% and 1.0, respectively and for subject generalization, a multilayer perceptron resulted in the best accuracy and AUC of 80% and 0.86, respectively, both with regress-N normalized data. Conclusion: The integration of gait data and ML to predict MS may provide a viable patient-centric approach to aid clinicians in disease monitoring and relapse treatment. This work is the first attempt to employ and demonstrate the potential of ML for this domain. Significance: The results of this study have future implications for the way regression normalized gait features may be clinically used to design ML-based disease prediction strategies and monitor disease progression in PwMS.

&#x200B;

[Butterfly Diagram of the Center of Pressure trajectory during a subject's walk](https://preview.redd.it/9vqlbv5xs2q61.png?width=620&format=png&auto=webp&s=9678ea41aa31370bcf4be7088b4c77fcae234704)

**Authors:** Rachneet Kaur, Zizhang Chen, Robert Motl, Manuel Enrique Hernandez, Richard Sowers (University of Illinois at Urbana-Champaign)"
fripperML,MachineLearning,1617772778.0,[D] Is it always recommended to scale features to be predicted the same way as the training data was scaled?,"I don't know if this has been asked before... And the question might seem somewhat silly, so let me start briefly with the standard approach:

* The general advice I have always seen is that, if you decide to scale your data (which, in general, is recommended), you should fit\_transform a scaler during the training process and then apply the same scaler to the data to be predicted.
* This makes a lot of sense, and allows the model to predict from one only sample to whatever number of samples.

I am not questioning that. But I have the feeling that sometimes it is not the best approach. Let me give you an example:

* Let's suppose that we are dealing with a model that predicts probability of default of a person using economic features, like the annual income, amount of debts, etc.
* Let's suppose that the only preprocessing we make is scaling of the data.
* Let's suppose that we trained the model with data from 2018 and we are still using this model in production.

In this setting, it is very likely that there is some kind of drift, worsening the performance, and so one could retrain the model using more recent data. But my intuition is that, at least partially, one could reduce the problem by scaling the features of the instances to be predicted using the information of the year in course. I think so because those kind of economic features are all affected by inflation, so, for example, an annual income of 80K in 2018 is comparable to an annual income of, say, 85K in the present year, and scaling with a ""moving ruler"" can be better than scaling with a ""fixed ruler"".

This approach could be useful whenever the feature has a mean that changes in time. What do you think? Do you think it does make sense?

Of course, one limitation is that it is not easy to implement (the only way I can think of is when you use your model to make batch predictions for big enough chunks, so that scaling the features according to the distribution of those chunks is meaningful)."
jnbrrn,MachineLearning,1616769447.0,[R] Baking Neural Radiance Fields for Real-Time View Synthesis,"Real-time photorealistic neural rendering in your browser. [https://nerf.live](https://nerf.live)

Live demos: [https://nerf.live/#demos](https://nerf.live/#demos)

Paper PDF: [https://nerf.live/#baking\_neural\_radiance\_fields\_for\_real\_time\_view\_synthesis.pdf](https://nerf.live/#baking_neural_radiance_fields_for_real_time_view_synthesis.pdf)

Explainer video: [https://www.youtube.com/watch?v=5jKry8n5YO8](https://www.youtube.com/watch?v=5jKry8n5YO8)"
hardmaru,MachineLearning,1616737639.0,[R] Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields,
SanjivGautamOfficial,MachineLearning,1619603499.0,[P] MLOverflow - A Webapp for ML/DL,"Hello ML/AI enthusiasts,

I created a small (minimalistic design ) webapp for Machine Learning feeds, papers and events and this webapp literally has **Feeds**, **Events** and **Papers** section.

Here is the link: [https://mloverflow.com](https://mloverflow.com/) (I am currently adding contents, but having more people engaged on it would be delightful)

Let's check the features of our webapp if you could spare a moment:

**Feeds** is where you share your medium articles or anything that you find interesting over internet related to ML/AI. May it be linkedin posts about ML/DL. Even reddit link about your project you did related to AI/ML. You made an impression detection app and uploaded in youtube? How about sharing it on this website? Any medium blogs you wrote after reading a paper? Share the **link**.

**Events** is where you share **link** of events related to ML/AI.

**Papers** is where we discuss research papers. People can comment different blogs/videos **links** which will be useful for other users to come.

**COMMENT SECTION:**

We do have comment section for Feeds and Papers, so it would be easier for people to post related ML/DL posts.

There'll be 4 types of comments. **Audio, Video, Text, Explain**.

Audio, Video,Text will contain links of websites that will have audio, video or blog post respectively. The **links** not the actual video/audio. **Explain** is where you clarify/write about things there in comment section where you don't need help with links.

**MOTIVATION:**

What's the point of creating a site when we already have [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) subreddit, Medium for article sharing and of course when we can google search?

I believe this will make navigation for things easier and having a central place for ML/DL related posts would make it convenient for people interested in ML/DL to further consolidate their understanding.

**Miscellaneous**:

I am not really a great developer, but I think it might help few enthusiasts out there as I am one of them.

Note: *There is no option for uploading audio/video/gifs/image as of now as I don't see a point of it. But if we need it in future, I would be happy to add them as well.*

P.S: There is a suggestion/report section. So any bugs or any features you think I need to resolve/add, kindly tell me. You can log in and go to [https://mloverflow.com/report](https://mloverflow.com/report) and write it down.

Thank you for reading till here. Cheers!"
mikegartrell,MachineLearning,1617917905.0,[N] Call for papers: KDD 2021 Workshop on Bayesian Causal Inference for Real-World Interactive Systems,"[https://bcirwis2021.github.io](https://bcirwis2021.github.io)

August 14 - 18, 2021 (final workshop date TBD)

&#x200B;

\---

Submission deadline: May 10, 2021, anywhere on Earth

Format: 3 page extended abstract + references + appendices, ACM Proceeding Template

Submission website: [https://cmt3.research.microsoft.com/BCIRWIS2021](https://cmt3.research.microsoft.com/BCIRWIS2021)

\---

&#x200B;

Increasingly we use machine learning to build interactive systems that learn from past actions and the reward obtained. Theory suggests several possible approaches, such as contextual bandits, reinforcement learning, the do-calculus, or plain old Bayesian decision theory. What are the most theoretically appropriate and practical approaches to doing causal inference for interactive systems?

We are particularly interested in case studies of applying machine learning methods to interactive systems that *did* or *did* *not* use Bayesian or *likelihood* based methods, with a discussion about why this choice was made in terms of practical or theoretical arguments. We also welcome submissions in the following areas:

* Offline evaluation of recommender and interactive systems.
* Comparison of Bayesian, off-policy and other heuristic approaches for offline metrics.
* Probabilistic approaches applied to contextual bandits and reinforcement learning approaches.
* Probabilistic approaches to incrementality and attribution.
* Non-Bayesian approaches and trade-offs with Bayesian/Likelihood approaches.
* Bayesian methods in a production environment.

&#x200B;

Organizers

* Nicholas Chopin (ENSAE)
* Mike Gartrell (Criteo AI Lab)
* Dawen Liang (Netflix)
* Alberto Lumbreras (Criteo AI Lab)
* David Rohde (Criteo AI Lab)
* Yixin Wang (UC Berkeley)"
TrainYourMonkeyBrain,MachineLearning,1616507632.0,[D] Polyline prediction literature,"I've encountered multiple occasions where I had the need to predict a line or curve, often represented as a polyline, in a 2D or 3D image. So far I've always obtained these by postprocessing standard CNN predictions like segmentations and the like. 

I was wondering, is there any existing research in CNN-based models that directly predict polylines? I have been unable to find any. I can imagine that a dedicated loss function based on somehow minimizing the area between a predicted and a ground truth curve would work best, but I'm running into the issue that it's difficult to determine the points that define each of the line segments, aka how many points should one predict, and where should they be placed best? 

Does anyone have literature or experience for this?"
pinter69,MachineLearning,1619361089.0,[R] Introduction to Photogrammetry and Points2Surf (ECCV 2020) - Link to free zoom lecture by the author in comments,
CarlJohnson2222,MachineLearning,1618958279.0,"[D] How long does it take to publish a research paper? How long is the the time from the moment you think of an idea, to the moment you submit the research paper to a journal (not including the time it takes for the journal to approve your paper)?","As the question states above, for those of you who know more about research, how long does it take to publish a paper? What is a normal amount of time to be able to conduct the research and then fully finish writing the paper and submit it?"
p_ranav,MachineLearning,1616342193.0,[P] Monocular Depth Estimation - I ran a number of fairly well-known pre-trained models and looked at the average,
DaredevilMeetsL,MachineLearning,1617157406.0,[D] What would you advice to make the most of a virtual conference?,"I am (presenting at and) attending a conference (International Symposium on Biomedical Imaging, a venue dedicated for medical and biological imaging and analysis) next month virtually. It is not a huge conference ([technical program](https://embs.papercept.net/conferences/conferences/ISBI21/program/ISBI21_ProgramAtAGlanceWeb.html)) like CVPR, NeurIPS, etc. (MICCAI is much larger for medical imaging) because the focus of the venue is quite niche.

&#x200B;

As a PhD student, such conferences could be quite useful for forging professional contacts and finding potential collaborators. What advice/suggestions would you have to make the most of my experience from a virtual conference? Ideally, I would like this to be the opportunity to networks and build connections. Any advice/suggestions would be appreciated. Thank you.

&#x200B;

* A very common advice is to look at the conference program beforehand to select some of the most relevant works. But this is limiting in that I do not know much about the work merely from the title and the abstract.
* Another (minor) inconvenience is the time zone: the event is in CET which makes the conference program from \~4 AM to \~9 AM for me."
jj4646,MachineLearning,1618637941.0,[D] Is there such a theorem in machine learning?,"Is there an official theorem in statistics and machine learning that states: ""for a machine learning algorithm to ""work"" (generalize well), the test data must be ""similar"" to the training data?""

I know this is common sense, but I have been searching all over the internet to see an official reference of this principle. Is this formally codified somewhere? Is this referenced in the literature of PAC theory, which in turn, provides the basis for all machine learning algorithms?"
ZenDragon,MachineLearning,1617152883.0,[Discussion] What do you think would happen if a GPT model was continuously fine-tuned on its own I/O?,"**Edit: The AI does not simply talk to itself. It talks to a human and the responses from *both* parties are used for further training. (In batches, paired together with symbols to indicate who is saying what)**

This would be quite computationally expensive obviously but suppose I've got the hardware and a lot of time and patience. Suppose I have daily conversations with the system, and the input/output of every interaction is immediately fed back in as training. (And we set the learning rate higher than normal because strong fitting would actually be good in this case)

Since natural conversation with a single person would take a long time to generate an appreciable volume of text, I was thinking maybe there'd also be ""introspective"" periods of training where it generates both sides of the conversation by itself, pretending there's still a human participant.

We could even add a simple ""judgement"" pass where the weight of fine-tuning for the AI's output is adjusted depending on how the human responds.

Worth a try or completely stupid idea?"
Ok_Reality2341,MachineLearning,1617487648.0,[D] How large can you go with CNN input images?,"How large can you reasonably go with CNNs in regards to the input image? I am training on a week worth of accelerometer signal data (at 100 data points per second) converted to a spectrogram. So, the higher the resolution of the spectrogram the better.

Is something like 6000x100 feasible?

I tried on a 600x100 image and it wasn't enough resolution to pick up the details of the full week. Any papers that relate to using CNN on large images is also a plus."
ubcengineer123,MachineLearning,1618605741.0,[R] Graphs for training loss per epoch in publications,"Hello all,

I'm researching and writing a paper on using ML (CNN architectures) in semantic segmentation for medical imaging. I'm hoping to create figures that look something like [This Graph](https://www.researchgate.net/profile/Mohammad-Pashaei-4/publication/339987654/figure/fig13/AS:870150437883905@1584471439655/Average-loss-per-epoch-for-training-and-validation-steps.ppm), but for me (70,000 training image patches), the epoch goes to a steady-state value after \~2-3 epochs? 

Has anyone encountered this before? Should I lower the batches per epoch? That won't be accurate because each epoch should consist of the full training dataset. 

Few more small details:

* I'm using weighted binary cross entropy to account for positive - negative label imbalance
* Default Adam optimizer

Can anyone suggest possible solutions or why it's happening? It's not a bad thing per se because the model works, but I won't be able to get nice looking figures if I reaches steady-state so quickly.

Thank you!"
zawerf,MachineLearning,1617278796.0,[D] Cheating Detection (from a recent a Google Code Jam competition),"Problem statement can be found here: https://codingcompetitions.withgoogle.com/codejam/round/000000000043580a/00000000006d1155

Rough summary is: you have 100 players and 10000 questions, each player has a skill level `S_i` and each question has a difficulty `Q_j`. `S_i ~ Uniform(-3, 3)` and `Q_j ~ Uniform(-3, 3)`. A player with skill `S_i` will answer a question with difficulty `Q_j` correctly with probability `sigmoid(S_i - Q_j)`. Except for one cheater who will answer correctly with probability `0.5 * sigmoid(S_i - Q_j) + 0.5`. You have the results of what each player answered for each question. Identify the cheater.

To pass you only need to find the cheater 86% of the time, but it's possible to do much better.

Since this was a coding competition you were expected to come up with a simple heuristic (see the ""Analysis"" tab for the intended solution).

But I'm curious how stats/ML people would approach this instead."
allasamhita,MachineLearning,1617799679.0,[P] Accelerate Your Machine Learning and Data Workflows to Production using Flyte!,"Imagine the pain behind orchestrating your ML workflows. You'd have to take care of maintaining the model artifacts, caching the results, facilitating backtracking to an error source, sharing your results with your team members, containerizing the jobs, and ensuring scalability all the time. This isn't cheap. A lot has to happen in the backend to ensure that your ML workflows are running seamlessly. If you're a part of a more prominent company, it's very much vital to ensure that yours is a fool-proof ML service spanning multiple teams. Here, **Flyte** could be your solution. 

**Flyte** makes it easy to create concurrent, scalable, and maintainable workflows for **machine learning** and **data processing (you heard it right! It's ML + data processing).**

Flyte is used in production at **Lyft**, **Spotify**, **Freenome**, and others. At Lyft, Flyte has been serving production model training and data processing for over four years, becoming the de-facto platform for teams like Pricing, Locations, ETA, Mapping, Autonomous, and more. In fact, Flyte manages over 10,000 unique workflows at Lyft, totaling over **1,000,000 executions** every month, **20 million tasks**, and **40 million containers!**

You can check out Flyte here: [https://github.com/flyteorg/flyte](https://github.com/flyteorg/flyte), and here is the [website](https://flyte.org/)."
the_travelo_,MachineLearning,1617795448.0,[D] what's the best approach to document a machine learning project?,"At my company we""re struggling to correctly document projects. We use confluence for governance but haven't found a proper rythm to document everything involved in the development of the project.

What tools/processes/code or anything do you use to solve this problem?"
cereal_final,MachineLearning,1620446819.0,[P] How to get better performance with styleGAN2-ada for cartoons,"I'm trying to generate pokemon with styleGAN2-ada, and I'm not getting the [best results](https://imgur.com/a/h9u6nVh). I would say 50% look like legit pokemon, but the other 50% are kinda trash like the image I linked. I tried training it longer, but I believe the [model collapsed](https://imgur.com/a/5ZIvJ27). How can I improve results? The dataset is 5k images of pokemon headshots like [this.](https://imgur.com/a/bxZu3vU)"
JFHermes,MachineLearning,1618835916.0,[D]Ethics in famous Machine Learning papers.,"Hi,

I've been asked to write an Ethics Review for my STAT class at University. Most of the papers you would think to do have already been done, and I know there is a lot of great research happening in AI so I thought I would ask for some advice here.

I just need a single paper. It doesn't have to be unethical or even gray, but it would be better if I had something to talk about. 

So yeah, just throwing it out there in case anyone in here knows of one off the top of their head. Thanks for your time!"
LosinCash,MachineLearning,1617995159.0,[D] Looking for some clarification on Big Sleep variables.,"Hi all, hoping a few may be able to produce some insight into the variables inside of the Big Sleep ( hoping u/Whiskey has a moment to respond). 

I've installed and am running the Big Sleep on a Nvidia Jetson Xavier (both in Python and as a Jupyter notebook) and am using up about 16gb of it's ram, so plenty left to abuse. When drilling down through the variables there are a couple that have me stumped and I'd like to know more about them while I'm experimenting with them. 

Specifically: 

seed
gradient accumulate
torch_deterministic
class_temperature 

Thanks for any help or resources you can point me towards."
timscarfe,MachineLearning,1617499307.0,"[D] Christian Szegedy - Formal Reasoning, Program Synthesis [Video Show]","[https://youtu.be/ehNGGYFO6ms](https://youtu.be/ehNGGYFO6ms) 

Dr. Christian Szegedy from Google Research is a deep learning heavyweight. He invented adversarial examples, one of the first object detection algorithms, the inceptionnet architecture, and co-invented batchnorm. He thinks that if you bet on computers and software in 1990 you would have been as right as if you bet on AI now. But he thinks that we have been programming computers the same way since the 1950s and there has been a huge stagnation ever since. Mathematics is the process of taking a fuzzy thought and formalising it. But could we automate that? Could we create a system which will act like a super human mathematician but you can talk to it in natural language? This is what Christian calls autoformalisation. Christian thinks that automating many of the things we do in mathematics is the first step towards software synthesis and building human-level AGI. Mathematics ability is the litmus test for general reasoning ability. Christian has a fascinating take on transformers too.  

Touching on;

A Promising Path Towards Autoformalization and General Artificial Intelligence \[Szegedy\]

[https://link.springer.com/chapter/10.1007/978-3-030-53518-6\_1](https://link.springer.com/chapter/10.1007/978-3-030-53518-6_1)

Learning to Reason in Large Theories without Imitation \[Bansal/Szegedy\]

[https://arxiv.org/pdf/1905.10501.pdf](https://arxiv.org/pdf/1905.10501.pdf)

MATHEMATICAL REASONING VIA SELF-SUPERVISED SKIP-TREE TRAINING \[Rabe .. Szegedy\]

[https://openreview.net/pdf?id=YmqAnY0CMEy](https://openreview.net/pdf?id=YmqAnY0CMEy)

LIME: LEARNING INDUCTIVE BIAS FOR PRIMITIVES OF MATHEMATICAL REASONING \[Wu..Szegedy\]

[https://arxiv.org/abs/2101.06223v1](https://arxiv.org/abs/2101.06223v1)

DEEP LEARNING FOR SYMBOLIC MATHEMATICS \[Lample\]

[https://arxiv.org/pdf/1912.01412.pdf](https://arxiv.org/pdf/1912.01412.pdf)

It’s Not What Machines Can Learn, It’s What We Cannot Teach \[Yehuda\]

[https://arxiv.org/pdf/2002.09398.pdf](https://arxiv.org/pdf/2002.09398.pdf)

Investigating the Limitations of Transformers with Simple Arithmetic Tasks \[Nogueira\]

[https://arxiv.org/pdf/2102.13019.pdf](https://arxiv.org/pdf/2102.13019.pdf)

Provable Bounds for Learning Some Deep Representations \[Arora\]

[https://arxiv.org/pdf/1310.6343.pdf](https://arxiv.org/pdf/1310.6343.pdf)

Neural nets learn to program neural nets with fast weights \[Schmidhuber\]

[https://people.idsia.ch/\~juergen/fast-weight-programmer-1991-transformer.html](https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html)

How does Batch Normalization Help Optimization? \[Ilyas\]

[https://gradientscience.org/batchnorm/](https://gradientscience.org/batchnorm/)

How to Train Your ResNet 7: Batch Norm

[https://myrtle.ai/learn/how-to-train-your-resnet-7-batch-norm/](https://myrtle.ai/learn/how-to-train-your-resnet-7-batch-norm/)

Training a ResNet to 94% Accuracy on CIFAR-10 in 26 Seconds on a Single GPU – \[KUHN\]

[https://efficientdl.com/how-to-train-a-resnet-efficiently/#7-batch-norm-does-reducs-internal-covariate-shift](https://efficientdl.com/how-to-train-a-resnet-efficiently/#7-batch-norm-does-reducs-internal-covariate-shift)"
Mjjjokes,MachineLearning,1617939562.0,[R] CPU algorithm trains deep neural nets up to 15 times faster than top GPU trainers,"Link: https://techxplore.com/news/2021-04-rice-intel-optimize-ai-commodity.html?fbclid=IwAR3uvvw6fOHDMliJxSi3AVoW1JNwtYkDIUcf0Tmuc9dWwdAH8irtTMABYjs

""The whole industry is fixated on one kind of improvement—faster matrix multiplications,"" Shrivastava said. ""Everyone is looking at specialized hardware and architectures to push matrix multiplication. People are now even talking about having specialized hardware-software stacks for specific kinds of deep learning. Instead of taking an expensive algorithm and throwing the whole world of system optimization at it, I'm saying, 'Let's revisit the algorithm.'""

From the article"
pcaversaccio,MachineLearning,1618587670.0,[R] GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,
meldiwin,MachineLearning,1617392472.0,"[N] ""DeepDream"" Questions for Alexander Mordvintsev, a research scientist at Google","&#x200B;

Hello,

For those who maybe famiilar with DeepDreams  a [computer vision](https://en.wikipedia.org/wiki/Computer_vision)  program created by  Alexander Mordvintsev, we (IEEE Soft Robotics  podcast) are going to have him on the podcast, if you have any questions you can send  them here: [https://docs.google.com/forms/d/e/1FAIpQLSdThTdL5BKFBRRpoA0scZWFDDq4HIJwqGt2SLSugh9NdA0LNA/viewform?vc=0&c=0&w=1&flr=0&gxids=7628](https://docs.google.com/forms/d/e/1FAIpQLSdThTdL5BKFBRRpoA0scZWFDDq4HIJwqGt2SLSugh9NdA0LNA/viewform?vc=0&c=0&w=1&flr=0&gxids=7628)

&#x200B;

&#x200B;

https://preview.redd.it/kqdkz2aoctq61.png?width=922&format=png&auto=webp&s=e693219b6b5b893492e4aab6c9b81bc8fc52e08c"
SieunPark,MachineLearning,1617422929.0,[R] How to download the DIV8K dataset for Super-Resolution?,"[DIV8K](https://people.ee.ethz.ch/~timofter/publications/Gu-ICCVW-2019b.pdf) is a dataset used for Super-Resolution. This was used in the 2019 AIM challenge and 2020 NTIRE challenge.

The link to the challenge is [https://competitions.codalab.org/competitions/22217#learn\_the\_details-evaluation](https://competitions.codalab.org/competitions/22217#learn_the_details-evaluation).

But unfortunately, I couldn't find a link to download the dataset anywhere on the internet. How can I download the DIV8K dataset?

Thank you."
OShackathon,MachineLearning,1618620959.0,[P] Darknet on AMD Hardware,[https://github.com/AlexeyAB/darknet/compare/master...os-hackathon:master](https://github.com/AlexeyAB/darknet/compare/master...os-hackathon:master)
iarai-weather4cast,MachineLearning,1617294382.0,[N] Join our new IARAI Multi-sensor Weather Forecast Competition!,"Join our new Multi-sensor Weather Forecast Competition!

* Study multi-channel weather movies.
* Predict weather products in various earth regions.
* Apply transfer learning to new earth regions.

**Goal**  
The goal of the competition is a short-term prediction of selected weather products in three regions  (**core challenge**) and apply the transfer learning to predict weather products in the three additional regions (**transfer learning challenge**).

Following our recent success of our Traffic4cast competitions at NeurIPS in 2019 and 2020, this challenge similarly presents weather forecast as a video frame prediction task. The competition goal is to predict the next 32 images (8 hours in 15 minute intervals) of the weather movies. The images contain four channels encoding the following weather products: temperature (on accessible surface: top cloud or earth), convective rainfall rate, probability of occurrence of tropopause folding, and cloud mask, based on meteorological satellites data obtained in collaboration with AEMET/ NWC SAF. Each pixel in the images represents the area of about 4 km x 4 km, and  each region contains 256 x 256 pixels.  The regions span varying  landscapes including mountains, deserts,  islands and seas, and others.

**The challenge offers real-world benchmark for few shot and transfer learning and allows testing multi-sensor data fusion!**

Join us and learn more about the data on our website: [weather4cast.ai](https://weather4cast.ai)

**Prizes**  
The winners of the core competition and the transfer learning competition will be awarded the following prizes:

* 1st place –  a voucher or cash prize worth 5,000€ to the participant/team;
* 2nd place – a voucher or cash prize worth 3,000€ to the participant/team;
* 3rd place – a voucher or cash prize worth 2,000€ to the participant/team.

**Deadlines**

* Competition starts: April 1, 2021
* Submission deadline: May 31, 2021, 23:59 AOE.
   * held-out dataset available: June 1, 2021.
* Abstract submission and held-out dataset prediction: June 6, 2021, 23:59 AOE.
* Announcement of the winners: June 21, 2021."
StrasJam,MachineLearning,1617867075.0,[D] Facebook's use of Softmax in multi-label classification,"I was reading this [paper](https://arxiv.org/pdf/1805.00932.pdf)  put out by a group of researchers at Facebook where they found that using a softmax and CE loss function during  training led to improved results over sigmoid + BCE. During training they change the one-hot label vector such that each '1' is divided by the  number of labels for    the given image (e.g. from \[0, 1, 1, 0\] to \[0, 0.5,  0.5, 0\]).

However, they do not mention how this could then be used in the  inference stage, because the required threshold for selecting the  correct labels is not clear and would theoretically need to be set based upon the expected number of labels for the image (which is information which wouldn't be available at inference).

Has anyone else read this paper or have an idea how this could work?"
jj4646,MachineLearning,1619296637.0,[D] relationship between svm and neural networks,"Is this correct?

Svm's project the data into higher dimensions to look for patterns and then project them back into lower dimensions to make the final decision; whereas neural networks directly project the data into a lower dimension to make the decision?"
zy415,MachineLearning,1619705753.0,[D] IJCAI 2021 Paper Acceptance Result,IJCAI 2021 paper acceptance results were released. Creating a discussion thread for this year's results.
huseinzol05,MachineLearning,1618732918.0,"[P] Malaya-Speech, Speech-Toolkit library for Malay language, powered by Deep Learning Tensorflow","## Features

-   **Age Detection**, detect age in speech using Finetuned Speaker Vector.
-   **Speaker Diarization**, diarizing speakers using Pretrained Speaker Vector.
-   **Emotion Detection**, detect emotions in speech using Finetuned Speaker Vector.
-   **Gender Detection**, detect genders in speech using Finetuned Speaker Vector.
-   **Language Detection**, detect hyperlocal languages in speech using Finetuned Speaker Vector.
-   **Multispeaker Separation**, Multispeaker separation using FastSep on 8k Wav.
-   **Noise Reduction**, reduce multilevel noises using STFT UNET.
-   **Speaker Change**, detect changing speakers using Finetuned Speaker Vector.
-   **Speaker overlap**, detect overlap speakers using Finetuned Speaker Vector.
-   **Speaker Vector**, calculate similarity between speakers using Pretrained Speaker Vector.
-   **Speech Enhancement**, enhance voice activities using Waveform UNET.
-   **Speech-to-Text**, End-to-End Speech to Text for Malay and Mixed (Malay and Singlish) using RNN-Transducer.
-   **Super Resolution**, Super Resolution 4x for Waveform.
-   **Text-to-Speech**, Text to Speech for Malay and Singlish using Tacotron2 and FastSpeech2.
-   **Vocoder**, convert Mel to Waveform using MelGAN, Multiband MelGAN and Universal MelGAN Vocoder.
-   **Voice Activity Detection**, detect voice activities using Finetuned Speaker Vector.
-   **Voice Conversion**, Many-to-One, One-to-Many, Many-to-Many, and Zero-shot Voice Conversion.
-   **Hybrid 8-bit Quantization**, provide hybrid 8-bit quantization for all models to reduce inference time up to 2x and model size up to 4x.

## Pretrained Models

-   **Wave UNET**, Multi-Scale Neural Network for End-to-End Audio Source Separation, https://arxiv.org/abs/1806.03185
-   **Wave ResNet UNET**, added ResNet style into Wave UNET, no paper produced.
-   **Wave ResNext UNET**, added ResNext style into Wave UNET, no paper produced.
-   **Deep Speaker**, An End-to-End Neural Speaker Embedding System, https://arxiv.org/pdf/1705.02304.pdf
-   **SpeakerNet**, 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification, https://arxiv.org/abs/2010.12653
-   **VGGVox**, a large-scale speaker identification dataset, https://arxiv.org/pdf/1706.08612.pdf
-   **GhostVLAD**, Utterance-level Aggregation For Speaker Recognition In The Wild, https://arxiv.org/abs/1902.10107
-   **Conformer**, Convolution-augmented Transformer for Speech Recognition, https://arxiv.org/abs/2005.08100
-   **ALConformer**, A lite Conformer, no paper produced.
-   **Jasper**, An End-to-End Convolutional Neural Acoustic Model, https://arxiv.org/abs/1904.03288
-   **Tacotron2**, Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, https://arxiv.org/abs/1712.05884
-   **FastSpeech2**, Fast and High-Quality End-to-End Text to Speech, https://arxiv.org/abs/2006.04558
-   **MelGAN**, Generative Adversarial Networks for Conditional Waveform Synthesis, https://arxiv.org/abs/1910.06711
-   **Multi-band MelGAN**, Faster Waveform Generation for High-Quality Text-to-Speech, https://arxiv.org/abs/2005.05106
-   **SRGAN**, Modified version of SRGAN to do 1D Convolution, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, https://arxiv.org/abs/1609.04802
-   **Speech Enhancement UNET**, https://github.com/haoxiangsnr/Wave-U-Net-for-Speech-Enhancement
-   **Speech Enhancement ResNet UNET**, Added ResNet style into Speech Enhancement UNET, no paper produced.
-   **Speech Enhancement ResNext UNET**, Added ResNext style into Speech Enhancement UNET, no paper produced.
-   **Universal MelGAN**, Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform Generation in Multiple Domains, https://arxiv.org/abs/2011.09631
-   **FastVC**, Faster and Accurate Voice Conversion using Transformer, no paper produced.
-   **FastSep**, Faster and Accurate Speech Separation using Transformer, no paper produced.
-   **wav2vec 2.0**, A Framework for Self-Supervised Learning of Speech Representations, https://arxiv.org/abs/2006.11477

Check latest documentation at https://malaya-speech.readthedocs.io/

Check github repository at https://github.com/huseinzol05/malaya-speech"
marksteve4,MachineLearning,1617752955.0,[D] state of art for Speaker Diarization?," I've tried [Resemblyzer's](https://github.com/resemble-ai/Resemblyzer) method, yet it always either cut out too much of his voice, or included too much of others. It also required that i have a clip of him talking, and the quality of that clip heavily impacted its performance."
malia912,MachineLearning,1620140015.0,YOLOV5 with a second stage classifier [D],"[D]
Hello everyone. This is a question regarding YOLOV5 In detect.py module I saw a second stage classifier but it is set to false.

PROBLEM STATEMENT: I want to classify images with a secondary classifier. In first stage YOLOV5 just draws bounding box around images of Dogs (basically localising the dogs in the image) and with the help of SECONDARY CLASSIFIER want to classify dogs into respective breeds.

RESULT: bounding box around the dog with names of the

breed.

Has anyone tried this approach?

Thank you for your time."
ztzyz615,MachineLearning,1619087386.0,[D] Any statistical theory for mixture density network?, I want to ask whether there are any paper or work in relation to the statistical theory of mixture density network ([https://publications.aston.ac.uk/id/eprint/373/1/NCRG\_94\_004.pdf](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf))? Like the convergence rate or estimation error bound. I did a lot of searching but failed to find. Many thanks!
PaganPasta,MachineLearning,1618989276.0,[D] Adding under review/submitted papers on Resumé?,"Recently, I have been skimming through resumes of potential candidates for a research role and came across plenty of them which add ""submitted"" or ""under-review"" @CVPR, ICML etc. to their publication list. 

Is this a common practice ? It becomes annoying when the paper is not on arxiv and the candidate wishes to present it but also can't share it. Most of the times these papers are underwhelming which lack rigour on many fronts. I just see it as a poor effort at adding the reputed conference names to the resumé. Or perhaps, I am missing the utility of this information."
say_wot_again,MachineLearning,1618230205.0,[R] What Will it Take to Fix Benchmarking in Natural Language Understanding?,
hnipun,MachineLearning,1617549237.0,[P][D] Dynamic Hyper-parameters,"*Hyper-parameters control the learning process of models. They are set before training starts, either by intuition or a hyper-parameter search. They either stay static or change based on a pre-determined schedule. We are introducing dynamic hyper-parameters which can be manually adjusted during the training based on model training stats.*

**Github:** [**https://github.com/lab-ml/labml**](https://github.com/lab-ml/labml) **App:** [**https://app.labml.ai/**](https://app.labml.ai/)

## What are hyper-parameters?

Hyper-parameters are parameters that control the learning process of models, such as the learning rate, batch size, and weight decay. The model might not learn if the hyper-parameters are not set correctly. Setting hyper-parameters is a key part of deep learning research. Researchers find them based on intuition or by running a hyper-parameter search. In a hyper-parameter search, the model is trained repeatedly with different hyper-parameters to find the best set of hyper-parameters. Hyper-parameter search becomes costly as the number of hyper-parameters increases and the model training time increases.

Some hyper-parameters change during the training based on a pre-determined schedule. For example, you could slowly decrease the learning rate, or you could decrease the coefficient of an auxiliary loss as the model learns. Finding these schedules is nearly impossible with a hyper-parameter search, and are usually determined based on the intuition of the researchers.

## Why is it hard to determine hyper-parameters?

Setting hyper-parameters require quite a bit of experience with the kind of models and sizes you are training as well as the dataset. For instance, consider fine-tuning a pre-trained language model to classify tweets. You get a pre-trained language model as the backbone and attach a layer (or two) as the classification head. First, you freeze the parameters of the backbone and train the head for a certain number of updates, and then you unfreeze all parameters and train all the parameters. The number of steps to keep the backbone frozen is generally set to 1 epoch. This is a hyper-parameter. And the common practice of freezing for 1 epoch might be too small or too large depending on the size of the model as well as the dataset size. Someone who has worked with similar models and datasets will have a good intuition on this hyper-parameter. If you are new, you will have to try training the model to get a feel about it.

## ⚙️ Introducing Dynamic Hyper-parameters

Dynamic hyper-parameters are hyper-parameters that researchers can adjust while the model is being trained. This allows researchers to actively control how the model trains, instead of letting the model train with a pre-determined set of hyper-parameters. Dynamic hyper-parameters help train the model faster and better on a single training session. Also, they let researchers play around with the hyper-parameters during a single training run to gather insights.

Sometimes researchers save the model checkpoints and restart the training with changed hyper-parameter values. This has a similar effect to dynamic hyper-parameters but it's quite cumbersome to do.

## How does it work?

You need to create a dynamic hyper-parameter and register them along with other configurations.

    from labml import experiment
    from labml.configs import FloatDynamicHyperParam
    
    lr = FloatDynamicHyperParam(2.5e-4, range_=(0, 0.1))
    
    experiment.configs({
      'learning_rate': lr,
      ...,
    })

Then can call the dynamic hyper-parameter to get the current value. For example:

    def train(batch):
      optimizer.set_lr(lr())
      optimizer.step()

The call `lr()` will return the current learning rate set in [labml.ai](https://labml.ai) [app](https://github.com/lab-ml/app).

[*This*](https://github.com/lab-ml/labml/raw/master/guides/dynamic_hp.png) *is a screenshot of the mobile web interface for changing dynamic hyper-parameters. In this* [*Demo*](https://app.labml.ai/run/6eff28a0910e11eb9b008db315936e2f/hyper_params) *demo we adjusted the learning rate, clipping range, and the number of training epochs (per sample) to speed up the training of a* [*PPO agent*](https://nn.labml.ai/rl/ppo/experiment.html) *for Atari Breakout. A standard learning rate decay and other static hyper-parameter values would have taken a lot of training updates to get over the score of 1.*

## Example use-cases

**Freezing pre-trained layers**: When fine-tuning a language model, you can train with the backbone frozen until the rate of improvement of loss drops, and change the hyper-parameter affecting which layers are frozen. This is better and faster than going with the common practice of keeping the backbone frozen for 1 epoch for all models and datasets.

**Learning-rate warm-up and decay**: The learning rate can be manually increased during the initial training updates. You could decide how long to warm up for based on the loss curves. Similarly, you can decay the learning rate when the loss values stabilize. This allows you to use higher learning rates initially to speed up the training.

**Increase sequence length**: Recurrent models train faster when the BPTT length is shorter. But you need higher BPTT lengths to improve accuracy. Therefore, it is a common practice to start with a shorter BPTT length and increase it later. Again deciding when to do this beforehand is hard. Changing this dynamically is a lot easier.

**Adjusting regularization parameters**: You can start with lower weight decay and lower dropout probabilities initially. Especially if you are not sure about the representation capacity of the model. You can then increase these regularization parameters later when the validation loss stops improving (higher variance).

**Adjusting reinforcement learning hyper-parameters**: Reinforcement learning tends to have more hyper-parameters. Most of which need to change during training, such as discount factor, entropy-bonus coefficients, learning-rate, etc.  Pre-determining them is almost impossible without observing a few training runs, and those training runs go many hours or days even for simple gaming environments. Changing these during training based on the agent's performance and other stats is a lot easier.

## What's next

**Updating hyper-parameter schedules**: Our current implementation only allows users to update hyper-parameter values. This can take too much user time. For instance, let's say based on current loss curves the user figures out that he wants to drop the learning rate from `1e-3` to `2.5e-4` during the next 100,000 updates. With our current implementation, she would have to make several manual  changes. We want to let users set and update hyper-parameter schedules so that user has to manually intervene only when necessary.

**Rewind**: Often when training with dynamic hyper-parameters you feel like experimenting with them. Sort of like a small hyper-parameter search while the model is training. But when things go wrong you want to reset. To enable this we are working on a simple rewind (or undo) option, where the user could restart at  any checkpoint, with a couple of taps on the screen."
init__27,MachineLearning,1620470429.0,[P] Video/Discussion on Building an Aircooled rig to accommodate 3x GPUs,"Video URL: [https://www.youtube.com/watch?v=S0-lM6mZJn0](https://www.youtube.com/watch?v=S0-lM6mZJn0&t=2s)

Hi everyone! 

This project started with me trying to find guides on effectively air-cooling >=2 3090 GPUs for a DL Box. I couldn't find any detailed discussions on picking the correct parts so after a lot of iterations, I was able to build one myself. 

I decided to publish a video discussing the parts and my reasoning behind selecting the same so that anyone could use this as a template to build their own rigs. 

PC Part list: [https://pcpartpicker.com/list/gFGtF8](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa05NYlpndUVnMzhldjV5MldNSUU5NUo2MVRSQXxBQ3Jtc0ttQ3ozU01ENFNpaWFDVW8zVFZGR3p6aktDeHdxb0JpV3otZEdpNTA1SnBNVXZkbUk1QjlSaTlCOTZEMWR4Uzhwd2dHMkFSbmtiLUZxcE1BVllYRGVCWUYtMFVKYTBZQVhOYlFNX3pOZWNBTTZDenU2UQ&q=https%3A%2F%2Fpcpartpicker.com%2Flist%2FgFGtF8)​ (Note it doesn't show the correct GPUs, I have an MSI Gaming X Trio 3090, a MSI Ventus 3x & an A6000)

Other resources: 

Apart from this, I would highly encourage referring the solid writeup by u/emilwallner: [https://www.emilwallner.com/p/ml-rig](https://www.emilwallner.com/p/ml-rig) as well as the legendary Tim Detter's blog: [https://timdettmers.com](https://timdettmers.com)

Happy to answer any Qs. TIA!"
phenomenonical,MachineLearning,1620462828.0,[D] Tech stack and resources for a ML project,"What would be the best tech stack / what resources would I need for this potential ML product?

* Assume that input data is already stored on Azure (but could possibly be stored on internal servers or other cloud provider)
* End product would be a web app that shows predictions on a map. The web app would ideally have registered users and would collect some light data on user info and activity
* Machine learning model would collect new data, be retrained, and make new predictions annually
* Model maintenance would be done by my company (unless this is not a good idea?)
* The clients are municipal administrations, so budget is small and they would only provide url hosting
* We are in the EU so any user data collected would need to be GDPR compliant

Background: I am a solo-ing data scientist (MSc in data science but no actual ML work experience so I haven't seen precedents) trying to get ML up and running at a large (15,000 person) firm. I came up with a good idea to weave in ML with a traditional service that the firm provides, but now I need to figure out how actually do the thing. I've done a proof of concept with some toy data and it gave promising results.

Could the end-to-end product be done within Azure? Is there a cheaper option since the ML model would only need to be updated annually? My company has difficulty funding innovations that are not tied directly to a client project, so I essentially have no budget until I get a client buy-in.

What other team members would it make sense to bring on? Should I outsource any part of this to an external company? I can handle the model building, but I have no idea what happens after I make an API to broadcast the predictions. There is a company lawyer that oversees the country's GDPR compliance (so not too sure how much free time he has). Also, I currently work closely with a developer-ish person who was involved in creating a web app, but not too sure how much technical knowledge he has. Whenever I press him for details he seems to have no clue (for example, he had never heard of Azure before I mentioned it. He said something about using GitHub for the web app.)."
Daddy_Long_Legs,MachineLearning,1619199043.0,[D] Hyperparameter tuning with a budget constraint on total number of model parameters,"Is this possible to do with any of the hyperparameter frameworks out there? I have a convolutional network with the hypterparameters being kernel size, number of layers, and number of channels, and would like to search through those parameters while placing a limit on the total number of parameters present in the model due to where I'm deploying it. I found this [paper](https://arxiv.org/pdf/1902.00532.pdf) but could not find any code implementing their strategy. Has anyone implemented something like this already?

Facebook's Ax allows something similar, but only with linear combinations of parameters which is tough since the number of parameters for a conv layer is superlinear (((m \* n \* d) + 1) \* k). Seems like this should be a common problem, so curious why there isn't more out there on this as well."
MacaronFraise,MachineLearning,1619255019.0,[D] Improve KNN calculation time,"Hey guys,

I am currently working on a machine learning project using KNN and I have a performance issue. At the moment, my KNN is loaded on a Flask server hosted on Heroku. Whenever I want to make a prediction, I send to the flask server the parameters of the prediction.

The problem I encounter is that it takes too much time to make a single prediction (about 1 second). For my project, I actually need to make 5000 predictions in one go.

So I'm posting here in order to ask you for some solution about it. Is there a way to optimize KNN so that it can handle 5000 predictions quickly ? Or, should I host my KNN on another hosting service with more computing power, more adapted to Machine Learning like Azure ?

All that is quite new for me. Thanks a lot for your help

UPDATE : After a few tests, I realized that I am using all the 500MB RAM available on Heroku. I don't know if it is a normal figure or if I my code is not optimized and using too much memory"
dptzippy,MachineLearning,1619208491.0,[D] What are some projects/tutorials that I could use to get started with ML?,"Hello, everybody! I have been trying to learn more about ML, and I have been improving, but I can't find any projects/tutorials that explain exactly what is going on. I am looking for any free, preferably in written form, tutorials that can help me, and others, start out with ML.

Side-question: I have been trying to find a way to create a program/model that can be ""fed"" text files, which add to its vocabulary, and can be used to generate text. I can't find something to do this, and I am sure that it's out there, so I wanted to see if anybody knew of such a project/framework/tutorial. 


I am new to Python/ML, but I have been programming for several years, so I might need some dumbing-down of specific stuff, but I can probably understand broader concepts.

Thanks!"
pircherth,MachineLearning,1619201704.0,[R] The structure dilemma in biological and artificial neural networks,"The role of internal structure in information processing networks in artificial and biological based models and how this structure changes during learning. We showed that the structure leads to the edge weights, but the structures are not problem exclusive.

https://www.nature.com/articles/s41598-021-84813-6"
RchGrav,MachineLearning,1618790215.0,[D] I made a script that does all the work to deploy GPT-NEO on Windows 10. (Please Test),"Envisioned Purpose: Windows 10 GPT-NEO Local Demo for the Average Joe.

[https://gist.github.com/RchGrav/8bb3769a825540a4587e73ce6ea26053/](https://gist.github.com/RchGrav/8bb3769a825540a4587e73ce6ea26053/)

So what this does is deploy miniconda3 and configures the base environment which then downloads and runs one of the GPT-NEO models on either a CPU or CUDA capable device (GPU w/ CUDA installed).  I chose miniconda3 because the concept behind this was to allow someone to run GPT-NEO locally for experimentation or a foundation as a base environment.   If this doesn't work for you please report back with some details.  This is a private GIST at the moment but I may put it on my Github page if people say this would be a benefit to anyone who only has a Windows system and is struggling to get a working system going.   I'm only starting off on my journey of actually working with machine learning hands on, but I have been doing deploy scripts for years in the IT field.

Some technical notes:  The script installs and uses the choco command (modeled after apt on linux) from [chocolately.org](https://chocolately.org) to bootstrap miniconda3, it uses the echo to generate a few other supporting scripts for requesting elevation and to reload the path and environment variables so that its a one step process..  run the script.  Finally the script also creates a example python file that executes to text generating text, but also a condition for the main script so if it is run again it won't try redeploying everything, it just drops to the GPT-NEO demo.

There is some notes at the bottom of the Gist which will be helpful for anyone looking to run on a GPU, or use the larger transformer model.

Minimum Recommended Specs: Tons of Cores, 16GB of or more..   Fast storage (NVMe / SSD) suggested, 12GB of HDD space for the smaller model, 2x that for the bigger model, and 3x that for both.)

To run on GPU: I don't know..  I have a GeForce RTX 3090, so that works for sure.. you will need to install CUDA from NVidia, this much I do know.

Side Note: Personally, I was hoping to get GPT-NEO up and working on a Jetson Nano board, but even with swap I think its going to be a really tight fit.. sounds like its gonna cost me more than $100 if I wanna pretend to be Michael Knight and converse with my car.  :-P

Please share feedback..  This may help others who are looking for a quick and easy bootstrap and way to play with GPT Text Generation.  It shouldn't mess with Anaconda at all since miniconda3 runs in its own environment, so as far as I can surmise I can't see it messing up anything on your system.."
TheCockatoo,MachineLearning,1618909454.0,[D] How to do stratified train/test split for semantic segmentation?,"In other words, how to ensure similar class distributions in the train and test sets when some samples have (arbitrarily) more pixels of class A than B, and others have more pixels of class B than A?"
techsucker,MachineLearning,1618502126.0,"[R] Brown University Researchers Introduce DeepONet, A Model Based On Deep Neural Network, To Approximate Both Linear and Nonlinear Operators (Paper and Github link included)","Researchers from Brown University have built DeepONet, a novel neural network-based model that can efficiently learn both linear and nonlinear operators. This novel model was inspired by earlier studies led by researchers at Fudan University. 

A **continuous function** does not have any abrupt changes in value. More precisely, small changes in continuous function’s output can be assured by restricting to sufficiently small changes in its input. Many studies show that artificial neural networks (ANN) are highly efficient approximators of continuous functions. However, not many studies have yet focused on their ability to approximate nonlinear operators.

Inspired by the papers published by Chen and Chen at Fudan University, which discusses the functional approximation using a single layer of neurons, the researchers decided to explore the possibility of building a neural network that could approximate both linear and nonlinear operators

Summary: [https://www.marktechpost.com/2021/04/15/brown-university-researchers-introduce-deeponet-a-model-based-on-deep-neural-network-to-approximate-both-linear-and-nonlinear-operators/](https://www.marktechpost.com/2021/04/15/brown-university-researchers-introduce-deeponet-a-model-based-on-deep-neural-network-to-approximate-both-linear-and-nonlinear-operators/) 

Paper: [https://www.nature.com/articles/s42256-021-00302-5](https://www.nature.com/articles/s42256-021-00302-5) 

Github: [https://github.com/lululxvi/deeponet](https://github.com/lululxvi/deeponet)"
maroxtn,MachineLearning,1617669407.0,[D] Is there better options than beam search in translation ?,"Are there better sampling strategy than beam search used by big companies in translations (such as google translation ...) ? 

When I train a transformer for translation, I get a lot of repeated ngrams when using beam search, and longer sentences."
crack_pop_rocks,MachineLearning,1620581059.0,Any suggestions for deep learning textbooks that have a neuroscience perspective? [Discussion],"I was wondering if anybody experts here have any suggestions on textbooks that address deep learning from a neuroscience perspective? There are several good publications on the subject, but I am having difficulty finding something that is more comprehensive.

I've been studying ML for about 6 months now with pretty steady progress, and just started getting into neural networks. My background is in neurobiology/cognitive neurosciences, so the fundamental principles behind DLNN model architecture and behavior are already familiar. Deep learning is much more statistics oriented than neuroscience given the nature of modeling and the ease of access to quantitative data, but I'm actually surprised how little biological systems are discussed/referenced, especially given how much we know about the various mechanisms for modulating learning.

&#x200B;

[Synaptic Integration in Dendritic Trees](https://sci-hub.se/10.1002/neu.20144)

[Functional Significance of Passive and Active Dendritic Properties in the Synaptic Integration by an Identified Nonspiking Interneuron of Crayfish](https://journals.physiology.org/doi/full/10.1152/jn.00680.2006)

&#x200B;

Also--if there are any PhDs reading this, look into jellyfish. They have primordial neural nets composed of just ganglia, which anatomically seems the most similar to deep learning model architectures.

&#x200B;

Abstract

Jellyfish nerve nets provide insight into the origins of nervous systems, as both their taxonomic position and their evolutionary age imply that jellyfish resemble some of the earliest neuron-bearing, actively-swimming animals. Here, we develop the first neuronal network model for the nerve nets of jellyfish. Specifically, we focus on the moon jelly Aurelia aurita and the control of its energy-efficient swimming motion. The proposed single neuron model disentangles the contributions of different currents to a spike. The network model identifies factors ensuring non-pathological activity and suggests an optimization for the transmission of signals. After modeling the jellyfish’s muscle system and its bell in a hydrodynamic environment, we explore the swimming elicited by neural activity. We find that different delays between nerve net activations lead to well-controlled, differently directed movements. Our model bridges the scales from single neurons to behavior, allowing for a comprehensive understanding of jellyfish neural control of locomotion.

&#x200B;

[https://elifesciences.org/articles/50084](https://elifesciences.org/articles/50084)"
bachier,MachineLearning,1617909974.0,"[D] CVPR 2021 paper ""The Affective Growth of Computer Vision""","[https://authentic.sice.indiana.edu/publications/Su\_Crandall-AffectiveGrowthCV-CVPR21.pdf](https://authentic.sice.indiana.edu/publications/Su_Crandall-AffectiveGrowthCV-CVPR21.pdf)

Authors: Norman Makoto Su, David J. Crandall

Abstract: The success of deep learning has led to intense growth and interest in computer vision, along with concerns about its potential impact on society. Yet we know little about how these changes have affected the people that research and practice computer vision: we as a community spend so much effort trying to replicate the abilities of humans, but so little time considering the impact of this work on ourselves. In this paper, we report on a study in which we asked computer vision researchers and practitioners to write stories about emotionally-salient events that happened to them. Our analysis of over 50 responses found tremendous affective (emotional) strain in the computer vision community. While many describe excitement and success, we found strikingly frequent feelings of isolation, cynicism, apathy, and exasperation over the state of the field. This is especially true among people who do not share the unbridled enthusiasm for normative standards for computer vision research and who do not see themselves as part of the “incrowd.” Our findings suggest that these feelings are closely tied to the kinds of research and professional practices now expected in computer vision. We argue that as a community with significant stature, we need to work towards an inclusive culture that makes transparent and addresses the real emotional toil of its members."
uhtiloah,MachineLearning,1617282793.0,AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks,
cloudone,MachineLearning,1618589798.0,[R] Efficient Large-Scale Language Model Training on GPU Clusters,
ilia10000,MachineLearning,1617749313.0,"[D] Is ""data"" plural in modern machine learning literature?","As the title suggests I'm trying to figure out whether modern machine learning literature considers ""data"" to be a plural noun (e.g. The data are sparse) or a singular/mass noun (e.g. The data is sparse). My PhD supervisor argues that the plural form is correct, but I feel like it just doesn't sound quite right (at least in CS/ML contexts) when it hits my ear.

Google searches suggest that while the plural form is historically considered to be the correct one, in general, usage of ""data is"" is several times higher than usage of ""data are"". Apparently, in scientific literature, their usage is about equal.

Does anyone have any statistics or comments on this topic specifically for contemporary machine learning literature?"
givdwiel,MachineLearning,1616408714.0,[P] pyRDF2Vec 0.2.0 is out!,"# pyRDF2Vec 0.2.0 is out!!

This  release is packed with many new features and optimizations under the  hood. An entire overview of what's new can be found in our CHANGELOG ([https://github.com/IBCNServices/pyRDF2Vec/releases/tag/0.2.0](https://github.com/IBCNServices/pyRDF2Vec/releases/tag/0.2.0)). An overview of some major updates:

&#x200B;

**0) What is RDF2Vec?**

RDF2Vec is a technique to generate embeddings for nodes or entities in a Knowledge Graph (often represented in RDF format, hence the name).

The technique is unsupervised and therefore task-agnostic. You can generate embeddings for nodes and use those for multiple downstream tasks.

A concrete example is a graph containing information on chemical compounds, an example snippet of this data:

`<Compound rdf:about=""#d193"">`

`<cytogen_sce rdf:datatype=""&xsd;boolean"">true</cytogen_sce>`

`<hasAtom rdf:resource=""#d193\_13""/>`

`<mouse_lymph rdf:datatype=""&xsd;boolean"">false</mouse_lymph>`

`<amesTestPositive rdf:datatype=""\&xsd;boolean"">false</amesTestPositive>`

`<hasAtom rdf:resource=""#d193\_10""/>`

`<hasBond rdf:resource=""#bond3301""/>`

`<hasBond rdf:resource=""#bond3299""/>`

`<hasBond rdf:resource=""#bond3308""/>`

`<hasAtom rdf:resource=""#d193\_3""/>`

`<hasBond rdf:resource=""#bond3305""/>`

`<hasAtom rdf:resource=""#d193\_12""/>`

`<hasAtom rdf:resource=""#d193\_8""/>`

`<hasBond rdf:resource=""#bond3311""/>`

`<cytogen_ca rdf:datatype=""&xsd;boolean"">false</cytogen_ca>`

`</Compound>`

Which can then be turned into numerical representations through RDF2Vec:

&#x200B;

https://preview.redd.it/xdqhk1zev4p61.png?width=882&format=png&auto=webp&s=61e0975d79a3dc7a2865968d51cb904ddeb24423

&#x200B;

&#x200B;

**1) Rudimentary Literal Support**In  addition to node embeddings, pyRDF2Vec can extract numerical  information from the neighbourhood around a node. Users can specify a  path of predicates that can be followed to obtain numerical information.

https://preview.redd.it/w1ueuoev2ko61.png?width=1691&format=png&auto=webp&s=cd4fc03c5123ed4e44569e339ee3ba789ad15ae1

**2) Online Learning**Originally,  the entire model had to be re-trained when the underlying knowledge  graph changed. This is no longer the case, pyRDF2Vec 0.2.0 now features  online learning to update your embedding model dynamically.

https://preview.redd.it/5wasofiy2ko61.png?width=692&format=png&auto=webp&s=5d97a0e84a1df78b37f8a3e9fb223a8eb92f4c7d

**3) Reverse Walking**The  original walking algorithm extracted children starting from a certain  root recursively. In pyRDF2Vec 0.2.0, parents can be extracted as well.  This enables better interaction with the underlying windowing used by  Word2Vec.

https://preview.redd.it/rijl8pdz2ko61.png?width=651&format=png&auto=webp&s=427c6d5b9bfed1b1552f822ec239935f3dee789c

**4) Blazing Fast Walking**Many optimizations have  been made under the hood to speed up the walk extraction. Optimizations  include: multiprocessing, caching, asynchronous operations, etc...  Speedups of an order of magnitude can easily be achieved!

&#x200B;

https://preview.redd.it/3thhykc03ko61.png?width=1603&format=png&auto=webp&s=8ecd858414d667f5c2fae3e166fb412f61b4871e

**EXTRA)**

A blog post on this will be published shortly.   Moreover, we have noticed that pyRDF2Vec is being increasingly used within different studies, which is great to see!!!

We have compiled an overview: [https://pyrdf2vec.readthedocs.io/en/latest/posts-papers.html](https://pyrdf2vec.readthedocs.io/en/latest/posts-papers.html)"
hyunwoongko,MachineLearning,1616353077.0,[P]Summarizers: Easy to use controllable summarization package,"&#x200B;

https://preview.redd.it/4tlzfppwhfo61.png?width=2130&format=png&auto=webp&s=dabb97a246c2e904f84baf200ecf01022916d79f

Hello, I'm studying natural language processing. I made a package this weekend, and I'm writing a post for sharing because I think some of you might be able to use it well.

&#x200B;

The package name is summarizers, and as the name suggests, it is an easy tool to summarize text. However, summarizers support a variety of controllable summarization options, like above image, as well as a variety of domains such as general article summaries, paper summaries, and patent summaries.

&#x200B;

For more information, please visit [https://github.com/hyunwoongko/summarizers](https://github.com/hyunwoongko/summarizers)."
davidbun,MachineLearning,1617030776.0,"[N] do well in a CVPR/Kaggle Plant Pathology Challenge, win $500","**tl;dr top 20 teams that use package activeloopai/hub in CVPR's Plant Pathology Kaggle challenge win $500**

Hey r/MachineLearning,

My team and I have created Hub ([github.com/activeloopai/Hub](http://github.com/activeloopai/Hub)). The package makes unstructured dataset of any size accessible from any machine at any scale, and helps seamlessly stream data to PyTorch and TF, as if it were local. As part of our ongoing effort to support the machine learning community, we will support teams who do well in this year's CVPR FGVC8 challenges, starting with the [Plant Pathology Challenge](https://sites.google.com/view/fgvc8/competitions/plantpathologychallenge2021?authuser=0). Teams that place in the top 20 for these challenges (on the Kaggle leaderboard) and use the package in their solution will receive $500.For additional information, see the [wiki](https://github.com/activeloopai/Hub/wiki/Hub's-Plant-Pathology-2021-Challenge) or let me know here! [Here's a short notebook](https://github.com/mynameisvinn/Hub-Tutorials/blob/master/Pushing%20Plant%20Pathology%20Dataset%20to%20Hub.ipynb) on how to use Hub with the CVPR  dataset.  


**Features of Hub you might find relevant during the challenge:**

* Create large datasets with huge (10\^5 x 10\^5) size arrays and store locally, on hub storage, or any cloud.
* Collaborate with your team on the same dataset.
* Version control the dataset from the API itself.
* Filter datasets to only get the samples you need.
* Create data pipelines and transform the data.
* Easily access and visualize any slice of the dataset without downloading the entire dataset.
* Directly plug Hub datasets into tensorflow and pytorch and start training.
* Transfer datasets across different locations easily.

Good luck,

davidbun"
user01052018,MachineLearning,1616920279.0,[D] Pointer sentinel mixture model - Why is $P_{ptr}$ is $V$ dimensional?,"I have been checking this paper [https://arxiv.org/abs/1609.07843](https://arxiv.org/abs/1609.07843) . In the research paper, they said the model is capable of predicting not only rare or less frequent words but also unseen words. Now I noticed $P\_{ptr}$ is of size V. That means if we have some way encoded the information of an unseen word in those V sized vector. I wonder how can it be possible for an unseen word. When you are training you are considering a |V| size vocabulary which didn’t appear in softmax vocabulary?

&#x200B;

To be clear, suppose my own RNN vocabulary contains 3 words.

\[“Reddit”,”Users”,”<UNK>”\].  Now suppose I have 4 unseen  words in the text like

\[“Quora”,”Stackoverflow”,”Lichess”,”Wikipedia”\]. How would a 3 size vocabulary handle it?"
ykilcher,MachineLearning,1616435112.0,[D] Paper Explained - Perceiver: General Perception with Iterative Attention (Full Video Analysis),"[https://youtu.be/P\_xeshTnPZg](https://youtu.be/P_xeshTnPZg)

Inspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver's weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:20 - Built-In assumptions of Computer Vision Models

5:10 -  The Quadratic Bottleneck of Transformers

8:00 - Cross-Attention in Transformers

10:45 - The Perceiver Model Architecture & Learned Queries

20:05 - Positional Encodings via Fourier Features

23:25 - Experimental Results & Attention Maps

29:05 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2103.03206](https://arxiv.org/abs/2103.03206)"
AerysSk,MachineLearning,1620222682.0,[D] A small dataset with high resolution?,"Hello everyone. I am testing my augmentation strategy. Currently I am using CIFAR, but it is too small (32x32) to visualize the results. Is there any dataset that is small in size (like < 100 images, or smaller), but has high resolution (above 100x100 maybe)?

I am using PyTorch. If there is the code to load it too, I am greatly appreciated."
MrAcurite,MachineLearning,1619615551.0,[D] Recommendations for increasing training stability with extremely small batches?,"Howdy folks,

I'm currently in a situation where my batch size basically cannot exceed 2. If I had a cluster, I'd be able to do much better than that, but I don't, so I can't. This, I think, is leading to stability issues, because if the phenomena that the model is taking as input are homogeneous in a given step, the model will simply try and optimize towards giving the appropriate output generally, rather than conditionally.

What should I be trying to do? Should I be bumping up the betas in the Adam optimizer? Holding onto gradients over multiple batches and then applying them all at once? Any ideas?"
yashchandak,MachineLearning,1619781819.0,[R] Universal Off-Policy Evaluation,"Hi everyone,

I am happy to share with you our first steps towards a universal off-policy estimator (UnO) --- one that provides off-policy estimates and high-confidence bounds for any parameter of the return distribution. 

[https://arxiv.org/abs/2104.12820](https://arxiv.org/abs/2104.12820)

(joint work with Scott Niekum, Bruno Silva, Erik Learned-Miller, Emma Brunskill, and Phil Thomas)

We use UnO for estimating and simultaneously bounding the mean, variance, quantiles/median, inter-quantile range, CVaR, and the entire cumulative distribution of returns in the off-policy (or counterfactual) setting. We also discuss Uno's applicability in various settings, including fully observable, partially observable (i.e., with unobserved confounders), Markovian, non-Markovian, stationary, smoothly non-stationary, and discrete distribution shifts.

UnO can be useful in many critical applications that require thinking about metrics beyond the expected return. For example, medical settings (where tail-sensitive risk measures like value-at-risk/CVaR are essential to avoid catastrophic outcomes), online recommendation (where metrics like median/inter-quantiles are essential to tackle high noise in data collection), or human-machine interaction (where metrics like variance/entropy are essential to quantify uncertainty in the system's outcomes).

We believe that we have barely scratched the surface of this direction, and we welcome any and all feedback!"
SeasonedLeo,MachineLearning,1617925125.0,Multi classification issue [R],"Hello All

I am working on a Multi-classification problem with three classes. I am using light   Gbm model. My F1score on testing data set is pretty good for all three classes  over 0.95. However, when i am trying classify using this model on a new set . model misclassification  Is huge . I am not able understand how can this possible happen . Any pointers or ideas ? 

All dat have same features .

Thanks for help"
dokluch,MachineLearning,1620367447.0,[D] Deploying ML model for inference on user devices,"Hi! 
I would like to create a simple app with a couple of pre-trained ML models that would run on a variety of user devices - Windows and macOS machines with Nvidia or AMD or even no GPU.

Are there any guidelines on the such infrastructure? I am currently looking into tflite, but I'm not sure it is the right way.

Thank you!"
cdossman,MachineLearning,1618397655.0,[D] Understanding Hinton’s Capsule Networks Series,"Why Capsules neural network architecture is so important, the intuition behind it, and a dive into the technical details. 

Part I: [Intuition](https://pechyonkin.me/capsules-1/)   
Part II: [How Capsules Work](https://pechyonkin.me/capsules-2/)  
Part III: [Dynamic Routing Between Capsules](https://pechyonkin.me/capsules-3/)  
Part IV: [CapsNet Architecture](https://pechyonkin.me/capsules-4/)"
SrData,MachineLearning,1619252993.0,[D] Recovering images from bottlenecks or training dataset.," I'm wondering if it is possible to generate the images used to train a CNN or if it is possible to generate the images in the input from the bottleneck. Do you know some papers,  or post that I could check?

I've been investigating this: [https://arxiv.org/pdf/1710.09926.pdf](https://arxiv.org/pdf/1710.09926.pdf) (  Image Compression: Sparse Coding vs. Bottleneck Autoencoders ) but although similar is not the same.

My point to investigate this is to understand what is the level of privacy of the images if the bottleneck produced by those images are stored or going beyond, what is the level of privacy of the images used to train a  model."
runcep,MachineLearning,1617622899.0,[D] Are there attempts at a large German-language LM?,"I am aware of a few smaller attempts (huggingface has a German GPT-2 model, there is also Zamia‘s model), but I wonder if there are, at a university or a research group, projects that want to build a proper German-language LM in the size of the GPT-2 XL model and up? 

I am especially interested in learning what their training set is made up of. 

I saw that The Pile is interested in building open-source models for different languages but could find anything about a German one. 

Any hint is appreciated!"
AhmedAl93,MachineLearning,1617316167.0,[D][R] Solutions for handwritten text generation,"Hello,
The aim of my post is to discuss the possible solutions for synthetic handwritten text generation under specific constraints.

Context: Currently working as a data scientist in a french company, I encountered some issues when dealing with handwritten text recognition, mainly because the amount of available data is simply not enough (~4k images of handwritten text)

The first idea that came to my mind is to generate a large ""artificial"" handwritten text dataset by assembling single handwritten characters (check out Chars74k dataset), so I made a script that takes a string as input, assembles character images and provides an image containing handwritten text.

This strategy gave really promising results: I trained a text recognition model (called ""deep text recognition benchmark"") on the artificial dataset (120k images) then finetuned it on my real dataset and got 72% accuracy.

However, even if this result is good, it can be improved mainly by generating handwritten text that is as similar to the real one as possible.

I was thinking about using GAN models, but the problem is that they need what I'm lacking in the first place: big amount of data.

Do you have any ideas about solving this ""dilemma"" ?
Also, I thought this dilemma can be seen as PhD thesis proposal, and I'm planning to discuss with my managers about it, so if you can share your views on this, it would be great !"
VeterinarianTight102,MachineLearning,1618891530.0,[P] Awesome Semantic Search,"[EDIT] Thanks for the current support!

A repository for Papers , Tools, Projects , Libraries  and Datasets , conf related to MultiModal Semantic Search tasks.
Currently it  has mainly Text based stuff.

https://github.com/Agrover112/awesome-semantic-search

I believe there is a Plethora of content out there related to the topic , but less consolidation at one place."
techsucker,MachineLearning,1618244638.0,[R] Researchers At The University of Tokyo Present A Frequency-Based Inpainting Method To Generate Missing Image Portions,"Researchers from the University of Tokyo introduce a frequency-based inpainting method that can use both frequency and spatial information to generate missing image portions. 

Image inpainting is a computer vision (CV) technique that fills in the missing pixels in an image. It allows the removal of unwanted objects from a photo or recreates missing regions of occluded images. Inpainting is popularly used to predict missing image data. However, it is difficult to synthesize the missing pixels realistically and coherently.

Summary: [https://www.marktechpost.com/2021/04/12/researchers-at-the-university-of-tokyo-present-a-frequency-based-inpainting-method-to-generate-missing-image-portions/](https://www.marktechpost.com/2021/04/12/researchers-at-the-university-of-tokyo-present-a-frequency-based-inpainting-method-to-generate-missing-image-portions/) 

Paper: https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-30/issue-02/023016/Image-inpainting-using-frequency-domain-priors/10.1117/1.JEI.30.2.023016.full?SSO=1"
andyljones,MachineLearning,1617866914.0,[R] Scaling Scaling Laws with Board Games,"**Studying a sequence of small problems can let you extrapolate the behaviour of orders of magnitude larger problems!**

[Paper](https://arxiv.org/abs/2104.03113), [tweet thread](https://twitter.com/andy_l_jones/status/1380049754458034176).


Here we work on a board game, which is about the friendliest domain possible for this scaling behaviour. But there's potential for it to show up in many other places. If you're a resource-constrained researcher, I think it's a really promising avenue to look into.

Keen to hear your thoughts!"
elbogotazo,MachineLearning,1617384423.0,Discovering column mappings [R],"I have a challenge to work on at work and am trying to figure out the approach. We have an internal system that stores transactional data in a tabular form.

We receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. The amount field may have 3 digits behind the comma, where our system expects 1 digit or what our system calls ""amount"" might be called ""quantity1"" in the incoming files etc.. ). There will be multiple amount, date, free text and categorical fields and there will be relationships between those fields. Note that the source data comes from external parties and I have no control over the format of the incoming files. 

We have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. Im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.

I've been looking into a few things : using NLP\spacy to train a model that recognises patterns in the column data. E.g. Numeric + period + comma is likely to correspond to amount. I've also looked at modeling the data and extracting an RDF representation using a open source tool called Karma to see if I can train a model on a network graph. But really struggling to see how to implement this. Regex would only get us part of the way there and I'm really trying to see if there is a scalable way to implement this. 

Is anyone aware of the formal name of this type of problem and if there are tried and tested approaches\implementations out there that I could build upon?"
Mundane_Definition_8,MachineLearning,1619678595.0,"[D] I'm kaggler and hate waiting result, what makes you feel good?","Hi guys.

Usually, as a programmer, I enjoy watching my result immediately.

But in ML, Although I want to create my model, I hate waiting my result, and I don't know I really like AI.

My viewpoint makes me limit as a programmer grow.

&#x200B;

Where's your happy point while you are creating yours?

And, What do you do while your model is trained?"
marksteve4,MachineLearning,1617819843.0,[D] Does anyone use huggingface hosted interface?,I tried a bunch (many are disabled by default). But not much useful. Am I missing sth? Thought that the source for HF to make money ...
DietMediocre8993,MachineLearning,1617816736.0,[D] ML Engineer with Data Engineer,"Hello fellow enthusiasts. I have an upcoming interview for a Data Engineering role where one of the rounds is with a Data Scientist on Problem Solving, I am not sure what to focus for my prep or even what kind of questions I can expect, any suggestions?

It seems the focus would be around production, how DE and Scientists work/collaborate together, what do you think?"
Yuqing7,MachineLearning,1617162546.0,[N] Microsoft & Princeton’s Text-Game Agents Achieve High Scores in Complete Absence of Semantics,"A research team from Princeton University and Microsoft Research discover autonomous language-understanding agents are capable of achieving high scores even in the complete absence of language semantics, indicating that current RL agents for text-based games might not be sufficiently leveraging the semantic structure of game texts.

Here is a quick read: [Microsoft & Princeton’s Surprising Discovery: Text-Game Agents Achieve High Scores in Complete Absence of Semantics](https://syncedreview.com/2021/03/30/microsoft-princetons-surprising-discovery-text-game-agents-achieve-high-scores-in-complete-absence-of-semantics/)

An early version of the paper *Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents* was featured in the NeurIPS 2020 [workshop](https://wordplay-workshop.github.io/modern/) *Wordplay: When Language Meets Games.* The updated paper is available on [arXiv](https://arxiv.org/pdf/2103.13552.pdf)."
tars9999,MachineLearning,1617288820.0,Mixture of experts - tradeoffs vs traditional single nets [D],"I was personally surprised that ""branched NN's"" (seemingly called Mixture-Of-Experts) was not a more popular (or sucessful) technique, (eg. for imagenet entries) but it's appeared in some recent stories.

Is there any research comparing training times, model size, evaluation time between 1 dense net and mixture-of-experts ?

e.g. how would it pan out if you tried to do imagenet with it? (eg 33 branches of 33 categories each, or 10 vs 100 ..)

also is there any research on the tradeoffs of more branches ('heirachical MoE') (like 1000 caterogies = 10x10x10)

Perhaps for a particular problem there is an optimum branching depth, and it just happens below a certain size the optimum number of branches is just '1'.

&#x200B;

I'm guessing that single nets work better than I expected because the features cooperate more.. are there attempts at MoE models with some common trunk shared by the branches 

&#x200B;"
jostmey,MachineLearning,1616375436.0,[R] Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets,"Preprint: [https://arxiv.org/abs/2103.10472](https://arxiv.org/abs/2103.10472)

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies."
Yuqing7,MachineLearning,1617987945.0,"[N] TUM, Google, Nvidia & LMU München's CodeTrans Pretrained Models Crack Source Code Tasks With SOTA Performance","A research team from Technical University of Munich, Google, Nvidia and LMU München proposes CodeTrans, an encoder-decoder transformer model which achieves state-of-the-art performance on six tasks in the software engineering domain, including Code Documentation Generation, Source Code Summarization, Code Comment Generation, etc.

Here is a quick read: [TUM, Google, Nvidia & LMU München's CodeTrans Pretrained Models Crack Source Code Tasks With SOTA Performance](https://syncedreview.com/2021/04/09/tum-google-nvidia-lmu-munchens-codetrans-pretrained-models-crack-source-code-tasks-with-sota-performance/)

The CodeTrans code is available on the project [GitHub](https://github.com/agemagician/CodeTrans). The paper *CodeTrans: Towards Cracking the Language of Silicone’s Code Through Self-Supervised Deep Learning and High Performance Computing* is on [arXiv](https://arxiv.org/ftp/arxiv/papers/2104/2104.02443.pdf)."
OnlyProggingForFun,MachineLearning,1619005027.0,[D] Will Transformers Replace CNNs in Computer Vision?," I recently wrote an article, ""[Will Transformers Replace CNNs in Computer Vision](https://pub.towardsai.net/will-transformers-replace-cnns-in-computer-vision-55657a196833)?"" showing that transformers can be applied to not only text but also images and other types of inputs. I did that by covering a paper called the [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) where it gives a way to apply transformers' architecture in computer vision. I know that many others are quite promising, like [The Perceiver](https://arxiv.org/abs/2103.03206), but my question is: Do you think transformers are better suited for computer vision than convolutional neural networks? Are they viable in computation time and results compared to CNNs, optimizing the properties of images, especially for classification?"
m_nemo_syne,MachineLearning,1617723592.0,[R] Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers,"Timers and Such! New speech dataset for timers, alarms, unit conversion, and math.

Use it to test your speech models or to train your own offline voice assistant.

Paper: [https://arxiv.org/abs/2104.01604](https://t.co/1Itx4KKVyo?amp=1)

Code (SpeechBrain): [https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such](https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such)

Pre-trained model on Hugging Face: [https://huggingface.co/speechbrain/slu-timers-and-such-direct-librispeech-asr](https://huggingface.co/speechbrain/slu-timers-and-such-direct-librispeech-asr)"
jj4646,MachineLearning,1618972573.0,[D] Complexity of Time Series Models: ARIMA vs. LSTM,"1) When it comes to time series analysis, I am trying to understand what makes newer models such as LSTM's capable of capturing more complex patterns in the data compared to older modeis such as ARIMA? In statistical learning theory, there is something called the VC Dimension of an algorithm (https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension) - the VC dimension is apparently what describes the relartive level of complexity a machine learning algorithm can capture. Does this concept of VC Dimension carry over to models in time series analysis? Is it possible to show that LSTM's have a higher VC dimension compared to ARIMA style models? Supposedly, neural network based time series models were developed because modeols like ARIMA was unable to provide reliable estimates for bigger and complex datasets. Mathematically speaking, what allows a LSTM to capture more variation and complexity in a dataset compared to ARIMA?




2) Lately, I have seen many people starting to suggest that Convoloution Neural Networks (CNN) can also be used for the purpose of time series forecasting. Just as a general question: in what instances would it be better to use a CNN for time series forecasting compared to an LSTM?

Thanks"
AirZealousideal1342,MachineLearning,1618671696.0,[D]What is this in ontonotes dataset?,"I found in ontonotes, there are many punctuations which are preceded by a slash (e.g. '/.', '/?'). Is it an error. If it is, why it has not been be fixed until now(verion12)?

https://preview.redd.it/br0p5tng0rt61.png?width=3810&format=png&auto=webp&s=9e5e48cb5be7bf1e38ab3df053c637be8cc09d97

But some of then are normal.

https://preview.redd.it/atckgg9vrvt61.png?width=3732&format=png&auto=webp&s=a1ed061d8d28e988be47db12d9d69c691414d108"
Stingeymingey,MachineLearning,1617265463.0,[D] What's out there in terms of Video comprehension/understanding?,"Hi, 

I'm wondering if people could point me to any papers or projects that are working on video / subsequent frame understanding. Not just object detection, but using previous frames as context to guide future predictions.

Thanks."
cloudone,MachineLearning,1616563801.0,[N] Pieter Abbeel launched a new podcast,"First guest is Andrej Karpathy

https://www.therobotbrains.ai/"
BodybuildingPhD,MachineLearning,1616747965.0,[D] Does the NN really learns probability distribution?,"I am currently trying to understand how does the Neural Net learns probability distribution.

At first glance, this seems impossible, since basically neural net is a deterministic non-linear function.

As for the VAE, the encoder of VAE takes such x as an input and results in the main parameters of distribution(mostly, mean and covariance of gaussian distribution). This seems plausible; just by adding some random variable epsilion \~ N(0, I) you can construct Normal distribution.

But for Decoder, though many literature assumes the decoder has the form of probability distribution(which is P(X|Z)), yet they don't. Unlike the encoder part, there is no *randomness* in the structure of VAE decoder.

Moreover many decoder of the VAE assumes that they have normal distribution, but how could they be so sure?

Is there anything that I am missing?"
Koyset,MachineLearning,1617640968.0,"[Discussion] ""Developers should take philosophy classes to make ethical AI""","Hello, community!

I am still a third-semester CS Student but had already a rather confusing workshop at my student job at a telecommunications firm. It started with two consultants who both have an undergrad in social science but don't code. They provided us within a day with AI ethics guidelines that we developers should implement during our work.   
I asked how to make the guidelines like ""de-bias your code"", ""make inclusive algorithms"" into a (functional) code, since it's my first job, but they just said I should take additional philosophy classes against my hidden bias. 

How do implement ethical guidelines in code? Does every company has their own guidelines? Sorry, I was very uncomfortable asking further questions since I didn't understand the overall workshop. Hope you can provide some real-life examples from your jobs."
the21st,MachineLearning,1619516433.0,[P] I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool,"Hi there! I'm Simon, an engineer at Deepnote, previously I worked as an ML team lead in an e-commerce company. I recently built a new feature in Deepnote and I just wanted to brag a little bit because I'm proud of it ☺️

It's native SQL cells with Python interop, check out an [example project](https://deepnote.com/project/RNA-Exploration-Duplicate-xlaRWiCeRNqQ0Id0v2RY5A/%2Fnotebook.ipynb). Full docs for this feature are [here](https://docs.deepnote.com/features/sql-cells).

This was possible thanks to standing on the shoulders of open-source giants such as pandas and jinjasql, which I hugely appreciate and it makes me truly happy.

I'd appreciate any feedback and your thoughts on what innovation you'd love to see around notebooks in the future 🙂"
New-Sound8660,MachineLearning,1616886907.0,[R] Teaching a machine to paint like a painter comparing photos and associated paintings,"Hello everybody, I'm just approaching ML and I'd need some guidance.

Here's  the idea: I have a set of photos and a set of human made paintings  based on those photos, that try to replicate as faithfully as possible  the photos. Can I ""give"" to the algorithm the two sets of images, asking  him to ""learn"" how to create ""paintings"" in the style of the painter ho  realized the dataset, starting from new photos? Any suggestion about  which books, articles, courses, blogs should I follow to learn how to do  this would be greatly appreciated, thanks so much!"
vladiliescu,MachineLearning,1616591206.0,[D] [P] How do you use tools like AutoML?,"Hi girls and guys.

I was wondering - do you use tools like AutoML in your day to day life? Like, anything from H2O to Google's AutoML to auto-sklearn.

Asking since personally I only find AutoML useful for training an initial baseline. You know, when you've just managed to get a dataset that's just clean enough to be useful, and you're itching to see if this time you'll need to use something other than LightGBM. It's always LightGBM though. Always. 🥺

Anyway, apart from this initial phase I don't really use the tool - what I do when working with the [Azure flavor](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml) at least, is reverse engineer the model, take whatever I can and adapt whatever I cannot take. Kinda like [Picasso](https://quoteinvestigator.com/2013/03/06/artists-steal/), I imagine.

I've written a short(-ish) blog post on creating your own models based on Azure AutoML-trained ones. You'll find it [here](https://vladiliescu.net/reverse-engineering-automated-ml/). It's focused on time series, but the underlying principles should apply to regression and classification models, too.

Hope you like it. And I appreciate any and all feedback you may have - I'm really curious how you use these tools, and what you think of my approach."
otso_ai,MachineLearning,1618355393.0,[P] otso Annotator - A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists working in teams.,"*otso has just launched our Annotator, and have built with the needs of many in this sub! Check it out!*

**A cloud-based Text Annotator built for Machine Learning Engineers and Data Scientists.**

We have been working on otso Annotator for over two years. It began as an internal tool, used to manage annotation and data labelling for our own machine learning projects. As the tool and the interface developed, we began providing it to select enterprise customers. We’ve received so much significant positive feedback from our clients, that we decided to launch the Annotator as a standalone product.

otso Annotator provides three key benefits; the user experience, which prioritises ease-of-use and understanding. The project management features, which let you allocate and manage annotation tasks. Finally, as a cloud-first tool, you no longer need to have annotators use a CLI to get started, which makes it a much easier tool for teams to use.

**Why a focus on the user experience of teams?**

Text annotation is best done in a team environment. Ideally, machine learning engineers and data scientists will set up and run projects, while subject matter experts provide annotations. We have built otso Annotator with these different user types in mind - enabling seamless project setup for project admins and an easy and keyboard enabled annotation experience for annotators.

With this public launch, we are granting users and teams that sign up during April an extended trial period of 30 days.

To check it out, head to [otso.ai/annotator](https://otso.ai/annotator).No credit card required."
artificial_intelect,MachineLearning,1616734142.0,[P] NAS repos,When looking at git repos that have implemented MNasNet or EfficientNet they only ever seem to implement the network found by the neural architecture search. Does anyone know of a git repo that implements the Proximal Policy Optimization that can find EfficientNet (or some other similar NAS algorithm repo)?
tanelai,MachineLearning,1618087578.0,[P] Using PyTorch + NumPy? A bug that plagues thousands of open-source ML projects.,"Using NumPy’s random number generator with multi-process data loading in PyTorch causes identical augmentations unless you specifically set seeds using the worker\_init\_fn option in the DataLoader. I didn’t and this bug silently regressed my model’s accuracy.

How many others has this bug done damage to? Curious, I downloaded over a hundred thousand repositories from GitHub that import PyTorch, and analysed their source code. I kept projects that define a custom dataset, use NumPy’s random number generator with multi-process data loading, and are more-or-less straightforward to analyse using abstract syntax trees. Out of these, over 95% of the repositories are plagued by this problem. It’s inside PyTorch's official tutorial, OpenAI’s code, and NVIDIA’s projects. Even Karpathy admitted falling prey to it.

For example, the following image shows the duplicated random crop augmentations you get when you blindly follow the official PyTorch tutorial on custom datasets:

https://preview.redd.it/pccy5wskpes61.png?width=1652&format=png&auto=webp&s=f292d0282ad954cbac2c693a9656d62fa0dd9682

You can read more details [here](https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/)."
PT_Lightning,MachineLearning,1619457247.0,[N] Lightning Transformers - Train HuggingFace Transformers at scale with PyTorch Lightning,"[Lightning Transformers](https://github.com/PyTorchLightning/lightning-transformers) is for users who want to train, evaluate and predict using HuggingFace models and datasets with PyTorch Lightning. Full customizability of the code using the LightningModule and Trainer, with Hydra config composition for quick and easy experimentation. No boilerplate code required; easily swap out models, optimizers, schedulers, and more without touching the code. Check out the blog post: [Training Transformers at Scale with PyTorch Lightning](https://pytorch-lightning.medium.com/training-transformers-at-scale-with-pytorch-lightning-e1cb25f6db29) for more information or the [documentation](https://lightning-transformers.readthedocs.io/).

https://i.redd.it/ufmoebjbwjv61.gif"
Mephistothelessa,MachineLearning,1616657297.0,[D] What is the best way to publish an image dataset?,"Hello people! As a small team, we looked around for a dataset we needed for a project but couldn't find one. So we decided to properly create one ourselves and publish it for people to use in the future. We want to properly ""publish"" our dataset, meaning we don't want to use Kaggle for this. Maybe a dedicated website might work, we want it to look professional. Maybe Github pages may be used?

What are your suggestions? What do you think is the best way to publish a dataset?

Open to all suggestions and curious about what you guys think. All inputs are appreciated. Cheers."
artificial_intelect,MachineLearning,1616542211.0,[D] PipeMare paper discussion,"or maybe PipeMare paper rant...

&#x200B;

A while back I read a paper on mitigating the effects of asynchronous pipelined training called [PipeMare](https://proceedings.mlsys.org/paper/2021/hash/6c8349cc7260ae62e3b1396831a8398f-Abstract.html). Their methods didn't seem novel or super helpful so I ignored the paper and that was that. Then I noticed that it was accepted into a conference: MLSys2021.

So now I guess it's worth putting my thoughts online.

PipeMare proposes two methods to mitigate asynchronous pipelined NN training.

Issue 1: the type of asynchronous pipelined NN training they mitigate for is Pipelined Backpropagation (Petrowski et al., 1993). Petrowski et al. (1993) aren't even cited in the paper. Just because PipeDream doesn't cite Petrowski et al. (1993) does not mean you shouldn't. Note Pipelined Backpropagation has 2 issues: inconsistent weight & delayed gradients. PipeDream uses weight stashing to eliminate inconsistent weight but still has delayed gradients. PipeMare eliminates the overhead of weight stashing with ""Discrepancy correction"", but doesn't really deal with delayed gradients except for using an lr warmup.

The two methods PipeMare proposes are:

T1 - Learning rate rescheduling: a type of learning rate warm-up where the warm-up period is based on the pipeline delay.

T2 - Discrepancy correction: a type of backward weight prediction for reconciling the weights used in the forward and backward pass. While T2 deals with weight inconsistency, it does not mitigate for gradient delay.

Issue 2: If T1 is a type of learning rate warm-up why does the paper not show a baseline run with just a regular learning rate warm-up? My guess is that a regular learning rate warm-up would do just as well as this new convoluted method T1.

Issue 3: In Table 3 the PipeMare paper shows that T1 on its own works just as well as T1 + T2. Why use T2? It doesn't seem to help and just has overhead. PipeDream's weight stashing eliminates weight inconsistency; T2 only mitigates for weight inconsistency and does not eliminate it. In [2019 Chen et al](https://arxiv.org/abs/1809.02839) proposed a method called SpecTrain. In their work, they show that weight inconsistency is not a big issue and eliminating it by use weight stashing is useless. If weight inconsistency is not a big issue (as shown in the SpecTrain paper), why use T2 (especially since Table 3 shows that it is useless)?

T1 isn't novel / there is no evidence that a simple lr warm-up wouldn't do just as well. T2 looks like it's useless. The methods are neither novel nor useful. How did reviewers at MLSys not see this?

I mean the paper is interesting. The pipelined execution model is interesting. The analysis is interesting, but the mitigation methods (ie the paper's contributions) are incremental at best. How does this get in?

Personally, I think the [SpecTrain paper](https://arxiv.org/abs/1809.02839) (similar topic, better mitigation but the method analysis isn't that good) is a much better paper that should have been published at conf but wasn't. NOTE: I am not an author on the SpecTrain paper.

If anyone is attending MLSys2021, could you question the authors on the points brought up in this post. My only request is that the questions be asked nicely. Like I said their analysis of delayed optimization is still really interesting, and they do explore the world of fine-grained pipelined training ie they're actually exploring non-mainstream execution models; even if Pipelined Backpropagation has existed since 1993, it hasn't really been used on modern NN.

&#x200B;

EDIT:

In the paper, Figure 7 and Figure 8 show that when the pipeline depth is **artificially** increased to be very large, T2 becomes useful, but at that point, pipelined training has a hard time achieving the same accuracy as SGD; at that point maybe you shouldn't even be using pipelined training at all?"
memgamemotron,MachineLearning,1619183370.0,[D] data science and ML bootcamp check!,"Anyone here completed any data science boot camps from Metis, flatiron, brain station, general assembly and others? I’m looking to see if these are legit and if they’ve helped you land a job in data science after completing the boot camps. They advertise a 92% hire rate from top tech companies, so curious if anyone can contribute any thoughts."
TopIndependent5791,MachineLearning,1618733779.0,[D] Which AWS tools/services are necessary to learn for Machine Learning Engineer?,"What AWS tools I need to learn in order to be more ""qualified"" for the industry?

I have been personally involved in machine learning through college and personal projects, but I feel I lack some entry level industry knowledge?

What would be the right resources for the AWS?"
mgl96,MachineLearning,1617264240.0,[N] Trankit v1.0.0 - An open-source Transformer-based Multilingual NLP Toolkit for 56 languages is out.,"Hi everyone,

We've just released the version v1.0.0 for our Transformer-based Multilingual NLP toolkit named Trankit, which outperforms the popular SOTA Stanford NLP (Stanza) in many tasks over 56 different languages.

💥 💥 💥 The new version v1.0.0 offers:

* **A trainable transformer-based pipeline for fundamental NLP tasks over 100 languages**.
* **90 new pretrained transformer-based pipelines for 56 languages**. The new pipelines are trained with XLM-Roberta large, which further boosts the performance significantly over 90 treebanks of the Universal Dependencies v2.5 corpus. For **English**, Trankit is significantly better than Stanza on sentence segmentation (**+9.36%**) and dependency parsing (**+5.07%** for UAS and **+5.81%** for LAS). For **Arabic**, our toolkit substantially improves sentence segmentation performance by **16.36%** while **Chinese** observes **14.50%** and **15.0%** improvement of UAS and LAS for dependency parsing. Performance on other languages is also significantly improved. The detailed comparison between Trankit, Stanza, UDPipe, Spacy on other languages can be found [here](https://trankit.readthedocs.io/en/latest/performance.html#universal-dependencies-v2-5) .
* **Auto Mode for multilingual pipelines**. In the Auto Mode, the language of the input will be automatically detected, enabling the multilingual pipelines to process the input without specifying its language. Check out how to turn on the Auto Mode [here](https://trankit.readthedocs.io/en/latest/news.html#auto-mode-for-multilingual-pipelines).
* **Command-line interface** is now available to use. This helps users who are not familiar with Python programming language can use Trankit more easily. Check out the command-line tutorials on this [page](https://trankit.readthedocs.io/en/latest/commandline.html).

**Trankit is written in Python** and can be easily installed via pip. Our code and pretrained models are publicly available at: [https://github.com/nlp-uoregon/trankit](https://github.com/nlp-uoregon/trankit)

We also created a documentation page and a demo website for Trankit.

Documentation page: [https://trankit.readthedocs.io/en/latest/index.html](https://trankit.readthedocs.io/en/latest/index.html)

Demo website: [http://nlp.uoregon.edu/trankit](http://nlp.uoregon.edu/trankit)

Technical details about Trankit can be found in our paper: [https://arxiv.org/pdf/2101.03289.pdf](https://arxiv.org/pdf/2101.03289.pdf)

Thank you for your time reading this post!

Hope you enjoy Trankit!"
RyanAI100,MachineLearning,1620584248.0,[D] Pooled Contextualised Embeddings for NER | Research Papers Summary 017,
hardmaru,MachineLearning,1620197291.0,[R] MLP-Mixer: An all-MLP Architecture for Vision,
orenog,MachineLearning,1619488275.0,[D] Node Collapse with StyleGAN2 - ada,"Hey reddit, I have a problem.

I trained StyleGAN2 ada on 3100 Hearthstone's cards,the results were amazing, but they started look more and more the same, untill it got to a point where it just generated the same 3 cards! 1 spell and 1 or 2 minions, and all of them with a very similar artwork.

I tried again with all of the augmentation pipeline options on and got the same results! It node collapsed after 20 ticks and just got less and less variation as it went on!


I wanted to ask, what are the ways to deal with node collapse?  And what are the ways to deal with node collapse specifically in StyleGAN2 ada.

The dataset seems like it has a lot of variation, it's 3100+ cards, they look similar for the most parts, but there are different types of cards and each card has a unique artwork, but somehow the generated artworks looked the same after 40 ticks! 

I really want it to works, any advice or direction towards solution will be super helpful, I don't even know what's causing it.

Thanks

Edit: apparently it's mode collapse, not node collapse"
Farconion,MachineLearning,1620326515.0,[D] Any value in negative / null results?,"I am in the middle of completing a small independent research project  under a ML professor at my college. Ideally my project was aiming to  reimplement / replicate the results of a previous paper from a couple of  years ago that had a decent number of citations (\~100), and then extend  upon their work in some simple but worthwhile ways.

However  after reimplementing their code (really old PyTorch that had no  compatibility with my hardware) and running some basic experiments, I  failed to replicate even their most basic results. I plan on running  more experiments to further confirm this, but I'm not getting my hopes  up.

Obviously there are any number  of reasons for this - changes in PyTorch, errors in my implementation  (I feel confident in it but still), etc. - so trying to write or publish  something saying ""technique X doesn't work because I couldn't replicate  it"" doesn't feel worthwhile or correct.

Does  anyone have any tips on the best way to utilize null results like this?  I was hoping to use this project as a part of my grad school apps, but  in its current state I'm not sure what the best way to frame it - or  even if I can."
robust_melon,MachineLearning,1619475121.0,[D] ethical considerations with launching a website with vehicle accident density forecasts,"I have been working on a project for predicting the distribution of accidents over city locales over short-term forecasting horizons (~1 hour). I wish to create a public page/dashboard that is updated hourly with these forecasts.

**Assuming that I have a great model, which predicts where accident hot spots are geographically likely to occur in the next hour -what are the ethical ramifications, if these forecasts were made public?** Should I assume that someone with ill intent can, and will, use this information with criminal intent (e.g. infer where emergency response resources will likely be)?


Edit: bolded for emphasis"
downtownslim,MachineLearning,1618939161.0,[R] The Power of Scale for Parameter-Efficient Prompt Tuning,
RyanAI100,MachineLearning,1619377883.0,[D] BERTweet (SOTA) for Named Entity Recognition in Social Media | Research Papers Summary 015,
biotechdood,MachineLearning,1617993459.0,[D] Looking for an interesting paper in the field of biotech and AI,"Hi,

I  will be having a presentation about a chosen paper in a few days and am  looking for revolutionary scientific discoveries that contain AI in the  domain of bio/biotech/pharma/bioprocess engineering/etc. I would be  really thankful if you could help me out.

Ideally  it shouldn't be too complicated for laymen coming from the bio area and  I would appreciate if it was something (supposedly) revolutionary such  as AlphaFold 2.

Thank you in advance and kind regards

\*I would also appreciate some paper recommendations in the field of computer vision and microscopy"
hotpot_ai,MachineLearning,1617654345.0,[D] Overview on SOTA methods for deep learning model compression,"**Intro**

This post covers model inference optimization or compression in breadth and hopefully depth as of March 2021. This includes engineering topics like model quantization and binarization, more research-oriented topics like knowledge distillation, as well as well-known-hacks.

Each year, larger and larger models are able to find methods for extracting signal from the noise in machine learning. In particular, language models get larger every day. These models are computationally expensive (in both runtime and memory), which can be both costly when served out to customers or too slow or large to function in edge environments like a phone.

Researchers and practitioners have come up with many methods for optimizing neural networks to run faster or with less memory usage. In this post I’m going to cover some of the state-of-the-art methods. If you know of another method you think should be included, I’m happy to add it. This has a slight PyTorch bias (haha) because I’m most familiar with it.

**URL**

[https://rachitsingh.com/deep-learning-model-compression/](https://rachitsingh.com/deep-learning-model-compression/)"
federico-bianchi,MachineLearning,1620243697.0,[N] Coveo SIGIR Data Challenge for Ecommerce,"For those interested, [Coveo](https://www.coveo.com/) is organizing the SIGIR Data Challenge.

Useful to apply Deep Learning/ML skills on one of the biggest eCommerce datasets ever released, including fine-grained shopping behavior, search queries with clicked/unclicked items, and product meta-data from catalogs. Great chance to get creative and write a research paper!

[Challenge website](https://sigir-ecom.github.io/data-task.html)

[Repository](https://github.com/coveooss/SIGIR-ecom-data-challenge)

*Important Dates*:

* Registration ends: June 1st, 2021
* Final leaderboard: June 15th, 2021
* SIGIR eCom Full-day Workshop: July 15th, 2021"
Chriscbe,MachineLearning,1617135089.0,[R] Please point me in the right direction: decision trees or possibly something better,"ML Practitioners:

I am a scientist who studies how to purify recombinant proteins. I am generally proficient with Python. I am looking for ways to fortify decsion making during purification of proteins created in E coli and cell culture. What parts of ML would be best to learn in order to improve planning/ decision making in a decion-tree like format? I have seen some literature on the issue but I'd like to start from a point where I can understand, for example, which parts of (sci-kit learn?) might address the issue, etc.

Thanks for any help"
amaigmbh,MachineLearning,1620394984.0,"[N] i.am.ai Newsletter - Updated AI Conference Calendar, Crowdsourced Speech Recognition, Dinos and more","[Hello there](https://www.youtube.com/watch?v=eaEMSKzqGAg), just here to share the [latest edition](https://www.am.ai/en/blog/newsletter-011/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit) of our i.am.ai Newsletter. Hope this interesting for some and not too annoying self promotion. Read the whole thing below and feel free to [subscribe](https://i.am.ai/newsletter/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit). Feedback welcome :)

# WHATS IMPORTANT

## 📍 ICLR 2021 Conference

The landmark Deep Learning conference [ICLR](https://iclr.cc/) is taking place from May 3rd to May 7th. For a second time the gathering is held completely virtual. Researchers from around the world are gathering for poster sessions, keynotes and workshops. Out of 2997 papers, 860 were accepted for the conference. The official **Outstanding Paper Awards** have been awarded for 8 papers of ""exceptional quality"" leading up to the conference. You can find a full list [here](https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab).

Meanwhile, Topbots made its own selection of ICLR conference papers, with **""breakthrough potential""**. These include the Visual Transformer (ViT), DETR, DeBERTa and Performers papers we previously introduced in this Newsletter. Read more about each breakthrough paper on [topbots.com](https://www.topbots.com/iclr-2021-research-papers/).

 

## 📍 More to Come

ICLR only marks the beginning of the annual Conference Summer. With the global health situation only improving slowly, more conferences have moved to virtual meetings. Find an **updated version of the AI Conference Calendar** below and more about [the most important AI conferences here](https://www.am.ai/en/blog/ai-conferences-2021/?utm_campaign=Newsletter+11+Post+r/MachineLearning&utm_medium=Social&utm_source=Reddit).

https://preview.redd.it/y0rbh01ecpx61.png?width=2052&format=png&auto=webp&s=d2bc5cb19e7abcc7229bd161a208d765f3006797

 

## THINGS WE FOUND WORTH SHARING

🚀 **Contribute** – The **crowdsourcing project Common Voice** aims to create a free database for speech recognition in 50+ languages. Since 2017 Mozilla asks volunteers to record sample sentences and review recordings of others. They recently saw a USD 1.5 million investment by Nvidia and are continuously looking for more participants. [Contribute here (switch the language in the top right)](https://commonvoice.mozilla.org/en).

 

  📚 **A personal recommendation from** [**AMAI**](https://linkedin.com/company/amai-gmbh) **CEO** [**Jürgen Stumpp**](https://www.linkedin.com/in/juergen-stumpp/):

>""The book **""Real World AI: A Practical Guide for Responsible Machine Learning""** provides executives with a quick but thorough overview of the steps necessary for successful AI projects. Moreover, it can help soon-to-be university graduates to prepare for work in the field.""

In their [book](https://www.goodreads.com/book/show/57347702-real-world-ai), Alyssa Rochwerger, Director of Product at Blue Shield and Appen CTO Wilson Pang, share their practical experiences and present an approach that claims a 3x higher success rate for AI projects compared to industry average. Learn more about **""Real World AI: A Practical Guide for Responsible Machine Learning""** in this [authors interview](https://www.forbes.com/sites/tomtaulli/2021/05/05/real-world-ai-a-great-guide-for-managers/).

 

📄 **Paper** – DINO: In the paper ""Emerging Properties in Self-Supervised Vision Transformers"" ([arXiv](https://arxiv.org/abs/2104.14294)), researchers from Inria, Facebook AI and Sorbonne University introduced **DINO**. Short for ""self-**di**stillation with **no** labels"", this method can segment images in a self-supervised manner, meaning without labels required upfront.

Read more about DINO and the accompanying PAWS on [venturebeat.com](https://venturebeat.com/2021/04/30/facebook-details-self-supervised-ai-that-can-segment-images-and-videos/) and the Facebook AI Blog [ai.facebook.com](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/). One-man paper discussion group, **Yannic Kilcher**, walks his viewers through the paper in detailed 39 minutes on [youtube.com](https://www.youtube.com/watch?v=h3ij3F3cPIk).

[\\""These self-attention maps for selected heads were generated using DINO with videos of a horse, a BMX rider, a puppy, and a fishing boat.\\"" - from Facebook AI](https://reddit.com/link/n6yspn/video/xghum660cpx61/player)

   

📈 **Markets** – Microsoft acquired Nuance in April, a Massachusetts company focussing on AI-driven speech recognition. At USD 19.7 billion this merger marks the second highest acquisition in Microsoft's history (LinkedIn was acquired for 26.2 billion in 2016). Nuance Communications is behind the speech recognition capabilities of Apple's voice assistant, Siri. More on [axios.com](https://www.axios.com/microsoft-readies-deal-frenzy-bbc807a2-fd5b-48a7-99d1-cf06d0a41547.html).

 

🧑‍⚖️ **Regulation** – The European Commission released the **Artificial Intelligence Act**, a [108-page](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence) proposal for regulation of AI. More on the proposal and the legislative hurdles ahead, on [politico.eu](https://www.politico.eu/article/6-key-battles-europes-ai-law-artificial-intelligence-act/).

Outside the EU, Chinese regulators have begun to enforce **data localization** on local companies. From now on data can only be stored in China for certain applications such as facial recognition ([biometricupdate.com](https://www.biometricupdate.com/202104/china-pushes-standards-for-face-biometrics-and-plenty-more)).

 

🤗 **Tongue in Cheek** – After [XKCD](https://xkcd.com/2456/) posted 12 types of scientific papers, [Natasha Jaques](https://twitter.com/natashajaques) and [Max Kleiman-Weiner](https://twitter.com/maxhkw), two PhDs from MIT, put together [12 Types of Machine Learning Papers](https://twitter.com/natashajaques/status/1387859601555554304).

[from XKCD with edits by Natasha Jaques](https://preview.redd.it/2mow1e47cpx61.jpg?width=1080&format=pjpg&auto=webp&s=0904984b236700acef5a8ba13b858ebaef0a466e)

 

👁 **Brief** – 15 Graphs You Need to See to Understand AI in 2021: In the last issue we shared the Stanford [2021 AI Index Report](https://hai.stanford.edu/research/ai-index-2021). From that Eliza Strickland takes 15 visualizations to highlight the most important developments. Scroll through on [ieee.org](https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/the-state-of-ai-in-15-graphs).

 

🎓 **Education** – One of the best educational resources for AI is Andrew Ng's Deep Learning  specialization on Coursera. The course just received the 2021 update. All programming exercises are now in Tensorflow 2 and the syllabus includes Transformer Networks. Find the updated course here [coursera.org](https://www.coursera.org/specializations/deep-learning).

 

## EVENTS 

📅 May 12 (online, 16:00 CET) – **How AI turns existing Business Models Upside Down** – ""Join online to interactively hear some astonishing inputs about disruptive AI business models from the Harvard Post Doc Johannes"" from Merantix Labs. Organized by KI-Garage fro the institute of Entrepreneurship and Innovation Research from Stuttgart University and our partner the German Digital Hub Initiative. – Register [here](https://www.bwstiftung.de/de/veranstaltungen/how-ai-turns-existing-business-models-upside-down)."
czhu12,MachineLearning,1617908894.0,[P] A torchvision transforms visualizer,"The `torchvision.transforms` library is used in almost all CV projects but the documentation is a bit hard to visualize. I threw [an app together](https://pytorch-transforms-builder.chriszhu.me/) for myself that some of my teammates also found useful so I figured I'd share it with the community.

It's built with Streamlit and hosted on google cloud run, which I've found to be a nice way to deploy these kinds of things.

See it [here](https://pytorch-transforms-builder.chriszhu.me/)

Let me know if this is useful for anyone else. I'm happy to spend a few weekends making it better / more helpful!

https://preview.redd.it/nnez3sv1zzr61.png?width=958&format=png&auto=webp&s=fc42086e7971a9ff0e9b5b3f5af43d673ee068f0

https://preview.redd.it/g1cktqv1zzr61.png?width=955&format=png&auto=webp&s=8131af14c77fc705f5c2c0801a5184ab17bb0e7e"
Combination-Fun,MachineLearning,1617887342.0,[R] Video explaining Normalization Free Nets paper,"Here is a video explaining the idea of the NF-Nets paper titled ""High-Performance Large-Scale Image Recognition Without Normalization"". Hope its useful. Enjoy: [https://youtu.be/AzKFgjrbR2o](https://youtu.be/AzKFgjrbR2o)"
jary93,MachineLearning,1620411298.0,Structured Ensembles: an Approach to Reduce the Memory Footprint of Ensemble Methods,
ashubham,MachineLearning,1617060191.0,[P] Prediction Trees in Pure Javascript,"I recently learnt about compact prediction trees and thought, this would be a great use case to do in the web browser. Hence,built this library [https://github.com/ashubham/CPT](https://github.com/ashubham/CPT)

Feedback, welcome."
PeupleDeLaMer,MachineLearning,1618982723.0,[R] Looking for Paper Recommendations for characterising model performance/ Assurance,"Hello!

I'm relatively new to ML research and I'm reading up on literature that can help a user characterise the performance of a model in order to understand which points in the input space it is considered reliable and which points not. Some examples include [Algorithmic Assurance](https://paperswithcode.com/paper/algorithmic-assurance-an-active-approach-to), and [Adversarial Attacks](https://openai.com/blog/adversarial-example-research/) (this is not exactly what I'm after but on the right track)

I haven't found a huge amount of work related to this so I figured I'd ask :)"
techsucker,MachineLearning,1617380494.0,[N] Introducing PyTorch Profiler – The New And Improved Performance Debugging Profiler For PyTorch,"The analysis and refinement of the large-scale deep learning model’s performance is a constant challenge that increases in importance with the model’s size. Owing to a lack of available resources, PyTorch users had a hard time overcoming this problem. There were common GPU hardware-level debugging tools, but PyTorch-specific background of operations was not available. Users had to merge multi-tools or apply minimal correlation information manually to make sense of the data to retrieve the missing information.

The PyTorch Profiler came to the rescue, an open-source tool for precise, efficient, and troubleshooting performance investigations of large-scale deep learning models. 

Summary: [https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/](https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/)

Source: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/"
hyunwoongko,MachineLearning,1617286558.0,"[P] Asian language BART models (English, Chinese, Japanese, Korean and ECJK mixed)","https://preview.redd.it/wznspj3klkq61.png?width=1028&format=png&auto=webp&s=a234ac4af8d1ab75f5f413413d0bf1a1e928084e

Hello. I am hyunwoongko who is studying natural language processing. In many Asian languages such as Chinese, Korean and Japanese, the pre-trained sequence to sequence model is necessary, but it is currently lacking.

&#x200B;

So I made Chinese, Japanese, Korean, and English Bart models by pruning the embedding layers of the facebook mBART model. The mBART model is a multilingual language model with 25 languages. However, if we only need one language (e.g., Japanese), the mBART's vocab and token embeddings are unnecessary but take up space. Therefore, I have organized only the necessary tokens in each language so that this model can be used in the monolingual setting.

&#x200B;

Please check the following link for detailed usage.

[https://github.com/hyunwoongko/asian-bart](https://github.com/hyunwoongko/asian-bart)"
kongxianxingren,MachineLearning,1620585692.0,[D] Are Centroidal Voronoi tessellation and Voronoi tessellation unsupervised learning in machine learning?,"The centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation in which the generating point of each Voronoi cell is also its centroid (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators.

&#x200B;

The Voronoi tessellation is a partition of a plane into regions close to each of a given set of objects.

&#x200B;

The following is the simple picture of centroidal Voronoi tessellations which I found in [wiki](https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation).

&#x200B;

https://preview.redd.it/ircxp2xr35y61.png?width=560&format=png&auto=webp&s=4af24b1912d7f7455e4c03baaa95e57f4a112d03

&#x200B;

For me, it likes the K-means algorithm and it can be concluded as a clustering method in unsupervised learning. Am I right? The reason I ask is that I didn't see anybody related CVT to the machine learning algorithms."
Uncommented_python,MachineLearning,1616368763.0,[D] An example of machine learning bias on popular. Is this specific case a problem? Thoughts?,
mrathi12,MachineLearning,1618417597.0,[D] Addressing Gender Bias in Neural Machine Translation,"Hey everyone,   
I made a video discussing how we can fix bias in Neural Machine Translation: [https://youtu.be/p21FjLMf0Fw](https://youtu.be/p21FjLMf0Fw)  


Every so often, we get a viral tweet about how when it comes to assigning gender from a gender-neutral language like Hungarian, Google Translate defaults to the stereotypical translations.  
This video looks at how we evaluate gender bias (using datasets such as WinoMT [https://www.aclweb.org/anthology/P19-1164.pdf](https://www.aclweb.org/anthology/P19-1164.pdf)), a discussion of how the current solution by Google Translate adds to intellectual debt, and finally what I believe is the most practical approach to debiasing large language models - Domain Adaptation. [https://arxiv.org/abs/2004.04498](https://arxiv.org/abs/2004.04498)    


Personally I think that curating a large dataset on the scales we need is impractical, and that it's much more practical to fine-tune on small curated datasets: [https://link.springer.com/article/10.1007/s10676-021-09583-1](https://link.springer.com/article/10.1007/s10676-021-09583-1)

Would be interested to hear your thoughts as to what you think the most practical approach for debiasing is.   


Twitter thread discussing this: [https://twitter.com/mukulrathi\_/status/1382094171977216000?s=20](https://twitter.com/mukulrathi_/status/1382094171977216000?s=20)   
Transcript: [https://mukulrathi.co.uk/google-translate-is-biased/](https://mukulrathi.co.uk/google-translate-is-biased/)"
CrypticParagon,MachineLearning,1617351673.0,[D] When does stratified k-fold cross-validation provide worse performance than standard k-fold?,"I used sklearn to train a random forest model with all default parameters. The dataset is about ~550k data points, 7 features. Binary classification, 0 or 1. All 7 features are simple likelihoods of label 1 given a specific value of a categorical feature.

I'm using the built-in cross-validator, 10 folds. The only thing I'm changing is standard vs. stratified, and stratified performs slightly worse. Why would this be?"
jj4646,MachineLearning,1619239890.0,"[D] Relationship Between Kernels, Neural Networks and Gaussian Process"," ""At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit , thus connecting them to kernel methods ""

Can someone please provide an intuitive example as to why there is some sort of connection between neural networks, gaussian processes and kernels?

Thanks

Source: [https://arxiv.org/pdf/1806.07572.pdf](https://arxiv.org/pdf/1806.07572.pdf)"
Seankala,MachineLearning,1620131965.0,[D] Do you guys take out time to study math?,"For a few months now I've been taking out time everyday just to study math. I'm in the 4th semester of my MSCS degree, but my undergrad background wasn't in STEM and I always feel like I lack a strong mathematical foundation. Also, when I read papers with a lot of mathematical formulation sometimes I get intuitively understand what the equations mean but often it takes me hours or even days of Googling.

I'm just curious whether anyone else goes out of their way to brush up/study math stuff. Sometimes it feels futile but I try and tell myself otherwise."
jj4646,MachineLearning,1619239577.0,[D] Automatic Feature Engineering during Deep Learning,"I have often heard that one of the reasons that deep learning methods are preferred over other machine learning methods is because algorithms like deep neural networks do not require the analyst to spend as much time ""selecting variables for the model"" (i.e. feature engineering, feature selection, feature extraction).

Apparently, deep neural networks are able to ""intelligently"" (in the background) consider and create many different combinations of ""features"" that are ""conducive"" to the modelling problem.

Naturally, I was curious about this claim. Intuitively, I understand that through the hidden layers, weights and activation functions, neural networks are making ""new combinations"" of features that are passed forward and are ultimately used for making predictions on new data.

Beyond this, I am not sure what to think - are there any references/papers that have documented (either theoretically or empirically) that deep neural networks are able to largely ""take care"" of the task of ""feature engineering"" compared to traditional algorithms like regression models, decision trees and random forest? Have any experiments been done where many irrelevant features were added to a dataset, and a deep neural network was able to ""ignore"" them?"
NotThatGuy97,MachineLearning,1617633514.0,[D][P] Feedforward Noise Cancelling Project. Looking for some advice before i dive into details.,"  

Hi I am beginner with ML and finished one minor project yet using a simple neural network.

Now I am heading towards a new project which is more complex and searching for good approach.

Since I have not much experience, I want to introduce you the project and hoping for some ideas. 

(Which approach makes sense NN, RNN, RL or alternative approaches?)

The main goal of the project is to find parameters (amplitude (A\_i)  and phase  (P\_i) at fixed frequencies (i) (i=50Hz, 100Hz… ) for a sinusoid ) for a signal generator. The generated signal is then added to the “main” signal in order to cancel out some noise (which consist of these n x 50Hz frequencies). Basically, it is a noise channeling problem and can be solved by a feedforward algorithm.

The input is a measurement (duration 2-8 seconds) of the main signal. If a Fourier transformation (FFT) is performed the amplitudes of the 50Hz harmonics (50Hz,100Hz,150Hz…) are clearly observable (in the FFT). With the correct A\_i and P\_i the amplitude of the frequency i can be suppressed (so the frequency is compensated by the additional introduced signal created by our signal generator). The goal is now to find these perfect A\_i and P\_i for each frequency i.

The first measurements have shown that for each FFT\_amplitude(frequency i), a unique minimum can be found and a gradient towards this minimum has been observed ( simplified: FFT\_amplitude= (A\_i)\^2+(P\_i)\^2) .

The loop for the optimization would look like:

1. Measurement of the main signal --> FFT

2. Input (n x 1 array) : \[FFTamptitude\_1,…., FFTamptitude\_n\]

3. Optimization algorithm (NN, RNN, RL ) ?

4. Output (n x 2 array): \[\[A\_1,P\_1\],\[A\_2,P\_2\],…. ,\[A\_n,P\_n\]\]

5. Signal processing

6. Measurement of the main signal with the added signal

&#x200B;

The Loss function could look like:

Loss = sum ( FFTamptitude\_i ), (trying to minimize the loss function)

&#x200B;

Since FFT\_amplitude= (A\_i)\^2+(P\_i)\^2, a training simulation can be created in order to “pre”-train the algorithm. 

&#x200B;

&#x200B;

I hope I made the situation clear.

One problem to me:

The output is an array of parameters, but the loss function has to be calculated with the measurement of point 6 (see above). I would have to treat the output as a layer in order to get the backpropagation working right? (Please add some advice)

Furthermore, the algorithm has always to wait for the Measurement. Is that an Issue? I want to use Keras.

&#x200B;

What do you think? Are NN, RNN, RL capable to solve this task?

Which would be the best approach? What problems do you see? 

Do you know any comparable project or paper? (Please share it)"
timscarfe,MachineLearning,1618582221.0,[D] Francois Chollet interview on ML Street Talk," In today's show we are joined by Francois Chollet.

He has extremely interesting views on intelligence as generalisation, abstraction and an information conversation ratio. He wrote on the measure of intelligence at the end of 2019 and it had a huge impact on my thinking. He thinks that NNs can only model continuous problems well, which have a smooth learnable manifold and that many ""type 2"" problems which involve reasoning and/or planning are not suitable for NNs. He thinks that many problems have type 1 and type 2 enmeshed together. He thinks that the future of AI must include program synthesis to allow us to generalise broadly from a few examples, but the search could be guided by neural networks because the search space is interpolative to some extent.  

&#x200B;

Video: [https://youtu.be/J0p\_thJJnoo](https://youtu.be/J0p_thJJnoo)

Pod: [https://anchor.fm/machinelearningstreettalk/episodes/51-Francois-Chollet---Intelligence-and-Generalisation-ev1i79](https://anchor.fm/machinelearningstreettalk/episodes/51-Francois-Chollet---Intelligence-and-Generalisation-ev1i79)"
anon-burner-5981,MachineLearning,1620412872.0,[D] Parting Gifts for Lab Director and Ph.D. Student,I've spent the last two years working for an amazing lab. Next year I'll be leaving. They have had a tremendous impact on me and I'd like to be able to give them something to show my appreciation. What would be good gifts for a mentor and Ph.D. student?
Square365,MachineLearning,1617649803.0,[D] Good video dataset labeling services? (Frame By Frame),"Hello i'm sort of new to ML and I have been trying to make a dataset to train image recognition with yolov4 for a few weeks now. Im using DarkLabel and i have an aproximate of 24k frames from 34 videos labeled, im planning to label another 500 but I would like to know if there are any good services that label at that rate. I have seen amazon mturk but they only do box in images and in videos they only do the topic. 

I got an estimate from someone willing to do 10videos/5$, but I would like to get your opinions on this."
AristocraticOctopus,MachineLearning,1619491330.0,[N] Toyota subsidiary to acquire Lyft's self-driving division,"After Zoox's sale to Amazon, Uber's layoffs in AI research, and now this, it's looking grim for self-driving commercialization. I doubt many in this sub are terribly surprised given the difficulty of this problem, but it's still sad to see another one bite the dust.

Personally I'm a fan of Comma.ai's (technical) approach for human policy cloning, but I still think we're dozens of high-quality research papers away from a superhuman driving agent.

Interesting to see how people are valuing these divisions:
>Lyft will receive, in total, approximately $550 million in cash with this transaction, with $200 million paid upfront subject to certain closing adjustments and $350 million of payments over a five-year period. The transaction is also expected to remove $100 million of annualized non-GAAP operating expenses on a net basis - primarily from reduced R&D spend - which will accelerate Lyft’s path to Adjusted EBITDA profitability."
Symbiot10000,MachineLearning,1618176734.0,[D] Practical benefits of unlocking vGPU functionality in NVIDIA cards for ML?,"Over the weekend there has been a lot of [attention](https://news.ycombinator.com/item?id=26754351) [given](https://old.reddit.com/r/hardware/comments/mnord0/unlock_vgpu_functionality_for_consumer_grade_gpus/) to a [new software method](https://github.com/DualCoder/vgpu_unlock) to unlock vGPU functionality in consumer NVIDIA cards.

What does this mean, if anything, for smaller ML outfits that could now divide up GPU resources across VMs and subdivide the GPU across different ML tasks?"
Competitive-Rub-1958,MachineLearning,1618587934.0,[D] Can some other organization/company replicate GPT-3 for their own use?,"GPT-3 in itself does not create much of a new 'innovation' per se, being an overfitted model on a huge amount of data and a large number of parameters, probably based on it's predecessors architecture more or less.

So, is it easy for companies like Google (FAANG companies) to replicate GPT-3 sized NLP models for their own use?

But the Million dollar question - if it is indeed easy for them to do so, how much advantage does OpenAi have against them?

what prevents google from launching a new Model bigger and better, and offering it to consumers cheaper than OpenAi+MS can?"
lukeiy,MachineLearning,1617422679.0,[P] A TF implementation of AdamW with a One-Cycle policy,"&#x200B;

https://preview.redd.it/3wuc17rluvq61.png?width=677&format=png&auto=webp&s=7b5955c84bc3fb8e049fd2079eac6c2e0559cc78

[https://github.com/LukeBolly/OneCycleAdamW](https://github.com/LukeBolly/OneCycleAdamW)

When the article from [fast.ai](https://fast.ai) was originally posted, the reception was.... mixed. I've recently implemented it though, and for my models it sped up training and remains stable at higher learning rates. I added a small example to compare it against regular Adam + AdamW. Let me know if you have any issues.

Cheers!"
proximauri,MachineLearning,1616976163.0,[D] Face recognition: classification vs distances between embeddings,"Hi,  I am new to face recognition methods. I have noticed that one popular approach to use pre-trained models on new datasets is to measure distances between two embeddings.

I don't understand however why this is preferred over using a pre-trained network without the top layer and just fine tune it with a new fully connected layer on top of the model.

Honestly I don't even understand how to train the model with the first approach. Could someone clarify it to me please?"
alexandrea_pierrick,MachineLearning,1617958906.0,[D] Unsupervised document similarity state of the art,"I have a set of N documents with lengths ranging from 0 to more than 20000 characters. I want to calculate a similarity score between 0 and 1 between all pairs of documents where a higher number indicates higher similarity. Assume below that deploying a supervised model is infeasible due to resource constraints that are not necessarily data science related (gathering labels is expensive, infrastructure cannot be approved for supervised models for whatever reason etc).

&#x200B;

\*\*Approaches I have considered\*\*:

1. tf-idf

2. Smooth Inverse Frequency (SIF) embeddings and its developments (uSIF, p-SIF). [https://openreview.net/pdf?id=SyK00v5xx](https://openreview.net/pdf?id=SyK00v5xx) [https://www.aclweb.org/anthology/W18-3012/](https://www.aclweb.org/anthology/W18-3012/) [https://arxiv.org/abs/2005.09069](https://arxiv.org/abs/2005.09069)

3. BERT or bert-like embeddings, e.g., [https://arxiv.org/abs/2010.06467](https://arxiv.org/abs/2010.06467)

4. Hierarchical Optimal Transport for Document Representation (HOTT): [https://papers.nips.cc/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html](https://papers.nips.cc/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html)

&#x200B;

\*\*Question\*\*:

Is there an unsupervised technique that has been shown in a peer-reviewed setting to achieve higher accuracy (or F1 or similar) on long texts (more than, say, 10000 characters) than HOTT?

&#x200B;

\*\*Background\*\*:

The HOTT paper benchmarks various approaches with a k-NN classifier and shows that HOTT performs best, but not dramatically better than tf-idf (HOTT has 0.52 vs tf-idf's 0.66 normalized error). Note that while the HOTT algorithm is unsupervised the datasets in the paper are labeled, otherwise a benchmark would not be possible.

The SIF papers mostly deal with the STS datasets which are not long texts. p-SIF has a benchmark on the Reuters dataset, but uses a SVM supervised approach. Interestingly, the HOTT paper finds that SIF does not perform well with the k-NN approach with 0.79 normalized error.

In many cases BERT requires pre-training and if it does not, its max or average pooled performance (pooling over BERT layers) appears to be worse than glove embeddings ([https://arxiv.org/abs/2010.06467](https://arxiv.org/abs/2010.06467) page 114).

I have also not been able to find unsupervised benchmarks for Doc2Vec, Universal Sentence Encoder (USE).

&#x200B;

There is additionally the question of how to calculate the similarity once the embedding is obtained (e.g., [https://www.aclweb.org/anthology/N19-1100.pdf](https://www.aclweb.org/anthology/N19-1100.pdf)) but that is out of scope for this question unless it affects comparison between unsupervised benchmarks (e.g., the k-NN approach can use various distance metrics which may affect accuracy).

&#x200B;

If the benchmarks in HOTT are representative and no other methods exist that perform substantially better it is tempting to make the conclusion that tf-idf is still a strong approach since it is so simple to implement and understand (it is certainly simpler than HOTT). If that is the case I think it is a remarkable conclusion given the deep learning developments in the last 5-10 years.

edit: added ""(pooling over BERT layers)"""
noodlepotato,MachineLearning,1619701885.0,[D]Anyone reading Probabilistic Machine Learning: An Introduction? (Murphy's new textbook),"(Already posted in /r/learnmachinelearning but no answer)

How is it so far? Is there any additional/difference from the other edition, like notation-wise, I heard the first edition has pretty inconsistent notations. I'm asking because I'm planning to buy the 2012 Machine learning: a probabilistic perspective physical book but after checking the table of contents of the new one, I might consider buying a physical book of this edition instead. Just want to know what others think who's reading it now"
Guest_Basic,MachineLearning,1617344944.0,[D] AUC vs F-measure for binary classification for unbalanced target variable,"I'm working on building a binary classification predictive model. My target variable is extremely unbalanced (50K 1s and 14Million 0s). 

As I understand AUC is not a very good metric to evaluate my model and F-measure might be a better alternative. 

Question#1: is my assumption correct?


The model I built has a decent AUC (~0.85) and a really low F-measure (0.3). This makes me think that I've actually built a really bad model. However, all the existing published literature only claim an AUC of 0.6 and none of them report F-measure.

Question#2: If my goal is to build a better model than what currently exists... should I taking a victory lap right now?"
amourav,MachineLearning,1619529111.0,[D] Distributed data platform / framework,What are your thoughts on AWS Sagemaker and/or Horovod for distributed data training? What is your method of choice?
moajjem04,MachineLearning,1618508700.0,[D] Best Literature Review Practices,"My supervisor has assigned me a new topic to research on. Typically in any project, I mostly handle the experiment part. This time I thought I should do literature review as well. I know it is a huge deficiency on my part but I want to know about your best practices, dos and don'ts beyond searching google scholar with keywords.

tl;dr: help me senpai by providing me with your knowledge!🙌🙌🙌"
SlickBlueML,MachineLearning,1619025945.0,[P] Multilingual translation like Google Translate with PyTorch (+explanation),"I recently put together a tutorial for multilingual translation, but I think the code alone might be useful to people. It includes a demo you can play around with if you get it running and supports Chinese, Japanese, and English right off the bat. With the small amount of training it works surprisingly well.

I have it in a Colab so you don't need to worry about setup if you use that. If you do use the non-pro version of Colab though, make sure to change the model repo from  `google/mt5-base` to `google/mt5-small` or else you will get out of memory errors from CUDA.

Github code: [https://github.com/ejmejm/multilingual-nmt-mt5/blob/main/nmt\_full\_version.ipynb](https://github.com/ejmejm/multilingual-nmt-mt5/blob/main/nmt_full_version.ipynb)

Colab code: [https://colab.research.google.com/drive/1eGSCod03SjWD\_YOfwb33kMJOeDZGu7lP?usp=sharing](https://colab.research.google.com/drive/1eGSCod03SjWD_YOfwb33kMJOeDZGu7lP?usp=sharing)

There is also a video series that goes with it for anyone interested: [https://www.youtube.com/watch?v=HuZq5KkLx8Q&list=PL\_49VD9KwQ\_ObGMW5g9hMOLnDY01NHv91&index=2&t=1s](https://www.youtube.com/watch?v=HuZq5KkLx8Q&list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91&index=2&t=1s)"
wecmiw,MachineLearning,1618074938.0,[D] How do you share your models/ demos with others?,"Hi, I am a Phd student working with deep learning  and I often have to show demos of my models. I currently simply share results in a presentation and use notebooks when I want to show more explicit experiments on the spot.  Is there another way to do that? I could see this being done a lot especially in industry where you have to talk to many stakeholders?

I am aware of gradio but I was wondering if there are other ways.

Any insight is appreciated! :)"
hubert0527,MachineLearning,1617996004.0,[R] InfinityGAN: Towards Infinite-Resolution Image Synthesis,"[\\""Synthesizing infinite-resolution images from finite-resolution inputs.\\"" A 1024×2048 image composed of 242 patches, independently synthesized by InfinityGAN with spatial fusion of two styles. The generator is trained on 101×101 patches \(e.g., marked in top-left\) sampled from 197×197 real images. Note that training and inference \(of any resolution\) are performed on a single GTX TITAN X GPU.](https://preview.redd.it/4drmg9ez47s61.png?width=1285&format=png&auto=webp&s=5d5a1112aa090c1707e9e64a1e49c757480b2e3b)

***TL;DR***  We propose InfinityGAN towards a new problem of synthesizing infinite-resolution images. The model is trained with images of limited resolution, and generalizes to arbitrary resolutions at testing. We further demonstrate several applications with a trained generator in spatial style fusion, image outpainting, and image inbetweening.

**Project page**: [https://hubert0527.github.io/infinityGAN/](https://hubert0527.github.io/infinityGAN/)

**Paper**: [https://arxiv.org/abs/2104.03963](https://arxiv.org/abs/2104.03963)

We will release our code soon here: [https://github.com/hubert0527/infinityGAN](https://github.com/hubert0527/infinityGAN)"
SensitiveAnteater420,MachineLearning,1616960506.0,[D] Would it be possible to make a model that predicts where a picture is taken?,"I love OSINT and geolocating, and I'm a Python dev wondering if I can extend my boundaries. The title is self explaining: Is it possible to make a model that predicts where a picture is taken, and are there existing datasets for it?"
InsightFinder,MachineLearning,1617028825.0,[News] Women in computer science leadership talk on 4/9,"**Event alert - Women in Tech Leadership discussion with execs from Bank of America, Dell, and InsightFinder!**

On April 9, 2021 at 11 AM EST, **InsightFinder** **CTO and Founder Helen Gu** will participate in Women in Tech Leadership panel hosted by NC State's Women in Computer Science (WiCS) organization. Professor Gu teaches at NC State and is also a faculty leader of WiCS.  She will be joined by industry leaders Liz Holland, Vice President of Dell Technologies, Betsy Brady, Managing Director at Bank of America and moderated by NC State professor and WiCS faculty leader Dr. Lina Battestilli.

These leaders have agreed to share their stories - including but not limited to early career decisions, watershed career moments, and how they would advise young students and professionals today.  All are welcome to attend. **To join, register** [here (website form)](https://insightfinder.com/events/) **or** [here (google form)](https://docs.google.com/forms/d/e/1FAIpQLSe0HyYVPgLjvQiwOBls5IFMcZtdgRYnRZ3Yot8mN8Cn2Wr35g/viewform)**.**"
asivokon,MachineLearning,1617719141.0,[N] Grammarly releases a grammatical error correction (GEC) dataset for the Ukrainian language,"This dataset contains 20,715 sentences annotated for grammatical errors and fluency correction. The license is CC-BY 4.0.

This blog post provides some context: [https://www.grammarly.com/blog/engineering/announcing-ua-gec/](https://www.grammarly.com/blog/engineering/announcing-ua-gec/)

The data and code are on Github: [https://github.com/grammarly/ua-gec](https://github.com/grammarly/ua-gec)

Paper (draft): [https://arxiv.org/abs/2103.16997](https://arxiv.org/abs/2103.16997)

I am one of the authors. Happy to answer your questions :)"
shreyansh26,MachineLearning,1620580380.0,[P] BERT - Annotated Paper + Paper Summary,"Everyone who is interested in NLP or even DL and ML for that matter, has definitely heard about the BERT family of models. BERT, RoBERTa, DistilBERT and many many more. This paper ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"" first introduced this and it has now completely changed the way AI practitioners are solving and looking at NLP problems these days.

As a part of my Paper Notes series, I have gone through the paper and created an informative summary of the paper. This time it goes a bit longer than the previous paper summaries, but it had to be done. The paper contained many tiny interesting nuggets that I had to include. Check out the links below and happy reading!

Paper Summary -  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf)"
techsucker,MachineLearning,1618972153.0,[R] Researchers Introduce a Convolutional Neural Network (CNN)-Based Model that Automates the Distinction Between Natural Images and Computer-Generated Images (CGI),"With the increasing performance accuracy of computer software systems, the realistic appearance of computer-generated images (CGI) and deepfakes often leads to assuming them as authentic images.

Researchers at the Changsha University of Science and Technology and Hunan University in Hunan, China, have recently developed an image source pipeline forensics method based on convolutional neural networks (CNN) to automate the distinction between natural images and CGI. The work announced in the *International Journal of Autonomous and Adaptive Communications Systems* describes that the CNN-based model is fine-tuned using a database of 10000 images.

Summary: [https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/](https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/) 

Paper: http://www.inderscience.com/offer.php?id=114295"
Yuqing7,MachineLearning,1617064776.0,[N] DeepMind & Alberta U Introduce Novel Search Algorithm: Policy-Guided Heuristic Search with Guarantees,"A research team from DeepMind and Alberta University proposes Policy-guided Heuristic Search (PHS), a novel search algorithm that uses both a heuristic function and a policy while offering guarantees on the search loss that relate to both the quality of the heuristic and the policy.

Here is a quick read: [DeepMind & Alberta U Introduce Novel Search Algorithm: Policy-Guided Heuristic Search with Guarantees](https://syncedreview.com/2021/03/29/deepmind-alberta-u-introduce-novel-search-algorithm-policy-guided-heuristic-search-with-guarantees/)

The paper *Policy-Guided Heuristic Search with Guarantees* is on [arXiv](https://arxiv.org/pdf/2103.11505.pdf)."
vwxyzjn,MachineLearning,1619399521.0,[P] Open Reinforcement Learning Benchmark 0.5.0,
ilikepancakez,MachineLearning,1619018686.0,On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs [R],
Xxyjoel,MachineLearning,1617007872.0,ML + Infrastructure [P],"Hey All,

I   have been in the data science and machine learning space for the   majority of my career, but have more recently spent time in meddling around with infrastructure.  Could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so I built a tool to help manage costs.

It still requires some policy finagling, so it's not self service yet, however, I'd love y'alls candid feedback on the tool - [BlueArch.io](https://bluearch.io/)

Apologies  if this is against the sub's rules... sharing your work can be scary but I'm pretty excited about project."
thermokopf,MachineLearning,1617657632.0,[D] Can the same convolutional network be used on different image sizes?,"Convolutional networks perform ""convolutions"" on an image, or array of pixels, but basically decreases the size of the image, and then inputs this to a typical feedforward network.

How does this generally handle different image sizes?

For example I train a network on a bunch of 28x28 images. Will I be able to use this network to make predictions on 64x64 images or some other size?"
flaviojuvenal,MachineLearning,1619445763.0,[P] Entity Embed: fuzzy and scalable Entity Resolution using Approximate Nearest Neighbors,"[https://github.com/vintasoftware/entity-embed](https://github.com/vintasoftware/entity-embed)

Entity Embed is based on and is a special case of the [AutoBlock model described by Amazon](https://www.amazon.science/publications/autoblock-a-hands-off-blocking-framework-for-entity-matching).

It allows you to transform entities like companies, products, etc. into vectors to support **scalable Record Linkage / Entity Resolution using Approximate Nearest Neighbors.**

Using Entity Embed, you can train a deep learning model to transform records into vectors in an N-dimensional embedding space. Thanks to a contrastive loss, those vectors are organized to keep similar records close and dissimilar records far apart in this embedding space. Embedding records enables [scalable ANN search](http://ann-benchmarks.com/index.html), which means finding thousands of candidate duplicate pairs of records per second per CPU.

This is the first Deep Learning project we launch. Hope it's useful! Please feel free to reach me with feedbacks."
hardmaru,MachineLearning,1617347384.0,[R] On the role of planning in model-based deep reinforcement learning,
,MachineLearning,1619991678.0,"[D] What is the reason behind the recent explosion in NLP-based research, jobs, and tools?","I've noticed that NLP has been having quite a renaissance moment for the  past 5 years or so. To me, it kinda came out of nowhere, but I'm sure experts saw it coming. Anyways, I see so much NLP research being published, as well  as every company seeking people with NLP knowledge? Why is this? What  has been the cause that's led to this explosion of NLP in the past 5-6 years?"
mhj,MachineLearning,1619270649.0,"[P] Implementations of Apriori, Eclat and FP-Growth in Go",
ObjectiveDue9905,MachineLearning,1617205094.0,[D] How important is controls theory in machine learning?,"Hi,  I was wondering how important is control theory in machine learning as  they seem to go hand in hand in some applications. I'm currently a  computer engineering major student and have taken mostly only CS courses  when it comes to machine learning but control theory is a course in my ECE (Electrical and Computer Engineering) department that I  was interested in taking as it seems to have some applications that I  could see myself using with hardware and software in the future."
samk2104,MachineLearning,1617093257.0,[D] Not all independent variables available for same time period..how to handle such situations for ML models?," I have 9/10 independent variables for which information is available for last 2 years (including dependent variable). For 1/10 variables the information is only available for last 2 months as this feature was recently launched. The simplest way would be to build a model on 2 months for which all variables are available, but number of data points are not enough.

Is there a way I can incorporate the new feature in model i.e. using 2 months data, but for other 9/10 features I still build model using 2 years of data?"
NotAHomeworkQuestion,MachineLearning,1616962377.0,"[D] Instead of taking an approach like Invariant Risk Minimization, why is it not enough to control for environmental factors (confounders) by including them as regressors?","I've only just started diving into this fascinating topic so please excuse my ignorance. I really enjoyed reading the IRM paper but it left me wondering why we couldn't accomplish something similar by including the environmental variables as regressors as people do in causal inference? For example, in the MNIST coloring application, we could have the final layer of our model take the top layer of the usual plain-vanilla CNN as well as an indicator for the color of the image. We have thus 'controlled for' color confounding in our image so that the CNN part of our model architecture accounts for everything but that. As we are worried about shenanigans with future data points having the effect of color being reversed, we thus create prediction on future data points by ignoring the color effect on the prediction. This (I think) would give similar results to graying out the image which was shown to give excellent performance. What am I missing here?"
rom1504,MachineLearning,1616491364.0,[P] fromconfig: A library to instantiate any Python object from configuration files.," [fromconfig](https://github.com/criteo/fromconfig) acts as a generic command line interface from configuration files *with absolutely no change to the code*.

[fromconfig](https://preview.redd.it/x953l8lzwqo61.png?width=755&format=png&auto=webp&s=c517361d86fe26d47f92ba43757161307c68d651)

It is particularly well suited for **Machine Learning** (see [examples](https://github.com/criteo/fromconfig#machine-learning)). Launching training jobs on remote clusters requires custom command lines, with arguments that need to be propagated through the call stack (e.g., setting parameters of a particular layer). The usual way is to write a custom command with a reduced set of arguments, combined by an assembler that creates the different objects. With fromconfig, the command line becomes generic, and all the specifics are kept in config files. As a result, this preserves the code from any backwards dependency issues and allows full reproducibility by saving config files as jobs' artifacts. It also makes it easier to merge different sets of arguments in a dynamic way through references and interpolation."
TheOverGrad,MachineLearning,1619902564.0,Memory-Efficient Semi-Supervised Continual Learning (IJCNN2021 oral),
cowgod2007,MachineLearning,1617853595.0,[D] Sourcing medical data?,"Hey all!

For ML projects, how does one source / buy / obtain medical labeled data for training data? e.g. labeled radiology imaging data

Thanks!"
JST99,MachineLearning,1619188054.0,[D] How to properly version control ML models amid rapid experimentation?,"Coming from a software engineer background, I've been using git to version control my files, along with a `config.json` that includes all model and training hyperparameters. A trainer class would read a specified configuration file, train the model, and save the best one with the highest validation score.

This pipeline worked in the beginning when I was iterating at a slow pace. Recently, however, I'm increasingly realizing that saved checkpoint files are obsolete because non-trivial changes have been made to the model architecture since the last experiment. By checkpoints, I'm really referring to `torch.load(some_state_dict)`, but my question is framework agnostic.

I could, of course, check when the experiment was conducted and git checkout the repository to that specific point in time. However, part of me believes that this is an incredibly common thing for ML engineers, and that there must be a more elegant solution. So far, my research has brought me to

* [Sacred](https://sacred.readthedocs.io/en/latest/index.html), which I got from [this post](https://www.reddit.com/r/MachineLearning/comments/3npg0d/how_to_keep_track_of_experiments/) 
* [Keepsake](https://keepsake.ai)
* [DVC](https://dvc.org), which appears tangential since my question pertains more to models, not data

Thanks in advance for sharing your insight!"
charles96322,MachineLearning,1618968095.0,[D] How Valuable Would Cutting Your ML Models Computation Time (at Inference) By 30-50% Be?,"Hello everyone,

I'm currently working on a personal project where I try to optimize deep learning algorithms with respect to the hardware on which it is deployed and so far I've had some pretty decent results; The idea is that some hardware handles some computation better than other. You can then build a simulator that shows you the utilization of each blocks in your neural network and arrange your model so that it keeps the same capacity, but improves the computation time significantly. (e.g. see this paper https://arxiv.org/pdf/2003.02838.pdf)

Eventually, the goal would be to select a piece of hardware on which your model will be deployed on and optimize your model by the click of a button.

I'm now wondering if a solution like that would made any business sense, and I'd love to know more about your use cases.

So, for those of you who are deploying machine learning models to edge devices:

1) What kind of application do you deploy on IoT devices and is latency typically a problem?

2) How much time do you spend optimizing a model for computation time?

3) How valuable would cutting computation time by 30-50% at inference be to you?

I'm wondering if there are applications (e.g. robotics/medical/...) where this 30-50% would become very valuable and I'd love to speak with someone working in such a field"
ijovab,MachineLearning,1620598087.0,[D] What are some promising areas in privacy-preserving learning in medical data?,"So with the EU's new proposal, and general problems related to the usage of medical data the topic seems to be becoming fairly important. I've been reading up on federated learning, continual learning, and differential privacy recently. What do you think are some of the most promising areas to simplify and guarantee the safety of the medical image data during training?   


Any paper suggestions also appreciated."
binaryfor,MachineLearning,1618678509.0,[P] Flashlight - A C++ standalone library for machine learning. Open-sourced by Facebook.,
kamil-rafalko,MachineLearning,1618305825.0,[Project] Better neurobiological research with AI,"How to do instance segmentation of star-shaped cells called astrocytes in 3D with already-known computer vision techniques? This task may seem simple at first, but turns unexpectedly tricky... I've shared details about the process of finding a solution to this problem in a blog post: [https://blog.softwaremill.com/better-neurobiological-research-with-ai-d91eacaf7976](https://blog.softwaremill.com/better-neurobiological-research-with-ai-d91eacaf7976)

I hope it would be interesting for you. Any comments are welcome."
proximauri,MachineLearning,1617183404.0,"[D] Is uploading dataset to personal Google Drive and use it in Google Colab against ""No distribution agreement""?","Hi, so I a have a machine learning database, for which I have accepted an agreement. In agreements, it is stated that distribution of data to any third party is prohibited.  


My question is now, if uploading dataset to my personal GDrive and load it in Google Colab is against this? I don't want to share it with anyone."
dh27182,MachineLearning,1619591958.0,[D] Open source projects for interpretability,"Are there any good open source projects for model interpretability?

I've been catching up with several [distill.pub](https://distill.pub) articles recently and the authors show some impressive visualizations. Unfortunately, it seems to not be open source. Wonder if anyone's used/built any projects for visualizing and inspecting models."
ykilcher,MachineLearning,1619540955.0,[P] We gave GPT-3 random ingredients and cooked the recipe it came up with (Video),"[https://youtu.be/hIoCn\_9QTVU](https://youtu.be/hIoCn_9QTVU)

We went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.

&#x200B;

Our Rules:

1. All Vegan

2. Follow the recipe as closely as possible

3. We must finish our plates

&#x200B;

The Recipe:

1. Boil the potatoes and carrots.

2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. 

3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. 

4. Add the soy cream, stir and cook for three minutes. 

5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. 

6. Cut the bread in small squares and fry in the vegan butter until golden brown.

7. Cut the limes into cubes and squeeze the juice into the bean mixture. 

8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.

9. Pour the bean mix into a blender. 

10. Bake for 5 minutes in the oven at 180C. 

11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. 

12. Cut the bell pepper into cubes and add to the pot. 

13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. 

14. Add the avocado. 

15. Add the chickpeas. 

16. Add the chocolate.

17. Serve on bread with mustard and pommegrenade on top.

&#x200B;

VIDEO OUTLINE:

0:00 - The Plan

2:15 - Ingredients

4:05 - What is GPT-3?

6:10 - Let's cook

12:25 - The Taste Test

&#x200B;

GPT-3 on Wikipedia: [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)

GPT-3 Paper: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)"
Stargor14,MachineLearning,1616875179.0,[D] Efficient ways of quantitatively classifying price movements for algorithmic trading using machine learning,"Hi there everyone, I'm currently a high school student and I've recently been experimenting with algorithmic cryptocurrency trading. I started off with generic conditional strategies, however after those proved relatively inefficient and simple, I tried moving onto a machine learning approach. 

I've been using XGBoost and python and I'm trying to figure out an efficient way to classify short term price movements, which in this case is the dependent variable I'm trying to predict with my model. I've tried using the % change over the next 5-10 candles but that didn't work too well. I'm not really looking for a specific answer, tbh I'm just curious on everyone's thoughts on this application (predicting short term price movements) of random forests. If you think my post was a bit vague feel free to ask away. Any answers are appreciated!"
RenYang_ETHZ,MachineLearning,1619719239.0,"[N] NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset, Methods and Codes","We have organized the NTIRE 2021 Challenge on Video Enhancement in conjunction with CVPR 2021. We propose a large-scale diverse video database for the challenge. The proposed methods in the challenge advance the state-of-the-art on enhancing compressed video. The homepage includes the database and the open-source codes (keep updating) of the benchmark: [https://github.com/RenYang-home/NTIRE21\_VEnh](https://github.com/RenYang-home/NTIRE21_VEnh)

The dataset and methods reports are at 

[https://arxiv.org/abs/2104.10782](https://arxiv.org/abs/2104.10782)

[https://arxiv.org/abs/2104.10781](https://arxiv.org/abs/2104.10781) 

We hope the database and the benchmark benefit future research in this direction!

The NTIRE workshop will be held on the first day (June 19) of CVPR 2021, more than welcome to attend our workshop."
bin_wang_osl,MachineLearning,1617079701.0,[N] Live IEEE-NASPI Contest calls for ML experts' participation!,"Can machine learning better solve power system problems? It's up to you experts! Here is a live contest co-hosted by IEEE-NASPI (from Apr. 12 to Jun. 11, 2021), see more details at [https://www.naspi.org/node/890](https://www.naspi.org/node/890). You are very welcome to register and solve an important real-world problem rooted in power and energy systems. If you need someone with a power system background to form a team, don't worry, here's a way to find one: [https://docs.google.com/spreadsheets/d/1zA0QdLk2-7OIczh-8ia6zmzuIrDnd7iQ6xkpNJOKvHA/edit#gid=173757192](https://docs.google.com/spreadsheets/d/1zA0QdLk2-7OIczh-8ia6zmzuIrDnd7iQ6xkpNJOKvHA/edit#gid=173757192)."
AirZealousideal1342,MachineLearning,1618023079.0,[D] Why we must use weight demodulation in stylegan2,"Why we cannot do scale-specific style control if we use the architecture in figure(c)? and must use architecture in figure(d)?

The paper says

>In practice, style modulation may amplify certain feature maps by an order of magnitude or more. For style mixing to work, we must explicitly counteract this amplification on a per-sample basis—otherwise the subsequent layers would not be able to operate on the data in a meaningful way.

But I cannot understand it. And I cannot undertand why weight demodulation solves this problem.

&#x200B;

https://preview.redd.it/y4ljnsdmf9s61.png?width=1072&format=png&auto=webp&s=5529e6c8c3c97fa7a0520a57ece0443fb4d5a34c"
Cosack,MachineLearning,1617493961.0,[Project] Estimating fine-tuning cost,"With model sizes growing larger and larger, it's been mentioned time and again that retraining is prohibitive for individuals. Fine tuning? More reasonable. While of course the tuning intensity would depend on the task and size of available data, how would you go about estimating the cost of tuning with some fixed amount of data? Say I'm looking to fine-tune GPT-Neo (open-source GPT-3) with its 2.7B parameters on some 200k short inputs of 7.5M tokens. How could I estimate the cost?"
ByteHubAi,MachineLearning,1617650022.0,[P] ByteHub: simple timeseries data preparation in Python,"Hi everyone! Sharing a project I've been working on to help make time-series data easier to store, access, and transform when building machine-learning models. It’s a Python-based feature store, and is available as an [open-source library](https://github.com/bytehub-ai/bytehub) or as a low-cost [cloud-hosted service](https://bytehub.ai).

For a bit more background I've [written some more](https://medium.com/bytehub-ai/making-feature-stores-simple-3ae0d0dcac30) about why we built it. In summary we want to help data scientists save time when building machine-learning models with something that is simple to use - i.e. compatible with Jupyter notebooks and with no complex infrastructure to setup and manage. I'd appreciate feedback from anyone interested in checking it out!"
gospodin_dan,MachineLearning,1619896827.0,[N] Fresh paper: Detecting objects in images by describing them with keywords 🔥,
evanatyourservice,MachineLearning,1618239686.0,[2102.11600] ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks,
ilikepancakez,MachineLearning,1620598303.0,Open Catalyst 2020 (OC20) Dataset and Community Challenges [R],
dev_bes,MachineLearning,1618317666.0,[R][P]MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis,"In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.

The main goal of our work is the democratization of AI. To start working with a modern AI-based high-fidelity face generation, you need just two python commands:

    pip install random_face
    python -m random_face.demo

&#x200B;

Paper: [https://arxiv.org/abs/2104.04767](https://arxiv.org/abs/2104.04767)

Code: [https://github.com/bes-dev/MobileStyleGAN.pytorch](https://github.com/bes-dev/MobileStyleGAN.pytorch)

Python library: [https://github.com/bes-dev/random\_face](https://github.com/bes-dev/random_face)

YouTube videos: [https://www.youtube.com/playlist?list=PLstKhmdpWBtwsvq\_27ALmPbf\_mBLmk0uI](https://www.youtube.com/playlist?list=PLstKhmdpWBtwsvq_27ALmPbf_mBLmk0uI)"
bci-hacker,MachineLearning,1617472841.0,[D] Trustworthy Machine Learning talk | ideas and potential research,"Hey all,

I recently gave a [talk](https://www.youtube.com/watch?v=2CQ7EuTCurc) at Virginia Tech on Trustworthy Machine Learning and discussed research ideas on model interpretability such as influence functions, gradient based attributions, and structure learning.

would love for you to check it out and provide feedback, if possible :)

i'm still an UG so i apologize for the rigor of academic talks. tryin my best to improve haha

would love it if you sub cuz i love making videos on my research :D"
grid_world,MachineLearning,1618074601.0,[R] Finding important neural network connections,"Most of the research work related to neural network pruning revolves around iterative pruning ever the general idea is to prune p% of connections per iterative round either locally or globally, structured vs. unstructured. A common criterion is absolute magnitude weight based pruning (Han et al., Frankle et al.).

Since this is an iterative pruning technique, the number of such rounds are large to say prune from 0% to 99.5%.

Is there some other pruning technique to overcome this shortcoming? It's kind of like trying to identify the important connections before the entire training process such that this iterative process can be skipped."
levi97zzz,MachineLearning,1618010691.0,[D] how useful is OS knowledge in AI/ML?,"Question in title, I am choosing between several classes for my next semester, one of them is a class in OS. How useful is OS knowledge if I want to get further in AI/ML in the future?"
peaked-too-early,MachineLearning,1619992949.0,"[D] With ICLR starting, how do you make the most of an online conference?","Before the end-times, in-person conferences would be productive (for me, at least) as you could make acquaintances (which later lead to Ph.D. internships/job offers/collaborations) and could naturally learn about others/convey your own research through just casual conversation in a poster hall. The fact that we blocked off our entire schedules for the conference week obviously helped us focus on the conference, but we don't have that luxury anymore.

I've tried just winging it at online ML/ML-adjacent conferences with little success. Do you have any tips/resources on how to make the most out of this suboptimal situation?"
abpan8,MachineLearning,1618930656.0,[P] Open data about logistics, Could you suggest some datasets or a good site to take data from by  scraping to do some forecasting on the cost of transporting goods by truck  in the US?
medwatt,MachineLearning,1617829959.0,[D] Explain Energy Based Models,"I am an electrical engineering student with some basic practical understanding of how neural networks work. I understand the basic ideas of the multi-layer perceptron (MLP), convolutional layers, etc and I have even implemented a binary neural network using Keras. As an electrical engineering student, I have not had/taken any hardcore machine learning courses (or even optimization theory). Probably, they have not yet worked their way into the curriculum of the university I'm attending. Most of my knowledge in the field are from self-interest.

I recently saw project that involves the building hardware accelerators for machine learning tasks. The project requires me to have some background in energy based models. Unsurprisingly, I had never heard of this before. I came across some papers online and even found Yann LeCunn's video lectures on the topic. Needless to say, I wasn't able to understand much. I am, at the moment, unfamiliar with a lot of the ideas in this field. So, I would like if someone could give be an explanation on why energy based models are interesting, how they differ from probabilistic models, how they can be trained, what applications are they most suitable for, etc. Maybe, this will make it easier to revisit the papers and understand them on second reading."
Egan_Fan,MachineLearning,1617299603.0,[D] Statistical Significance in Deep RL Papers: What is going on?,"I'm an ICML reviewer, and I've been reading author responses.  I'm primarily an RL researcher, and so many of the papers I reviewed used deep networks + RL.  I rejected 3-4 papers because their empirical results relied on 3-5 trials (and the authors did not perform any sort of hypothesis testing/statistical analysis...not that that would have helped with so little data).  One of the author responses said something like, ""well, everyone else does the same thing, and the computational cost is very high"".  It's not an excuse, but they are not wrong on either point.

Why is this seen as acceptable?  In other fields (e.g., a medical journal), manuscripts with 3-5 data points and no statistical analysis would be immediately rejected, and rightfully so (and if the authors responded and said ""well we couldn't afford a larger study"", no one would see that as a legitimate excuse).  However, **none of the other reviewers on these papers are raising these concerns**.  Why am I the only one with these concerns?  **Why are papers like these getting accepted at top conferences, and even winning best paper awards?**  Am I missing something, or is this a deep problem with our field (in which case I should stick firmly with “reject” for these papers)?

Thank you in advance for thoughtful replies and discussion."
fripperML,MachineLearning,1616972625.0,"[D] What’s the simplest, most lightweight but complete and 100% open source MLOps toolkit?","I know this has been asked many times and in many different ways. And there are tons of blog posts, articles, videos and courses addressing this and comparing hundreds of tools, libraries, frameworks… And that’s part of my problem: I am facing so many options that I feel like Buridan’s ass, dying of starvation for not knowing what to do.

Although I don’t want to write too much, I need to speak a little about our situation, in order to put the question in our context.

**Our Team**

Our Team is small. We have only four people, which could be qualified as beginner data scientists. One of us has a profile that is a little bit more “engineer”, so data engineer could be more suitable for him. Anyway, we don’t have much experience, neither in Python Projects nor in Machine Learning. What we have is passion and love for ML!

For a couple of years, we have been functioning with SAS, but now we plan to change to the Python landscape, as it is much more vivid and exciting. In the last year, we have made two projects in Python, but without using any good practices at all. Every step was made by hand and prone to error, models were neither monitored nor even deployed (they only were used for making some batch predictions), projects were not properly structured, documentation was painful…

So we know that we need to change it before it becomes unmanageable.

We don’t expect the size of the team to grow fast. Let’s say in a couple of years we can expect 10-12 people working with us (the organization knows the importance of Machine Learning, but economic issues can be an obstacle).

**Our Projects**

For the moment, we have only made “classical” Machine Learning. I mean: no Deep Learning. We have used Pandas and Scikit-Learn, XGBoost, etc. And only in Batch mode. But we expect it to change in less than a year, because we will need to train image classifiers, so it will need to be:

\- Trained using a deep learning convolutional network.

\- Integrated with other applications (that are coded in Java) and fast (real-time).

Other change we expect is to need more distributed computing, as we will need to manage some huge databases that simply do not fit in a pandas dataframe. These are the most important challenges we face.

**Our Company**

We work for a big company, which also imposes some restrictions to us. Mainly:

We do not have budget to spend in MLOps solutions, so everything has to be open and free.

We won’t hire data scientist / data engineers for the moment.

There are some tools, uses by other teams, that we should use as part of the MLOps stack, although they are not the best in the class.

Regarding the last item, a short list of this set of restrictions is the following:

* We have a Cloudera Express installation. It’s the most basic and cheaper Cloudera option, so it does not come with any tool for Machine Learning management. It only gives to us HDFS, Impala, Spark and a set of nodes to run Python scripts on them.
* We have Control-M as the orchestrator and workflow manager tool.
* We have DataStage as the ETL tool.
* We use SVN as the code version system (yes, no git).
* We deploy our projects using a very simplified and self-made version of Docker. It’s a little bit awkward and I think that, if we push a little bit, we could convince the organization to let us use Docker. But if Docker is reachable, Kubernetes is out of our capabilities.
* We have Jenkins for CI.
* We have Visual Studio Code professional licenses.

**Toolset**

With this premises, I have two different and opposed concerns or even fears.

* Fear of not using enough tools and good practices and arriving in a couple of years to a state where we cannot manage our own code, project and models.
* Fear of using so many tools that they impose a burden our small team cannot bear.

It’s clear that we need some MLOps, but how much, I don’t know. I will review some things I have been reading, and I hope you can help me choosing the right tools.

**Python Programming**

It looks like we will program using Visual Studio. We will use a remote interpreter, because we will run things on the Cloudera Nodes, although we will program locally and integrate the code with a SVN repository.

Do we need tools for standardizing our code, like PyLint, Flake8, MyPy or Black? Would you recommend any of those?

**CI and Deployment**

We will use Jenkins. For deployment of our code, is Docker a no brainer, a minimum standard? I tend to think so from what I read, but I’d like to be sure and to have good arguments.

Do we need more tools?

**Project Scaffolding**

I have been reading about PyScaffold, CookieCutter and, best of all (from my point of view), Kedro. I think we will stick to Kedro template, because it offers much more functionality, and I like to think of each project as a set of pipelines to be run. What do you think of Kedro?

**Documentation**

Would you recommend having separate documents, or generating the documentation from the projects, using Sphinx or another similar tool? I tend to prefer the second option, because the first one very likely tend to generate obsolete docs. But I don’t know if the “burden” of the second is too big, and if the generated docs can suffice for a typical ML project.

**Project registry**

Is there any tool that could be used as a “project registry”, like a simple web app where we could navigate through our projects, read the docs and thinks like that? I don’t know. If not, the registry will be the SVN repo with all our projects as folders, and that’s all.

**Data Exploration and Preparation**

I think that Matplotlib, Seaborn and Pandas should suffice, and when things go big, we should use PySpark, Scala or even plain SQL in Impala. However, I know Dask exists, and newer tools like Koalas or Vaex. What do you think?

For creating data transformation pipelines, we will use Kedro, although there are lots of tools that look interesting, like Dagster.

When we enter the “deep learning” realm, can we keep using the same tools? Should we use another framework like TFX? I’d prefer not, cause learning one framework is hard, and two is worse. If a solution is valid for all our projects it’s better. Or TFX is valid for “classical” ML and Deep Learning?

**Tests**

I think unit testing can be too much burden for us. But I have come to Great Expectations library and think it’s well suited for ML projects. Would you recommend it as an important part of our MLOps stack?

By the way, there is a Kedro-Great Expectations plugin, so we could benefit from that.

**Feature Store**

Is it really needed, especially considering our team size and experience? If so, I have read about Feast and Snorkel.

**Data Versioning**

Is it really needed, especially considering our team size and experience? If so, I have read about DVC.

**Experimenting**

I think it’s an important piece, although I wonder if we really need a tool or we could use our own standard of reports and artifacts to follow what we have tried. But the risk that it goes unmanageable is high.

Kedro has a journal, I don’t know if it can suffice. Also it has a Kedro-MLFlow plugin, so that we could benefit from using MLFlow as the experiment tool.

I have also read about Guild, that seems really lightweigh and easy. I don’t know much more.

**Training**

I developed my own library for doing nested cross validation and, within the same function:

\- Optimizing hyperparameters (both of model and pipeline).

\- Generating a report of the training, to assess the quality of the model.

It’s build mainly on top of Skopt. I did it pip installable, it’s here:

[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)

So we might use it in our traning workflow (at least in some projects), along with the typical models like XGBoost, LightGBM and all Scikit-Learn. But when we need other frameworks like TensorFlow or Keras, we will see.

**Model Registry**

I think it’s an important piece, although I don’t know if we even could build our own with an standard database. If not, MLFlow seems a mature option.

**Model serving**

I am not sure if it’s included in the previous point or not. Anyway, I have read about Streamlint and FastAPI. Would you recommend any of those?

Is Apache Kafka needed for real time predictions?

**Visualization**

With this I mean sharing with the organization basic web apps with customizable plots, explainable predictions and things like that. I have read about panel, which has the ability of transform a Jupyter Notebook into a simple web app. It might be interesting.

**Model monitoring**

Is there a good free tool for monitoring the models and detecting loss of accuracy, data drift and things like that? Or we should better generate our own script of monitoring to be run periodically?

**BigData**

As I said before, we plan to use mainly Spark when need.

I know it’s a lot of info. Maybe I have overcomplicated myself and I should use only 20% of what I think I should. Or maybe not. I have no idea. Any help will be GREATLY appreciated. Thanks in advance.

**EDIT**  


I should add a couple of things, the first one is spread among two comments:-  I work for the Spanish public Administration. So we do have lots of data to use and explore. This also has to do with the rigidity of our budget and the other things I explained. This also explains why cloud is not an option for us: there are data protection, legal and even political reasons that forbid us to have any data outside of our scope. It's a pity, because I think AWS (or another provider) can help in having part of this stack covered.

As we already use DataStage, the IBM provider told us that soon it will be unified with Cloud Pak For Data, so we might be having soon a Cloud Pak For Data license. I have mixed feelings for that product, do you think it might benefit us more than the opposite?"
jj4646,MachineLearning,1619315323.0,[D] how accurate were the statistical models you developed on real-world data?,"When it comes to real-world data, how accurate were the statistical models you developed? Were these models able to consistently and accurately make predictions? 

E.g. for supervised binary classification, has anyone been able to develop a model that had high accuracy, high sensitivity and high specificity?"
Yuqing7,MachineLearning,1618849550.0,[N] DeepMind 'Podracer' TPU-Based RL Frameworks Deliver Exceptional Performance at Low Cost,"A research team from DeepMind introduces Anakin and Sebulba, two architectures that demonstrate reinforcement learning platforms based on TPUs can efficiently deliver exceptional performance at scale and with low cost.

Here is a quick read: [DeepMind 'Podracer' TPU-Based RL Frameworks Deliver Exceptional Performance at Low Cost](https://syncedreview.com/2021/04/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost/).

The paper *Podracer Architectures for Scalable Reinforcement Learning* is on [arXiv](https://arxiv.org/pdf/2104.06272.pdf)."
xiikjuy,MachineLearning,1619173343.0,[D] Should I report the pretraining results?,"Hello,

I am using a Transformer-based model to pretrain  my unlabeled large data and then finetune the pretrained model on my labelled smaller dataset, just  a regular learning pipeline following the idea of BERT.

My question is, should I report the pretraining results (like the learning curve)? or just need to describe the pretraing steps, optimizers, etc.

For literature using Transformer-based models with this learning paradigm ( pretrain+finetune), it seems most papers didn't report the pretraining results, just the results on donwstream tasks.

Shouldn't they first show that the pretrained model is well-pretrained?

or it doesn't matter that much, even a model performs poorly in pretraining phase but somehow the checkpoint works well on downstream tasks, then no problem at all?

Thanks.

Edit: ""report"" here  means showing it on a regular ML/DL conference paper."
ilikepancakez,MachineLearning,1619293570.0,Team Polk’s Bryan Pellegrino Talks About His AI Research And How It Helped Formulate Strategies To Win $1.2 Million [P],"https://www.cardplayer.com/poker-news/25778-team-polk-s-bryan-pellegrino-talks-about-his-ai-research-and-how-it-helped-formulate-strategies-to-win-1-2-million

Really interesting use of game theory. Had the honor of watching this happen live. A moment of silence to the human /r/poker players please."
alexirpan,MachineLearning,1618168907.0,[D] Thoughts on industry research vs academia,"Hi all,

I didn't go to grad school, going straight to indsutry instead, and I've been working in ML for about 5 years now. I thought it'd be interesting to look back on how that turned out. The post is here: [https://www.alexirpan.com/2021/04/07/grad-school-5years.html](https://www.alexirpan.com/2021/04/07/grad-school-5years.html)

I got feedback from all across the ML career spectrum (straight to ML engineer, in PhD, industry to academia, post PhD), and have tried to address all their experiences, so hopefully it matches up with reality and is helpful if you're considering a similar decision."
glassAlloy,MachineLearning,1617285861.0,[P] How to group every data point with HDBSCAN to some group to have no noise?,"**TASK**

\- I am clustering products with about 70 dimensions ex.: price, rating 5/5, product tag(cleaning, toy, food, fruits)

\- I use HDBSCAN to do it

&#x200B;

**GOAL**

\- The goal is when users come on our site and I can show similar products to what they viewing.

&#x200B;

**QUESTION**

\- How to get all data point to be part of a group, so the goal is to not to have any noise?

&#x200B;

**CODE**

    clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=1).fit(data)
    color_palette = sns.color_palette('Paired', 2000)
    cluster_colors = [color_palette[x] if x >= 0
                      else (0.5, 0.5, 0.5)
                      for x in clusterer.labels_]
    cluster_member_colors = [sns.desaturate(x, p) for x, p in
                             zip(cluster_colors, clusterer.probabilities_)]
    plt.scatter(*projection.T, s=20, linewidth=0, c=cluster_member_colors, alpha=0.25)
    
    
    labels = clusterer.labels_
    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    print('Estimated number of clusters: %d' % n_clusters_)"
cbsudux,MachineLearning,1616679371.0,[D] Cheapest GPU server options for deploying a side-project?,"Hey guys, what are your cheapest options for deploying ML models?

\- AWS, GCP cost 250-300 USD/mo for using a T4/P4 instance. Too much for a side project.

\- [vast.ai](https://vast.ai), independent GPU providers cost around 100-150 USD per/month which is still a bit pricey.

**1) Are there any pay-as-you go options (aws elastic equivalent)?**  
**2) Are there are any cheap options for 50-80 USD/mo?**"
KirillTheMunchKing,MachineLearning,1620480849.0,[D] Solving computer vision without convolutions! MLP-Mixer explained.,"# [MLP-Mixer: An all-MLP Architecture for Vision](https://t.me/casual_gan/35)

This paper is a spiritual successor of Vision Transformer from last year. This time around the authors once again come up with an all-MLP (multi layer perceptron) model for solving computer vision tasks. This time around, no self-attention blocks are used either (!) instead two types of ""mixing"" layers are proposed. The first is for interaction of features inside patches , and the second - between patches. See [more details](https://t.me/casual_gan/35).

[Model architecture overview](https://preview.redd.it/na599eawfwx61.png?width=1280&format=png&auto=webp&s=fb29ae62876fb41b7c14e53b1dbc5b1cc9bbd0f6)

\[[7 minute paper explanation](https://t.me/casual_gan/35)\] \[[Arxiv](https://arxiv.org/pdf/2105.01601.pdf)\]"
st-memory,MachineLearning,1616860882.0,[P] Generating Galaxies using StyleGAN2-ADA,"[This](https://www.youtube.com/watch?v=kxPe8Oux0jQ) is a video of a [StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada) trained on images from [Hubble](https://esahubble.org/images/archive/category/galaxies/) that involves travelling through the latent space with different truncation values (such that the variety of images increases in exchange for quality as the video progresses). The images were scraped and then manually cleaned as a fair few either had diagrams or ""defects"". The dataset is of \~1,000 images and the network has been trained until no further improvements in FID were noted."
roVinchi,MachineLearning,1620639086.0,[D] Will the talks of ICLR21 be publicly available?,"and if so, is there a known ""release"" date? I could not find any information on the website itself."
Yuqing7,MachineLearning,1617322595.0,[N] Google Research's SOTA GNN 'Reasons' Interactions over Time to Boost Video Understanding,"A research team from Google Research propose a message-passing graph neural network that can explicitly model spatio-temporal relations, use either implicitly or explicitly representations of objects, and generalize previous structured models for video understanding.

Here is a quick read: [Google Research's SOTA GNN 'Reasons' Interactions over Time to Boost Video Understanding](https://syncedreview.com/2021/04/01/google-researchs-sota-gnn-reasons-interactions-over-time-to-boost-video-understanding/)

The paper *Unified Graph Structured Models for Video Understanding* is on [arXiv](https://arxiv.org/pdf/2103.15662.pdf)."
sharvil,MachineLearning,1620146526.0,[P] ArxivDiff: view diffs of arXiv paper revisions,"I built a tool to show diffs between any two revisions of a paper on arXiv. Just take any arXiv URL and replace arxiv.org with arxivdiff.org, e.g. https://arxiv.org/abs/2009.09761 becomes https://arxivdiff.org/abs/2009.09761

edit: my first Reddit awards! Thank you so much, fellow..um.. net surfers."
ottawalanguages,MachineLearning,1619997683.0,[D] correct application of autoencoders for classification,"Can autoencoders be performed on data the same way as principal component analysis? Can you perform dimensionality reduction on your data using autoencoders, and then use random forest on the reduced data? Or is this counterproductive?"
CauchySchwartzDaddy,MachineLearning,1619047967.0,[D] Is it just me or is it getting harder and harder to get access to cloud GPUs with regions being out of resources almost all the time,"I have about 10 VMs set up in google cloud in basically every region where I can get a V100 and yet multiple times a day I cant access any of them due to regions being out of resources.  Maybe this is just a gcloud thing combined with a cutthroat GPU market.

I'd be interested to know if anyone else has this problem."
xela-sedinnaoi,MachineLearning,1618815073.0,"[P] [D] The benefits of training the simplest model you can think of and deploying it to production, as soon as you can.","I’ve had many successes with this approach. With this in mind, I’ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this Agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a Scikit-Learn model, using FastAPI with [Bodywork](https://github.com/bodywork-ml/bodywork-core) (an open-source MLOps tool that I have built).

How does this compare to your experiences? I’d be interested to get people’s thoughts, as my background is largely with structured data."
prestodigitarium,MachineLearning,1617376157.0,"[P] Gourdian Free Dataset Download: Daily weather of the world, back to 1929","Hi there! 

Have you ever thought that it'd be useful to train a model on historical weather data for some chunk of the world, but didn't want to deal with grungy data wrangling on a massive dataset to get it into a convenient format? 

Have you ever just been curious what the average temperature of Egypt in 1985 was?

Well, a friend and I made a webpage that lets you filter the 28 gig NOAA Global Summary of the Day weather dataset down to a small fraction of that, and download just the part you care about, as a csv, here:

**https://gourdian.net/g/eric/noaa_gsod.global_summary_of_day**

Table preview on the left, geographic and time filters and download button on the right. Delivers it as a single, clean csv, which should be easy to import into Pandas, R, a database, or whatever else you like to use to work on tabular data. CSV works with everything!

A bit about our goals and what we're trying to build:

* Filter (optional), click the button, CSV arrives on your hard drive
* Downloads are always a single CSV, no bundles with weird directory structures, no other formats
* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want
* Open licensed datasets are free to download
* No signup required for downloading open datasets
* Search within and across datasets

Basically, our focus is on building something very simple - it won't be as powerful at fine-grained querying as something like BigQuery, but it should be easier to get going with.

This is a super early version of our web/javascript download client (we've never shown it publicly before, our other client is a python library), and we're trying to figure out what to make better, so we'd love any feedback, especially if it breaks for whatever reason. What can we do that would make your life easier?

Please note, **it doesn't work well on mobile yet** - we figured that not too many people would want to browse/download datasets there, and it's just the two of us, and... priorities. But if we're wrong, please let us know!"
SQL_beginner,MachineLearning,1619323632.0,[D] Reservoir Computing/Echo State Networks vs RNN's and LSTM's,Has anyone ever heard of Reservoir Computing or Echo State Networks (https://en.wikipedia.org/wiki/Reservoir_computing)? Does anyone have any idea in what situations they should be used compared to models such as RNN and LSTM?
marcovirgolin,MachineLearning,1619510512.0,[R] Model Learning with Personalized Interpretability Estimation,"For high-stakes applications (e.g., cancer treatment), AI cannot be used lightly and recklessly. We need models we can trust and, to achieve trust, interpretability is a key factor.

The field of eXplainable AI (XAI) concerns *both* methods to explain the behavior of black-boxes such as deep neural networks, and methods to generate white-boxes, i.e., models that are interpretable (think of, e.g., sparse linear models, small decision trees, symbolic expressions).
There are very good reasons why the latter are more desirable than the former, see e.g., the famous paper by Cynthia Rudin: https://arxiv.org/abs/1811.10154

We proposed a new proof-of-concept work that looks at whether XAI for interpretable model generation can and should be *personalized*. (What follows is essentially taken from the abstract.)
In fact, current algorithms for the synthesis of *potentially* interpretable models rely on objectives or regularization terms that represent interpretability only coarsely (e.g., model size) and are not designed for a specific user. 
Yet, interpretability is intrinsically subjective. 
We propose an approach for the synthesis of models that are tailored to the user by enabling the user to steer the model synthesis process according to her or his preferences. 
We use a bi-objective evolutionary algorithm to synthesize models with trade-offs between accuracy and a user-specific notion of interpretability. The latter is estimated by a neural network that is trained concurrently to the evolution using the feedback of the user, which is collected using uncertainty-based active learning. To maximize usability, the user is only asked to tell, given two models at the time, which one is less complex. With experiments on two real-world datasets involving 61 participants, we find that our approach is capable of learning estimations of interpretability that can be very different for different users. Moreover, the users tend to prefer models found using the proposed approach over models found using non-personalized interpretability indices.

Preprint: https://arxiv.org/abs/2104.06060 
(Accepted to appear at the EC+DM workshop at GECCO 2021)"
Last-Programmer2181,MachineLearning,1617364678.0,"[R] Why can a single large SL model be broken down into smaller SL models, and have better accuracy?","For reference, I am using Learning Classifier Systems (LCS) to perform supervised learning on a dataset.

I have a large synthetically generated set of data, 11 different inputs, and the predicted action as output. After generating nearly a million different data points, I trained a SL model and achieved roughly 70% classification accuracy. I did the normal hyperparameter sweeping, and accuracy varied anywhere from 60-70%. The data is very unique, in the sense that there are many different combinations of the eleven inputs, leading to twenty-seven different possible classifications (actions). (I know these numbers are pretty specific, but I just want to give a sense of what I'm dealing with).

The actions could range from, 'do nothing', 'do something X', and 'do something Y'. Where 'do something X' had A many possible variants/similar actions, and then same with 'do something Y'

What I decided to do was break this one larger SL problem, into small chunks. I broke the single larger problem into four much more manageable models, one leading into the next. (Model A -> B -> C -> D, ultimately predicting the same result as the larger singular model)

The first model would simply decide 'do nothing' or 'do something', where I combined all of the 'do something X and Y' into a single prediction. And achieved a model prediction accuracy of 95%+. And so on, where I kept making the models more and more specific, ultimately leading me to the same twenty-seven different possible classifications I had in the one larger model. (Each model had an accuracy of 95%+, for reference).

Each model would always have the same 11 inputs, I just removed the data points no longer relevant, based on the previous model's choice.

Why would one larger model, trying to do everything, have a much lower classification accuracy than multiple models (doing the same thing) that have a classification accuracy of 95%+?"
fedetask,MachineLearning,1616519398.0,[R] RL Papers using graph techniques on sampled trajectories,Are there papers that construct a graph from states-actions-rewards collected and do something with it? I just find this idea intriguing but I haven't found much about it
downtownslim,MachineLearning,1617383464.0,[R] Scaling Local Self-Attention for Parameter Efficient Visual Backbones,
programmerChilli,MachineLearning,1619417306.0,[D] Huawei just announced that they trained a 200 billion transformer model on an entirely Chinese stack,"My tweet about it: https://twitter.com/cHHillee/status/1386541907950465028

They trained a 200 billion parameter decoder-only dense transformer for 40B tokens on 2048 Huawei Ascend 910 chips. Moreover, this was all done using MindSpore, Huawei's ML framework.

In contrast, GPT-3 was a 175B parameter model trained for 300B tokens. 

On its own, this is already quite impressive. Even though they've only done 40B tokens, this is the biggest model yet out of China, and represents one of the biggest models yet in the world. 

However, the thing that's really impressive to me is that this was done with an all-Chinese stack: Huawei Mindspore as the framework, compiling down to Huawei Ascend chips. 

I'd known that Huawei was working on AI chips, but I was unaware that they had matured to the point that they could feasibly train a model of this scale.

Code: https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-AIpha

Paper: https://t.co/8wQepOVIYq?amp=1"
OnlyProggingForFun,MachineLearning,1618063607.0,[News] From Amputee to Cyborg with this AI-Powered Hand! 🦾[Nguyen & Drealan et al. (2021)],"**Papers involved for this arm**:

1. Nguyen & Drealan et al. (2021) A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based Finger Control: [https://arxiv.org/pdf/2103.13452.pdf](https://arxiv.org/pdf/2103.13452.pdf)
2. Luu & Nguyen et al. (2021) Deep Learning-Based Approaches for Decoding Motor Intent from Peripheral Nerve Signals: [https://www.researchgate.net/publication/349448928\_Deep\_Learning-Based\_Approaches\_for\_Decoding\_Motor\_Intent\_from\_Peripheral\_Nerve\_Signals](https://www.researchgate.net/publication/349448928_Deep_Learning-Based_Approaches_for_Decoding_Motor_Intent_from_Peripheral_Nerve_Signals)
3. Nguyen et al. (2021) Redundant Crossfire: A Technique to Achieve Super-Resolution in Neurostimulator Design by Exploiting Transistor Mismatch: [https://ieeexplore.ieee.org/document/9355574](https://ieeexplore.ieee.org/document/9355574)
4. Nguyen & Xu et al. (2020) A Bioelectric Neural Interface Towards Intuitive Prosthetic Control for Amputees: [https://www.biorxiv.org/content/10.1101/2020.09.17.301663v1.full](https://www.biorxiv.org/content/10.1101/2020.09.17.301663v1.full)

**Video demo**:

[https://youtu.be/wNBrCRzlbVw](https://youtu.be/wNBrCRzlbVw)"
FormerYogurtcloset17,MachineLearning,1617233499.0,"[D] How can I augment an existing model with new training data, preferably on the edge?","How Can I Augment an Existing Model with New Training Data?

I built an app where I use MobileNet model from TensorFlow Lite to detect an object in a live steaming camera. 

Now, I wish to ReTrain the model or somehow ""Augment"" with new training data, i.e. new photos. 

How can I accomplish such an objective at the Edge, i.e. on the device? 

or even on a remote server ""fast""?"
satprepnow124,MachineLearning,1618955447.0,[P] Time Series Forecasting," Hey, I have a dataset of police complaints and I want to do time series forecasting. I got officers' wages and years of promotions and years in which complaints were filed. I want to predict future complaints based on wages , past complaints, and when they were promoted.

Any suggestions for methods to use? Sorry, I have just never done time series forecasting, so I'm a bit confused."
pinter69,MachineLearning,1617552299.0,[R] Graph Convolutional Networks in Videos and 3D Point Clouds - Dr. Ali Thabet - Link to free zoom lecture by the author in comments,
WigglyHypersurface,MachineLearning,1618503893.0,[DISCUSSION] How do the different versions of the bootstrap work with deep neural networks?,"I've been looking into deep learning methods as a possible way of imputing missing data, and this has lead me to some questions about how deep neural networks interact with the various versions of the bootstrap.

In a statistical missing data context, the goal is to estimate a posterior predictive distribution for the missing data conditioned on the observed data. You then ""fill in"" the missing data with draws from this distribution, leading to *m* versions of your dataset, where the observed data is the same, but the missing data vary. You do your analysis *m* times and use some simple formulas to combine the analyses, which gives you a nice clean picture of how much uncertainty in the missing data reduces your confidence in whatever hypothesis your testing.

Ok, now the deep net part. In the statistics literature on missing data, there is this idea of ""properness"" which says that you want your posterior distribution for the missing data to reflect all sources of uncertainty, in the model you used to fill in the missing data. Your model for filling in the missing data either needs to be fully bayesian, or approximate a fully bayesian model if that isn't possible.

The simplest way to do an approximate proper posterior in missing data world is often to use either a parameteric or nonparametric bootstrap. For parametric bootstrap, you 1) train a model (say, learning mu and sigma in a linear regression), 2) sample predicted values for the outcome variable, (ie make a new outcome variable by sampling from the learned mu and sigma) 3) retrain the model using the sampled values as the new dependent variable, and 4) make whatever predictions/inferences you want from this second model. If you do this a bunch of times you'll aproximate a bayesian posterior. Nonparametric bootstrap you resample you data with replacement and train on the resampled data. Again, repeated many times you'll get an approximate posterior.

So my questions are, for a deep neural network:

1) What advantages/disadvatages does a deep net using variational bayes versus a deep net using either version of the bootstrap have? Are these any known or expected biases when doing either form of bootstrapping with a deep neural network? 
2) Will you cause a bias if you don't train the network from scratch for multiple iterations of the bootstrap? Would it be a problem if, for example, you resample the data, but carry-over the weights from a previous bootstrap iteration? 
3) Is there a good resource on how changing to a fully bayesian deep network changes what sort of choices you should make about dropout, batch normalization, activations, etc?"
timscarfe,MachineLearning,1619855005.0,[D] Unadversarial Examples video with Hadi Salman (MIT lab),"Performing reliably on unseen or shifting data distributions is a difficult challenge for modern vision systems, even slight corruptions or transformations of images are enough to slash the accuracy of state-of-the-art classifiers. When an adversary is allowed to modify an input image directly, models can be manipulated into predicting anything even when there is no perceptible change, this is known an adversarial example. The ideal definition of an adversarial example is when humans consistently say two pictures are the same but a machine disagrees. Hadi Salman, a Ph.D student at MIT (ex-Uber and Microsoft Research) started thinking about how adversarial robustness  could be leveraged beyond security.  He realised that the phenomenon of adversarial examples could actually be turned upside down to lead to more robust models instead of breaking them. Hadi actually utilized the brittleness of neural networks to design unadversarial examples or robust objects which are objects designed specifically to be robustly recognized by neural networks.  

Video: [https://youtu.be/\_eHRICHlg1k](https://youtu.be/_eHRICHlg1k)

Pod:  https://anchor.fm/machinelearningstreettalk/episodes/52---Unadversarial-Examples-Hadi-Salman--MIT-e1015k2

In the first 10 mins I give an intro covering the MIT features not bugs papers, non-robust features etc. 

Adversarial Examples Are Not Bugs, They Are Features

[https://arxiv.org/pdf/1905.02175.pdf](https://arxiv.org/pdf/1905.02175.pdf)

Adversarial Robustness as a Prior for Learned Representations

[https://arxiv.org/pdf/1906.00945.pdf](https://arxiv.org/pdf/1906.00945.pdf)

Image Synthesis with a Single (Robust) Classifier

[https://arxiv.org/pdf/1906.09453.pdf](https://arxiv.org/pdf/1906.09453.pdf)

&#x200B;

Unadversarial Examples: Designing Objects for Robust Vision

[https://arxiv.org/pdf/2012.12235.pdf](https://arxiv.org/pdf/2012.12235.pdf)

&#x200B;

Do Adversarially Robust ImageNet Models Transfer Better?

[https://arxiv.org/pdf/2007.08489.pdf](https://arxiv.org/pdf/2007.08489.pdf)

&#x200B;

A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks

[https://arxiv.org/pdf/1902.08722.pdf](https://arxiv.org/pdf/1902.08722.pdf)

&#x200B;

Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers

[https://arxiv.org/pdf/1906.04584.pdf](https://arxiv.org/pdf/1906.04584.pdf)

&#x200B;

Denoised Smoothing: A Provable Defense for Pretrained Classifiers

[https://arxiv.org/pdf/2003.01908.pdf](https://arxiv.org/pdf/2003.01908.pdf)

&#x200B;

ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness

[https://arxiv.org/abs/1811.12231](https://arxiv.org/abs/1811.12231)"
pinter69,MachineLearning,1618759421.0,[R] Putting visual recognition in context - Link to free zoom lecture by the authors in comments,
ploomber-io,MachineLearning,1617292408.0,[D] Incremental builds for ML pipelines,"Hi everyone, I'd like to get some perspective on incremental builds. When developing an ML pipeline, I often have to revisit a processing step  (e.g., update SQL or Python script). Since the output is now outdated, I  have to rerun the pipeline, but I can skip unaffected tasks to save some time; as the pipeline grows, this has a significant impact. The most common example is Make. Incremental builds are a must for me since it allows me to modify something and bring everything up-to-date quickly.

A few orchestrators have this feature: [Ploomber](https://github.com/ploomber/ploomber)  (which I'm developing), DVC, drake (in R), but others don't: dagster,  kedro, prefect. Surprisingly, users from the latter group do not seem to miss (or maybe be aware of) that feature.

My only guess is that people who mostly work in Deep Learning don't see much of a benefit of a make-like tool because there are fewer pre-processing steps.

Am I missing anything? How do you ensure all tasks are using the most recent data?  And more importantly, how do you quickly get all outputs up-to-date?"
thejuror8,MachineLearning,1616768727.0,[D] Class-incremental learning and Reviewer 2,"Disclaimer: I am not one of the authors (nor am I connected to the authors in any shape or form) of the following ICLR 2021 rejected paper, for which the review is accessible at:

https://openreview.net/forum?id=mu0WNwWWWCE

While the results presented seemed promising to me (although I'm quite fresh to CIL), I was surprised to read reviewer two's conclusion:

> In summary, the premise of 'class incremental' learning appears weak to me. In practice (in vision applications, ..), most often labeling is manual and highly time consuming, and that's the major bottleneck (getting sufficiently many accurate labels on many classes). As the labels arrive, training or retraining (time and space, batch (re)training) is not the issue with existing batch training methods and computational power.

Besides the fact that this is such a classic reviewer 2 move, traction around papers presenting new CIL methods in vision-specialized conferences has been quite significant, especially recently, which suggests at least some degree of relevance.

Which leads to my question: is there a consensus on the relevance of CIL, and are there people sharing reviewer's opinion on its (im)practicality?"
Haunting-Garbage-364,MachineLearning,1620036485.0,"[D] Companies that sell ""creative"" AI/ML products and services","Hey everyone! I am writing my final thesis on Artificial Intelligence use in creative sectors, and I am on the lookout for creative sector companies that use AI specifically to automate the creation of their product/service... For example, this company: [https://ironov.artlebedev.com](https://ironov.artlebedev.com/) Is selling logos designed by AI. I am looking for companies doing something similar, but cannot find any... If any of you have any ideas or companies you know, please let me know, I would really appreciate it!"
hardmaru,MachineLearning,1617335153.0,[R] EfficientNetV2: Smaller Models and Faster Training,
downtownslim,MachineLearning,1618984939.0,[N] Cerebras launches new AI supercomputing processor with 2.6 trillion transistors,">[Cerebras Systems](https://venturebeat.com/2020/11/17/cerebras-wafer-size-chip-is-10000-times-faster-than-a-gpu/) has unveiled its new Wafer Scale Engine 2 processor with a record-setting 2.6 trillion transistors and 850,000 AI-optimized cores. It’s built for supercomputing tasks, and it’s the second time since 2019 that Los Altos, California-based [Cerebras](https://cerebras.net/) has unveiled a chip that is basically an entire wafer.  
>  
>Chipmakers normally slice a wafer from a 12-inch-diameter ingot of silicon to process in a chip factory. Once processed, the wafer is sliced into hundreds of separate chips that can be used in electronic hardware.  
>  
>But Cerebras, started by SeaMicro founder Andrew Feldman, takes that wafer and makes a single, massive chip out of it. Each piece of the chip, dubbed a core, is interconnected in a sophisticated way to other cores. The interconnections are designed to keep all the cores functioning at high speeds so the transistors can work together as one.

Full text: [https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/](https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/)"
TheoreticallyBlank,MachineLearning,1619406224.0,[R] Fractional pooling layers in CNNs,Have there been any recent publications or surveys related to improvements by replacing standard pooling layers with fractional ones?
Yuqing7,MachineLearning,1618933498.0,"[R] Rice University, IBM & USC Study Pushes Quantum State Tomography Beyond Current Computation Capabilities","A research team from Rice University, IBM and USC combine compressed sensing, non-convex optimization and acceleration techniques to introduce a new algorithm — Momentum Inspired Factored Gradient Descent (MiFGD) — that pushes QST beyond current capabilities. 

Here is a quick read: [Rice University, IBM & USC Study Pushes Quantum State Tomography Beyond Current Computation Capabilities](https://syncedreview.com/2021/04/20/rice-university-ibm-usc-study-pushes-quantum-state-tomography-beyond-current-computation-capabilities/).

The paper *Fast Quantum State Reconstruction via Accelerated Non-Convex Programming* is on [arXiv](https://arxiv.org/pdf/2104.07006.pdf)."
FreddeFrallan,MachineLearning,1616355252.0,[P] [R] Pre-trained Multilingual-CLIP Encoders,"&#x200B;

https://preview.redd.it/n4vkz1ceofo61.png?width=1739&format=png&auto=webp&s=bf33b2791d15b1af6e40d5fd5e5c3742d2b62ad2

We have started creating a set of pre-trained text encoders which match the [OpenAI CLIP](https://openai.com/blog/clip/) image encoders. See our [Github page](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Distil%2040) for more information and list of currently available models.

We have currently only performed a minor qualitative evaluation on:

* French
* Russian
* Spanish
* German
* Green
* Swedish

The model seemingly yields reasonable results for all tested languages ([See results here,](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Distil%2040) in which every column is a Softmax for the corresponding image over the given texts)

The idea of creating multilingual CLIP encoders via teacher learning coupled with machine translation is not very creative, so it is unlikely we will write any proper papers of these. So we thought we might as well share the preliminary models directly.

We will be releasing bigger Multilingual models as soon as they finish training.

Hope you guys find them useful, enjoy!"
hhh312,MachineLearning,1619813787.0,[D] Optimizing the top of a network only,"Hey Guys,

I'm using the backbone of a bart model from the HuggingFace transformers library, but fine tune an additional head. What I do right now is that, where I provide the weights to the optimizer, I only hand in the weights of the head, however, I suspect, during the training, the no\_grad is not applied to the non-head weights, and hence, a lot of computational resources are wasted. Is that the case? How can I enforce no grade to all other weights?

&#x200B;

Thanks"
jj4646,MachineLearning,1618864084.0,"[D] Has anyone ever heard of ""scissor plots"" being used in machine learning?","https://imgur.com/a/d2t6gII

I came across this interesting graph called ""scissors plot"". I have never heard about it before - has anyone else heard about it? Is this a well known plot? 

It would be interesting to know if there was some way to roughly approximate the ""N-o"" point, perhaps the ""N-o"" point could be used to decide if it makes more sense to use ""complex"" models or ""simple"" models."
JollyEye3,MachineLearning,1620281484.0,[D] Anomaly detection in sequential data under budget constraint,"I am working on problem where I need to detect anomalies in a collection of N sequential examples with couple requirements:

1) In each example, there can be multiple anomalies i.e., an example may contain 1 or 100 anomalies. However, the goal is to detect atleast one anomaly per example and detecting a single anomaly is good enough.

2) The number of detected anomalies allowed is B. This is because we only have a fixed annotation budget to confirm the detected anomalies. A human reviewer will review and confirm the anomalies.

There is training data available for the problem. This is a constrained optimization problem where we need to maximize the number of examples covered with as few anomalies detected per example (but atleast one).

Any thoughts on a constrained optimization view of the problem? Is there any research or papers around this topic of cost-constrained anomaly detection in corpus of sequential data?"
dhekurbaba,MachineLearning,1620448897.0,[D] just accepted an offer upon graduation..... what do you do in-between?," doing a phd has really messed up my work-life balance..... this will be my first job upon graduation, and joining is 2 weeks from now, meaning i have this many days with nothing to do

i was thinking of emailing my (to be) boss and asking him to refer some tools/concepts to me so that i can teach myself these things, but was wondering if this is the norm..... or should i not do that and just play video games instead

what do you guys do?"
Brahimce,MachineLearning,1618321567.0,[R][P] How to handle equality constraints in mutation of evolutionary algorithms?,I am new in evolutionary algorithms field. I have a chromosome of 6 variables (real variable) where the sum of these variables equal to one. I am looking for mutation formulas that can generate a new chromosome respecting the equality constraint ( the sum of new chromosome should always equal to one).
LakeTurbulent5878,MachineLearning,1617513587.0,"[D] Having published at top ML conference, how to be nominated as a reviewer?","I used to believe that somebody will invite me after publishing a paper at NeurIPS/ICML/ICLR, but after publishing two first-authored papers at these conferences, I didn't receive the invitation to be a reviewer. I actually quite enjoy reviewing papers and I would be happy to be a reviewer. I think I am qualified at least, but my advisor doesn't mainly work in the ML field so he has no idea how to nominate me. Then who should I contact with? Who has the right to nominate reviewers? Just a reviewer, AC, or PC?"
ptoews,MachineLearning,1620036509.0,[D] CPU choice for machine learning server (Epyc vs. Threadripper),"We are planning on building a rig with 4 RTX 3090 and 128 GB RAM. The application area is computer vision, so preprocessing will most likely be necessary. I've read about DALI which might be useful, but we can't be sure yet.

We are currently looking at Threadripper vs. Epyc. Are there any benchmarks or experiences on how these two line ups compare in image preprocessing tasks?

So far from what I've read Threadrippers have higher clock speed, but run hotter and support less memory capacity and bandwidth, whereas Epyc is the opposite. But how does this translate to a border for applications in ML?

As a side question, how important is core count for preprocessing? Apparently 2 cores per GPU is recommended, but does it scale after that?"
statsIsImportant,MachineLearning,1618647712.0,[D] Looking for the extreme classification + Language modelling video,"Hi, I am looking for a video from a ICML 2020 workshop.

""[ **Invited Talk 2 - Historical perspective on extreme classification in language modeling - Tomas Mikolov** (Talk)](https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5719#collapse6942)  ""

If somebody can point me to a website or provide link, it would be great."
elTope,MachineLearning,1618161239.0,[D] Industry vs Learning process gap,"Maybe this hole post doesn't rasonate with anyone, but still. I always have the same feeling when I try to learn about machine learning stuff. I am obviously constantly amazed by what's being developed and archived by ML in industry/research, but whenever I enroll in a course, search for a guide, etc, I can't wrap my head around how all of this ends up interacting in a production environment. Many courses may explain some models and attach to it some toy example, but when it comes to implementing an end to end solution to a specific problem I'm still rather clueless. 
Maybe this is just me, or maybe is the best way to learn and I lack enough information to know better. If neither of those are the cases, I would like to know if someone could provide with different aproaches, method or resources to remove some of the abstraction.
If I'm just being a bitch you can tell me that also.
Thanks for your attention."
jhanytime,MachineLearning,1618156293.0,[D] Video - Why would you use graphs for machine learning data?,"I'm a PhD student studying machine learning and applications in transportation systems and autonomous systems (think RL and robotics). While there are several ""GCN made easy"" videos out there on Youtube, I feel like these videos often miss the forest for the trees (especially since GCN is just 1 algorithm that was developed in 2016...) videos often don't cover the broader historical context of how GNNs were developed and how different variations allow them to model new types of systems. 

This is the first video in a series I'm making about graphs, graph neural networks, and the application areas where they have the potential to make big impacts. Please let me know what you think of the video and if you learned anything new from it!

https://youtu.be/mu1Inz3ltlo"
srcho,MachineLearning,1619696052.0,[R] Ethical consideration in AI(Machine learning) decision-making process,"Dear community,

I desperately need your help!!

As part of my Master’s thesis at the Universiteit van Amsterdam, I am conducting a study about AI, Machine Learning, Ethical consideration, and its relationship to decision-making outcome quality! I would like to kindly ask your help to participate in my survey. This survey is only for PEOPLE WHO HAVE EXPERIENCE IN THE DECISION-MAKING PROCESS WITH BUSINESS PROJECT before. **If you have working experience with AI, Machine learning, or deep learning, it would be even better!!! Please fill this survey to support me!!**

The survey link is: https://uva.fra1.qualtrics.com/jfe/form/SV\_5bWWZRfReTJmGSa

This survey takes about 5 minutes maximum. To find out the relationship, I need your help with sufficient participants. Please fill out this survey and contribute to helping me to finish my academic work! Feel free to distribute this survey to your network!

I am looking forward to hearing your answers!"
emilwallner,MachineLearning,1617697075.0,[P] How I built a €25K Machine Learning Rig,"Link: [https://www.emilwallner.com/p/ml-rig](https://www.emilwallner.com/p/ml-rig)

Hey, I made a machine learning rig with four NVIDIA RTX A6000 and an AMD EPYC 2 with 32 cores, including 192 GB in GPU memory and 256GB in RAM ([**part list**](https://docs.google.com/spreadsheets/d/1VMtiLZbgLAChKscBAbC1VovVwsI_8Y0BBAW8R06rc5I/edit?usp=sharing)).

I made a 4000-word guide for people looking to build Nvidia Ampere prosumer workstations and servers, including:

&#x200B;

* Different budget tiers
* Where to place them, home, office, data center, etc.
* Constraints with consumer GPUs
* Reasons to buy prosumer and enterprise GPUs
* Building a workstation and a server
* Key components in a rig and what to pick
* Lists of retailers and build lists

Let me know if you have any questions!

Here's the build:

&#x200B;

[Four RTX A6000 with EPYC 2](https://preview.redd.it/1h7fbtb2iir61.jpg?width=1456&format=pjpg&auto=webp&s=63aa35971c13f372a842d231d1a58c83ae6bc437)"
lkhphuc,MachineLearning,1618873310.0,[R] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning,"**ZeRO-Infinity at a glance:**  ZeRO-Infinity is  a novel deep learning (DL) training technology for  scaling model  training, from a single GPU to massive supercomputers  with thousands of  GPUs. It powers unprecedented model sizes by  leveraging the full memory  capacity of a system, concurrently  exploiting all heterogeneous memory  (GPU, CPU, and Non-Volatile Memory  express or NVMe for short). Learn  more in our paper, “[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://www.microsoft.com/en-us/research/publication/zero-infinity-breaking-the-gpu-memory-wall-for-extreme-scale-deep-learning/).” The highlights of ZeRO-Infinity include: 

* Offering  the system capability to train a model with over 30  trillion  parameters on 512 NVIDIA V100 Tensor Core GPUs, 50x larger than  state  of the art. 
* Delivering  excellent training efficiency and  superlinear throughput scaling  through novel data partitioning and  mapping that can exploit the  aggregate CPU/NVMe memory bandwidths and  CPU compute, offering over 25  petaflops of sustained throughput on 512  NVIDIA V100 GPUs.
* Furthering the mission of the DeepSpeed team to democratize large model training by allowing data scientists with *a single GPU* to fine-tune models larger than Open AI GPT-3 (175 billion parameters).
* Eliminating   the barrier to entry for large model training by making it simpler and   easier—ZeRO-Infinity scales beyond a trillion parameters without the   complexity of combining several parallelism techniques and without   requiring changes to user codes. To the best of our knowledge, it’s the   only parallel technology to do this.

![](https://www.microsoft.com/en-us/research/uploads/prod/2021/04/1400x788_deepspeed_nologo-1.mp4)

From the blog post: [https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/).

Massive  props to Microsoft and the DeepSpeed team for their work. I'm thrilled  every time I see a new ZeRO paper and DeepSpeed release on Github."
cedricdb,MachineLearning,1618850067.0,[R] [D] Label info in Adversarial Autoencoders,"I have a question about the Adversarial Autoencoders paper by Makhzani et al, 2016 ([https://arxiv.org/abs/1511.05644](https://arxiv.org/abs/1511.05644)).

Let's look at Figure 8, which concerns the architecture for Semi-Supervised Adversarial Autoencoders. A softmax is used to obtain soft labels for the input image. This soft output is encouraged to be as close to a categorical sample by the objective of the upper GAN. The question is: what happens with the soft label when optimizing decoder? Are you supposed to draw a hard sample from it, or take the argmax?

In the semi-supervised setting it's not that important, but later on in Figure 10 (Dimensionality Reduction with Adversarial Autoencoders), this architecture is reused. Here the softmax output is used as a cluster head selector. Is this a soft or hard selector?

The way I see which makes kind of sense, is as follows: to optimize the upper GAN, you use soft labels. But to optimize the decoder using reconstruction error, you sample from it. But this implies that the gradients of the reconstruction error will not flow through the softmax layer.

Can someone clarify this for me? Thanks!"
roma-glushko,MachineLearning,1618823116.0,[P] How I built my Deep Learning workstation,"Recently I have built my deep learning workstation and shared my experience in the following blogpost:

[https://www.romaglushko.com/blog/how-i-built-my-ml-workstation/](https://www.romaglushko.com/blog/how-i-built-my-ml-workstation/) ⬅️

I have tried to cover all aspects of building machine learning PC:

\- some theory on choosing PC parts for ML

\- hardware installation and troubleshooting guides

\- software and CUDA setup

I hope it's going to be helpful 🙌"
touchanimize,MachineLearning,1618075882.0,[P] Fine tuning Magenta ML model,"Hi all,

I'm a bit of a novice with machine learning as I'm an undergrad working on fine-tuning the Onsets and Frames model by Google ([https://arxiv.org/pdf/1710.11153.pdf](https://arxiv.org/pdf/1710.11153.pdf)) using this code base: [https://github.com/greenbech/onsets-and-frames](https://github.com/greenbech/onsets-and-frames). I'm trying to fine tune this model with jazz music, which is different from the classical music used to train the original model. I was a bit unsure of how to go about this process, and I was hoping if anyone could given any insights on things that I could do. I've done the following thus far:

* Acquired dataset of jazz music and converted them to MIDI
* Have trained different mixes of jazz music with the larger MAESTRO dataset used to train the Onset and Frames model. I've gotten some F1 scores with these mixes.

I was thinking of doing some incremental training progress with a checkpoint and maybe some in-batch dataset mixing. Do I need to do a hyper parameter search again? I'm not exactly sure of how to move forward, so any ideas would be much appreciated. Thank you guys for your time!"
omnipotent_i,MachineLearning,1620326101.0,[D] Paths to become a Productive Non-Academic Researcher,"Hello, I’ve been very much fortunate to have been working in a Research Role post Bachelors in a Start-Up. We’ve been trying for almost 2 years to get accepted in conferences such as ICLR, NeurIPS etc. Have always been unsuccessful. Research is interesting, Unfortunately I couldn’t afford to get into academics due to situations. Is there any other parts I could take to go ahead in this path? Is it a Fantasy to get accepted in Conferences having come from a Non-Academical and Research Labs of Top Companies to provide clear directions and Guidances?  Thanks."
meowklaski,MachineLearning,1618758483.0,[P] MyGrad: Drop-In Autodiff for NumPy,"[https://github.com/rsokl/MyGrad](https://github.com/rsokl/MyGrad)

 

MyGrad is a lightweight library that adds automatic differentiation to NumPy – its only dependency is NumPy!

MyGrad's primary goal is to make automatic differentiation accessible and easy to use across the Python/NumPy ecosystem. As such, it strives to behave and feel exactly like NumPy so that users need not learn yet another array-based math library. 

  
\`\`\`

 \>>> import mygrad as mg 

\>>> import numpy as np 

\>>> x = mg.tensor(\[1., 2., 3.\])  # like numpy.array, but supports backprop! 

\>>> f = np.sum(x \* x)  # tensors work with numpy functions! 

\>>> f.backward() # triggers automatic differentiation  

\>>> x.grad # stores \[df/dx0, df/dx1, df/dx2\] 

array(\[2., 4., 6.\]) 

\`\`\`

  
It works by leveraging NumPy's new(ish) protocols for overriding its functions. Thus MyGrad could eventually be used to bring autodiff to CuPy, xarray, sparse array, and other array-based libraries.

This has proven to also be a very useful library to help folks learn about auto-diff and machine learning. I first created it to support a class that I teach, but it has become a fully-fledged autodiff library since then!"
diffgram-anthony,MachineLearning,1618524641.0,[P] Diffgram - Open Annotation Platform,"Sharing [https://github.com/diffgram/diffgram](https://github.com/diffgram/diffgram)

This has been something I (Anthony) have been working on for the last 2+ years in closed source. Recently we have grown to be a small team.

What makes Diffgram different?

We list some [benefits here](https://github.com/diffgram/diffgram#benefits) but if I had to pick one thing, it's that it's a complete system. You can be up and running [in 2 minutes](https://youtu.be/y0LE7QPXxE0) on docker. And scale to ""big tech co"" level on multiple [k8s clusters.](https://diffgram.readme.io/docs/open-installation-production) 

Over time, the goal is continue to define all the abstractions needed to smoothly work with data in anyway you desire on any system. This goes far beyond UI customizations, or specific speed up approach implementations, and really is a complete ""all in one"" system.

Would love your feedback!

&#x200B;

https://preview.redd.it/ack84fz3vet61.png?width=3385&format=png&auto=webp&s=b535a93c3c5f0d90e4d93326c999174d5466a9aa"
JuanPRamirez,MachineLearning,1617358083.0,[D] What would you say are the biggest hurdles for people looking to get into ML?,"Hey all!

So for context I am a junior year undergrad currently taking my first course in ML. Hopefully looking to get deeper into academia and either become a Researcher or a professor in the field. I originally planned for this post to ask what I can do to better prepare myself for what's to come, but I feel like it's better to gauge that by first asking what the common major hurdles are. Whether it be academic, industry, or just life hurdles that commonly show up. Any info is highly appreciated!"
tomkoker,MachineLearning,1616612505.0,"[P] Torchsort - Fast, differentiable sorting and ranking in PyTorch","Introducing Torchsort, an implementation of ""Fast Differentiable Sorting and Ranking"" [(Blondel et al.)](https://arxiv.org/abs/2002.08871) in PyTorch, complete with a custom C++ and CUDA kernel for fast performance.

    pip install torchsort

[https://github.com/teddykoker/torchsort](https://github.com/teddykoker/torchsort)

Differentiable sorting and ranking operations open the door to new loss functions. For example you can easily implement Spearman's rank coefficient using Torchsort, and have a model learn to output predictions with a monotonic relationship to the targets:

    import torch
    import torchsort
    
    def spearmanr(pred, target, **kw):
        pred = torchsort.soft_rank(pred, **kw)
        target = torchsort.soft_rank(target, **kw)
        pred = pred - pred.mean()
        pred = pred / pred.norm()
        target = target - target.mean()
        target = target / target.norm()
        return (pred * target).sum()
    
    pred = torch.tensor([[1., 2., 3., 4., 5.]], requires_grad=True)
    target = torch.tensor([[5., 6., 7., 8., 7.]])
    spearman = spearmanr(pred, target)
    # tensor(0.8321)
    
    torch.autograd.grad(spearman, pred)
    # (tensor([[-5.5470e-02,  2.9802e-09,  5.5470e-02,  1.1094e-01, -1.1094e-01]]),)

The algorithm itself is O(n log n), and runs quite fast on CPU and GPU (even with large batch sizes and sequence lengths) thanks to the custom Isotonic regression kernel. I hope this is helpful tool for the ML community!"
SentientHero,MachineLearning,1616998376.0,[Project] Resumeasy: Our first attempt to create a data product: An application to recommand relevant jobs and useful skills based on user profile.,
crubier,MachineLearning,1619634130.0,"[P] Labelflow, the open source image labeling and dataset cleaning platform.","Hi, all! Announcing Labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.

We are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  We were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can’t easily be shared. 

So we started building Labelflow, an image labeling tool with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. Let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!"
abhijithneilabraham,MachineLearning,1619615635.0,Question Answering on Covid-19 data [research],"Hi, I have uploaded my first model to huggingface, a lonfgormer model for question answering on covid -19 data. You can find the source code [here](https://github.com/abhijithneilabraham/Covid-QA)

Give a star if you like the project, and also test it on the huggingface API, link given in readme.

&#x200B;

&#x200B;

https://preview.redd.it/dmk0f9iazwv61.png?width=822&format=png&auto=webp&s=e9ad7fa71b7ceff047e61c949599aad3949158c5"
Ziinxx,MachineLearning,1618165769.0,[P] Trained StyleGAN2-ADA on Naruto picture and plugged it through Lucid Sonic Dreams.,
episodeyang,MachineLearning,1620097999.0,"Paper claims scale invariance, Yet implicitly uses data augmentation?",
KirillTheMunchKing,MachineLearning,1616775591.0,[D] Encoding in Style (Pixel2Style2Pixel - pSp) explained,"Have you guys seen the results from the pSp encoder?
 I found the paper extremely useful for my research on GAN inversion, and latent space projection for deep learning based image editing.  

If you want to know the main ideas of the paper ""Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation"" (pixel2style2pixel or pSp) by Richardson et al. head over my [telegram channel](https://t.me/casual_gan), where I break down the main ideas from popular GAN papers.   

In case you missed it, Pixel2Style2Pixel is nowadays used in many image editing apps because it has simple, yet effective ideas and it just works! Read 

more here: [https://t.me/casual\_gan/16](https://t.me/casual_gan/16)"
jj4646,MachineLearning,1619157754.0,[D] neural tangent kernel,"Has anyone heard of the ""neural tangent kernel""? I originally had thought this was an activation function for a neural network. 

Looking here:
 https://en.m.wikipedia.org/wiki/Neural_tangent_kernel

""the neural tangent kernel (NTK) is a kernel which describes the evolution of deep artificial neural networks during their training by gradient descent. It allows ANNs to be studied using theoretical tools from Kernel Methods.""

Can someone please help me understand what this means? Why are neural tangent kernels important?

Thanks

https://rajatvd.github.io/NTK/"
Rishit-dagli,MachineLearning,1616898697.0,[P] Implementing Geoffery Hinton's latest idea paper," I am glad to today present my attempt to implement Geoffery Hinton's latest idea paper about representing part-whole hierarchies in neural networks. Also doing ML more the way the human brain does it!  
[https://github.com/Rishit-dagli/GLOM-TensorFlow](https://github.com/Rishit-dagli/GLOM-TensorFlow)

Consider giving it a star if you like it."
opensourcecolumbus,MachineLearning,1619664415.0,"[Project] Framework to build AI powered search with just 7 lines of code. Supports semantic, text, image, audio & video search","Before [this open-source project(Jina)](https://github.com/jina-ai/jina), one has to depend on closed source solutions to implement neural search. With Jina helps you build our own semantic search engine that can

* Text to text search
* Image to image search
* Text to image search
* Audio to audio search
* Text to audio search
* Text to video search

**Being Open-Source(Apache 2.0 License),** you can modify it, host it on your infrastructure and be in complete control of your data.

**How is it different than Solr/Elasticsearch?**

* Solr/ElasticSearch implements Symbolic Search(rules-based based)
* Jina implements Neural Search(based on pre-trained deep learning models) which results in better semantic search and new capabilities such as cross-modal(e.g. text to video) and multi-modal(e.g. text+image/video to image/video/text) search

Appreciate your feedback/questions"
RoyalScores,MachineLearning,1619528216.0,[D] Is Object Detection a sub-optimal way to do triage and diagnosis in Medicine?,"Currently, Microscopy Object Detection is the fastest feasible method to do  diagnosis of parasitosis' for triage in underdeveloped countries.

Basically, the model is given hundreds of microscopy images of feces and applies Object Detection to all of them in order to find at least one out of a few possible cases of infection.

The way this impacts the problem is that the bulk of negative images can be eliminated by the model, and just the images that might contain eggs of parasites will be analyzed by a biomedic This person will then review the detection and classify it accordingly.

Recently it has come to me that the classification part of the scheme isnt important at all, and the problem could be solved with a approach more similar to Anomaly Detection, by creating a network that can classify images as ""suspicious of infection"" or negative images.

Are there any current researches going on about the specific problem of triage in ML, or is this problem novel? It seems urgent to solve this issues as the WHO has declared it intends to eliminate all NTDs by 2030 and the biggest obstacle to achive this is scalable cheap diagnosis."
hardmaru,MachineLearning,1616392175.0,[R] A rapid and efficient learning rule for biological neural circuits,
hardmaru,MachineLearning,1620018399.0,[R] DriveGAN: Towards a Controllable High-Quality Neural Simulation,
bendee983,MachineLearning,1617367196.0,[D] Machine learning business models in robotics,"Boston Dynamics' latest robot, Stretch, is boring in comparison to the company's previous robots. It can't dance, backflip, and do any of the other tricks that Spot, Handle, and Atlas could do. But it might be the most commercially successful robot the company has created so far.

Successful autonomous mobile robots hinge on versatility/robustness on the one hand, and cost-efficiency on the other.

On the versatility side, they follow the rules of machine learning: The narrower the domain, the more robust the ML model. You can have a robot that can do many tricks and fail often, or one that can do a few tricks but very robustly. Stretch fits this description perfectly. It does one thing (move boxes) in a predictable environment (flat grounds in warehouses), so you can rely on it to work safely and robustly in most cases. And given BD's long history in computer vision and robotics, they can push the limits of versatility beyond their competitors without compromising robustness/safety.

On the cost-efficiency side, since BD has been acquired by Hyundai, they will be in a better position to manufacture their robots at low costs, and then ship further enhancement props to make them even more versatile.

So, Stretch is not cool, but has the potential to turn BD into a profitable company. Meanwhile, it can continue on working to push the limits of science with its research on humanoid and biped robots.

It's kinda like the patent-clerk job Einstein held in the early 1900s. It helped him pay the bills while he used his idle time to develop some of the most important scientific theories of history.

Read the full analysis of BD's new robot and what it means for the company's future here:

[https://bdtechtalks.com/2021/04/01/boston-dynamics-stretch-robot/](https://bdtechtalks.com/2021/04/01/boston-dynamics-stretch-robot/)"
GiuPaolo,MachineLearning,1618831031.0,[R] Sparse Reward Exploration via Novelty Search and Emitters,"Excited to announce that our work on dealing with **sparse rewards environments** through **Novelty Search and emitters** has been accepted at GECCO 2021 for publication!

You can find it here: [https://arxiv.org/abs/2102.03140](https://arxiv.org/abs/2102.03140)

The code instead is released on: [https://gpaolo.github.io/SERENE/](https://gpaolo.github.io/SERENE/)

Check it out and if you have any question do not hesitate to ask!

**Abstract:**

Reward-based optimization algorithms require both exploration, to find rewards, and exploitation, to maximize performance. The need for efficient exploration is even more significant in sparse reward settings, in which performance feedback is given sparingly, thus rendering it unsuitable for guiding the search process. In this work, we introduce the SparsE Reward Exploration via Novelty and Emitters (SERENE) algorithm, capable of efficiently exploring a search space, as well as optimizing rewards found in potentially disparate areas. Contrary to existing emitters-based approaches, SERENE separates the search space exploration and reward exploitation into two alternating processes. The first process performs exploration through Novelty Search, a divergent search algorithm. The second one exploits discovered reward areas through emitters, i.e. local instances of population-based optimization algorithms. A meta-scheduler allocates a global computational budget by alternating between the two processes, ensuring the discovery and efficient exploitation of disjoint reward areas. SERENE returns both a collection of diverse solutions covering the search space and a collection of high-performing solutions for each distinct reward area. We evaluate SERENE on various sparse reward environments and show it compares favorably to existing baselines."
huggingface,MachineLearning,1617809911.0,[R] A prompt is worth a thousand data points: combining GPT3-style prompting and traditional fine-tuning,
MushiML,MachineLearning,1619776404.0,[D] Temperature term in SimCLR or MoCo papers.," Hi!

I read an interesting article on SimCLR and it was quite helpful. 

[https://amitness.com/2020/03/illustrated-simclr/](https://amitness.com/2020/03/illustrated-simclr/)

What is the real purpose of term temperature in the loss function? Please can anyone help in understanding it with some intuitive example. Also, I found this temperature term in the MoCo paper; both of them means the same? I found the following comment on this blog post ([https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e](https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e)), but I don't think that I really understood what does it mean.

""Chen et al. found that an appropriate temperature parameter can help the model learn from hard negatives. In addition, they showed that the optimal temperature differs on different batch sizes and number of training epochs.""

Thanks"
OnlyProggingForFun,MachineLearning,1619878190.0,[R] Infinite Nature: Fly into an image and explore it like a bird!,
tdls_to,MachineLearning,1619374980.0,[D] Can you train a privacy-aware language model,"Pretrained language models memorize training data which can be uncovered by probing the model with appropriate prompts. This has some serious privacy implications. Here is a paper discussing the same; I would love to hear more about this.

[https://arxiv.org/abs/2104.07504](https://arxiv.org/abs/2104.07504)"
kk_ai,MachineLearning,1619004138.0,[D] Convenient libs to use for new research project at the intersection of GNN and RL.,"If you were to start new research project (starting point is the ""rough idea"" what you want to do), that can potentially take 1+ years, what tools would you pick?

There is number of libraries for each domain (GNN, RL), and they are based on TF/Keras, PyTorch or JAX. To that end, it's a bit overwhelming to review all of them to make an informed choice.

I did my homework before, so I have some candidates in mind which I am intentionally not sharing, because I don't want to bias the discussion towards any particular direction.

In general, I don't mind learning new libraries, if it promises some flexibility in the future and has growing community of users.

Pls advice, thx."
hardmaru,MachineLearning,1617242521.0,[R] Fast Adaptation with Linearized Neural Networks,
techsucker,MachineLearning,1619587796.0,"[R] Researchers at JAIST, the Japan Advanced Institute of Science and Technology, Have Proposed a Model that Allows Voices to Mimic and Control the Generated Speech’s Speaker Identity","Voice Conversion (VC) method used to modify the speaker’s identity without altering the linguistic content. Non-linguistic information is vital for having natural (human-to-human) communication. By changing the non-linguistic information, such as adding emotion to speech, VC can make human-machine communication sound more natural. This allows people to get more information from speech and thus socialize better. 

Humans use several languages for communication, and we often need machine translators for speech-to-speech conversions. Prof. Akagi from JAIST explains that conventional (monolingual) VC models face challenges when we apply them to a “cross-lingual” VC (CLVC) task. For example, changing the speaker’s identity led to an undesirable modification of linguistic information.

Summary: [https://www.marktechpost.com/2021/04/27/researchers-at-jaist-the-japan-advanced-institute-of-science-and-technology-have-proposed-a-model-that-allows-voices-to-mimic-and-control-the-generated-speechs-speaker-identity/](https://www.marktechpost.com/2021/04/27/researchers-at-jaist-the-japan-advanced-institute-of-science-and-technology-have-proposed-a-model-that-allows-voices-to-mimic-and-control-the-generated-speechs-speaker-identity/) 

Paper: [https://ieeexplore.ieee.org/document/9367139](https://ieeexplore.ieee.org/document/9367139)"
jj4646,MachineLearning,1619072335.0,"[D] is the ""curse of dimensionality"" still as relevant as it was 20 years ago?","I have been reading some good examples that explain (in layman's terms) what is the curse of dimensionality. 

These examples first considers a circle inside a square (2 dimensions: example 1) - and then considers a sphere inside a cube (3 dimensions: example 2). This is to illustrate the fact that the cube in example 2 is a lot more ""emptier"" (ratio of volume between sphere and cube) compared to the square in example 1. As the number of dimensions increase (e.g. the cube becomes a hypercube in 4 dimensions), it can be mathematically shown that the ratio of emptiness increases more and more. In this analogy, the sphere represents the data and the cube represents the space which the data belongs to. These examples show us that in higher dimensions, we need exponentially more and more data to fill this space - thus, in higher dimensions, data becomes more ""sparse"", and this sparsity makes it harder to fit machine learning algorithms (I understand this is intuitively, but I don't know if there is a mathematical explanation behind why sparsity gives machine learning algorithms a hard time - perhaps sparsity makes some of the matrix calculations harder to calculate? ). Furthermore, it can be shown using ""Chernhoff's Inequality"" that in higher dimensions, data is probabilistically more likely to occupy the extremity regions of the space, further exacerbating the curse of dimensionality.

All this being said - how are modern machine learning examples (e.g. deep neural networks) able to overcome the curse of dimensionality? Some of these CNN (Convolutional Neural Networks) deal with pictures that are by nature high dimensional data, and thus likely to suffer from the curse of dimensionality. Yet companies like Google and Microsoft are constantly developing neural networks that are able to successfully make predictions on pictures. 

By the looks of it, it would appear that ""the curse of dimensionality is dead"" (or rather, it doesn't affect us that much as it once did). How are modern neural networks able to handle the curse of dimensionality? I was reading about techniques called ""manifold learning"" which are able to extract important information from the data and reduce the number of dimensions (thereby mitigating the curse of dimensionality). Just a thought - somewhere within the architecture and all the hidden layers in neural networks, is some form of ""dimensionality reduction"" taking place?"
Spotums,MachineLearning,1617791320.0,[D] Docos on ML like AlphaGo - The Movie,I really enjoyed watching AlphaGo - The Movie on youtube and was wondering if there are any similar documentaries related to Machine Learning that are worth watching as entertainment/infotainment without being too dry?
jfischer,MachineLearning,1617812523.0,[P] Datahut.ai: A directory of data science and data engineering projects,"https://datahut.ai is a new free website that provides statistics and analysis on the most popular data science and data engineering projects. My wife and I created this site because we were spending a lot of time researching which projects best fit a given use case, both for our clients and for our personal side projects. We cover over 100 projects, but are just scratching the surface. We’d love your feedback on what topics to cover and what additional content you’d like to see. Thanks!"
soulslicer0,MachineLearning,1618352118.0,[D] Bayesian Machine Learning for KITTI Depth Estimation,"This code predicts depth from RGB images. But instead of producing depth alone, it produces a multimodal depth distribution for each pixel, in the form of a categorical distribution. This is useful for weeding out uncertain 3d points, or in downstream adaptive depth sensing tasks. We solve the task of Monocular, Stereo and Lidar Upsampling based depth estimation using the same architecture. The core network architecture is taken from PSM-Net and Neural RGBD

I note that bayesian machine learning has not really been explored in detail in research, especially when looking at multi-view stereo or sensor fusion.

\[Writeup\] [https://github.com/soulslicer/probabilistic-depth/blob/main/pics/explanation.pdf](https://github.com/soulslicer/probabilistic-depth/blob/main/pics/explanation.pdf)

\[Code\] [https://github.com/soulslicer/probabilistic-depth](https://github.com/soulslicer/probabilistic-depth)"
harish-2306,MachineLearning,1616831163.0,[P] Looking for a teammate in implementing a neat algorithm in Python with C++ as the backend.,"Hi, I'm  20 and doing my major in the field of Data Science. I've planned to  create a python library for neat with a C++ backend so it'll faster than  the already existing neat library. This is my first time writing a  library but I'm very well experienced with Python, C++ and DL. I have  thought of writing the wrapper in SWIG.

We  cloud dicuss and change the technology or plans if needed. I'll be a  fun person to work with. So if you are interested to join drop a  message.

Thanks :)"
Inevitable_Engineer5,MachineLearning,1617281572.0,[D] Genetic Algorithm: the chromosome representation for Sliding Puzzle Solver?,"Hello, I want to solve the game: Sliding Puzzle Solver via Genetic Algorithm. But I don't have any idea, how should be the chromosome representation for the problem. For example I can encode each movement via bits (00 - down, 11-up, 01-right, 10-left). Its' OK but the  recombination will not work with this chromosome representation, because not all movements are allowed. Do you have any idea? thanks"
ykilcher,MachineLearning,1620491472.0,[D] Paper Explained - Involution: Inverting the Inherence of Convolution for Visual Recognition (Full Video Analysis),"[https://youtu.be/pH2jZun8MoY](https://youtu.be/pH2jZun8MoY)

Convolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

3:00 - Principles of Convolution

10:50 - Towards spatial-specific computations

17:00 - The Involution Operator

20:00 - Comparison to Self-Attention

25:15 - Experimental Results

30:30 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2103.06255](https://arxiv.org/abs/2103.06255)

Code: [https://github.com/d-li14/involution](https://github.com/d-li14/involution)"
emystats,MachineLearning,1616606233.0,"[P] Best way to calculate ""performance"" in a probability estimation task","Thank you in advance for taking the time to read this long post!

I am working on a task in which participants estimate the probability that a series of beads are extracted from one of two hidden jars. The beads are extracted one by one, with replacement. 

The [two jars](https://i.stack.imgur.com/5fO8Hm.png) contain beads in two colors, yellow and black, in different proportions. *Jar A contains 85% yellow beads and 15% black beads, and jar B contains 85% yellow beads and 15% black beads.*

While the jars are hidden to the participant, he or she is aware of the difference between them - therefore he or she can estimate the probability that a sequence of beads is extracted specifically from one of the two jars. 

**Example**

After each extraction, the participant always answers the same question: ""What's the probability that the sequence was extracted from jar A?

Then, if the first bead extracted is yellow, it's more likely that bead was extracted from jar A; if the second bead is also yellow, it's now even more likely that the sequence of beads was extracted from jar A, and so on.

The estimation, of course, changes as the participant is shown more beads. All participants are shown the same sequence.

**The actual task**

At ""Event 0,"" the participant is asked the question before seeing any beads. This is why the estimation in this [plot](https://i.imgur.com/xl7qxlk.png) is at 50%. 
In the plot, you can see a red line: that's the estimations for one participant. On the x-axis, you can see the extraction number (or event number), on the y-axis the probability estimation.
In black you can see the ideal observer's estimation, that is, the correct probabilities.

Then the participant is shown a sequence of beads that have already been extracted. In this case, [8 yellow beads and 2 black beads](https://i.stack.imgur.com/P2ZTpm.png). 

That's why, at this point (Event 1), the sequence is most likely coming from jar A.

After that, the participant is shown more beads, extracted one by one. They all happen to be black (the participant does not know this beforehand, of course.) That's why the probability estimation slowly decreases.

[Final sequence](https://i.stack.imgur.com/nrBBmm.png)





**The problem**

I would like to define a ""performance profile"" for each participant, based on how he or she responds to the task. Then I would like to be able to correlate this ""profile"" with some psychometric results (average responses to surveys.)

About the ""performance profile"", I would like to have a good idea of how much the participant is far off from the ideal observer. I thought I could just calculate the distance between each pair of points on the y-axis and sum them up. Probably the absolute distance or the squared distance would be better, but I would also like to retain the ""sign"" (in other, similar tasks, the participants' responses both overestimate and underestimate the ideal observer's estimation.)

Using the distance, I could easily correlate the ""performance"" with the personality data.

**Question**

Does this make sense? I was wondering if there are better ways to perform this type of analysis. Is there a way I can retain more information about the participant's choices? For example, I thought I could fit one curve to the participant's response and one curve to the ideal observer's estimation and evaluate the difference between the parameters defining the curves, but I am not sure about how to go about that.

Someone has suggested that I instead perform a [time series k-means clustering](https://drkeithmcnulty.com/2020/03/02/clustering-time-series-data-in-r/) to group participant's responses. I am not familiar with this analysis but that could be an idea. But if I perform a clustering analysis, could I then see how each cluster performs with respect to the personality criteria? For example, ""people in cluster A are particularly high in X criterium."" I also thought about performing a [PCA](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/) to see how the personality criteria are correlated (another analysis I am not too familiar with!) The question is how to relate the psychometric results to the performance.

By the way, [here](https://i.stack.imgur.com/BVMSq.png) you can see an image of all the participants. 

If you have any ideas, or if you can recommend online example or tutorials I would really appreciate it!

R code for one participant:

    
    library(ggplot2)
    library(scales)
    
    # participant's probability estimations
    
    participant <- structure(list(event = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), prob_est = c(0.46, 
     0.98, 0.89, 0.72, 0.53, 0.21, 0.24, 0.12, 0.09, 0.01)), class = ""data.frame"", row.names = c(""1"", 
    ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10""))
    
    # ideal observer's probability estimations
    
    ideal_observer <- structure(list(event = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), prob = c(0.5, 
    0.99996979903057, 0.999828885289768, 0.999031123657329, 0.994534412955466, 
    0.969798657718121, 0.85, 0.5, 0.15, 0.0302013422818792)), row.names = c(NA, 
    10L), class = ""data.frame"")
    
    plot <- ggplot(data=subset(participant, event<=9), aes(x = event, y = prob_est, col=""red""))  + 
            geom_point(cex=1.5)+
            geom_line(lwd=0.9)+
            labs(x=""Event Number"", y=""Probability"") + 
            scale_y_continuous(breaks=pretty_breaks(n=10), limits = c(0,1))+
            scale_x_continuous(breaks=pretty_breaks(n=10))+
            geom_line(data=subset(ideal_observer, event<=9), aes(x = event, y = prob),col=""black"",lwd=0.9)+
            geom_point(data=subset(ideal_observer, event<=9), aes(x = event, y = prob),col=""black"",cex=1.5)
    plot

    # calculating discrepancy from ideal performance
    difference <-  sum(participant[,2] - ideal_observer[,2])
    difference
    #> [1] -2.743364

 <sup>Created on 2021-03-24 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>"
SQL_beginner,MachineLearning,1619536458.0,[D] Rules for Determining how much Data should he used in a Model,"This is a concept i always struggled with: in statistics, is ""more data always better""? 

Suppose you 50 years of data about hospital visits. You are interested in supervised classification. You have predictors such as age, height, weight, blood type, salary, etc. You are interested in predicting if the hospital stay will be less than 1 day or more than 1 day. This can be easily solved using random forest.

My dilemma is: using all 50 years of data might be able to capture a wide variety of patterns  ... but since we are interested in predicting future information, maybe some of the older data is less relevant and might surpress more current trends?

How do you deal with this problem?"
Rina-Panigrahy,MachineLearning,1619364646.0,"[R] Google-Workshop: Conceptual Understanding of Deep Learning, May 17. Join Us.","Please join us for a virtual Google workshop on “[Conceptual Understanding of Deep Learning](https://sites.google.com/view/conceptualdlworkshop/home)” 

**When**: May 17th 9am-4pm PST. 

**Where**: [Live over Youtube](https://www.youtube.com/watch?v=g5DGBWjiULQ),

**Goal:** How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological strides in recent decades, there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form. The goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning, characterizing the class of functions that can be learned, coming up with the right learning architecture that may (provably) learn multiple functions, concepts and remember them over time as humans do, theoretical understanding of language, logic, RL, meta learning and lifelong learning.

The speakers and panelists include **Turing award** winners Geoffrey Hinton, Leslie Valiant, and Godel Prize winner Christos Papadimitriou ([full-details](https://sites.google.com/corp/view/conceptualdlworkshop/home)).   

**Panel Discussion:** There will also be a panel discussion on the fundamental question of “**Is there a mathematical model for the Mind**?”. We will explore basic questions such as “Is there a provable algorithm that captures the essential capabilities of the mind?”, “How do we remember complex phenomena?”, “How is a knowledge graph created automatically?”, “How do we learn new concepts, function and action hierarchies over time?” and “Why do human decisions seem so interpretable?”

Twitter:[ \#ConceptualDLWorkshop](https://twitter.com/search?q=%23ConceptualDLWorkshop&src=recent_search_click). Please  [Retweet](https://twitter.com/rinapy/status/1384311169519788032).Hope to see you there!

Rina Panigrahy

([http://theory.stanford.edu/\~rinap](http://theory.stanford.edu/~rinap))"
wattnurt,MachineLearning,1620102631.0,"[D] Are there any ML algorithms that can learn a simple ""X+1"" problem?","I've had this idea for a while, a very simplistic problem statement:

Suppose you have N binary inputs and N binary outputs. The simple problem is: for any arbitrary input, copy that to the output, but set one more output bit to 1. Any of them will do. As an example, input is 0010010, a correct output would be 0011010.

Now, N should of course be large enough to not just exhaustively learn the input/output set during training. An interesting side effect of this problem is also, for any input there are usually several correct outputs (as many as there were 0s on the input).

Are there any  ML algorithms that can learn something like this? I should note that I'm not so much interested in some heavily catered solution (e.g. where the problem statement has been encoded in the feature set or the model architecture), but more from a general learning power point of view. Are there algorithms that can learn this extremely simple rule?"
Yuqing7,MachineLearning,1617727057.0,"[N] IBM, UMich & ShanghaiTech Papers Focus on Statistical Inference and Gradient-Boosting","A team from University of Michigan, MIT-IBM Watson AI Lab and ShanghaiTech University publishes two papers on individual fairness for ML models, introducing a scale-free and interpretable statistically principled approach for assessing individual fairness and a method for enforcing individual fairness in gradient boosting suitable for non-smooth ML models.

Here is a quick read: [Improving ML Fairness: IBM, UMich & ShanghaiTech Papers Focus on Statistical Inference and Gradient-Boosting](https://syncedreview.com/2021/04/06/improving-ml-fairness-ibm-umich-shanghaitech-papers-focus-on-statistical-inference-and-gradient-boosting/)

The papers [*Statistical Inference for Individual Fairness*](https://arxiv.org/pdf/2103.16714.pdf) *and* [*Individually Fair Gradient Boosting*](https://arxiv.org/pdf/2103.16785.pdf) are on arXiv."
ykilcher,MachineLearning,1618846736.0,[D] Paper Explained - NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (Full Video Analysis),"[https://youtu.be/CRlN-cYFxTk](https://youtu.be/CRlN-cYFxTk)

View Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:50 - View Synthesis Task Description

5:50 - The fundamental difference to classic Deep Learning

7:00 - NeRF Core Concept

15:30 - Training the NeRF from sparse views

20:50 - Radiance Field Volume Rendering

23:20 - Resulting View Dependence

24:00 - Positional Encoding

28:00 - Hierarchical Volume Sampling

30:15 - Experimental Results

33:30 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)

Website & Code: [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)"
jj4646,MachineLearning,1619155542.0,"[D] can someone please explain the ""representer theorem"" in simpler term?","https://en.m.wikipedia.org/wiki/Representer_theorem

Can someone please try to explain the ""representer theorem"" in simpler terms?  Why is it considered important in the realm of machine learning?

Thanks"
ClaudeCoulombe,MachineLearning,1619512223.0,"[D] Who first advanced the ""manifold hypothesis"" to explain the stunning generalization capacity of deep learning?","The manifold hypothesis states that natural data lies on a low-dimensional manifold (kind of local euclidian subspace) within the high-dimensional space where the data is encoded. I've already found [\[Clayton, 2005\] Algorithms for manifold learning](http://cseweb.ucsd.edu/~lcayton/resexam.pdf)  but it was written before the term ""deep learning"" was  invented by Hinton in 2007 and [\[Brahma, Wu, She, 2015\] Why deep learning works: A manifold disentanglement perspective](https://diginole.lib.fsu.edu/islandora/object/fsu:406789/datastream/PDF/view) which mentioned ""manifold hypothesis"" as a fact but without any citation."
niels_vg,MachineLearning,1617358952.0,Predict Company Sales - Design choices; how to store incoming sales/which model to choose? [P],"Hi Folks,

I am trying to predict the bookings of a stand-up comedian cafe. There are a lot of features I can use which have an affect on the number of sales. (e.g. day of the year, weather, average sales last month, day of the week, average sales on the specific day of the week etc.)

The goal is to create a model which is able to predict the number of sales for a given day once every hour, starting 10 days (240 hours) before the show and stops one hour before the deadline of the show. We have multiple predicting variables which are always known and do not change, these are:

* **avg\_30**. Average daily sales in the last 30 days
* **avg\_60**. Average daily sales in the last 60 days
* **vacation**. hot/cold variable which indicates whether it is a vacation day
* **LongtermGrowth**; the number of days the company already exists to measure continuous growth
* **DayofYear**. The day of the year. ( min: 1, max: 365)
* **Weekday**. Each day of the week hot/cold encoded. (moday: 0/1 etc.)

Apart from the above variables we have one more indicator (maybe the most important of all) which highly correlates with the eventual sales we are trying to predict. This is a combination of the two variables:

* **currentHour**. The number of hours before the start of the show (min: 0, max: 240)
* **Current sales.** The number of sales already obtained

***My Question***: Is what would be the best method to store all of the data above in such a way I can sufficiently train a prediction model on it. For the *currentHour* and *CurrentSales* I am in doubt between the following two methods.

1. I can simply use two column in the X data. where (1) indicates the number of hours before the deadline and (2) indicates the number of sales already obtained. However, this doesn't provide us any information on how the sales were obtained.
2. I can create a column for each hour before the deadline (240 columns in total). named hr\_01 until hr\_240. Where the value of each hour would be the number of sales yet obtained during that timestamp. The downside of this solution would be that 240 of the 246 columns only hold this data; is this a good idea? This will most likely drastically increase computational time?

*Concluding* I have the following two questions: **(1)** What would be the best of the two methods described above to store the sales yet obtained?, if you have a better idea than the two scenario's I created above, i would be glad to hear it! Furthermore, **(2)** what type of prediction model would you think results in the best performance? Currently I am seeing high performance with Linear Regression (Lasso & Ridge). I can hardly believe this would result in best performance.

Thanks in avance."
jacobgil,MachineLearning,1619376340.0,[Project] Recent Class Activation Map Methods for CNNs and Vision Transformers,"[https://github.com/jacobgil/pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)

&#x200B;

CAM based methods are a family of pixel-attribution methods that try to highlight the parts in the image that contribute to a model output.

These methods assign weights to spatial 2D activations in the network, and them sum them to get a 2D saliency map. 

&#x200B;

This project includes a PyTorch implementation (that you can pip install) for several Class Activation Map methods, including a few very recent ones:

* Grad-CAM ([https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391))
* Grad-CAM++ ([https://arxiv.org/abs/1710.11063](https://arxiv.org/abs/1710.11063))
* XGrad-CAM ([https://arxiv.org/abs/2008.02312](https://arxiv.org/abs/2008.02312))
* Ablation-CAM ([https://ieeexplore.ieee.org/abstract/document/9093360/](https://ieeexplore.ieee.org/abstract/document/9093360/))
* Score-CAM ([https://arxiv.org/abs/1910.01279](https://arxiv.org/abs/1910.01279))

And works for Vision Transformers (tested with DeiT), as well as for CNNs (tested with torchvision.models).

&#x200B;

I hope it will be useful, and that it can be a convenient starting point for developing and comparing new methods!"
TheCollaboratory,MachineLearning,1619194117.0,[P] Introducing The Collaboratory: A place to explore and discover research based on your interests,"Keeping track of new research is an increasingly time-consuming task in many fields. Fast-paced and interdisciplinary domains like ML are even harder to stay on top of, and require researchers to self-curate from a variety of sources (bookmarks, newsletters, arXiv reviews, etc.) on the topics relevant to them. The Collaboratory is a webservice that aims to ease the pain of exploring and keeping up with the research that’s most relevant to you.


Our platform automatically sources and recommends the latest research relevant to your interests, based on papers you seed into “reading lists”. These recommendations are powered by large language models that a) transform research descriptions into semantic embeddings, and b) compare those with pre-computed embeddings from a growing index of over 100M+ papers and datasets. Our models ensure that your recommendations are highly relevant, and our indexing efforts ensure that they are timely.


We were inspired to solve this problem after encountering it in our own daily lives, checking dozens of sources a week to stay on top of the literature in a few different fields. We think this tool will be really useful for people spinning up on new topics, people whose interests lie in-between fields, and for people who need to keep up with a stream of research that moves quickly.


Please take a look at the site and let us know what you think: [thecollaboratory.ai](https://thecollaboratory.ai). 


The platform remains a work in progress, but we want to keep making it better - please let us know how we can make it better for you!"
kaia_1527,MachineLearning,1618075767.0,[D] Learning resources: multi-object localization,"Hey everyone, I'm learning about multi-object localization and was wondering whether anyone could recommend learning resources (with an end-to-end example). I went through the 2018 [fast.ai](https://fast.ai) [lesson](https://www.youtube.com/watch?v=0frKXR-2PBY) on object localization using the Pascal dataset but the examples are dated. I'm hoping to train a multi-object detector on a single class but have found the examples surprisingly sparse."
swifty540,MachineLearning,1619081651.0,[D] How repetitive are the Ambient Sounds?,"As a fan of the Ambient Sounds feature on the Apple HomePod, I was wondering about their repetitiveness and found this thread: [/r/HomePod/comments/dpu4me/does\_anyone\_else\_feel\_like\_the\_ambient\_sounds\_are](https://www.reddit.com/r/HomePod/comments/dpu4me/does_anyone_else_feel_like_the_ambient_sounds_are/)

I'm just not so sure about their actual repetitiveness as described in the thread and was wondering if Machine Learning could be used to figure out exactly how repetitive they actually are. Does anyone here have insights about tools that can detect repetitiveness in audio?"
mamrollahi,MachineLearning,1616864558.0,[D] Which similarity method has been used in WS353 dataset?,"Hello,

May I know, which similarity method has been used in [WS353 dataset](http://alfonseca.org/eng/research/wordsim353.html)?"
KirillTheMunchKing,MachineLearning,1619540579.0,"[D] Main ideas from ""EigenGAN Layer-Wise Eigen-Learning for GANs"" explained!","# [EigenGAN Layer-Wise Eigen-Learning for GANs](https://t.me/casual_gan/31)

The authors propose a novel generator architecture that can intrinsically learn interpretable directions in the latent space in an unsupervised manner. Moreover each direction can be controlled in a straightforward way with a strength coefficient to directly influence the attributes such as gender, smile, pose, etc on the generated images.

[Samples and architecture overview](https://preview.redd.it/zds1xws1sqv61.png?width=1280&format=png&auto=webp&s=2f0ba0e21e513c5ce7bfb13c7d5e6d4eef43a89f)

&#x200B;

[Direction traversal examples](https://reddit.com/link/mzs8ct/video/746okqodsqv61/player)

Check out:\[[5 minute paper explanation](https://t.me/casual_gan/31)\] \[[Arxiv](https://arxiv.org/pdf/2104.12476.pdf)\]"
Anomalix,MachineLearning,1619811263.0,[D] Any good enough DCGANs that work on low-end GPUs?,"I want to experiment a bit with DCGANs, but 4GB of VRAM is not enough  for something like StyleGAN, so I was wondering if there's anything that  can run on lower end GPUs that's good enough. I want to be able to do  256x256 (or, if possible, 512x512, but I'm betting that's impossible on  such a low amount of VRAM)."
blatant_variable,MachineLearning,1619388418.0,[R] Correcting Experience Replay for Multi-Agent Communication (ICLR 2021 Spotlight),"Hi, I'm first author of this paper on how RL agents can learn to communicate. In general, this is quite a challenge because as agents learn, their policies change, making multi-agent environments highly non-stationary. Our key insight is an Orwellian one -  we can use present information to alter past messages to improve future learning. This involves relabelling messages sampled from the replay buffer to reflect the current communication policies of each agent. We find this greatly enhances agents' abilities to learn, substantially improving the performance of multi-agent RL algorithms across a range of experiments.

Paper : [https://openreview.net/forum?id=xvxPuCkCNPO](https://openreview.net/forum?id=xvxPuCkCNPO)Video: [https://www.youtube.com/watch?v=piiNq-fGDCY](https://www.youtube.com/watch?v=piiNq-fGDCY)

Please ask any questions you may have - happy to answer!"
minimaxir,MachineLearning,1619970229.0,[P] Create Your Own AI-Generated Magic: The Gathering Cards (2-cell Colab Notebook),"https://colab.research.google.com/drive/1VOt090UzvltoBgMdUZmU5vwhi4X-6E_a

A couple years ago I trained GPT-2 on Magic: The Gathering cards and it worked extremely well, however it was overengineered due to the limits around GPT-2 at the time.

Now, with much better AI text generation tooling, I trained a bespoke tiny GPT-2 (~1M parameters) on encoded Magic card data using [aitextgen](https://github.com/minimaxir/aitextgen) and its schema capabilities. Anecdotally the quality of cards generated is much better than the RNN approaches of the past. The cards follow the color pie now!

I was able to make a Colab Notebook with surprisingly little code if you unnest it, including the card decoder. The model is small enough that you can run locally on a CPU if you download the notebook.

Let me know if you have any questions!"
forsakenMule,MachineLearning,1617823068.0,[D] Regularly retraining a churn prediction model,"I am trying to build a process which aims at predicting customer churn. And I am having a hard time finding good resources on how to tackle the problem of retraining the model.

Indeed while it's relatively ""easy"" to train a model for the first time given I have the right data, I am struggling to determine the best strategy to deal with regular retraining. 

Indeed let's assume I have trained a model wich performs well and the company decides to use it to derive actions on customers which have been flagged as potential churners. I am now faced with the issue that the new data which is available to me is biased by the actions that were taken due to the predictions of the model in production.

Reinforcement learning is not really an option as the feedback time is counted in years while the concept drift due to product changes and competitive pressure is rather in months.

Any idea or links on how go tackle this issue?"
nirmalya8,MachineLearning,1617254105.0,"""[D]"" Generating Medical Images using GANs","Hey people,

Medical  datasets either have less data or the datasets are very imbalanced. To  deal with this imbalance, I thought of synthetically generating medical  images of the class with less examples. So, I was looking for Papers on  Generating Medical Images with the help of Generative Adversarial  Networks.

Can I get some recommendations?

Constraints:

1. Dataset: No Constraint, preferably freely available
2. Either code for the paper or details about the architecture, loss functions, activation functions, etc should be there

Thank You!"
6rubtub9,MachineLearning,1619011427.0,[D] Meaning of semantic in machine learning,"Hi all, 

This may be an elementary question to ask, but as I am reading several computer vision papers I come across the phrases ""semantically useful features"", ""semantically strong features"", ""deep semantic features"" and so on.. 

I tried looking for the meaning of ""semantics"" in machine/deep learning domain but couldn't find any satisfying answer.

So can anyone explain what does it mean by ""semantically"" strong, deep, useful and so on.

Thank You"
seuqaj114,MachineLearning,1618486921.0,[P] Nimbo: Run jobs on AWS with a single command,"Hey everyone,

My friend and I just launched [Nimbo](https://nimbo.sh), a dead-simple CLI that wraps AWS CLI, allowing you to run code on AWS as if you were running it locally. GitHub: [https://github.com/nimbo-sh/nimbo](https://github.com/nimbo-sh/nimbo). Docs: [https://docs.nimbo.sh](https://docs.nimbo.sh).

We decided to build this because we were frustrated with how cumbersome using AWS was, and we just wanted to be able to run jobs on AWS as easily as we run them locally. At the same time, we wanted to make use of cheap spot instances (on Nimbo, this is a single parameter). All in all, we didn't like the current user experience.

For this reason, we also provide many useful commands to make it faster and easier to work with AWS, such as easily checking prices, logging onto an instance, or syncing data to/from S3 (you can see some useful commands [here](https://docs.nimbo.sh/useful-commands)).

Unlike other similar services, we are solely client-side, meaning that the code runs on your EC2 instances and data is stored in your S3 buckets (we don't have a server; all the infrastructure orchestration happens in the Nimbo package).

We have tons of ideas for Nimbo, such as one-command Jupyter notebooks on EC2, add docker support, and (my personal favorite) provide images with preloaded large datasets like ImageNet, so that you don't have to download and store it yourself - you simply spin the instance, and the dataset is available at `/datasets`.

We are happy to receive any feedback and suggestions you have."
michaelaalcorn,MachineLearning,1620422300.0,"[R] DeepMind - Game Plan: What AI can do for Football, and What Football can do for AI",
khalilmeftah,MachineLearning,1616538667.0,[P] Generating CryptoPunks Images with GPT-2," 

[ Generated CryptoPunks with GPT-2](https://preview.redd.it/5633kncstuo61.png?width=768&format=png&auto=webp&s=b6fc29063f8cc97b0362231ca0c5cfa2f67c6516)"
chimp73,MachineLearning,1619910179.0,"[D] How far can we get with one-shot learning, generalization and policy gradient?","[OpenAI research](https://arxiv.org/abs/2001.08361) shows that merely scaling up simple NNs improves performance, generalization and sample-efficiency.
Notably, fine-tuning GPT-3 converges after only [one epoch](https://github.com/cabhijith/GPT-3_Docs/blob/master/Fine-Tune.md).
This raises the question: Can very large NNs be so sample-efficient that they one-shot learn *in a single SDG updates* and reach human-level inference and generalization abilities (and beyond)?

Assuming such capabilities, I've been wondering what could an actor model look like that makes use of them: Chiefly, one could eliminate the large time horizons used in RNNs and Transformers, and instead continuously one-shot learn sensory transitions within a very brief time window, by predicting the next few seconds from previous ones. Further, one could dedicate some output neurons to driving some actuators and train them with policy gradient. Then long-term and near-term recall would simply be generalizations of one-shot learned sensory transitions, and, similarly, decision making would simply be generalization of one-shot learned modulations to the policy.

(To make clear what I mean by one-shot learning by SDG and recall by generalization: Let's say you are about to have dinner and you predict it is going to be pasta, but it's actually fish. Then the SDG update makes you one-shot learn what you ate that evening due to the prediction error. When asked what you ate the next day, then by generalization (from the context of yesterday, to the context of the question), you know it was fish.)

Further, one could use each prediction sample as an additional prediction target such that the model one-shot learns its own predictions as thoughts that have occurred; and by generalization and reward modulation, these thoughts become goal-driven, allowing the agent to ignore the prediction objective if it increases reward (e.g. pondering via inner monologue instead of listening). One would also need to feed the prediction sample as additional sensory input in each time step such that the model has access to these thoughts or predictions.

Then conscious thoughts are not in a latent space, but in sensory space. This matches the human mind, as we, too, cannot have thoughts beyond our model of the data generating process of sensory experience. Further, conscious thoughts would occur in brief time slices, which also matches human conscious thoughts, skipping from one thought to the other in almost discrete manner, with consciousness hence only existing briefly [during the forward passes](https://karpathy.github.io/2021/03/27/forward-pass/), and reality being re-interpreted each second afresh, tied together via one-shot learned contextual information in the previous steps.

By allowing the model to learn from imagined/predicted rewards too, imitation learning would be a simple consequence of generalization, namely by identifying the other agent with the self-model that naturally emerges.

The mere self-model of one's predictions or thoughts, being learned by predicting one's own predictions, seems  sufficient for thoughts to get strategically conditioned (by previous thoughts) such that they are goal-directed, again relying on generalization. I.e. the model may be conditioned to do X by a one-shot learned policy update, but by world knowledge it knows X only works in context Y (which establishes a subgoal). The model also knows that its thoughts act as predictors, thus, by generalization, in order to achieve X it generates a thought that the model expects to be completed in a manner that is useful to get to Y.

The architectural details may not matter much. Ignoring economic factors, there is not a large difference between different NN architectures so far. Even though Transformers [perform 10x better](https://arxiv.org/abs/2001.08361) than LSTMs (Fig. 7, left), there is no strong divergence, i.e. no evidence of LSTMs not being able achieve the same performance with about 10x more resources. Transformers seem to be mostly a trick to get large time horizons, but they are biologically implausible and also not necessary if you rely on one-shot learning tying together long-term dependencies instead of long time-horizons.

Generalization would side-step the issue of meticulously backpropping long-term dependencies by temporal unrolling or exhaustively backpropping value information throughout state space in RL. Policy gradients are extremely noisy, but human-level or higher generalization ability might be able to filter the one-shot learned noisy updates, because, by common sense (having learned how the world works though the prediction task), the model will conclude how the learned experience of pain or pleasure plausibly relates to certain causes in the world."
hiDDenthings63,MachineLearning,1616820874.0,[D] How do I make a model which takes a bedroom image as input give an output of different design of bedroom related to input image?,"I want to make a model for my project which take some interior design image as input and provide me some output with some different kind of design related to the input image.  I don,t know where to start I think it would use CNN. but don't what should be the track to make. I didn't find anything related to it on google."
KirillTheMunchKing,MachineLearning,1618595253.0,[R] Spatially-Adaptive Pixelwise Networks for Fast Image Translation (ASAPNet) by Shaham et al. - Explained,"# [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://t.me/casual_gan/27)

The authors propose а novel architecture for efficient high resolution image to image translation. At the core of the method is a pixel-wise model with spatially varying parameters that are predicted by a convolutional network from a low-resolution version of the input. Reportedly, an 18x speedup is achieved over baseline methods with a similar visual quality. More details [here](https://t.me/casual_gan/27).

[ASAPNet](https://preview.redd.it/ablptrd5pkt61.png?width=1225&format=png&auto=webp&s=95fed8d1fbaa57d3481a88284604436c6190ecec)

 If you are not familiar with the paper check it out over [here](https://t.me/casual_gan/27)."
bendee983,MachineLearning,1616616039.0,[D] Has anyone tried Newton's method for ML optimization?,"Is there any scenario where Newton's method would be better than SGD, Adam, etc.? If yes, why isn't it included in ML/DL frameworks (unless I'm mistaken)?"
deep-yearning,MachineLearning,1618494268.0,[D] What really makes neural networks generalizable?,"We usually employ regularization techniques (dropout, batch norm., early stopping, etc.) in helping prevent our models overfitting to the training set and perform well on the validation set. **However, this still does not usually help our models perform better on real data (test data), or data that is considered to be out-of-distribution of the training/validation sets.** This means that our models are not generalizable. 

This is clearly a huge area of research, but most papers that I have read do not lead to direct usable advice for our own projects, and are instead focused on more theoretical discussions of generalizability (please correct me if I am wrong and point me to better papers). So here is my question:

**What are your practical tips during training and development to get a model to actually perform well on real test data and out-of-distribution data?** Obviously, starting with training/validation data that matches the distribution of your expected test data is a great start - but we can't always predict or guarantee the distribution of our test data."
amirninja,MachineLearning,1617643791.0,[Discussion] Similarity between two datasets/matrices,"Hello,

I have  a dataset and created another dataset from it using tow different methods, methods A and method B. 

I would like to find out how close the newly created dataset is to original dataset? What metrics would make more sense in this case? Cosine similarity/Euclidean distance? 

If I have to capture the closeness/difference in a single number how about using Frobenius Norm? 

This [subredit](https://www.reddit.com/r/MachineLearning/comments/r6i99/proper_method_for_calculating_similarity_between/) seems to suggest Mantel test. However, I am not sure if thats the right one here? Any thoughts/help would be appreciated.

&#x200B;

Thanks!"
JEUNGHWAN,MachineLearning,1620611903.0,[P] AI Kant is willing to be ghostwriter only for you,"Hi, There! I dreamed of someone who writes a Philosophy essay in place of me when I majored in Philosophy. So I came up with the idea that I trained GPT-2 with ‘The Critique of Pure Reason’ of Kant and Kant becomes Ghostwriter and does write Kantian essay in place of students.

The project called Teachable NLP.

[https://forum.ainetwork.ai/t/teachable-nlp-kant-is-willing-to-be-ghostwriter-only-for-you/58](https://forum.ainetwork.ai/t/teachable-nlp-kant-is-willing-to-be-ghostwriter-only-for-you/58)

It is a program that helps fine-tuning natural language processing (NLP) models without complex code or a Graphics Processing Unit (GPU). So you can easily train and get your own NLP model.

There some use cases that you want it.

I'd appreciate any feedback and your thoughts!

Thanks.  


[Demo](https://reddit.com/link/n8t2ud/video/ogn2pttj97y61/player)"
broutonlab,MachineLearning,1620219389.0,[D] Have you ever faced attacks on deep learning model in your projects?,"Adversarial Attacks\[[blog post](https://broutonlab.com/blog/adversarial-attacks-on-deep-learning-models)\] are a serious threat to DL models. An attacker can intentionally create malicious inputs to fool the DL model and get the incorrect outputs. This can lead to serious troubles, e.g. in banking applications that do user identification by his face or in citizen surveillance systems at the airport.

The questions are,

1 - When developing deep learning models, do you take into account the fact that they may come under Adversarial Attacks in production?

2 - If so, what methods of protection do you employ?

3 - Have you ever tried to attack DL models in production😎?"
Yuqing7,MachineLearning,1619627967.0,[R] Google’s 1.3 MiB On-Device Model Brings High-Performance Disfluency Detection Down to Size,"A research team from Google Research proposes small, fast, on-device disfluency detection models based on the BERT architecture. The smallest model size is only 1.3 MiB, representing a size reduction of two orders of magnitude and an inference latency reduction of a factor of eight compared to state-of-the-art BERT-based models.

Here is a quick read: [Google’s 1.3 MiB On-Device Model Brings High-Performance Disfluency Detection Down to Size.](https://syncedreview.com/2021/04/28/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-7/)

 The paper *Disfluency Detection with Unlabeled Data and Small BERT Models* is on [arXiv](https://arxiv.org/pdf/2104.10769.pdf)."
vaseline555,MachineLearning,1619268301.0,[D] Has anybody implemented FedAvg? I have a question," 

Hello,

I am now implementing FedAvg using PyTorch.

Even after looking into some Github repositories, I am still confusing: testing models

My main question is:

(1) Should I prepare test dataset for each client?  
(i.e. each client has its own training/test dataset)

OR

should I use globally identical test set for all client?  
(since after finishing FedAvg, all the clients have the model with the same weights)

\---

(Detailed description)

I am now dealing with MNIST dataset with 10 clients.

Each client has only one digit: Client 0 has digit 0, Client 1 has digit 1, ... , Client 9 has digit 9.  
(for the simulation of non-IID setting)

After local training, each client sends parameters to the central server, the server then aggregates them, and finally re-distribute the averaged model to clients.

For testing the averaged model's performance, should I test only once using test dataset having all digits (the former case in the above question), or test 9 times using locally split test set (each having only one digit) and calculating mean to report the final performance?

\----

It is very confusing... If the latter is true, each client finally has the ability to encode the unobserved data (e.g. for Client 0 having only 0 digit sample, it is finally available to encode another digits (1-9) after FedAvg). Am I right?

Thank you in advance!"
martin1285,MachineLearning,1618585781.0,[D] Serializing and using KNN for Predictions,"Hello,

I am working on a problem where I would like to use the K-Nearest Neighbors Regressor to make predictions. I am working in Python and I am using Joblib to serialize the model in a pickle format.

I understand that the K-Nearest Neighbors is instance-based and requires the entire training set to make predictions. When running this model through the pipeline (ETL, train model, serialize model and make predictions), I am able to make predictions using just the pickled model.

My understanding is that in Python, the training data is serialized along with the algorithm and does not require me to load the entire training data when making predictions. Resulting from this, the size of the pickled object gets larger as the training data increases.

Is my understanding of this correct from a deployment perspective? This would be my first time deploying an instance-based model.

Thanks in advance!"
wdanilo,MachineLearning,1618363356.0,"[News] [Project] Enso 2.0 is out! Visual programming language for Data Science. It lets you code in a visual way in Python, Java, R, and JavaScript. Written in Rust and running in WebGL.","Hi, I'm Wojciech, one of the founders of Enso.

Enso is an award-winning interactive programming language with dual visual and textual representations. It is a tool that spans the entire stack, going from high-level visualization and communication to the nitty-gritty of backend services, all in a single language.

Enso is also a polyglot language - it lets you import any library from Enso, Java, JavaScript, R, or Python, and use functions, callbacks, and data types without any wrappers. The Enso compiler and the underlying GraalVM JIT compiler, compile them to the same instruction set with a unified memory model.

Check out:

* our **demo video**: [https://www.youtube.com/watch?v=fQvWMoOjmQk&t=2s&ab\_channel=Enso](https://www.youtube.com/watch?v=fQvWMoOjmQk&t=2s&ab_channel=Enso)
* our **website**: [https://enso.org](https://enso.org/)
* our **GitHub** (Enso is Open Source): [https://github.com/enso-org/enso](https://github.com/enso-org/enso)
* the **GraalVM** website (which Enso compiler bases on): [https://www.graalvm.org](https://www.graalvm.org/)"
hardmaru,MachineLearning,1616464018.0,[N] NeurIPS2021 will be using openreview.net to manage submissions,"According to the program chairs, “NeurIPS 2021 will be using OpenReview to manage submissions this year but the reviewing process will not be public.  As in previous years, submissions will be visible only to their assigned program committee.  All internal discussions will remain private both during and after the reviewing process.  After the notification deadline, accepted and opted-in rejected papers will be made public, together with their anonymous reviews and meta-reviews.”

So unlike ICLR, the review process will still be private, and the reviews would be released only afterwards. Rejected submissions by default would not be revealed, unless the authors opt-in.

https://openreview.net/group?id=NeurIPS.cc/2021/Conference"
techsucker,MachineLearning,1619392197.0,"[R] Scientists From The Max Planck Florida Institute For Neuroscience (MPFI) Have Developed ‘Gold Digger’, A Software Tool That Uses A Modified PIX2PIX Deep Learning Network","Electron microscopy (EM) is a method used for high-resolution images of biological and non-biological samples. It requires precise and time-consuming steps from sample preparation to image acquisition to produce the clarity and details required to visualize small cell structures with high resolution. Additionally, extracting the biological information out of EM-created images is a very laborious and time-intensive task. This is because the current EM analysis software usually requires the skilled eye to examine hundreds of pictures manually.

A team of scientists from the Max Planck Florida Institute for Neuroscience (MPFI) has applied neural networks to create a novel analysis software ‘Gold Digger,’ aimed at streamlining part of the lengthy process. [Diego Jerez and Eleanor Stuart, two high school data science students](https://techxplore.com/news/2021-04-gold-digger-neural-networks-nexus.html), started working on this project out of curiosity. But later, it turned into a more complex and interdisciplinary project.

Summary: [https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/](https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/) 

Paper: [https://www.nature.com/articles/s41598-021-87015-2](https://www.nature.com/articles/s41598-021-87015-2)"
moon-child-99,MachineLearning,1618115946.0,[P] How do I validate a subjective model?,"I have a clustering model that classifies as best and worst, essentially a trade off of all the features provided. Since the classes are subjective, how do I validate this as there aren't test cases with the right answer."
bendee983,MachineLearning,1616423246.0,[R] Adversarial training reduces safety of neural nets in robots,"Adversarial training results in a drop in pure accuracy of DL models, which is generally received as an acceptable tradeoff for robustness against adversarial attacks.

But when used in robots, the accuracy degradation can cause safety risks, according to a [research paper](https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/) by IST Austria, MIT, and TU Wien.

Key points:

\- Adversarial training has been designed for static image classification tasks, where each inference task is independent of others. Robotics deal with dynamic environments, where inferences are done in dependent sequences.

\- In robotics, it doesn't just matter how often errors happen but also *where* the errors take place, i.e. if they happen in consecutive cycles, they can result in the robot crashing

\- Classic adversarial training metrics only measure the number of misclassification errors. In robotics, it is also important how far the prediction error deviates from the correct label.

Read the coverage of the paper + interview with the lead author here:

[https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/](https://bdtechtalks.com/2021/03/22/adversarial-training-robot-learning/)

Read the full paper here:

[https://arxiv.org/abs/2103.08187](https://arxiv.org/abs/2103.08187)"
Rohit901,MachineLearning,1618904137.0,[D] When to use one-hot encoding of categorical variables?,"Hey all, I have 20 continuous input variables and 1 categorical variable which has 14 levels, so if I use one-hot dummy encoding then it will create 14 more variables as input, will it degrade model performance? I plan to use a simple multi-layer neural network and maybe add LSTM layer in it for classification task.

If it helps, I have 11,747 data points. I was planning to use create dummies method from pandas to encode categorical variables."
Appaulingly,MachineLearning,1618063524.0,[P] A machine learning tool now exists to detect Photoshop face warping!,"[Sheng-Yu Wang et. al.](https://arxiv.org/abs/1906.05856) have developed a ML tool to identify images that have been edited using Adobe Photoshop’s face aware liquify tool, a common tool utilised to warp facial features. The ML algorithm outperforms humans in identifying edited images and can even determine where editing has taken place in the image. I'd highly recommend checking out their works it's all fascinating.

I've created a subreddit ( r/FakeWarpBot ) which scrapes images from r/Instagramreality and r/KimKardashianPics and uses their tool to analyse the image for face warping. r/Instagramreality is subreddit where redditors post images purported to have been edited in different ways. But a lot of the time the suggested edits are too subtle for human determination, or in fact no-editing has taken place. So I hope that r/FakeWarpBot will provide a more certain determination of editing.

***You can now also post your own images to*** r/FakeWarpBot ***for analysis! I would stress that in no way is this machine earning tool a final determination on whether face warp has occurred.***

All credit goes to [Sheng-Yu Wang et. al.](https://arxiv.org/abs/1906.05856).

EDIT: spelling"
_rusht,MachineLearning,1619974802.0,"[P] Onepanel - open source, extensible deep learning platform, now includes an Ubuntu based deep learning desktop, hyperparameter tuning and a Python DSL for defining pipelines and workflows",
aselsiriwardena,MachineLearning,1618556255.0,[P] Simple UI for deep learning model,"Hi! I'm looking for some help with the project I'm working on.

It's a  simple image-to-image translation project. I want to create a UI for that model. The project was built using PyTorch.I'm looking for a very simple UI just to demonstrate like selecting an image from the left side and showing the resulting image on the right side.

Any suggestions? Are there any broiler plate codes?"
NahanTrogn,MachineLearning,1618195876.0,[Discussion] About pre-processing audio for Yamnet's input,"Hi community ML, 

This is first time I post a question in forum.

I read the input of [Yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet#input-audio-features) as:

""**Input: Audio Features**

As with our previous release VGGish, YAMNet was trained with audio features computed as follows:

All audio is resampled to 16 kHz mono. A spectrogram is computed using magnitudes of the Short-Time Fourier Transform with a **window** size of 25 ms, a window hop of 10 ms, and a periodic Hann window.

 A mel spectrogram is computed by mapping the spectrogram to 64 mel bins covering the range 125-7500 Hz. A stabilized log mel spectrogram is computed by applying log(mel-spectrum + 0.001) where the offset is used to avoid taking a logarithm of zero. 

These features are then **framed** into 50%-overlapping examples of 0.96 seconds, where each example covers 64 mel bands and 96 frames of **10 ms each**.""

So, my question is why each frame length 10 ms ( ""where each example covers 64 mel bands and 96 frames of **10 ms each""** )? But not 25 ms? I think that because 25 ms is a length of a window of STFT, while 10ms is only a ""window hop"".

Thanks in advance. Wish you a good day."
ottawalanguages,MachineLearning,1619755336.0,[D] How sensitive are statistical models to the richness of information within the data?,"I spent the last hour thinking and creating an example that illustrates my question : https://imgur.com/a/36gU0pb

My question indirectly relates to exploratory data analytics and feature selection for statistical models. Suppose you have some variables (let's assume they are categorical variables for this example) - when you make a histogram for these variables, they appear extremely skewed. On first glance, you would not want to include heavily skewed variables as inputs for a statistical model - e.g., if 99% of the variable is a single value, how informative and useful could it be to a statistical model?

But how do you know that these heavily skewed variables don't contain some very useful information in the 1%, that might really help you in making future predictions?

Sometimes, using the context of the problem (e.g. if you working on a biology problem, consult with the biologists) you might be able to gain some insights - other times, you can try to use some logic to figure out if these heavily skewed variables are in fact useful for the model ... but when you have big and complex data, how is it possible?

I posted an example above that illustrates this problem (https://imgur.com/a/36gU0pb) - I would be curious to know if any of you have dealt with similar problems in the past.

Thanks"
dgheere,MachineLearning,1619432105.0,[R] Survey on evaluation of algorithmically generated music. Can you tell the difference?,"Dear all,

At Utrecht University, I am conducting a scientific study on the evaluation of algorithmically generated music. I’m testing to see whether people can identify whether a piece of music is made by a computer or by Johann Sebastian Bach, specifically. It does not matter whether you are familiar with the style of Bach, or not.

I would very much appreciate it if you could take 5-10 minutes of your time to fill out this survey. It would greatly help me in writing my bachelor thesis. Your responses will, of course, be completely anonymous and be treated with confidentiality.

Thank you for your time!

https://survey.uu.nl/jfe/form/SV_865UclrBbCUhUb4"
Professional-Bag392,MachineLearning,1617261893.0,[D] Is there a way to evaluate model during training?,"I am working on a Machine Learning Project. I have set up a ML pipeline for various stages of project. The Pipeline goes like -

Data Extraction -> Data Validation -> Preprocessing -> Training -> Model Evaluation

Now as Model Evaluation is concerned, currently it happens after training is completed. Based on evaluation, if model is approved or rejected. Now what I want is model evaluation to take place during training itself at any point. Say at about when 60% of the training is complete, the training is stopped and model is evaluated, based on which if the model is approved, it resumes the training.

How can the above scenario be implemented?"
johndoe709,MachineLearning,1616437000.0,[D] Gender Bias in Persian to English Google Translate,"The third person pronoun in many languages is neutral or so-called epicene. In translating texts from these languages into languages in which the third person pronoun is not neutral, such as English, the translator usually either determines the gender of the pronoun in general (he/she) or assumes it using evidence. Below you can see the translation of a text from Persian to English done by Google Translate:

[Gender Bias in Persian to English Translation](https://preview.redd.it/3xs7qtl6bmo61.png?width=1366&format=png&auto=webp&s=c784c3326862a13173a25837c405186f81259de8)

I have to point out that Google Translate is a lot better than it was a few years ago. And it helped me write this text. I'm curious about the solution to avoid bias in language models. And is there a solution at all? Because these models are trained with texts that we humans have produced.

This post is inspired by the following post:

[https://www.reddit.com/r/europe/comments/m9uphb/hungarian\_has\_no\_gendered\_pronouns\_so\_google/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/europe/comments/m9uphb/hungarian_has_no_gendered_pronouns_so_google/?utm_source=share&utm_medium=web2x&context=3)"
Stanford_Online,MachineLearning,1619652145.0,[N] Free webinar - Hacking AI: Security & Privacy of Machine Learning Models,"Register for our upcoming webinar with Stanford Professor Dan Boneh. He will discuss recent work at the intersection of cybersecurity and machine learning, with an emphasis on adversarial machine learning. [https://learn.stanford.edu/security-privacy-machine-learning-models-webinar.html](https://learn.stanford.edu/security-privacy-machine-learning-models-webinar.html)"
papajan18,MachineLearning,1619619546.0,"[R] ""Understanding Human Intelligence through Human Limitations"" - A Great Article that Highlights How to Think of the Relationship Between the Study of Human and Machine Intelligence","https://www.sciencedirect.com/science/article/pii/S1364661320302151

Some highlights of the article:

* Human intelligence arises from optimizing for three main constraints: limited time (lifespan), limited computation (must fit inside a single brain), and communication (must be able to transfer solutions to other brains to sustain humankind). 

* The space of problems human intelligence is solving is a subset of potential problems we want artificial intelligence to solve. Most machines do not face the same constraints humans do, so they will not necessarily have to use the same solution. 

* There are some use cases of machine intelligence where one has limited time, bandwidth, and high latency communication. In such cases, understanding how humans can optimize around such constraints can provide valuable insight."
qudcjf7928,MachineLearning,1618165059.0,"[D] Has anyone looked at ""LSPE"" algorithm as portfolio rebalancing method?"," 

[http://proceedings.mlr.press/v108/uziel20a/uziel20a.pdf](http://proceedings.mlr.press/v108/uziel20a/uziel20a.pdf)

This "" Long-and Short-Term Forecasting for Portfolio Selection with Transaction Costs"" paper claims it can produce positive returns even during the down market times.

Typical problems with the classical methods of portfolio rebalancing was that they were commission fees oblivious, so their models and results were quite not realistic. Ever since then, there has been numerous ways found to incorporate the said commission fees .... etc

And then I came across this LSPE paper but the problem is i have no idea what they are talking about.

I get that there are long-term portfolios that get rebalanced every d days, while there are short-term portfolios that when mod(t,d) != 0, then the agent can choose to update to the short term portfolio.

But I have no idea after ""the transition paths"" part. What are transition paths? what purpose do they serve and what are their dimensions and how are they used?"
ykilcher,MachineLearning,1619899473.0,[D] Paper Explained - DINO: Emerging Properties in Self-Supervised Vision Transformers (Full Video Analysis),"[https://youtu.be/h3ij3F3cPIk](https://youtu.be/h3ij3F3cPIk)

Self-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI's new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for image retrieval and zero-shot k-nearest neighbor classifiers (KNNs).

&#x200B;

OUTLINE:

0:00 - Intro & Overview

6:20 - Vision Transformers

9:20 - Self-Supervised Learning for Images

13:30 - Self-Distillation

15:20 - Building the teacher from the student by moving average

16:45 - DINO Pseudocode

23:10 - Why Cross-Entropy Loss?

28:20 - Experimental Results

33:40 - My Hypothesis why this works

38:45 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)

Blog: [https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training)

Code: [https://github.com/facebookresearch/dino](https://github.com/facebookresearch/dino)"
Difficult_Parsnip876,MachineLearning,1620376445.0,[N] The Pastry A.I. That Learned to Fight Cancer,"[The Pastry A.I. That Learned to Fight Cancer | The New Yorker](https://www.newyorker.com/tech/annals-of-technology/the-pastry-ai-that-learned-to-fight-cancer)

&#x200B;

seems like the bread shops in my country should adopt this technology as well"
MaJhole007,MachineLearning,1618824980.0,[D] BERT Finetuning/Domain Adaptation,"Usually when bert is finetuned on downstream tasks (task adaptation) only small dataset (500 samples) is required. 
I am wondering how much data do we need to finetune it on a specific domain like Finance (domain adaptation) . And do you expect the resulting model to outperform original Bert on downstream tasks in Financial domain ? 
Thanks in advance.."
dojoteef,MachineLearning,1619234019.0,[R] Wordcraft: a Human-AI Collaborative Editor for Story Writing,
thunder_jaxx,MachineLearning,1616698147.0,[R] Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,"Abstract: 

In the standard Markov decision process formalism, users specify tasks by writing down a reward function. However, in many scenarios, the user is unable to describe the task in words or numbers, but can readily provide examples of what the world would look like if the task were solved. Motivated by this observation, we derive a control algorithm from first principles that aims to visit states that have a high probability of leading to successful outcomes, given only examples of successful outcome states. Prior work has approached similar problem settings in a two-stage process, first learning an auxiliary reward function and then optimizing this reward function using another reinforcement learning algorithm. In contrast, we derive a method based on recursive classification that eschews auxiliary reward functions and instead directly learns a value function from transitions and successful outcomes. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.

&#x200B;

Arxiv URL: [https://arxiv.org/abs/2103.12656](https://arxiv.org/abs/2103.12656)

*This is such a wonderful direction for RL. I am so excited papers are being written in this direction*"
jikkii,MachineLearning,1618864223.0,"[N] HuggingFace releases accelerate: A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision","HuggingFace releases a new PyTorch library: [Accelerate](https://github.com/huggingface/accelerate), for users that want to **use multi-GPUs or TPUs** without using an abstract class they can't control or tweak easily. With 5 lines of code added to a raw PyTorch training loop, *a script runs locally as well as on any distributed setup.*

They release an accompanying blog post detailing the API: [Introducing 🤗 Accelerate](https://huggingface.co/blog/accelerate-library).

Here's an example of what it looks like in practice:

[HuggingFace Accelerate in practice](https://preview.redd.it/me4g5rtmw6u61.png?width=1055&format=png&auto=webp&s=4839efdf6bfac121e1e3889c6df9235f47af7e06)

The library is fully open-sourced and available on PyPI and on GitHub; to learn more, check out the [documentation](https://huggingface.co/docs/accelerate/)."
fabien-campagne,MachineLearning,1618632546.0,[P] Multi-modality Perceiver implementation for Pytorch,"I forked Phil Wangs' repo implementing Perceiver for Pytorch and fixed two problems with the implementation:

1. The figure in the preprint and text indicate that the cross attention block is followed by a 'Latent Transformer'. The text indicates that the latent transformer has 6 blocks when training on ImageNet. This repo , to be fair, most re-implementations I have seen for Keras or JAX make the same mistake and use a single latent block, not a 'Latent Transformer' made up of several blocks. 
2. The signature of the forward method in this implementation cannot support training with multi-modality inputs. Multi-modality is when you want to train with video, audio and image for instance as inputs to the same model. The signature does not support multi-modality because each modality has a different number of dimensions, and positional encoding must be applied to each modality independently. This is not possible when accepting a single tensor as input to forward if positional encoding is done in the forward method.

The repo also offers an experimental contribution to help with text as one of the input modalities.

I have implemented fixes for these two issues in this fork: [https://github.com/fac2003/perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch), the package is available from PyPi (pip install [perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch))."
bayonetworking123,MachineLearning,1619155102.0,[P] Imputing values to training/testing data after model selection,"I'm developing a model to multiply impute missing values in covariate data from an experiment I ran. As expected, I selected a model, trained on some training data, based on test set fit. Am I running risk of leakage using this model to then predict missing values in the combined training/test data? 

This is a somewhat different scenario that I've typically experienced: I am trying to predict missing values in my *current* data without overfitting rather than predict data I will see in the future. I could use bootstrapped datasets/cross-validation instead of a testing set to avoid any issues, but I'm not sure if there is an issue here in the first place."
squidwardstrousers,MachineLearning,1616373467.0,[D] Why does computer vision get more attention than speech recognition?,I see a lot more tutorials and datasets geared towards computer vision than speech recognition. Why?
MonitorIndividual341,MachineLearning,1618801306.0,Prospects of Machine Learning and AI [Discussion],Just wanted opinions on the future of ml and AI. How major of a role will it play in the future? What about jobs? Also when should someone start learning the topic?
jwestonhughes,MachineLearning,1616338331.0,[D] Explicitly modelling inter-operator variability in segmentation,"I have a medical image segmentation dataset where each example is segmented by exactly one of twelve different segmenters, with on the order of thousands of segmentations each. I want to understand if each segmenter differs systematically; for example, does one segmenter usually draw tighter segmentations, or usually cut off a piece of the segmentation more often. Does anyone know of any existing computer vision methods for looking at this? I could swear I saw a publication recently on building a model to predict every segmenter's segmentation, but I can't find it now and am wondering if I dreamed it."
dlisfyn,MachineLearning,1619647752.0,[D] Need advice on how to generate HD point clouds visuals for a presentation,"Hi All

I've to show point cloud data as a part of presentation. I wanted to seek ideas on how can i render high definition point cloud visuals. Till now, I was taking screenshots from Meshlab and that didn't looked so good. Also, if someone is aware of how can i generate these rotating shape visuals ([here at 1:15](https://www.youtube.com/watch?v=1iuLxJmQII0) ) .

Thanks"
whyhateverything,MachineLearning,1617194894.0,[D] Good algorithm for clustering big data (sentences represented as embeddings)?,"Hi,

I have a lot of sentences (100.000) that are represented as embeddings through sentence transformers and I want to cluster them in the group (the number of sentences can be larger as well). All results on Google point out to Kmeans but I don't like it since it doesn't use cosine similarity, it's not scalable and very slow. At the same time, I am interested in finding a good algorithm that can help me cluster this amount of embeddings without losing quality and be time-friendly. I am also struggling in using other solutions since they also ask for the cluster number in advance and I cannit determine it for obvious reasons.

I must point out that I am not a professional machine learning engineer and even though I understand how to use some implementations and what are their disadvantages and advantages, I cannot rewrite optimizations on my own (I often see this happening in the research world where there are pros in data science, ML and AI).

Your help is very valuable and more than welcome!

Take care, I wish good health to everyone."
spiritualParkour,MachineLearning,1619002088.0,[P] Coral Dev board vs coral Dev board mini,"Hope this is the right sub to ask.   
I went through the site, but I couldn't understand what the difference was. [https://coral.ai/products/dev-board-mini](https://coral.ai/products/dev-board-mini)   
[https://coral.ai/products/dev-board/](https://coral.ai/products/dev-board/)   


It'd be great if someone can point out. Thanks!"
tdls_to,MachineLearning,1617115495.0,"[P] ML Product Challenge (free to participate, sponsored by a few startups)","hey people!

We are hosting a [machine learning product challenge](https://ai.science/_m/deep-learning-product-challenge-cohort-8?utm_source=rdt) where you can work with a small team (bring your team or meet peers and team up) to solve an interesting business problem using ML. There are  projects sponsored by a few startups as well as the opportunity to work on your own project:

* $3,000 prize: build a web app to automatically detect bias risk in ML models
* $3,000 prize: (tbc) build a recommender system web app 
* $1,000 prize: build an api to mine relationships between business use cases and technical concepts
* $300 prize: whatever you want to build

If you have any questions about this, please ping me on here, or join our [info session](https://www.eventbrite.ca/e/ml-product-challenge-info-session-xai-tickets-148260708771?aff=newsletter)

&#x200B;

ps. This challenge is free to join for everyone, all you need to do is to submit a proposal and then you'd be invited to participate (template for proposal and an instructional video series is provided)"
RyanAI100,MachineLearning,1619981364.0,[D] CrossWeigh: Training NER with Imperfect Annotations | Research Papers Summary 016,
mennasiam,MachineLearning,1616527200.0,Video Class Agnostic Segmentation Benchmark in Autonomous Driving [R],"Check  out our work on video class agnostic segmentation in autonomous driving  ( to identify unknown objects jointly with semantic/panoptic  segmentation).

Paper: [https://arxiv.org/pdf/2103.11015.pdf](https://arxiv.org/pdf/2103.11015.pdf)

Code: [https://github.com/MSiam/video\_class\_agnostic\_segmentation](https://github.com/MSiam/video_class_agnostic_segmentation)

Demo:  [https://www.youtube.com/watch?v=c9hMFHdTs6M](https://www.youtube.com/watch?v=c9hMFHdTs6M)"
Green_ninjas,MachineLearning,1620350367.0,[R] Easier machine learning conferences,"I know the NeurIPS deadline is coming up, but are there any easier peer-reviewed journals/conferences? I am a student and while I think my paper is of decent quality, I'm not sure it's up to the standards of NeurIPS without a faculty mentor."
remymess,MachineLearning,1619896317.0,[N] Hybrid seminar platform,"Hey friends! 👋🏽👋🏽

Hope you are doing great!

We are Alain and Remy, both PhD in Mathematics and Statistics. Together with friends, we've been building a **hybrid** online-physical platform **sculpted for academic seminars**.

While we really miss physical seminars, we find that online seminars are amazing for a myriad of reasons (time, cost, ecology, networking, ...). We do not want these perks to stop. We envision a future where seminars will consist in a hybrid mixture of in-person and online audience, and are building our platform around this vision. We aspire to leverage modern technologies in academia to:

&#x200B;

* Make the life of the seminar organiser **as easy as possible**
* Allow every academic **to open the doors of any academic seminar with a couple clicks**  — *while some talks are public, others require a registration that needs to be reviewed by an organiser*
* Offer a place for **online seminar participants to socialise** with each other before and after the event (Virtual cafeteria in the form of a dynamic gather.town)
* Provide a way for **physical and online seminar participants to interact with each other**
* Give every community from every institutions **an equal chance to shine worldwide**: only the quality of their event matters.

If our vision resonates with you, please have a look at the website [https://agora.stream](https://agora.stream/)! We have been witnessing an exponential increase in traffic since our release. Research groups and university societies from Oxford, UCL, Stanford, and all over the world are publishing content on a regular basis.

The platform's evolution is driven by feedback so feel free reach out to us if you have any question / suggestion or would just like to meet for an informal chat! [https://calendly.com/remy-mess/e-coffee](https://calendly.com/remy-mess/e-coffee) (Special thanks to many of you who engaged with us last time we posted here <3! We loved meeting you and hearing about your experiences / ideas!)

See you very soon! 💪🏽

Alain and Remy

NB1: At the moment, the **platform is completely free** (we do not want money to be a barrier for knowledge). Very soon, we will deploy premium account for users who would want to enjoy some luxury features (e.g. ability to host seminars with more than 400+ participants) or simply support us.

NB2: To not miss out the hottest seminars of the moment or to hear about our new features, follow us on Twitter or Linked! ([https://twitter.com/agorastream](https://twitter.com/agorastream) \+ [https://www.linkedin.com/company/agorastream/](https://www.linkedin.com/company/agorastream/) )

NB3: As an appetizer, here is a small selection of some fun features we got:

* **Online seminar participants can socialise** with each other before and after the event (Dynamic gather.town)
* **Integrated application form for potential future speakers** on every community page.
* **Ability to clap at the end of a seminar** by smashing your space bar! (Fully working in our dev environment, to be released soon)
* Mobile app allowing **physical and online audience to interact with each other** (Fully working in our dev environment, to be released soon)"
apatus,MachineLearning,1617707302.0,Multi-Task learning for unbalanced classes [P],"I'm currently working a project, which assigns four classes to an input. So for example a target could be:

`[0,1,1,0]`

The problem is that for each class there are a different amount of 0s and 1s in the data. In the first class there are 900 0s and 50 1s. The problem now is, that my model is just always deciding in the favor of the over represented class. This means, that for class 1 it just always guesses a number close to 0.

I can't find a solution for this problem. I read a little bit about oversampling, but it's really hard to get the distribution right, since every item can have any number of classes. I'm really not sure how to approach this problem. Does anyone have an idea how I could solve this?"
danparker276,MachineLearning,1616348052.0,"[D] Should I build my own AI bot for sms with azure ml studio, or use a 3rd party that say they understand speech better","So first off, cost isn't so much an issue and if it's then part of our IP, the better. But looking to build a concierge type bot where we take in responses and nurture leads to our live agents. There are products like verse that will offer a solution, but we already have all the sending logic and just need an API. Should we should build this ourself with a good AI/ML dev

We have 100million rows of data and either way we'll have to feed that into the system. I'm guessing whatever these 3rd parties have, with their language processing on top of azure cognitive services (luis, text processing), aws, google nlp...Probably that data processing will make more of a difference than what the 3rd party has, we would have to do that anyway. Some of the 3rd parties just work on top of those) And with the limited text for SMS, what they have in their logic isn't going to make much of a difference. They all say they have years of experience with conversation which makes them better.

Either way they all say, it will need to be live trained as well, when the bot doesn't know what to do, so I'm really wondering do these 3rd party blackboxes make much of a difference?"
bloodcarter,MachineLearning,1619885051.0,[D] Black Box Interpretations,"Hi folks. I wrote  an overview of the interpretation problem. Is there any method I didn't covered, you you think I should? Would love your advice.

[https://dasha.ai/en-us/blog/black-box-interpretations-ml](https://dasha.ai/en-us/blog/black-box-interpretations-ml)"
ottawalanguages,MachineLearning,1618956370.0,[D] Why do polynomials have a bad reputation for overfitting?,"We all must have heard by now - when we start learning about statistical models overfitting data, the first example we are often given is about ""polynomial functions"" (e.g. see the picture here: https://ardianumam.wordpress.com/2017/09/22/deriving-polynomial-regression-with-regularization-to-avoid-overfitting/) .

We are warned that although higher degree polynomials can fit training data quite well. they surely will overfit and generalize poorly to the test data.

My question is : Why does this happen? Is there any mathematical justification as to why (higher degree) polynomial functions overfit the data? The closest explanation I could find online was something called ""Runge's Phenomenon"" (https://en.wikipedia.org/wiki/Runge%27s_phenomenon ), which suggests that higher order polynomials tend to ""oscillate"" a lot - does this explain why polynomial functions are known to overfit data?

I understand that there is a whole field of ""Regularization"" that tries to fix these overfitting problems (e.g. penalization can prevent a statistical model from ""hugging"" the data too closely) - but just using mathematical intuition, why are polynomials known to overfit the data?

In general, ""functions"" (e.g. the response variable you are trying to predict using machine learning algorithms) can be approximated using older methods like fourier series, taylor series and newer methods like neural networks. I believe that there are theorems that guarantee that taylor series, polynomials and neural networks can ""arbitrarily approximate"" any function. Perhaps neural networks can promise smaller errors for simpler complexity?

But does anyone know why polynomials are said to have a bad habit of overfitting, to the extent that neural networks have largely replaced them?

Interesting paper: https://www.nber.org/system/files/working_papers/w20405/w20405.pdf"
jakedageek127,MachineLearning,1616442294.0,[P] Mars 2020 Images Sorted by Content Diversity,"Hi all,

Spun up this quick project over the weekend! The [main outreach site](https://mars.nasa.gov/mars2020/multimedia/raw-images/) for raw images from the Mars 2020 Perseverance rover is great, but it only sorts in chronological order. These missions by nature take multiple pictures of the same target, so the gallery pages can get a bit repetitive.

I made a new (very simple, webdev is not my passion :) ) gallery site that uses the DEMUD algorithm (unsupervised novelty detection) to prioritize novel and interesting images. The result is a very content-dense and content-diverse collection of images you can skim through more easily. 

http://jakehlee.com/m2020-content-diverse/#

The ""How Does This Work"" button explains the methodology, but I'll paste it below. I'm trying to update the results once in the morning and once in the evening, since images roll in throughout the day. I'll also be in the comments to answer any questions!

---

Because Perseverance takes many images of the same object by design (such as calibration targets), sorting them chronologically in a gallery results in many pages of similar content. To find all of the different objects and content in the gallery, you have to flip through hundreds of pages of thousands of images.

The DEMUD algorithm solves this problem by prioritizing the set of images with the most diverse and novel content. It quickly identifies one of every item and ranks them for you to look at. If you give DEMUD a massive orchard, it will return with one of every fruit. This can be very useful for quickly finding interesting things in a large dataset.

The DEMUD algorithm is also capable of explaining why it thought an item was interesting enough to bring to your attention. It's not implemented here, but check it out below.

**Steps**

1. Extract features from each image with ResNet-50's last fully-connected layer prior to softmax
2. Find interesting images with DEMUD
3. Display the top 500 in order of prioritization

**Resources**

- [DEMUD GitHub Repository](https://github.com/wkiri/DEMUD)
- ""Guiding Scientific Discovery with Explanations using DEMUD"", Wagstaff et al., AAAI 2013 [paper link](https://wkiri.com/research/papers/wagstaff-demud-13.pdf)
- ""Unusual ChemCam Targets Disocvered Automatically in Curiosity's First Ninety Sols in Gale Crater, Mars"", Wagstaff et al., LPSC 2014 [paper link](http://www.hou.usra.edu/meetings/lpsc2014/pdf/1575.pdf)
- ""Interpretable Discovery in Large Image Data Sets"", Wagstaff and Lee, ICML WHI 2018 [paper link](https://wkiri.com/research/papers/wagstaff-interp-18.pdf)
- ""Visualizing Image Content to Explain Novel Image Discovery"", Lee and Wagstaff, DMKD 2020 [paper site](https://jakehlee.github.io/visualize-img-disc)"
Alternative_Detail31,MachineLearning,1617110460.0,[D] How effective is explainable AI in getting rid of biases?," I was going through a few Y Combinator companies in the recruitments/manpower sector and came across this company: [https://www.hiresweet.com/](https://www.hiresweet.com/)

According to their landing page, they advertised that they could streamline the recruitment process and use explainable AI in-order to get rid of biases. What would their setup possibly look like? (Are they doing something like assigning influence percentages to parameters or something like that?)

Blog posts/additional reading material about the effectiveness of bias reduction with XAI is welcome!"
techsucker,MachineLearning,1617164953.0,[N] Researchers at MIT and Amazon Study Pervasive Label Errors in Test Sets that Destabilize Machine Learning Benchmarks,"Large labeled data sets are crucial for successful supervised machine learning (ML) across several domains such as image classification, sentiment analysis, and audio classification. However, machine learning (ML) datasets are not perfectly labeled. The processes used to develop datasets often involve automatic labeling or crowdsourcing, inherently error-prone techniques.

Prior work has majorly focused on noises in train sets of ML datasets. Not many studies concentrate on label errors in test sets, Yet they have diverse potential consequences. No study has looked at systematic error across the most-cited ML test sets. 

Benchmark test datasets are used to evaluate the ML models and validate the theoretical findings. If label errors occurred extensively, they could potentially undermine the framework by which we measure machine learning progress. Label errors in the test sets could mislead practitioners to incorrect conclusions about the model’s performance. 

Summary: [https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/](https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/) 

Paper: https://labelerrors.com/paper.pdf

Demo: https://labelerrors.com/"
GrettaGrove,MachineLearning,1619573475.0,[R] Self-Tuning Deep Reinforcement Learning,[https://arxiv.org/abs/2002.12928v2](https://arxiv.org/abs/2002.12928v2)
GabrieleValvano,MachineLearning,1618841919.0,[R] Interested in GAN and Attention? Take a look at Adversarial Attention Gates!,"Find out how adversarial conditioning of attention gates improves object segmentation!Last but not least: try our new weakly annotated dataset :)

* **Project page:** [https://vios-s.github.io/multiscale-adversarial-attention-gates…](https://t.co/uW2bxky9q3?amp=1)
* **Paper:** [https://arxiv.org/abs/2007.01152](https://t.co/8NGlbxHPq7?amp=1)
* **Code:** [https://github.com/gvalvano/multiscale-adversarial-attention-gates…](https://t.co/Sf2xcgXqCi?amp=1)
* **Dataset:** [https://vios-s.github.io/multiscale-adversarial-attention-gates/data…](https://t.co/NkavT8UGDe?amp=1)

&#x200B;

https://preview.redd.it/cagml0gq25u61.png?width=1902&format=png&auto=webp&s=82a02d27929c2f853ad155d4d53bda7a28aa9b13

**Abstract**:

Large, fine-grained image segmentation datasets, annotated at pixel-level, are difficult to obtain, particularly in medical imaging, where annotations also require expert knowledge. Weakly-supervised learning can train models by relying on weaker forms of annotation, such as scribbles. Here, we learn to segment using scribble annotations in an adversarial game. With unpaired segmentation masks, we train a multi-scale GAN to generate realistic segmentation masks at multiple resolutions, while we use scribbles to learn their correct position in the image. Central to the model’s success is a novel attention gating mechanism, which we condition with adversarial signals to act as a shape prior, resulting in better object localization at multiple scales. Subject to adversarial conditioning, the segmentor learns attention maps that are semantic, suppress the noisy activations outside the objects, and reduce the vanishing gradient problem in the deeper layers of the segmentor. We evaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical (PPSS) datasets, and we report performance levels matching those achieved by models trained with fully annotated segmentation masks. We also demonstrate extensions in a variety of settings: semi-supervised learning; combining multiple scribble sources (a crowdsourcing scenario) and multi-task learning (combining scribble and mask supervision). We release expert-made scribble annotations for the ACDC dataset, and the code used for the experiments, at [https://vios-s.github.io/multiscale-adversarial-attention-gates](https://vios-s.github.io/multiscale-adversarial-attention-gates).

&#x200B;"
sk81k,MachineLearning,1620485171.0,How big of a deal are scale free networks? [D],"I’m not sure if this is the right subreddit to post this in, but since network analysis has been up and coming in machine learning, I thought this might be the right place. I recently read Broido and Clauset’s “Scale Free Networks are Rare” paper and I was wondering how much of a difference modeling degree distributions as scale free vs non-scale-free actually matter. Is there a need for non-scale-free algorithms?"
rarboot,MachineLearning,1617027809.0,[D] Andrew Ng's data-centric vs model-centric Machine Learning,"Regarding [https://youtu.be/06-AZXmwHjo?t=1048](https://youtu.be/06-AZXmwHjo?t=1048) (the timestamp was a choice of mine). I actually referenced this video in the [MLOps](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/) post in a deeper comment thread, but I felt it deserved its own discussion.

My abridged analysis:

* Machine Learning and data science are currently model-centric - In ""project-oriented"" teams (without a backing product), I would actually rephrase it as ""experiment-centric"", in the sense that most workflows are something in the lines of ""Problem \[under\]specification"" -> ""SOTA Literature review"" -> ""Data collection"" -> ""Model\[s\] estimation"" -> ""MVP"".
* Ng advocates for a data-centric approach, which alleviates the problem underspecification issue, and shifts focus from the often fruitless model fiddling to what actually delivers value - quality data.
* It doesn't feel like an actual ""MLOps"" talk, in the sense that no actual ""project patterns"" or ""tools"" are presented.
* Given the last point, in the QA, I felt that Andrew Ng's [comments](https://youtu.be/06-AZXmwHjo?t=3182) about ""maturity"" of MLOps tools and internet tutorials, and the ""vertical"" approach advocacy are actually a veiled critiscism to the current MLOps landscape.

I was underwhelmed at first, given that it's Andrew Ng and MLOps is such a hot topic, but afterwards it really bugged me how *bad* data management can be, especially in the ""project-oriented"" scenario.

Any thoughts?"
hou_yz,MachineLearning,1620040910.0,[R] Visualizing Adapted Knowledge in Domain Transfer,"[https://arxiv.org/abs/2104.10602](https://arxiv.org/abs/2104.10602)

This paper is the first attempt at visualizing what the models learn during domain adaptation. Specifically, it is found that for the source and target networks to makes similar predictions (compensate for their knowledge difference), a target image is forced to be translated to a completely ***unseen*** source style. Such results also indicate that we can rely on *models* rather than *images* for style transfer. 

https://preview.redd.it/hg7n3hmt2ww61.png?width=1739&format=png&auto=webp&s=0aeaeb209b41400f420aa78eb7f4405e764eb599"
techsucker,MachineLearning,1619285060.0,"[N] MLCommons, AI Industry’s Performance Benchmark, Releases MLPerf™ Inference v1.0 Results To Understand The Power Usage of Machine Learning (ML) Models","Machine learning is an area that is being incorporated into almost every industry, and there has been a call for standard machine learning benchmarks similar to the SPEC benchmarks created primarily for the CPUs. These benchmarks would prove pivotal in comparing all the relative machine learning solutions available in the marketplace.

[MLCommons](https://mlcommons.org/en/) is an open engineering consortium that has been working in the direction of creating machine learning benchmarks for training and inference through its own platform MLPerf. MLCommons is usually listed as an industry-academic partnership that aims to advance the development and access of the latest AI and machine learning datasets. The benchmarks so created have been discussed and disclosed time and again to make people aware of the refinements taking place along the way. Recently, the company unveiled its platform Inference v1.0 and also released [2000 results](https://mlcommons.org/en/news/mlperf-inference-v10/) into the database. Not only this, but the company also disclosed a new power measurement technique of the platform that would look into providing additional metadata on these results.

Full Read: [https://www.marktechpost.com/2021/04/24/mlcommons-ai-industrys-performance-benchmark-releases-mlperf-inference-v1-0-results-to-understand-the-power-usage-of-machine-learning-ml-models/](https://www.marktechpost.com/2021/04/24/mlcommons-ai-industrys-performance-benchmark-releases-mlperf-inference-v1-0-results-to-understand-the-power-usage-of-machine-learning-ml-models/)"
GiuPaolo,MachineLearning,1618052104.0,[R] Last CFP for the Evolutionary RL workshop at GECCO 2021.,"Only 2 days before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)

In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.

Recent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.

Nevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.

The goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.

The topics at the heart of the workshop include:

* Evolutionary reinforcement learning
* Evolution strategies
* Population-based methods for policy search
* Neuroevolution
* Hard exploration and sparse reward problems
* Deceptive reward
* Novelty and diversity search methods
* Divergent search
* Sample-efficient direct policy search
* Intrinsic motivation, curiosity
* Building or designing behaviour characterizations
* Meta-learning, hierarchical learning
* Evolutionary AutoML
* Open-ended learning

Autors are invited to submit **new original work**, or **new perspectives on recently published work**  on those topics. Top submissions will be selected for oral presentation and be presented alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). 

&#x200B;

Important dates

* Submission deadline: **April 12, 2021**
* Notification: **April 26, 2021**
* Camera-ready: **May 3, 2021**

**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**"
CopperGenie,MachineLearning,1620269184.0,[D] What is the most human-like chatbot available to the public?,"I'm not necessarily talking about the most comprehensive bot, discussion-wise. But which bot appears to be the most consistently human-like in its responses?"
ykilcher,MachineLearning,1620314786.0,[D] Paper Explained - MLP-Mixer: An all-MLP Architecture for Vision (Full Video Analysis),"[https://youtu.be/7K4Z8RqjWIk](https://youtu.be/7K4Z8RqjWIk)

Convolutional Neural Networks have dominated computer vision for nearly 10 years, and that might finally come to an end. First, Vision Transformers (ViT) have shown remarkable performance, and now even simple MLP-based models reach competitive accuracy, as long as sufficient data is used for pre-training. This paper presents MLP-Mixer, using MLPs in a particular weight-sharing arrangement to achieve a competitive, high-throughput model and it raises some interesting questions about the nature of learning and inductive biases and their interaction with scale for future research.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:20 - MLP-Mixer Architecture

13:20 - Experimental Results

17:30 - Effects of Scale

24:30 - Learned Weights Visualization

27:25 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)"
vadimdotme,MachineLearning,1619763832.0,[R] Eindhoven Reinforcement Learning Seminar,"Hi, /r/MachineLearning! We are running a biweekly (online, for obvious reasons) [seminar on reinforcement learning at TU Eindhoven.](http://einreise.tilda.ws/) We discuss advanced topics like bayesian methods, curiosity and multi-agent RL - think [RL Theory Seminar](https://sites.google.com/view/rltheoryseminars) but with a more practical bent. For every topic we try to invite the best experts as guest speakers (DeepMind/OpenAI/Uber/Lyft/UvA/INRIA/Yandex/etc).  


[Check out our recordings on youtube](https://www.youtube.com/channel/UC7WCvhgSstO2tbbF4pJcS3w) and feel free to join us! We have an event today at 14:30 CET on distributed machine learning. No registration required, just [click to join](https://meet.jit.si/EindhovenReinforcementLearningSeminar)."
SQL_beginner,MachineLearning,1619534991.0,"[D] appeal of ""occam's razor"" in statistics and machine learning","People often refer to ""occam's razor"" in statistics, that simpler models are preferred to complex models, provided both models have similar performance. 

Why should we prefer a simpler model? Does this have to do with the variance-bias tradeoff?"
Advanced_Treat_7986,MachineLearning,1617701673.0,[Project] How I trained Spotify Podcast Speech Synthesis using Tacotron2,"\[Project\]

In this project a speech synthesis voice was trained on Spotify Podcasts with the text-to-speech deep learning model Tacotron2 (paper by Google, implemented by Nvidia). This medium article contains how the training was done, and the steps in a re-usable data pipeline to make it more scalable (which can be  re-applied to other ML projects). Code and the paper for this project can be found in the metdium article."
hardmaru,MachineLearning,1617069336.0,[R] ViViT: A Video Vision Transformer,
shinysamurzl,MachineLearning,1616671416.0,[D] Human Error Function," Is having a human as an error function a thing ? So like the Model create a result, a human gives it a score, it learns, and repeat. How would you go about implementing something like this ?"
AdelSexy,MachineLearning,1617816916.0,[D] Data imbalance and noisy labels,"I have the CV classification task with 3 classes. Dataset is imbalanced towards class 0. I also know that the labels are noisy: class 0 and 2 are being messed in ~10% of cases. 

Observation: I got better results on the test set (supposed to be clean) if I use default random sampler than in case of the weighted random sampler (samples images in batch with respect to their weight, which is opposite of class presence is the dataset. This is unexpected for me. Can noisy labels be the source of that? I am also using mixup, can affect bad on training because if noisy labels?"
MaxRek,MachineLearning,1620342257.0,"[Project], [Discussion] Do you know any time series annotation platform?","Last many months I am engaged in the generation and study of signal values ​​in time series, built on the data of the electronic document flow (electronic documents interchange) of large companies and their contractors, indicating significant drops in the indicators of companies.

I learned how to produce a couple of types of signals with this data, but there is one problem with them - frequent false positives. To combat this feature, I want to add markup. And then train the classifier...

For mark up purposes on time series data will be needed in near future annotation mechanism for my work project.

Requirements for functionality:
* Many-classes labeling
* In plot labeling or/and labeling by index/value/out of threshold
* for a large number of time series - the ability to maintain / fill several markup sessions at once
* editing marked values
* the ability to work with sub series of the last n values ​​of the time series
* tagged data statistics

This is a new area for me, and therefore I am at a crossroads - to implement this functionality myself or to look for ready-made solutions and try to implement them into the project.

Is existing something for covering the requirements?"
JosephLChu,MachineLearning,1619818568.0,[R] Work In Progress: A Drop In Replacement For Softmax With Uncertainty Calibrated Scores,"So, I've been working on an activation function that could work as a drop in replacement for softmax, with the main original intention to be to apply the Principle of Maximum Entropy as a prior that would allow the activation function to output better calibrated confidence scores or probabilities in terms of uncertainty.  It uses some formulas I came up with to convert between correlation scores (-1 to 1) and probabilities (0 to 1).  The idea is that certainty can be measured such that 1 = certain, 0 = fully unknown, and -1 = certainly not, and that there is a way to convert between this and 0 to 1 probabilities by setting 0 on the correlation scale to 1/N on the probability scale.  How this is relevant to a neural net is that in some sense, a pre-activation signal of 0 from a given node basically means that everything cancelled out, and that this maps to maximum uncertainty.  Anyways, I don't want to give away too many details given that it's unpublished.

After a lot of time and effort theorizing and then experimenting with many variants of the formula, I think I have something that works.  When I train a network with this new activation function in the output layer on for instance, MNIST, and then test it on notMNIST, the resulting outputs seem much less overconfident than an equivalent network trained with softmax or sigmoid.  However, the accuracy on MNIST itself doesn't change much.

In fact, on most tasks the new activation function gets very similar results to softmax when used in just the output layer.  So, I'm not sure how useful it actually is.

There is one other place where it might affect performance, though at this point I haven't tested this against the larger SOTA models.  I tried using it in place of softmax in the attention heads of a transformer network for language modelling.  The results seem promising, but I don't know if it'll scale or work across more tasks than the toy problem I was able to run on a single GPU.

What I'd like to know from the experienced researchers here is whether or not this project is worth trying to make into a paper and publishing.  While I think I may have something here, I'm not sure if that's my own bias of wanting all the time and effort spent to not be wasted.

Another problem I'm finding is that over the course of the time that I've worked on this project, I've accumulated many variants of the activation function, and determining which one is actually the best on all tasks and scales is proving tricky.  For a while I was concerned that I had several versions that each work situationally, but don't perform better reliably.  So in some sense I have a family of activation functions, some simpler and more elegant than others.

How does one deal with this kind of uncertainty?"
cdancette,MachineLearning,1617104591.0,[P] multimodal: a library for VQA / vision and language research,"Hi everyone, I am currently building a library for vision & language research: https://github.com/cdancette/multimodal

For now, it focuses only the Visual Question Answering (VQA) datasets (VQA v1, v2 and VQA-CP datasets), and provides the pretrained visual features (bottom-up and top-down attention), and evaluation metrics. Those features and dataset are a hassle to implement when you're starting to work on VQA, so I figured it would be nice to have a reference implementation that just works. I also implemented a model (the bottom-up model) for VQA as an example of how to use the library.

I plan to add other VQA datasets (GQA, CLEVR), more models, or other tasks like captioning.

EDIT: I just added the CLEVR dataset.

Let me know what you think about it, and what you would like to have in this library, like pretrained models, tasks, datasets.."
UBIAI,MachineLearning,1618595730.0,[D] Tutorial on how to train entity relation extraction classifier with transformers & use cases,"We recently published an [article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c) on how to do joint NER and relation extraction with transformers and are eager to hear other use cases.

Please checkout the the[ article](https://walidamamou.medium.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c), and share below your use case.

If you have any question, simply DM me and I will get back to you asap."
hardmaru,MachineLearning,1617852612.0,[R] SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,
bebbo203,MachineLearning,1616746707.0,[D] Random Network Distillation (RND) applied to robotic manipulators,"Is there some working application of this network to a manipulator?  
It seems like this algorithm is not applied to robotic anywhere. I've searched on Google Scholar and various search engines.   
Thank you all"
louloucha,MachineLearning,1618846331.0,[D] Explanation System on Time-Series Data," As part of my final year research project that I'm doing in my university laboratory. I'm working on designing an Agnostic (i.e Black Box) Explanation System (based on data mining techniques) for recommenders systems. Also, Right now, I'm only focusing on sequential data (i.e sequential purchasing behavior/ User sequential dynamics, etc).

Now, as I'm on the evaluation part my **Sequential Pattern Mining** algorithm based on a **Numerical Target**. I'm searching for some competitors that I could compare to, but AFAIK, there aren't any.

For that purpose, I'm asking you guys if you know any algorithm (baselines) that works on identifying explanation (or pattern) for Time Series data.

I've found a bunch of papers (linked below) that I'm slowly working through :

 

I've found a bunch of papers (linked below) that I'm slowly working through :

* [https://link.springer.com/chapter/10.1007%2F978-3-030-59051-2\_10](https://link.springer.com/chapter/10.1007%2F978-3-030-59051-2_10)
* [https://ieeexplore.ieee.org/document/8995349](https://ieeexplore.ieee.org/document/8995349)
* [https://arxiv.org/pdf/1810.05741.pdf](https://arxiv.org/pdf/1810.05741.pdf)
* [https://arxiv.org/ftp/arxiv/papers/2004/2004.12524.pdf](https://arxiv.org/ftp/arxiv/papers/2004/2004.12524.pdf)

Any help would be appreciated!"
Brahimce,MachineLearning,1618447831.0,[R][P] How to deal particle swarm optimization with equality constraints?,"I am facing a problem. I want to create a PSO system where each particle consists of 6 variables (real variable) and the sum of these variables should equal to one.

I generate the initial population, but how can I generate new population from the initial population, and the particles of the generated population equal to one ?"
Math_wizard369,MachineLearning,1620590419.0,[D] Annotated Research paper Walkthroughs using Jupyter Notebooks,"Hi, I just went through [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP. The post walks through the ""Attention is all you need"" paper line by line, there's also a clone of the post as a google colab where you can read along and run the code. I learned a lot going through this notebook and was wondering if anyone knew of anymore of these types of notebooks. I remember a website I found a while back that did this, but I haven't been able to find it. Any help would be greatly appreciated. Thank you."
Gletta,MachineLearning,1620281236.0,[N] Computer Vision News (with research and code!) - May 2021,"Dear all,

Have a peek at Computer Vision News of May!

Many articles about AI, Deep Learning and more...

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2021May/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2021-may-pdf/)

Dilbert on page 2. Free subscription on page 40.

Enjoy!

&#x200B;

https://preview.redd.it/4i2l82xayfx61.jpg?width=400&format=pjpg&auto=webp&s=f545c54bff9dcfd12f515cf90871e5a5c954f72e"
Yuqing7,MachineLearning,1619026057.0,[R] Pieter Abbeel Team Proposes Task-Agnostic RL Method to Auto-Tune Simulations to the Real World,"A research team from UC Berkeley and Carnegie Mellon University proposes a task-agnostic reinforcement learning method that reduces the task-specific engineering required for domain randomization of both visual and dynamics parameters.

Here is a quick read: [Pieter Abbeel Team Proposes Task-Agnostic RL Method to Auto-Tune Simulations to the Real World](https://syncedreview.com/2021/04/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-2/).

The paper Auto-Tuned Sim-to-Real Transfer is on [arXiv](https://arxiv.org/pdf/2104.07662.pdf)."
ennanco,MachineLearning,1617956035.0,"[N] Call for Papers: Special Issue ""Applied Machine Learning in NIR Technology"""," Hosting a special issue on Applied Science (JCR - 2.474) Submissions and diffusion are highly appreciated.

[https://www.mdpi.com/journal/applsci/special\_issues/NIR\_technology](https://www.mdpi.com/journal/applsci/special_issues/NIR_technology)

SPECIAL ISSUE INFORMATION

Since the 1980s, Near-Infrared Reflectance (NIR) scanners have been a common companion in laboratories. This technology allows the analysis of the electromagnetic spectrum in bands close to the visible spectrum. As a result, there is a large amount of information that can be analyzed in order to make predictions about a factor such as the quality of the sample.  This advantage of being able to quickly analyze the material without having to destroy the samples have made it so that, in recent times, this kind of equipment has been moved from the laboratory to portable devices in some cases as small as a credit card. Therefore, numerous applications have been developed under the umbrella of this technology, such as analysis of sewer water, determination of food safety, diagnosis of construction material, support on medical and vet diagnosis; the number of applications is near endless.

However, this blossom of applications usually goes along with the requirement of a model which identifies and uses the information in the captured spectra. The relationship between both factors is usually non-linear and complex, so it is necessary to use machine learning techniques to obtain it. Although, it has usually been performed through an analytical process, currently, the application of machine learning has been a step forward in terms of the precision and adjustment of the results. As NIR and machine learning are multidisciplinary technologies, many fields of science and industry could benefit from their use.

Especially, if we focus on feature selection of the bands on the spectra that contain the information required to solve the problem, the application of machine learning needs a process of adjustment to optimize the performance. The aim of this Special Issue is to present the latest advances that have been made in this field, combining both techniques, as well as to show the remaining challenges and the future in this area of research. That is why this Special Issue solicits submissions in, but not limited to, the following areas:

* Selection of features or bands of the NIR spectra, with a special interest in those applications using nano-scale NIR scanners due to its limited operational range;
* Studies of different preprocessing techniques and comparison with automatic features extraction process, such as PCA, LDA, evolutionary computation;
* Developments combining cloud computing, fog computing or edge computing with the processing of NIR spectra in different industrial processes;
* Application and comparative of the latest approaches of machine learning techniques, such as deep learning or ensemble models;
* Works with a particular interest to cover the explainable artificial intelligence in the frame of decision-making using NIR data and machine learning.

Keywords:  

signal processing; near-infrared; machine learning; artificial neural networks; support vector machines; k-nearest neighbour; Naïve Bayes; random forest; ensemble models; evolutionary computation; explainable artificial intelligence; ensemble methods; deep learning; feature extraction; Principal Component Analysis (PCA); food security; material analysis; drug identification; medical diagnosis; vet diagnosis; Internet of things (IoT)"
ml_abler,MachineLearning,1620639883.0,[D] ML and Quantum Computing,"Hi reddit,

I am not entirely sure if this is the right place to ask this but here I go.I recently got an opportunity to work in quantum computing team. The entire purpose of the team is to find applications of quantum computing in our firm. It sounds like an amazing opportunity but I am having a lot of reservations. Would love and appreciate your insights.

I come from an CS engineering background with a masters in AI/ML, my full time position at the moment is that of a data scientist working on clustering models. The quantum computing opportunity is available due to formation of a new team which has a lot of vacancies at the moment.

Although the prospects of Quantum AI/ML sounds hella interesting(although very niche), quantum computing itself seems like an extreme departure from my formal education with its heavy emphasis in physics. This is making me anxious

How viable is this opportunity for my career? How are the job prospects in the domain ( for someone with my background)? Would it help me in creating a niche for myself in data science ?I can admit shamelessly that I'd like a career which pays the big bucks at the same time keep me interested enough.

Would love your opinions"
MrAcurite,MachineLearning,1617202567.0,[D] Does anyone care about the quality of the prose in academic papers?,"I've been reading a lot of great literary works lately, and when I go back to reading academic papers, there is a very noticeable downgrade in the quality of the language employed. And I guess that that makes sense; when you want to communicate technical information, there's not a lot of room or reason to employ metaphors or colorful language.

But it still seems like there has to be some way to make these works better, to make them legitimately enjoyable to read instead of the linguistic equivalent of shoving stale bread down your throat. Obviously there is often a lot of beauty to be found when you appreciate the cleverness of new ideas being presented, but appreciating that is typically in spite of, rather than due to, the actual quality of the writing.

Is this something that anybody else here cares about?"
olegvol,MachineLearning,1617562096.0,[P] QSMM for building algorithmic neural nets,"I am working on QSMM, a framework for building algorithmic neural nets. In principle, they can produce quality adaptive behavior, do not require excessive computer resources for training and operating, and do not suffer from the problem of explainability of generated behavior.

The core method of operation is random selecting the next state of an algorithmic neural net according to probabilities of possible states calculated on the basis of cycles that occur in the neural net.

Currently, the framework has a few examples demonstrating its possible use cases, but their number shall increase in the future. One of the examples is utilizing the framework to adaptively parse a token sequence. I hope more people make use of implemented ideas to transform approaches behind the concept of artificial intelligence.

[http://qsmm.org](http://qsmm.org)"
vajra_,MachineLearning,1616699791.0,[D] Model Size calculation for sparse neural networks,"In general, how do we calculate the model size of sparse-d neural network (much of the weights are 0)? non-zero weights*4 bytes?"
deama15,MachineLearning,1616545017.0,[Project] Remastering old anime using machine learning,"So I've been doing some remastering using AI algorithms on an old anime, hajime no ippo, a boxing anime and have just finished it. The type of remastering I did was to enhance the resolution, clean up artifacts/noise, and frame interpolation. I've made my own subreddit, in there I've got more info, as well as some FAQs and samples. Check it out!

https://www.reddit.com/r/InterpolateAndEnhance/

Screenshot comparisons, first one is old, second one is new:

https://docs.google.com/presentation/d/1bT5pZIZBd2hOVYXBn1IJlFICMlqL5bhqQ_l0oN4P6hA/edit?usp=sharing

For now I'll be doing anime, the next series will be the old hunter x hunter. At some point I want to try out live action content, specifically action stuff like martial arts movies, or straight up action, but for now the interpolation algorithms aren't quite good enough, or well actually they kinda are, at least the high end ones, but good luck trying to interpolate anything with those on anything other than a RTX 3090."
machinemask,MachineLearning,1617699100.0,[D] Best system load monitoring tool?,"Hello, I'm looking for a tool that can monitor and display information such as cpu/gpu usage over time. I'm envisioning something like htop together with an interactive dashboard to filter the data on e.g. time or user. I'd like to install it on a company gpu server to get a better understanding of how it's used. We mostly use the servers for training computer vision models so that's what I'm most interested in.

Here are a few features that I think would be interesting to have.

* CPU usage. Blocking vs non-blocking
* GPU utilization and memory usage
* RAM usage
* Disk read and write
* Support for multiple machines
* Display saved data in a dashboard
* Filter displayed data on e.g. user, process, time

&#x200B;

I found some projects that do similar things. [Permon](https://github.com/bminixhofer/permon), [GPU Monitor](https://github.com/msalvaris/gpu_monitor), [NVTOP](https://github.com/Syllo/nvtop), [Weights & Biases](https://wandb.ai/site/articles/monitor-improve-gpu-usage-for-model-training)

What do you think? Is it a good idea to set up such a system? Is there software out there that I've missed?

Any experiences and suggestions are welcome!"
jhanytime,MachineLearning,1618156460.0,[D] Video - Introduction to graph neural networks (made easy!),"I'm a PhD student studying machine learning and applications in transportation systems and autonomous systems (think RL and robotics). While there are several ""GCN made easy"" videos out there on Youtube, I feel like these videos often miss the forest for the trees (especially since GCN is just 1 algorithm that was developed in 2016...) and videos often don't cover the broader historical context of how GNNs were developed and don't cover how different variations of these models allow them to model new types of systems. 

This is the second video in a series I'm making about graphs, graph neural networks, and the application areas where they have the potential to make big impacts. Please let me know what you think of the video and if you learned anything new from it!

https://youtu.be/cka4Fa4TTI4"
rish-16,MachineLearning,1620547959.0,[P] PyTorch Involution layer wrapper,"Hello everyone!

I'm currently in the process of learning how to implement papers from scratch.

Here's my implementation of the newly-introduced **Involution** layer from the paper ""Involution: Inverting the Inherence of Convolution for Visual Recognition"" by Li et al. presented at CVPR 2021.

Wrapper: [https://github.com/rish-16/involution\_pytorch](https://github.com/rish-16/involution_pytorch)

(Do forgive any implementation errors; any PRs / Issues welcome)

*Note: I'll be releasing a TensorFlow wrapper soon if time permits!*

If you like it, a ⭐️ would be greatly appreciated! It motivates me to continue building easy-to-use ML wrappers :D

Thank you!"
sim_inf,MachineLearning,1617905410.0,[D] Student Travel Grant for ACL/NAACL/EMNLP,"Is  there any ""student travel grant"" award for ACL conferences? (I mean  only ACL, NAACL, and EMNLP). If there is, how can one apply for it?

The conferences in other communities typically have this, and they receive the applications in various forms.

I can contact the chairs, or follow their websites or twitter  account... But I am posting this here to hear it from an NLP person who  knows this and perhaps has had first hand experience. Thanks."
ka-wei,MachineLearning,1618776983.0,[P] Cinemate - Movie Recommender System made with Tensorflow Rust,"For the last couple of months, I've worked on a movie recommender system using Tensorflow Rust. The model uses collaborative filtering with neural networks to recommend movies based on the movies you input. Unlike other models you will find on the internet, this model also takes a rating/popularity trade-off coefficient as input. If you prefer popular movies, this coefficient can be set to 0, but if you value a high average rating more, then the coefficient can be set to 1. The optimal number for you is probably in-between the two and the default value is \~0.7. A big goal of this website is fast recommendations - you will get recommendations in a matter of seconds due to the performance of the highly optimized prediction in Rust. 

You can visit the website here:  
[https://cinemate.me/](https://cinemate.me/)"
NoAnalyst4,MachineLearning,1620067314.0,[D] Does the ICML acceptance rate curtailment impact 2021 submissions?,"Are the new ICML acceptance rules applying to this year's papers or next year's submissions? This is my first ever submission to ICML and I am worried about my chances now that the changes are announced: [https://www.reddit.com/r/MachineLearning/comments/n243qw/d\_icml\_conference\_we\_plan\_to\_reduce\_the\_number\_of/](https://www.reddit.com/r/MachineLearning/comments/n243qw/d_icml_conference_we_plan_to_reduce_the_number_of/)

I am biased, but it seems really unfair to change the policy in the middle of a decision process."
Yuqing7,MachineLearning,1619109567.0,[R] Are Multilingual Language Models Fragile? IBM Adversarial Attack Strategies Cut MBERT QA Performance by 85%,"An IBM research team proposes four multilingual adversarial attack strategies and attacks seven languages in a zero-shot setting on large multilingual pretrained language models (e.g. MBERT), reducing average performance by up to 85.6 percent.

Here is a quick read: [Are Multilingual Language Models Fragile? IBM Adversarial Attack Strategies Cut MBERT QA Performance by 85%](https://syncedreview.com/2021/04/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-3/).

The paper *Are Multilingual BERT Models Robust? A Case Study on Adversarial Attacks for Multilingual Question Answering*  is on [arXiv](https://arxiv.org/pdf/2104.07646.pdf)."
hardmaru,MachineLearning,1617623139.0,[R] Towards General Purpose Vision Systems,
Superb-Drawer5214,MachineLearning,1617340770.0,[D] How important is the prestige of undergraduate school in getting into top ML PhD programs?," 

I've looked through most of the cv and resumes of PhD students in CMU, Berkeley, Stanford, University of Washington and MIT and most of them got their bachelor's degree from Berkeley, Stanford, MIT, CMU, Harvard, Princeton, and IIT. There were some students who graduated from other famous Ivy league schools, University of Washington, Peking, and Tsinghua. I could not view all of their cv and resumes because some of them did not upload their cv.

I am considering attending UMD but there were only three students who graduated from UMD who are doing their PhD in those schools."
byronbae,MachineLearning,1616680571.0,[P] Sound pollution mapping from GeoJSON,"I am undertaking a data science project within my job on a subject i'm very unfamiliar with. There are already some big problems such as extreme data scarcity but all of that aside I wondered if anyone could help me out with a starting point. To put it as simply as possible, I have a GeoJSON files that contain sound measurements at specific coordinates throughout cities and I would like to build a model that tries to predict the 'noisiest' points. Eventually the goal would be to include more types of related data such as real-time traffic etc etc. 

For now I have found this which seems the closest to my problem: https://omdena.com/heatmap-machine-learning/
It doesn't go into how any of these concepts were actually applied (technologies etc..) but the ideas and the type of data/outcomes is very similar to my goals.

I've played around with the data a bit already in notebooks, leaflet, arcQGIS but i'm having a bit of an issue with wrapping my head around how an entire workflow for this project could work since I want to go from mapping raw data points to then mapping key points identified through an analysis of those points.

Any insights would be greatly appreciated!"
Acrobatic-Egg-,MachineLearning,1619433901.0,[D] The Journey Of Problem-Solving Using Analytics," 

In my \~6 years of working in the analytics domain, for most of the Fortune 10 clients, across geographies, one thing I've realized is while people may solve business problems using analytics, the journey is lost somewhere. At the risk of sounding cliche, ***'Enjoy the journey, not the destination"".*** So here's my attempt at creating the problem-solving journey from what I've experienced/learned/failed at.

The framework for problem-solving using analytics is a 3 step process. On we go:

1. **Break the business problem into an analytical problem**  
Let's start this with another cliche - *"" If I had an hour to solve a problem I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions"".* This is where a lot of analysts/consultants fail. As soon as a business problem falls into their ears, they straightaway get down to solution-ing, without even a bare attempt at understanding the problem at hand. To tackle this, I (and my team) follow what we call the **CS-FS framework** (extra marks to those who can come up with a better naming).  
The CS-FS framework stands for the Current State - Future State framework.In the CS-FS framework, the first step is to identify the **Current State** of the client, where they're at currently with the problem, followed by the next step, which is to identify the **Desired Future State**, where they want to be after the solution is provided - the insights, the behaviors driven by the insight and finally the outcome driven by the behavior.  
The final, and the most important step of the CS-FS framework is **to identify the gap**, that prevents the client from moving from the Current State to the Desired Future State. This becomes your Analytical Problem, and thus the input for the next step
2. **Find the Analytical Solution to the Analytical Problem**  
Now that you have the business problem converted to an analytical problem, let's look at the data, shall we? \*\*A BIG NO!\*\*  
We will start forming hypotheses around the problem, **WITHOUT BEING BIASED BY THE DATA.** I can't stress this point enough. The process of forming hypotheses should be independent of what data you have available. The correct method to this is after forming all possible hypotheses, you should be looking at the available data, and eliminating those hypotheses for which you don't have data.  
After the hypotheses are formed, you start looking at the data, and then the usual analytical solution follows - understand the data, do some EDA, test for hypotheses, do some ML (if the problem requires it), and yada yada yada. This is the part which most analysts are good at. For example - if the problem revolves around customer churn, this is the step where you'll go ahead with your classification modeling.Let me remind you, the output for this step is just an analytical solution - a classification model for your customer churn problem.  
Most of the time, the people for whom you're solving the problem would not be technically gifted, so they won't understand the Confusion Matrix output of a classification model or the output of an AUC ROC curve. They want you to talk in a language they understand. This is where we take the final road in our journey of problem-solving - the final step
3. **Convert the Analytical Solution to a Business Solution**  
An analytical solution is for computers, a business solution is for humans. And more or less, you'll be dealing with humans who want to understand what your many weeks' worth of effort has produced. You may have just created the most efficient and accurate ML model the world has ever seen, but if the final stakeholder is unable to interpret its meaning, then the whole exercise was useless.  
This is where you will use all your story-boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state. This is where visualization skills, dashboard creation, insight generation, creation of decks come into the picture. Again, when you create dashboards or reports, keep in mind that you're telling a story, and not just laying down a beautiful colored chart on a Power BI or a Tableau dashboard. Each chart, each number on a report should be action-oriented, and part of a larger story.  
Only when someone understands your story, are they most likely going to purchase another book from you. Only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders, will they travel with you again.

With that said, I've reached my destination. I hope you all do too. I'm totally open to criticism/suggestions/improvements that I can make to this journey. Looking forward to inputs from the community!"
hardmaru,MachineLearning,1618498632.0,[R] Meta-Learning Bidirectional Update Rules. A new type of generalized neural net where neurons and synapses maintain multiple states. They show that backprop in classical neural nets can be seen as a special case of a two-state net where one state is used for activations and another for gradients.,
SimlaBurcu,MachineLearning,1619032775.0,[P] ColTraIn Hybrid Block Floating-Point (HBFP) Training Emulator,"We are excited to announce the release of the [ColTraIn HBFP Training Emulator](https://github.com/parsa-epfl/HBFPEmulator).

[HBFP](https://papers.nips.cc/paper/2018/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf) offers the **accuracy** of   32-bit floating-point with the numeric and silicon density of 8-bit   fixed-point for a wide variety of models (ResNet, WideResNet, DenseNet,   AlexNet, LSTM, and BERT). We foresee HBFP laying the foundation for   accurate training algorithms running on accelerators with an order of   magnitude denser arithmetic than conventional or novel floating-point   based platforms. The ColTraIn emulator repository includes several   example DNN models including CNNs, LSTMs and BERT for both HBFP and a   reference FP32 baseline.  
Check out the ColTraIn emulator at [https://github.com/parsa-epfl/HBFPEmulator](https://github.com/parsa-epfl/HBFPEmulator) or visit our website at [https://parsa.epfl.ch/coltrain/](https://parsa.epfl.ch/coltrain/) for more information!"
Svito-zar,MachineLearning,1617024327.0,[D] Is anyone using Automatic Bug Fixing Tools for Python?,"Many of us are making bugs. Some more often than other. I am one of those :) 

After just fixing a bug which was there for several month and screwed up many of my experiments, I wonder: can't ML find bugs for me?

Quick search on this topic resulted in many papers and even a few tools ([https://analyticsindiamag.com/5-python-bug-fixing-tools-essential-for-developers/](https://analyticsindiamag.com/5-python-bug-fixing-tools-essential-for-developers/)).

And I wonder - is anyone using those tools? Are they actually working? Can they fix copy-paste errors?"
mrgemy95,MachineLearning,1620443257.0,[D] JAX vs Pytorch gpu performance.," Does anyone know if there's a benchmark that compares JAX (or it's apis)  and pytorch in speed on gpus.

I'm aware of this comparison [https://dzone.com/articles/accelerated-automatic-differentiation-with-jax-how](https://dzone.com/articles/accelerated-automatic-differentiation-with-jax-how) but it just compare desne layes, I was looking for something that benchmark more complicated architectures."
projekt_treadstone,MachineLearning,1619738644.0,[D] Prototypical network in the medical domain,Is there any existing project work/implementation of prototypical network-based meta-learning for medical image. As medical image are heavier to load computationally than imagenet and I would like to know how they handled large pixel medical data. As when I resize a large medical image  as per standard prototypical architecture there is large drop in accuracy . With a large resize value it's not fitting in GPU to train.
hardmaru,MachineLearning,1616865606.0,[R] Out of Distribution Generalization in Machine Learning (Martin Arjovsky's PhD Thesis),
PebbleWrestler9000,MachineLearning,1617831182.0,[D] Issues with using LSTM networks to classify raw EMG signal data,"I am unsure if this is the correct subreddit to post this but I am currently having issues with an ML undergraduate research project and I was hoping to potentially get some guidance!

My current task is to take EMG signal data (time-domain electrical signal data) gathered from a user and classify what finger a person is engaging at any moment. This is a multi-class label classification problem, with 5 total labels possible. This is an example of how such an EMG signal looks like when plotted in Python:

https://preview.redd.it/gged4otfgtr61.png?width=471&format=png&auto=webp&s=faf6d9f21137d7dcbdd8dd324f2363e5722ce89b

As you can tell by that graph, I have normalized the amplitude range to the range \[0, 1\], and to ensure each time-series is of equal length, I append 0s to both the front and the end of the series to match the length of the longest sequence in the data-set. As a result, each time series is of approximately length 800, with 812 total series. The distribution of each label is approximately equal. The resulting tensor shape is  (812, 794, 1) , so 812 rows, 794 being the length of each sequence, and 1 value stored at each time-step.

I am currently using the following Keras model:

&#x200B;

https://preview.redd.it/frr0cnbygtr61.png?width=909&format=png&auto=webp&s=a727e10971f45efeade1bc2abb2a85889098dc6c

I use an LSTM network with the length of the sequences as the number of neurons, followed by a dropout, followed by a Dense layer, another dropout layer, and a final classification Dense layer. I am using an Adam optimizer algorithm, with a learning rate between 1e-2 to 1e-10. I also tweaked the L2 kernel regularizer values from the default to the range 1e-4 to 1e-7. 

My issue is I have unusually terrible performance, an example of this is: 

&#x200B;

https://preview.redd.it/781esb8phtr61.png?width=1412&format=png&auto=webp&s=cf8f0ca254e184585c74a18ecaa942a5fc48805a

The validation accuracy is consistently zero, the validation loss continuously increases as if its over-fitting, and the training accuracy plateaus around a value of about 0.2. I have run this model at 1000 epochs before, and validation accuracy nonetheless remained at 0 with the loss increasing. The training accuracy and loss continue to plateau even up to 1000 epochs.

I initially suspected overfitting could be an issue but after dealing with Dropout layers, I do not believe this is the case. Would anyone be will to chat and help me figure out what's going on with the model simply not learning?

Thank you!"
hardmaru,MachineLearning,1617937116.0,[P] Neuralink's Monkey Mindpong,"Blog post: https://neuralink.com/blog/

Their decoder, which presumably is trained with machine learning, takes in neural activity data from the monkey, and after calibration, enables the monkey to play pong directly from thought."
Jack_Hackerman,MachineLearning,1618764770.0,[D] How to normalize and merge two different datasets with similar deltas for Keras?,"Hi, I didn't get enough attention on stackoverflow, so ask here

I have two datasets to train model on. The first dataset have values within the range 1.0 - 1.1, and another one has 1000-1100 range. Essentially, datasets are very similar in a sense of percentage data change. So there is 1% data change between row1.value1 = 1.0 and row2.value1 = 1.011 in first dataset and the same is in another dataset (something like row1.value1 = 1000 and row2.value1 = 1010.09), but these raw values range and targets are different (but again - similar in percentage change).

1. How can I normalize them to make them similar to combine and not to break my network?
2. Is this possible at all?
3. Should I normalize targets? Should I normalize data that I feed into .predict function?

I am not trying to predict stocks, or something, but data is a financial data, that is needed for my job to make a regression for some bonds parameters.

&#x200B;

**UPDATE**

&#x200B;

I realized that I can normalize first and second dataset separately by just using set -= mean, set /= std, lol, but what should I do with targets? What if one target is 1 and another target is 1000? "
Yuqing7,MachineLearning,1617896959.0,[N] ContinualAI Releases Avalanche: An End-to-End Library for Continual Learning,"A research and development team from ContinualAI, including a large group of researchers from KU Leuven, ByteDance AI Lab, University of California, New York University and other institutions, proposes Avalanche, an End-to-End Library for Continual Learning based on PyTorch.

Here is a quick read: [ContinualAI Releases Avalanche: An End-to-End Library for Continual Learning](https://syncedreview.com/2021/04/08/continualai-releases-avalanche-an-end-to-end-library-for-continual-learning/)

The paper *Avalanche: an End-to-End Library for Continual Learning* is on [arXiv](https://arxiv.org/pdf/2104.00405.pdf)."
thedeepreader,MachineLearning,1620224443.0,[D] (Paper Overview) MLP-Mixer: An all-MLP Architecture for Vision," **Video**

[https://youtu.be/7FHmzEBNzro](https://youtu.be/7FHmzEBNzro)

**Paper**

[https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)

Code  
(Will be soon available by the authors)

[https://github.com/google-research/vision\_transformer](https://github.com/google-research/vision_transformer)

**Abstract**

Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. ""mixing"" the per-location features), and one with MLPs applied across patches (i.e. ""mixing"" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers."
niujin,MachineLearning,1620380538.0,[D] Predictive model for the shape of an entire histogram,"I am working on a project in market research. Survey respondents reply to a survey on a 10-point scale (1 = dislike, 5=indifferent, 10=like). The responses can be shown on a histogram.

I would like to develop an ML model to predict the shape of the histogram for a survey before it the survey is run in the wild. I have a database of past surveys by country, industry, etc. It is straightforward to predict a single value such as the mean of the histogram, but how to train a machine learning model to predict the entire histogram?

So far approaches I am considering:

* train 10 separate and independent models
* train a model for the mean and one for the stdev, and assume normal (a bad assumption I know as these histograms have many different shapes)
* design a custom machine learning architecture with 10 outputs

When I've trained a model to predict the mean of the histogram only, the best performance comes from ensemble models such as Random Forest, or using Auto ML such as Azure ML.

Has anybody tried something similar? I am new to market research but have been in data science for several years."
michaelaalcorn,MachineLearning,1619522727.0,[R] baller2vec++: A Look-Ahead Multi-Entity Transformer For Modeling Coordinated Agents,
hardmaru,MachineLearning,1619579460.0,[R] Why AI is Harder Than We Think,
donkey_strom16001,MachineLearning,1619388491.0,[D] The Rants of an experienced engineer who glimpsed into AI Academia (Briefly),"# Background

I recently graduated with a master's degree and was fortunate/unfortunate to glimpse the whole ""Academic"" side of ML. I took a thesis track in my degree because as an immigrant it's harder to get into a good research lab without having authorship in a couple of good papers  (Or so I delude myself ). 

I worked as a Full-stack SWE for a startup for 4+ years before coming to the US for a master’s degree focused on ML and AI. I did everything in those years. From project management to building fully polished S/W products to DevOps to even dabbled in ML. I did my Batchelor’s degree from a university whose name is not even worth mentioning. The university for my master’s degree is in the top 20 in the AI space.  I didn't know much about ML and the curiosity drove me to university.  

Come to uni and I focused on learning ML and AI for one 1-1.5 years after which I found advisors for a thesis topic. This is when the fun starts. I had the most amazing advisors but the entire peer review system and the way we assess ML/Science is what ticked me off. This is where the rant begins. 

# Rant 1:Acadmia follows a Gated Institutional Narrative

Let's say you are a Ph.D. at the world's top AI institution working under the best prof. You have a way higher likelihood of you getting a good Postdoc at a huge research lab vs someone's from my poor country doing a Ph.D. with a not-so-well-known advisor having published not-so-well-known papers. I come from a developing nation and I see this many times here. In my country academics don't get funding as they do at colleges in the US. One of the reasons for this is that colleges don't have such huge endowments and many academics don't have wealthy research sponsors.  Brand names and prestige carry massive weight to help get funding in US academic circles. This prestige/money percolates down to the students and the researchers who work there. Students in top colleges get a huge advantage and the circles of top researchers keep being from the same sets of institutions. I have nothing against top researchers from top institutions but due to the nature of citations and the way the money flows based on them, a vicious cycle is created where the best institutions keep getting better and the rest don't get as much of a notice. 

# Rant 2: Peer Review without Code Review in ML/AI is shady 

I am a computer scientist and I was appalled when I heard that you don't need to do code reviews for research papers. As a computer scientist and someone who actually did shit tons of actual ML in the past year, I find it absolutely garbage that code reviews are not a part of this system. I am not saying every scientist who reads a paper should review code but at least one person should for any paper's code submission. At least in ML and AI space. This is basic. I don't get why people call themselves computer scientists if they don't want to read the fucking code. If you can't then make a grad student do it. But for the collective of science, we need this.  

***The core problem lies in the fact that peer review is free. :*** There should be better solutions for this. We ended up creating Git and that changed so many lives. Academic Research needs something similar.

# Rant 3: My Idea is Novel Until I see Someone Else's Paper

The volume of scientific research is growing exponentially. Information is being created faster than we can digest.  We can't expect people to know everything and the amount of overlap in the AI/ML fields requires way better search engines than Google Scholar. 

The side effect of large volumes of research is that every paper is doing something ""novel"" making it harder to filter what the fuck was novel. 

I have had so many experiences where I coded up something and came to realize that someone else has done something symbolically similar and my work just seems like a small variant of that. That's what fucks with my head. Is what I did in Novel? What the fuck is Novel? Is stitching up a transformer to any problem with fancy embeddings and tidying it up as a research paper Novel? Is just making a transformer bigger Novel?  Is some new RL algorithm tested with 5 seeds and some fancy fucking prior and some esoteric reasoning for its success Novel? Is using an over parameterized model to get 95% accuracy on 200 sample test set Novel? Is apply Self-supervised learning for some new dataset Novel? If I keep on listing questions on novelty, I can probably write a novel asking about what the fuck is ""Novel"". 

# Rant 4: Citation Based Optimization Promotes Self Growth Over Collective Growth

Whatever people may say about collaboration, Academia intrinsically doesn't promote the right incentive structures to harbor collaboration. Let me explain, When you write a paper, the position of your name matters. If you are just a Ph.D. student and a first author to a paper, it's great. If you are an nth author Not so great. Apparently, this is a very touchy thing for academics. And lots of egos can clash around numbering and ordering of names.  I distinctly remember once attending some seminar in a lab and approaching a few students on research project ideas. The first thing that came out of the PhD student's mouth was the position in authorship. As an engineer who worked with teams in the past, this was never something I had thought about. Especially because I worked in industry, where it's always the group over the person. Academia is the reverse. Academia applauds the celebration of the individual's achievements. 

All of this is understandable but it's something I don't like. This makes PhDs stick to their lane. The way citations/research-focus calibrate the ""hire-ability"" and ""completion of Ph.D. thesis"" metrics, people are incentivized to think about themselves instead of thinking about collaborations for making something better. 

# Conclusion

A Ph.D. in its most idealistic sense for me is the pursuit of hard ideas(I am poetic that way). In a situation like now when you have to publish or perish and words on paper get passed off as science without even seeing the code that runs it, I am extremely discouraged to go down that route.  All these rants are not to diss on scientists. I did them because ""we"" as a community need better ways to addressing some of these problems.


P.S.
Never expected so many people to express their opinions about this rant. 

U shouldn’t take this seriously. As many people have stated I am an outsider with tiny experience to give a full picture.

I realize that my post as coming out as something which tries to dichotomize academia and industry. I am not trying to do that. I wanted to highlight some problems I saw for which there is no one person to blame. These issues are in my opinion a byproduct of the economics which created this system. 

Thank you for gold stranger."
RSchaeffer,MachineLearning,1619728964.0,[R] Help understanding NeurIPS 2013 Dirichlet Process Mixture Model paper,"Hi! I'm struggling to understand certain parts of the 2013 NeurIPS paper ""Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation"" ([https://proceedings.neurips.cc/paper/2013/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html](https://proceedings.neurips.cc/paper/2013/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html)). If anyone on this subreddit is familiar with Bayesian nonparametrics and has five minutes, I would really appreciate answers to the questions I posted at Math Stackexchange

[https://math.stackexchange.com/questions/4121431/online-stochastic-variational-inference-for-dirichlet-process-mixture-models](https://math.stackexchange.com/questions/4121431/online-stochastic-variational-inference-for-dirichlet-process-mixture-models)

Based on the paper's reviews,  reviewer 4 had these same questions and I think the author intended to  add answers to the supplement (the supplement starts with, ""This  document provides proofs of theorems presented in the paper"") but never actually got around to adding proofs or answers.

I've emailed the sole author but I have yet to hear back from him."
answersareallyouneed,MachineLearning,1617655429.0,[D] Questions on mathematical maturity for PhD/R&D ML,"My current job mostly involves implementing papers and adapting models to fit the needs of the company and math hasn't really been an issue in understanding/implementing papers(So far).

I believe that I lack the mathematical maturity for more competitive positions or to go back for a PhD. I've worked on research in the past, and I'm specifically interested in domain adaptation/synthetic data generation.

A few questions:

1. Is it common for people to come into this work with a ""general understanding of concepts"" and slowly build out the maturity? A lot of my mentors were math/physics majors who later came into machine learning and so perhaps my idea of ""what it takes"" are a bit skewed.

2. I have about 2 years before which I'd want to apply for/start a PhD. I plan on working full-time for those years. What's the most effective way to use this time? What do I study first?

A colleague recommended that I go through real-analysis. It also seems like there's a good portion of this subreddit who have gone through Bishop and The Elements of Statistical Learning at the very least.

TLDR: I'm interested in the field and would like to not be a mediocre mid-level professional. Any advice?"
ML_WAYR_bot,MachineLearning,1618171211.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 110,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)||

Most upvoted papers two weeks ago:

/u/rtrx3: [How Machine Learning Teams Share and Reuse Features](https://www.tecton.ai/blog/how-machine-learning-teams-share-and-reuse-features/)

/u/Justdis: [Efficient Exploration of Chemical Space with Docking and Deep-Learning](https://chemrxiv.org/articles/preprint/Efficient_Exploration_of_Chemical_Space_with_Docking_and_Deep-Learning/14153819)

/u/KirillTheMunchKing: [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery - SOTA StyleGAN image editing](https://t.me/casual_gan/18)

Besides that, there are no rules, have fun."
Farconion,MachineLearning,1620063831.0,[D] Effectively summarizing projects & research on a resume?,"does anyone have any tips for how to effectively summarize ML projects / research on a resume?

I find that since such projects & research are usually pretty niche, that trying to convey what you did is an uphill battle because most  people who read your resume aren't going to have any idea what you're  talking about - but you don't have enough space to cover the appropriate background knowledge

I get part of this is the jobs you apply to, but since I'm not banking on getting an ML position after college and a good chunk of my work is ML related - I'd like to make my background as approachable to those unfamiliar with the field / subfields related to my work"
mistermysterioyster,MachineLearning,1619897023.0,"[D] Paper Reading Group #018 - MP3: A Unified Model to Map, Perceive, Predict and Plan. (Link to full slides in comments!)",
hallavar,MachineLearning,1617297476.0,[Discussion] Any metric for evaluating non-image synthetic data ?,"Hello, evaluating generated data (obtained by AE, GAN etc..) is always a tricky question.

For images, most of the papers i've read used the Inception score or its evolution, the FiD.

However,  those metrics are based on how well a generated images can be  classified by a pretrained network (this is a summary, no need for  details here)

But if I generate  data that are not image related (like text, sequences, graphs). How can I  evaluate the generation of my model ?

A  good metric should evaluate the plausibility of the synthetic data, ie  the the realism of the generation given the training distribution; but  also its diversity : to ensure that the training distribution is well  understood by the model.

I know how to evaluate the first criterium (realism), but i have no idea how to evaluate the second

If anyone has ever worked on something like that, i will be glad to talk about his/her work.

Thank you in advance"
vijish_madhavan,MachineLearning,1618161739.0,"[P] SkinDeep, Remove Tattoos using Deep Learning. GitHub Link in comments.",
SolitaryPenman,MachineLearning,1619269805.0,[D] Looking for datasets for video time-series segmentation/labeling,"I am looking for video datasets for time-series segmentation. Note that I am NOT looking for datasets for semantic segmentation of videos but for labeling each time-step into one of K categories. An example of such a video would be where a person performs different actions (with more than one action in a single video). Unfortunately, all the datasets I could find only have a single action performed in the video (eg, Weizmann Human Action Dataset) and thus a single label per video. It would be great if someone could point me to such video dataset(s) where each time-step can have a different label."
hardmaru,MachineLearning,1617245208.0,[R] Going deeper with Image Transformers,
rarboot,MachineLearning,1618235282.0,[N] Microsoft buys AI speech tech company Nuance for $19.7 billion,"From [The Verge](https://www.theverge.com/2021/4/12/22379414/microsoft-buys-nuance-ai-speech-tech).

I may be wrong on this, but afaik it has been a while since Microsoft made such a huge acquisition of a company with an arguably heavily-convoluted internal ecosystem. It feels like MS did it for the data acquisition processes more than for the product portfolio, which IMO will be cannibalized. Any thoughts?"
mikegartrell,MachineLearning,1617282048.0,[N] Upcoming talks for Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale,"We have two upcoming talks in April for our ongoing online seminar series about Bayesian machine learning at scale.  The intended audience includes machine learning practitioners and statisticians from academia and industry.

&#x200B;

Upcoming talks, with Zoom registration links:

This coming Wednesday, 7 April

[Monte Carlo integration with repulsive point processes - Rémi Bardenet](https://criteo.zoom.us/webinar/register/WN_i_Vil4YtTpaFsH5Do_hdyg)

Monte Carlo integration is the workhorse of Bayesian inference, but the mean square error of Monte Carlo estimators decreases slowly, typically as 1/N, where N is the number of integrand evaluations. This becomes a bottleneck in Bayesian applications where evaluating the integrand can take tens of seconds, like in the life sciences, where evaluating the likelihood often requires solving a large system of differential equations. I will present two approaches to faster Monte Carlo rates using interacting particle systems. First, I will show how results from random matrix theory lead to a stochastic version of Gaussian quadrature in any dimension d, with mean square error decreasing as 1/N\^{1+1/d}. This quadrature is based on determinantal point processes, which can be argued to be the kernel machine of point processes. Second, I will show how to further take this error rate down assuming the integrand is smooth. In particular, I will give a tight error bound when the integrand belongs to any arbitrary reproducing kernel Hilbert space, using a mixture of determinantal point processes tailored to that space. This mixture is reminiscent of volume sampling, a randomized experimental design used in linear regression.

Joint work with Ayoub Belhadji, Pierre Chainais, and Adrien Hardy

&#x200B;

21 April

[Automatic Backward Filtering Forward Guiding for Markov processes and graphical models - Frank van der Meulen](https://criteo.zoom.us/webinar/register/WN_6SZkMHlfQHKL3kPalfm5GQ)

I discuss a structured way for efficient inference in probabilistic graphical models with building blocks consisting of Markovian stochastic processes. The starting point is a generative model, a forward description of the probabilistic dynamics. The information provided by observations can be backpropagated through the model to transform the generative (forward) model into a conditional model guided by the data. It approximates the actual conditional model with known likelihood-ratio between the two. The backward filter and the forward change of measure are suitable to be incorporated into a probabilistic programming context because they can be formulated as a set of transformation rules. The guided generative model can be combined with different approaches to efficiently sample latent states and parameters conditional on observations. Application settings include Markov chains with discrete state space, interacting particle systems, state space models, branching diffusions and Gamma processes.

&#x200B;

Past talks can be found [here](https://ailab.criteo.com/laplaces-demon-bayesian-machine-learning-at-scale/)."
strngelet,MachineLearning,1616679026.0,[P] boost T5 models speed up to 5x & reduce the model size by 3x using fastT5.,"I wanted to share this new library I've been working on and that I just open-sourced!. Here are some quick links :

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

[fastt5 logo](https://preview.redd.it/kno5bm8se6p61.png?width=1280&format=png&auto=webp&s=390250fe3018973b949fa8f34ba481a0b1236118)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5.` This code snippet from the repository's README gives a concise overview:

[fastt5 usage](https://preview.redd.it/glra7heve6p61.png?width=1496&format=png&auto=webp&s=ea659b124cb5290fc5e1ab4c1a1f6e9380dac0b3)

The fastT5 library exports the T5 model to onnx with `past_key_values`, then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x)."
MudlarkJack,MachineLearning,1617972696.0,[Discussion] is it possible to cross train a pre-existing model with a higher resolution data set than was used to train the original network?,"use case: 

Should have indicated in title that this is a GAN question.

For example I have previously trained a network on say 512x512 images. I want to cross train it on a completely new data set that contains 1024x1024 images to benefit from the normal time saving of cross training. Can that work or do the smaller resolutions in the original data set somehow preclude this?"
sideonion,MachineLearning,1619650542.0,[P] Are there non-profits who work on ML for social cause (fairness etc.) where I can work for a project and contribute?,"Geography  no bar -- but they should be okay with volunteers from any other  geographic location. I want to volunteer or work part time for a  non-profit who is working or researching on ML on things like fairness  and accountability. I have good tech background but I am not able to  find places where I can contribute and possibly research for social  cause.

Please don't recommend Allen AI because they only look for full time people and I can not move to USA."
regalalgorithm,MachineLearning,1620420803.0,[D] Invitation to help address AI misrepresentation and misconceptions,"TLDR: I run a site to debunk misconceptions of AI news, pls  positive response, so hope bringing it up again now that we could use more help is fine.

As I posted before, for more than 3 years I've been running this thing called [Skynet Today](http://www.skynettoday.com/) (the name is meant to be ironic/news-y), with the mission of ""Putting AI News In Perspective"", or in other words  debunk inaccurate portrayals of AI research in media and also put out articles that put things in perspective. 

As many people  here are researchers and feel annoyed at hype/misconceptions about AI, I  wonder if any of you might want to join our effort. 

We are basically a couple of grad students doing this in our spare time, and have not put out any new articles in a while due to being busy / not having much help (writing is a lot of work!). If  interested, please consider taking a look at our [contribution survey](https://www.skynettoday.com/contribute), or just message me. Thanks!

Some examples of articles we've put out include:

* [DeepMind’s AlphaFold 2—An Impressive Advance With Hyperbolic Coverage](https://www.skynettoday.com/briefs/alphafold2)
* [The State of Deepfakes in 2020](https://www.skynettoday.com/overviews/state-of-deepfakes-2020)
* [GPT-3: An AI Breakthrough, but not Coming for Your Job](https://www.skynettoday.com/briefs/gpt3)
* [IBM, Microsoft, and Amazon Halt Sales of Facial Recognition to Police, Call for Regulations](https://www.skynettoday.com/briefs/face-recog-police)
* [Boston Dynamics' robots — impressive, but far from the Terminator  ](https://www.skynettoday.com/briefs/boston-dynamics)

TLDR: I run a site to debunk misperceptions about AI, pls [join](https://www.skynettoday.com/contribute) if you wanna help"
git-commit-bt7274,MachineLearning,1617721034.0,[P] Corgi: Rust neural network/dynamic automatic differentiation library I have been working on,"Hello there, I've been working on a Rust automatic differentiation crate called 'corgi'. It's not to the level of the other crates out there, but I thought it would be worth sharing.

Github: [https://github.com/patricksongzy/corgi](https://github.com/patricksongzy/corgi)

Crate: [https://crates.io/crates/corgi](https://crates.io/crates/corgi)

Edit: Fixed example

Edit 1: added support for custom layers, and models, example [https://github.com/patricksongzy/corgi/blob/main/examples/custom.rs](https://github.com/patricksongzy/corgi/blob/main/examples/custom.rs)

Example:

* Dynamic computational graph

&#x200B;

    let a = arr![5.0];
    let b = arr![2.0];
    let mut c = arr![0.0];
    for _ in 0..10 {
        c = &c + &(&a * &b);
        if c[0] > 50.0 {
            c = &c * &a;
        }
    }
    c.backward(None);
    assert_eq!(c, arr![195300.0]);
    assert_eq!(c.gradient(), arr![1.0]);
    assert_eq!(b.gradient(), arr![97650.0]);
    assert_eq!(a.gradient(), arr![232420.0]);

&#x200B;

* Fully-connected neural network: [https://github.com/patricksongzy/corgi/blob/main/src/dense.rs](https://github.com/patricksongzy/corgi/blob/main/src/dense.rs)
* Custom operation (this needs some work) - in the README

The project still needs a lot of work, including:

* Improving the ergonomics of the \`arr!\` macro
* Implementing more than just fully-connected layers
* Efficiency improvements, and using BLAS"
WavyShapes,MachineLearning,1616431656.0,[P] Backprop: a library to easily finetune and use state-of-the-art models,"Hi everybody —

I'd like to share [Backprop](https://github.com/backprop-ai/backprop), a Python library I've been co-authoring for the last few months. Our goal is to make finetuning and using models as easy as possible, even without extensive ML experience.

We've currently got support for text and image-based tasks, with wrappers around models like Google's T5, OpenAI's CLIP, and Facebook's BART, among others.

Once you've got your training data, you can just import your model/task, and then finetune with a single line of code.

We've also got some features that make deployment for production easy, but for full transparency, deployment is through a paid platform we've developed that is by no means necessary to use the library.

If you decide to check it out, the docs are [here](https://backprop.readthedocs.io/en/latest/), and we've got some example notebooks in the repo's [examples](https://github.com/backprop-ai/backprop/tree/main/examples) folder.

We're happy with the progress we've made, but it's still early days — we'd really appreciate any thoughts and feedback we can get, so we can make Backprop more featureful and easier to use in the future."
ad1tyawagh,MachineLearning,1619612972.0,[Discussion] Why doesn't Google's Live Captioning feature generate punctuation marks?,"Posting here since it's something related to NLP.

I understand that during data cleaning, people usually remove punctuations, that's why there might not be any punctuations. But considering that punctuations hold semantic meaning, I don't understand why removing them is the general practice. 

Can someone shed some light on this?"
ilikepancakez,MachineLearning,1616419675.0,[R] Combinatorial optimization and reasoning with graph neural networks,
Red-Portal,MachineLearning,1616337731.0,[D] How many of you explicitly ask the reviewer to raise their score?,"Hi, 
Given that it's the ICML review period, I would like to ask your experience on ""explicitly asking the reviewer to raise their score."" I saw some people do this and got positive results but I'm curious if this is effective (of not negative) in general. In where I'm from, doing such a thing might be somewhat rude, as what to do with the score is totally up to the reviewer. Is there a polite phrase for asking such a thing?

Also, how many of you write something in the private feedback to the metareviewer box (even if there is no outright unjust conduct by the reviewer)?"
bendee983,MachineLearning,1618507723.0,[D] Microsoft's ML acquisition strategy,"This week, Microsoft announced the $19.7-billion acquisition of Nuance, a company that uses deep learning to transcribe clinical appointments (and other stuff). What's interesting about the deal is the [evolution of Microsoft's relation with Nuance](https://bdtechtalks.com/2021/04/15/microsoft-nuance-acquisition/), going from cloud provider to partner to owner. 

This is a successful strategy that only Microsoft (and maybe Amazon) is in a position to implement:

Step 1: Microsoft starts by investing in ML companies by giving them Azure credits and luring them into its ML platform. This allows Microsoft to help the companies develop and also learn from them (and possibly replicate their products if it's worth it). Multiple small investments as opposed to one large acquisition is a smart move because many companies are trying new things in ML/DL, few of which will be successful. With small investments, Microsoft can cast a wider net and make sure it is in a good position to make the next move.

Step 2: Microsoft enters partnership with companies that have successful products. This allows Microsoft to integrate their ML products into its enterprise solutions (e.g., Nuance's Dragon DL was integrated into Microsoft's cloud healthcare solution). Since these companies are building their ML tools on top of Azure's stack, the integration is much easier for both companies.

Step 3: Acquire really successful companies (Nuance has a great reach in the AI+healthcare sector). This allows Microsoft to gain exclusive access to the company's data, talent, technology, and clients. With the acquisition of Nuance, Microsoft's total addressable market in healthcare has reached $500B+. And it can integrate its ML technology into its other enterprise tools.

Nuance is just one example of Microsoft's ML acquisition strategy. The company is on a similar path [with OpenAI](https://bdtechtalks.com/2020/09/24/microsoft-openai-gpt-3-license/) and is carrying out [a similar strategy in the self-driving car industry](https://bdtechtalks.com/2021/01/21/microsoft-self-driving-car-strategy/)."
pinter69,MachineLearning,1616346925.0,[R] Compositional Zero-Shot Learning - Dr. Massimiliano Mancini (CVPR 2021) - Link to free zoom lecture by the author in comments,
Jemsdaan,MachineLearning,1617052129.0,[D] RTX 3080 cuda 10.0,"Hi guys,

for a uni project I need to replicate a project which uses tensorflow-gpu 1.13 and Cuda 10.0. The only GPU I have at my disposal is a RTX 3080.

Now, from my understanding, the RTX 3080 doesn't support Cuda 10.0, only Cuda 11. In order to use Cuda 11, tensorflow needs to be updated to 2.4.0, but this breaks alot of code. So my quesiton is: am I dead in the water? 

To me it seems like the only solution is to either train the models with a CPU (which has terrible perfomance) or upgrade all code so it is compatible with tensorflow 2.4.0, but this is out of scope of the project.

I'm not sure if this is the correct place to ask but if someone has a suggestion on how to get this to run on my GPU, please let me know!"
TheInsaneApp,MachineLearning,1619861540.0,[D] Types of Machine Learning Papers,
SPAMinaCanCan,MachineLearning,1618438536.0,[D] What are you thoughts on how the amount of classes can affect model accuracy,"This hopefully is a very basic question

I'm struggling to find good papers explaining how the amount of different classes affects different models 

For example, say you were building a semantic segmentation classifier for JPG images containing soft drink cans.

Your objective is to find all the cans contained within the image.

Is it better to construct your training data using a generic soft drink can class?

OR is it better to construct your training data using different colours of soft drink cans as seperate classes (i.e. red can, orange can, green can, etc...)?

I am wondering how each of these class naming conventions will affect the overall accuracy classifying every can.

What are your thoughts on this?

Also, if you want, comment on how it can extend to other things, such as cars, clothes, etc..."
AuspiciousApple,MachineLearning,1620578926.0,[D] What's the SOTA/best practice for finetuning pre-trained CNNs on smaller datasets (~10k images)? Any principled approaches? Any papers comparing different schedules?,"There's basically two ways to approach transfer learning: 

A) Treat the pretrained weights as just a particularly efficient weight initialisation method and train a model like normal, focussing on all the other hyperparams, especially data augmentation

B) Use a special protocol to finetune

For B, Francois Chollet recommends to freeze to the lower convolutional layers and only train the added head, and then optionally as a second step also finetune part or all of the lower convolutional layers with a very low learning rate.

[https://keras.io/guides/transfer\_learning/#introduction](https://keras.io/guides/transfer_learning/#introduction)

[https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

I also remember reading about other ideas, like applying a l2 penalty not to do magnitude of weights but to their deviation from the pre-trained weights.

Are these special schedules worth it? Is there any research on it?

I'm also happy for any other suggestions for finetuning."
hyunwoongko,MachineLearning,1617857667.0,[P] Try to talk with GPT3 (GPT-Neo),"&#x200B;

https://preview.redd.it/7nphdngxrvr61.png?width=2100&format=png&auto=webp&s=9e7df7710ddc52c46a66d76d150485425ac6baeb

The GPT-Neo model was released in the [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) repository by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT3 like causal language model trained on the [Pile](https://pile.eleuther.ai/) dataset.

&#x200B;

Today, I deployed a prompt-based conversation option using GPT-Neo in Openchat. You can try conversation with GPT-Neo using **only line of code.** Check here ([https://github.com/hyunwoongko/openchat](https://github.com/hyunwoongko/openchat)) if you want more detail informations. Thanks."
fromnighttilldawn,MachineLearning,1617599998.0,[D] How do you improve your model after obtaining the test error?,"I have a very basic question about training neural networks and ML models that I do not see being addressed in the literature. Consider the following scenario:

&#x200B;

* You chop your data up into train/validation/test with something like 70%, 10%, 20% ratio. 
* You train and perform validation and you feel pretty confident that your model will perform well.
* Then you run your model on the test set, you obtain a high error (or something that falls below your expectation). Yikes!
* Of course, at this stage, you would do anything to improve your model, but *you can't*: **if you try to do anything, you will be effectively using your test set as a training set.**

My question is whether there exist some best/correct practices to improve your model after obtaining the test error. 

I know in the literature there is a lot of hyperparameter tuning after obtaining the test set...I do not wish to follow in their footsteps. 

If not, are there some techniques that allow you to prevent this scenario from happening? 

Thanks in advance!"
hobogalaxy,MachineLearning,1620042624.0,"[P] General and feature-rich PyTorch/Hydra template for rapid and scalable ML research/experimentation, with a list of best practices","Hi all,

I've been looking for a way to make my research more efficient and scalable. After iterating over a couple of different frameworks and structures, I converged on the following template: [https://github.com/ashleve/lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template)

To get you a feel of how it might be useful for you, take a look at [\#Your Superpowers](https://github.com/ashleve/lightning-hydra-template#your-superpowers) section of the readme. It's based on PyTorch Lightning with Hydra and its plugins.

I developed it for my research team but I also meant it as a starting point for anyone who would like to learn about this technology stack. I find this combination simple to use but very powerful at the same time. I believe it's very convenient for small-team research, reproducing papers and generally for projects in which you need to maintain many curated configurations of your experiments.

How I find it useful:

* it allows us to painlessly scale from small experiments into hiperparameter search on multi-GPU or SLURM computing clusters (with frameworks like Optuna or Ax). The hyperparameter optimization requires minimal setup: you only need to declare config with hyperparameter ranges and hydra takes care of the whole iterating over jobs logic.
* possibility to use most experiment tracking frameworks (like neptune, wandb, MLflow or just csv files)
* easy configuration management with command line superpowers (no need for argparse thanks to Hydra)
* advanced training and debugging features from pytorch lightning (e.g. gradient accumualation, deepspeed integration, etc.)
* encapsulating datasets into lightning datamodules gives us a very convenient way for understanding and reusing datasets across projects

I feel like most ML people don't use those tools because they simply don't realize all the advantages (especially Hydra seems like a very useful addition to any deep learning project). I focused on structuring the readme in a way, which (I hope) will give you a quick overview - my hope is it can help to spread the word about those frameworks in a broaded community. It incorporates [best practices](https://github.com/ashleve/lightning-hydra-template#best-practices) and tricks I gathered over the last couple of months of playing around with it.

My typical workflow is the following:

1. I write a LightningDatamodule. I found it to be an intuitive way to encapsulate any dataset. LightningDatamodule is a simple abstraction providing methods for data download, split, transforms and exposing dataloaders. Would love to see more researchers try out this concept, even in projects which don't use pytorch lightning. Reading LightningDatamodule makes me immedietely see how the dataset is prepared, while it seems like most data science projects throw around data logic across different parts of the pipeline, making it hard to understand what's going on. You can see example of such datamodule [here](https://github.com/ashleve/lightning-hydra-template/blob/main/src/datamodules/mnist_datamodule.py)
2. I write a LightningModule. This basically just encapsulates my pytorch model code.
3. I add new experiment config specifying the paths to the LightningDatamodule and LightningModule.Now the training can be launched with some experiment tracker attached, like csv logger or tensorboard: `python` [`run.py`](https://run.py/) `experiment=simple_mnist logger=tensorboard`

Btw. the structure is partly based on data science cookie cutter project template. If you didn't hear about it I recommend you to check it out - I found it to be a great source of useful concepts for project organization:

[https://drivendata.github.io/cookiecutter-data-science/](https://drivendata.github.io/cookiecutter-data-science/)

I'd love to hear your thoughts! Let me know if you see some limitations or a room for improvement"
seuadr,MachineLearning,1617201468.0,Machine Learning and HVAC [D]," Hi all,

I am working in HVAC controls deploying a fault detection platform. The product that we have currently is rules based, which is find for many things, but, it is not so great at determining long term performance drift.

I've started a data science boot camp and am learning about machine learning with an eye towards applying this to our buildings/HVAC systems.

we'd like to use datasets to predict things ranging from a general expected energy use for a near future period of time (like, the next hour) to performance drift on sensors and other equipment (like a heating coils performance not meeting expectations for the current conditions)

i think this is all ""do-able"" but it doesn't seem to be an area that is discussed openly on the internet (or my google-fu is weak..)

so i'm not sure where to start. For instance, I've been collecting large datasets from this equipment for years and i'm pretty confident that we can gain useful insight from it, but, how to i automate the process of feeding that in and executing the model?

what kind of intervals should i consider?

can i have it execute on a short time interval, like say, 5 mins and write that out?

what kind of computing power would i need for this? - i currently have a vm with 8 cores and 32gb, i would expect that'd have some ponies to get the job done?

i know these are big, open questions - just looking for some guidance on where to start and see if you all might be aware of existing communities that could help.

Thanks in advance for your time and assistance!

Jared"
ai_painter,MachineLearning,1619460955.0,[D] Lambda GPU Cloud launches world's first RTX A6000 instances,"[Lambda launches RTX A6000 GPU Cloud Instances](https://lambdalabs.com/blog/introducing-nvidia-rtx-a6000-gpu-instances-on-lambda-cloud/)

* NVIDIA RTX A6000 instances are \~2x faster than NVIDIA RTX 6000 instances
* RTX A6000's have 48 GiB of VRAM per GPU

Disclaimer: I'm an engineer at Lambda."
Yuqing7,MachineLearning,1619192608.0,"[R] Facebook AI, McGill U & Mila Promote 'Translationese' to Boost NMT System Faithfulness","A research team from McGill University, Mila - Quebec AI Institute and Facebook AI proposes novel metrics and perturbation functions to detect, quantify and compare trade-offs between robustness and faithfulness in NMT systems, both on the corpus level and with particular examples.

Here is a quick read: [Facebook AI, McGill U & Mila Promote 'Translationese' to Boost NMT System Faithfulness.](https://syncedreview.com/2021/04/23/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-4/)

 The paper *Sometimes We Want Translationese* is on [arXiv](https://arxiv.org/pdf/2104.07623.pdf)."
jeromeharper,MachineLearning,1618965208.0,[D] Cov-19 binary classification dataset.," Hi, Guys, I am looking for a binary classification covid-19 dataset, the prediction label is positive or negative, this label could be as, whether some one is infected or not. So far I found dataset from kaggle. But most of them is consists a medical image such as XRay scan of chest. I am not looking for image data as the feature. I found one from kaggle the feature is age, gender, hypertension and etc which the label is positive / negative. Please help. Thank you in advance.. Cheers"
JasonTodd550,MachineLearning,1619729450.0,[R] I’m building the interface for our product. I’d love some feedback and insights.,"Hey folks,

I’m a frontend developer/designer working on building out an interface for our product, a platform for ML models to be hosted, sandboxed, and ran as an API. Currently our MVP is running, but we don’t have an interface for users to use. We’re looking through communities that are ML focused to try and get industry-target users to participate in our research for 1:1 validation.  Where better to look than  the sub dedicated to it.

The link to our test is [here](https://app.useberry.com/t/mK8Ywphn/), but I’m also super happy to get some one on one feedback too. If you’re interested in sending your thoughts directly to me, or even an interview with me and my partner, DM me!"
tstanislawek,MachineLearning,1617815710.0,[P] Curated List of Document Understanding (DU) Papers & Resources.,"Hi everybody,

In  the last few years, I spent a lot of time working on automating business processes of big companies and seeing rising interest in DU topic (especially in the Key Information Extraction field). Therefore, I  created a list [https://github.com/tstanislawek/awesome-document-understanding](https://github.com/tstanislawek/awesome-document-understanding) of resources to make easier to track all the papers out there which are relevant to this topic."
Yogi_DMT,MachineLearning,1618847448.0,[P] StoRM: Mutation-based hyperparameter tuner,"For those struggling to find a decent hyperparameter tuner (NN tuning for example), I have designed a tuner that attempts to remedy a lot of the issues associated with this type of parameter space (nested, categorical, conditional, etc.)

There is a runnable script in the examples folder that demonstrates StoRM's performance compared random tuning. Please feel free to post any feedback and let me know if it is useful for you.

[https://github.com/ben-arnao/StoRM](https://github.com/ben-arnao/StoRM)"
gabegabe6,MachineLearning,1618579597.0,[D] What tools can you recommend for GPU resource alocation?,"The problem is given: There is a team, and there are GPU servers with >1 GPUs. What is the best way, to signal who uses which GPUs for how long? E.g. I am thinking about a simple tool with which you can say, that from 2 days later, you'll need 5 GPUs, and you ""allocate it"" for that time. Of course, everyone in the team could see this.

I was planning to create such a simple CLI tool, but first wanted to get some feedback, on who uses what for this purpose.

(I really don't want to use excel sheets for this.)"
CauchySchwartzDaddy,MachineLearning,1619584963.0,[D] Are there light(-er) installs of pytorch for model deployment that aren't nearly a whole gb of space and just support loading a model and forward pass?,"I'm deploying some models on docker and flask and I find it kind of tedious that I have to install 750 mb of of pytorch if all I need to do is load a model and forward pass it considering most containers and deployment methods are pretty light in terms of space allotted, so I can't really afford such a huge install."
Equivalent-Choice-75,MachineLearning,1619383564.0,[D] Academia to Industry. How to deal with Research FOMO?,"I'm a Masters student at a top-10 US university. I spend almost all of my working hours doing ML research.

I'm going to industry in a few days and have FOMO about miss out on cutting edge research due to the nature of job (product facing rather than pure research). I'm sure many PhD students would've faced the same. 

How do you deal with this? Do you still read latest papers post joining? Or any other suggestions?"
CaptainMcThorn,MachineLearning,1617880707.0,[R] Hollow-tree Super: a directional and scalable approach for feature importance in boosted tree models,
numpee,MachineLearning,1617769571.0,[Discussion] Suggestions for well-written papers,"I recently encountered a post regarding the [quality of prose](https://www.reddit.com/r/MachineLearning/comments/mh7vrt/d_does_anyone_care_about_the_quality_of_the_prose/?utm_source=share&utm_medium=web2x&context=3) in academic papers.

It seemed like people generally have different views on what they consider to be well written. For example, some prefer clear, concise language without any 'colorful' language. And vice versa.

So my question is: Can you suggest some papers that you consider are very well-written? Would be nice if you gave a quick explanation of your preference in writing style as well. :)"
Headz0r,MachineLearning,1617960110.0,[D] Objective of openAIs Microscope,"Regarding the the Microscope application of openAI:

https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/44

** Question **

One can see two sets of images generated from the feature visualization. 
One is ""Channel optimization: objective results in a repeating pattern."" and the other is  ""Neuron optimization: objective shows spatial preferences."".

What optimization techniques are they refering to? How does generating these images differ for the respective classes?"
binaryfor,MachineLearning,1617468530.0,[P] Deep Daze - A simple command line tool for text to image generation using OpenAI's CLIP and Siren,
1846bdksy,MachineLearning,1617602310.0,[D] Top 4 CS PhD AI+Healthcare Research Topic Worries,"I am very grateful to have been accepted into a top 4 CS PhD program (MIT/Stanford/CMU/Berkeley) this year. My research interests were listed as AI+healthcare in my PhD application and the faculty at the school all assume I will pursue that area. However, I’ve recently started to have some hesitation about making AI+healthcare, more specifically computer vision + healthcare, my PhD research topic. The main reason is that I’m worried about post-PhD career outcomes with a topic like this. I’d really like to have the opportunity to work at Google Brain, FAIR, or potentially even quant trading companies in the future. On the other hand, I do think AI+healthcare is a promising field and may potentially also do a startup related to it. 

I’m curious, what do you all think of AI+healthcare as a PhD thesis topic? This is assuming I will still be making fundamental advances in AI, e.g. publishing in CVPR/ICCV/ECCV, but at a slightly lower frequency than a pure AI student due to additional papers in healthcare-related journals. Do you think this will hinder my career possibilities at all? Or am I overthinking this? Thank you very much. 

By the way, the alternative would be to try to pursue more “pure” AI research, without applications to healthcare."
ProbablyCloseEnough,MachineLearning,1616960156.0,[R] Slurm Interface Prototype Evaluation Survey (2 minutes),"If you have used Slurm to schedule computing jobs on shared computing resources, please evaluate the proposed interface for doing so by completing the survey in the following link.

[http://peersurvey.cc.gatech.edu/s/6b86204e847c40be9a980d612afbfa69](http://peersurvey.cc.gatech.edu/s/6b86204e847c40be9a980d612afbfa69) 

This continues my investigation from my [previous post](https://www.reddit.com/r/MachineLearning/comments/lfn7d9/r_slurm_interface_survey_2_minutes/?utm_source=share&utm_medium=web2x&context=3). I am doing this as coursework in a human computer interaction course. Thank you for participating!"
mrwafflezzz,MachineLearning,1619708471.0,[R] Question regarding sampling negative examples for supervised learning,"I'm asking this question in this sub because it's a rather difficult question. Bear with me here:

Let's say I have a match predictor. This predictor uses 2 sets of features: one set for the content (c) and one set for the user (u). It matches a user to content.

Both u and c are represented by vectors that are spatially meaningful in terms of distance between users u and distance between content c.

Let's say that I only have positive matches between u and c (label 1). Any combination that isn't a positive match is ambiguous in meaning.

Would it make sense to approach this in a semi-supervised manner by assuming any combination of u and c that I sample isn't inherently a negative example (label 0), but that it could in fact be 0 or 1?

Could I then give each sampled combination of u and c a label by looking at how similar the vector u is to the vector of other users that have consumed content c?

The idea is then that if a user is similar to other users that have consumed content c, then the likelihood of him consuming content c becomes higher.

Any feedback is welcomed"
fiddlerlabs,MachineLearning,1616372511.0,[R] A Practical Guide To Adversarial Robustness,"While adversarial machine learning is still a very young field (less than 10 years old), there’s been an explosion of papers and work around attacking such models and finding their vulnerabilities, turning into a veritable arms race between defenders and attackers.

Here is a brief summary of the field: [https://blog.fiddler.ai/2021/02/a-practical-guide-to-adversarial-robustness/](https://blog.fiddler.ai/2021/02/a-practical-guide-to-adversarial-robustness/)"
windy-city-wizard,MachineLearning,1618899153.0,[R] Two questions: computing class weights and large confusion matrix?,"1. What's an algorithm to compute class weights (to address imbalanced dataset) for multi-class classification (480 classes)?
2. What are some ways to make a large confusion matrix (480x480) easier to see? Initially, there are some obvious classes getting a lot (most) of the false positives, so that's not too bad. My guess is once I address those classes, it'll be a lot harder to extract meaningful insight from a very large confusion matrix.

https://preview.redd.it/9gaobryqs9u61.png?width=6561&format=png&auto=webp&s=e22782fad7927d9ae6f5012b1f503cc77d378e12"
minimaxir,MachineLearning,1619798940.0,[P] Easily Transform Portraits of People into AI Aberrations Using StyleCLIP,"So I've been doing a lot of experiments using [StyleCLIP](https://github.com/orpatashnik/StyleCLIP) to create some fun images. I've written a blog post with reproducible inputs for some of my _fun_ experiments involving Mark Zuckerberg + released some streamlined Colab Notebooks to get up and running.

https://minimaxir.com/2021/04/styleclip/

[Colab Notebook](https://colab.research.google.com/drive/13EJ1ATvTnE0N7I0ULLvRsta7J7HdNuBi?usp=sharing)

tl;dr StyleCLIP is essentially Photoshop driven by text, with all the good, bad, and chaos that entails."
zecharias99,MachineLearning,1617968121.0,[P] Chai: Open source framework for deploying chat AIs,"* Chai is an open-source platform which allows you to develop and deploy chat AIs. Check out our [website](https://chai.ml) and our [docs](https://chai.ml/docs), which include support for huggingface bots.
* Using [chai\_py](https://pypi.org/project/chaipy/) you can speak with your AI in just a few lines of code:

https://preview.redd.it/ucabdg9qv4s61.png?width=1364&format=png&auto=webp&s=b38821e3de68baa61e72498f05fa08e64f15605d

https://preview.redd.it/gkx2tes3w4s61.png?width=1125&format=png&auto=webp&s=582caaa0d515b7c902392d23066b41021711ac81

* Check it out! Try deploying [Facebook's Blenderbot](https://huggingface.co/facebook/blenderbot_small-90M) to chai and chatting with it in the mobile app."
RandomForests92,MachineLearning,1616527472.0,[P] I just published first version of my metrics library for ML projects,"I hope you are doing well. I just released my new open-source library - onemetric. It is useful for evaluating computer vision projects for now, but I hope it will become the default benchmarking library for all ML-related projects. I would be very grateful if you could take a look. Potentially suggest some metrics from outside Computer Vision that might be useful.

[https://github.com/SkalskiP/onemetric](https://github.com/SkalskiP/onemetric)

One Metrics Library to Rule Them All!"
bionet271,MachineLearning,1618480565.0,"[P] I implemented DeepMind's ""Perceiver"" in PyTorch","So deepmind published [this paper](https://arxiv.org/pdf/2103.03206.pdf) but I couldn't find any source code. My implementation isn't *completely* faithful (the positional encoding for example) and the paper also didn't have every detail, but it's pretty close. I just thought I'd share in case anyone wants to use it and/or help make it better. Let me know if I missed anything too, I'm still very much learning.

[https://github.com/louislva/deepmind-perceiver](https://github.com/louislva/deepmind-perceiver)"
mroc_lak,MachineLearning,1616578491.0,[D] What tools do you use for testing your computer vision systems?,"Hi all, 

I am interested in the best practices for developing computer vision applications for production. Are there tools you would recommend for testing the system? (Requirements based tests, unit tests, integrations tests, etc). Or do you have to write all the infra in-house?"
faridrashidi,MachineLearning,1619327497.0,[P] Collection of Kaggle Past Solutions (to learn ideas and techniques),"I have collected here \[1,2\] almost all available solutions and ideas with codes shared by top performers in the past Kaggle competitions. This list will gets updated as soon as a new competition finishes. It allows you to search over the Kaggle past competitions solutions and ideas.

\[1\] [https://github.com/faridrashidi/kaggle-solutions](https://github.com/faridrashidi/kaggle-solutions)

\[2\] [https://farid.one/kaggle-solutions/](https://farid.one/kaggle-solutions/)"
bert4QA,MachineLearning,1616667982.0,[R] Hurdles to Progress in Long-form Question Answering,
CKL-IT,MachineLearning,1620223941.0,"[N] 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support - John Snow Labs NLU 3.0.0","
## 200+ State of the Art Medical Models for NER, Entity Resolution, Relation Extraction, Assertion, Spark 3 and Python 3.8 support in  NLU 3.0 Release and much more
We are incredibly excited to announce the release of `NLU 3.0.0` which makes most of John Snow Labs medical healthcare model available in just 1 line of code in NLU.
These models are the most accurate in their domains and highly scalable in Spark clusters.  
In addition, `Spark 3.0.X`  and `Spark 3.1.X ` is now supported, together with Python3.8

This is enabled by the amazing [Spark NLP3.0.1](https://nlp.johnsnowlabs.com/docs/en/release_notes#300) and [Spark NLP for Healthcare 3.0.1](https://nlp.johnsnowlabs.com/docs/en/licensed_release_notes#301) releases.

# New Features
- Over 200 new models for the `healthcare` domain
- 6 new classes of models, Assertion, Sentence/Chunk Resolvers, Relation Extractors, Medical NER models, De-Identificator Models
- Spark 3.0.X and 3.1.X support
- Python 3.8 Support
- New Output level `relation`
- 1 Line to install NLU  just run `!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash`
- [Various new EMR and Databricks versions supported](https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.0.0)
- GPU Mode, more then 600% speedup by enabling GPU mode.
- Authorized mode for licensed features

## New Documentation
- [NLU for Healthcare Examples](https://nlu.johnsnowlabs.com/docs/en/examples_hc#usage-examples-of-nluload)
- [Instrunctions to authorize your environment to use Licensed features](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies)


## New Notebooks
- [Medical Named Entity Extraction (NER) notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/medical_named_entity_recognition/overview_medical_entity_recognizers.ipynb)
- [Relation extraction notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/relation_extraction/overview_relation.ipynb)
- [Entity Resolution overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/entity_resolution/entity_resolvers_overview.ipynb)
- [Assertion overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/assertion/assertion_overview.ipynb)
- [De-Identification overview notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/healthcare/de_identification/DeIdentification_model_overview.ipynb)
- [Graph NLU tutorial](https://github.com/JohnSnowLabs/nlu/blob/3.0rc1/examples/webinars_conferences_etc/graph_ai_summit/Healthcare_Graph_NLU_COVID_Tigergraph.ipynb)


## AssertionDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [assert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html) | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)                   |
| English  | [assert.biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html) | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)                   |
| English  | [assert.healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html)                   |
| English  | [assert.large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html) | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)                   |

##  New Word Embeddings

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [embed.glove.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [embeddings_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [embed.glove.biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html) | [embeddings_biovec](https://nlp.johnsnowlabs.com/2020/06/02/embeddings_biovec_en.html)                   |
| English  | [embed.glove.healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html) | [embeddings_healthcare](https://nlp.johnsnowlabs.com/2020/03/26/embeddings_healthcare_en.html)                   |
| English  | [embed.glove.healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html) | [embeddings_healthcare_100d](https://nlp.johnsnowlabs.com/2020/05/29/embeddings_healthcare_100d_en.html)                   |
| English  | en.embed.glove.icdoem | embeddings_icdoem          |
| English  | en.embed.glove.icdoem_2ng | embeddings_icdoem_2ng          |

## Sentence Entity resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | embed_sentence.biobert.mli | sbiobert_base_cased_mli          |
| English  | resolve | sbiobertresolve_cpt          |
| English  | resolve.cpt | sbiobertresolve_cpt          |
| English  | resolve.cpt.augmented | sbiobertresolve_cpt_augmented          |
| English  | resolve.cpt.procedures_augmented | sbiobertresolve_cpt_procedures_augmented          |
| English  | resolve.hcc.augmented | sbiobertresolve_hcc_augmented          |
| English  | [resolve.icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html) | [sbiobertresolve_icd10cm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10cm_en.html)                   |
| English  | [resolve.icd10cm.augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html) | [sbiobertresolve_icd10cm_augmented](https://nlp.johnsnowlabs.com/2020/12/13/sbiobertresolve_icd10cm_augmented_en.html)                   |
| English  | [resolve.icd10cm.augmented_billable](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html) | [sbiobertresolve_icd10cm_augmented_billable_hcc](https://nlp.johnsnowlabs.com/2021/02/06/sbiobertresolve_icd10cm_augmented_billable_hcc_en.html)                   |
| English  | [resolve.icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html) | [sbiobertresolve_icd10pcs](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icd10pcs_en.html)                   |
| English  | [resolve.icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html) | [sbiobertresolve_icdo](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_icdo_en.html)                   |
| English  | [resolve.rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html) | [sbiobertresolve_rxcui](https://nlp.johnsnowlabs.com/2020/12/11/sbiobertresolve_rxcui_en.html)                   |
| English  | [resolve.rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html) | [sbiobertresolve_rxnorm](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_rxnorm_en.html)                   |
| English  | [resolve.snomed](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html) | [sbiobertresolve_snomed_auxConcepts](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_en.html)                   |
| English  | [resolve.snomed.aux_concepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html) | [sbiobertresolve_snomed_auxConcepts_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_auxConcepts_int_en.html)                   |
| English  | [resolve.snomed.findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html) | [sbiobertresolve_snomed_findings](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_en.html)                   |
| English  | [resolve.snomed.findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html) | [sbiobertresolve_snomed_findings_int](https://nlp.johnsnowlabs.com/2020/11/27/sbiobertresolve_snomed_findings_int_en.html)                   |

## RelationExtractionModel

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | relation.posology | posology_re          |
| English  | [relation](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.direction](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html) | [redl_bodypart_direction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_direction_biobert_en.html)                   |
| English  | [relation.bodypart.problem](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html) | [redl_bodypart_problem_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_problem_biobert_en.html)                   |
| English  | [relation.bodypart.procedure](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html) | [redl_bodypart_procedure_test_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_bodypart_procedure_test_biobert_en.html)                   |
| English  | [relation.chemprot](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html) | [redl_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_chemprot_biobert_en.html)                   |
| English  | [relation.clinical](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html) | [redl_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_clinical_biobert_en.html)                   |
| English  | [relation.date](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls) | [redl_date_clinical_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_date_clinical_biobert_en.htmls)                   |
| English  | [relation.drug_drug_interaction](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html) | [redl_drug_drug_interaction_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_drug_drug_interaction_biobert_en.html)                   |
| English  | [relation.humen_phenotype_gene](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html) | [redl_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_human_phenotype_gene_biobert_en.html)                   |
| English  | [relation.temporal_events](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html) | [redl_temporal_events_biobert](https://nlp.johnsnowlabs.com/2021/02/04/redl_temporal_events_biobert_en.html)                   |



## NERDLModels

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|English  | [med_ner.ade.clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html) | [ner_ade_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinical_en.html)                   |
| English  | [med_ner.ade.clinical_bert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html) | [ner_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_clinicalbert_en.html)                   |
| English  | [med_ner.ade.ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html) | [ner_ade_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_ade_healthcare_en.html)                   |
| English  | [med_ner.anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html) | [ner_anatomy](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_en.html)                   |
| English  | [med_ner.anatomy.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html) | [ner_anatomy_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_anatomy_biobert_en.html)                   |
| English  | [med_ner.anatomy.coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html) | [ner_anatomy_coarse](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_en.html)                   |
| English  | [med_ner.anatomy.coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html) | [ner_anatomy_coarse_biobert](https://nlp.johnsnowlabs.com/2021/03/31/ner_anatomy_coarse_biobert_en.html)                   |
| English  | [med_ner.aspect_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html) | [ner_aspect_based_sentiment](https://nlp.johnsnowlabs.com/2021/03/31/ner_aspect_based_sentiment_en.html)                   |
| English  | [med_ner.bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html) | [ner_bacterial_species](https://nlp.johnsnowlabs.com/2021/04/01/ner_bacterial_species_en.html)                   |
| English  | [med_ner.bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html) | [ner_bionlp](https://nlp.johnsnowlabs.com/2021/03/31/ner_bionlp_en.html)                   |
| English  | [med_ner.bionlp.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html) | [ner_bionlp_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_bionlp_biobert_en.html)                   |
| English  | [med_ner.cancer](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html) | [ner_cancer_genetics](https://nlp.johnsnowlabs.com/2021/03/31/ner_cancer_genetics_en.html)                   |
| Englishs | [med_ner.cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html) | [ner_cellular](https://nlp.johnsnowlabs.com/2021/03/31/ner_cellular_en.html)                   |
| English  | [med_ner.cellular.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html) | [ner_cellular_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_cellular_biobert_en.html)                   |
| English  | [med_ner.chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html) | [ner_chemicals](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemicals_en.html)                   |
| English  | [med_ner.chemprot](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html) | [ner_chemprot_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_chemprot_biobert_en.html)           |
| English  | [med_ner.chemprot.clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html) | [ner_chemprot_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_chemprot_clinical_en.html)           |
| English  | [med_ner.clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html) | [ner_clinical](https://nlp.johnsnowlabs.com/2020/01/30/ner_clinical_en.html)           |
| English  | [med_ner.clinical.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html) | [ner_clinical_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_clinical_biobert_en.html)           |
| English  | med_ner.clinical.noncontrib | ner_clinical_noncontrib          |
| English  | [med_ner.diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html) | [ner_diseases](https://nlp.johnsnowlabs.com/2021/03/31/ner_diseases_en.html)           |
| English  | [med_ner.diseases.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html) | [ner_diseases_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_biobert_en.html)           |
| English  | [med_ner.diseases.large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html) | [ner_diseases_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_diseases_large_en.html)           |
| English  | [med_ner.drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html) | [ner_drugs](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_en.html)           |
| English  | [med_ner.drugsgreedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html) | [ner_drugs_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_greedy_en.html)           |
| English  | [med_ner.drugs.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html) | [ner_drugs_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_drugs_large_en.html)           |
| English  | [med_ner.events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html) | [ner_events_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_biobert_en.html)           |
| English  | [med_ner.events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html) | [ner_events_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_events_clinical_en.html)           |
| English  | [med_ner.events_healthcre](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html) | [ner_events_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_events_healthcare_en.html)           |
| English  | [med_ner.financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html) | [ner_financial_contract](https://nlp.johnsnowlabs.com/2021/04/01/ner_financial_contract_en.html)           |
| English  | [med_ner.healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html) | [ner_healthcare](https://nlp.johnsnowlabs.com/2021/03/31/ner_healthcare_de.html)           |
| English  | [med_ner.human_phenotype.gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html) | [ner_human_phenotype_gene_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_gene_biobert_en.html)           |
| English  | [med_ner.human_phenotype.gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html) | [ner_human_phenotype_gene_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_gene_clinical_en.html)           |
| English  | [med_ner.human_phenotype.go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html) | [ner_human_phenotype_go_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_human_phenotype_go_biobert_en.html)           |
| English  | [med_ner.human_phenotype.go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html) | [ner_human_phenotype_go_clinical](https://nlp.johnsnowlabs.com/2021/03/31/ner_human_phenotype_go_clinical_en.html)           |
| English  | [med_ner.jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html) | [ner_jsl](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_en.html)           |
| English  | [med_ner.jsl.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html) | [ner_jsl_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_biobert_en.html)           |
| English  | [med_ner.jsl.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html) | [ner_jsl_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_jsl_enriched_en.html)           |
| English  | [med_ner.jsl.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html) | [ner_jsl_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_jsl_enriched_biobert_en.html)           |
| English  | [med_ner.measurements](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html) | [ner_measurements_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_measurements_clinical_en.html)           |
| English  | [med_ner.medmentions](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html) | [ner_medmentions_coarse](https://nlp.johnsnowlabs.com/2021/04/01/ner_medmentions_coarse_en.html)           |
| English  | [med_ner.posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) | [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html)           |
| English  | [med_ner.posology.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html) | [ner_posology_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_biobert_en.html)           |
| English  | [med_ner.posology.greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html) | [ner_posology_greedy](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_greedy_en.html)           |
| English  | [med_ner.posology.healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html) | [ner_posology_healthcare](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_healthcare_en.html)           |
| English  | [med_ner.posology.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html) | [ner_posology_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_large_en.html)           |
| English  | [med_ner.posology.large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html) | [ner_posology_large_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_posology_large_biobert_en.html)           |
| English  | [med_ner.posology.small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html) | [ner_posology_small](https://nlp.johnsnowlabs.com/2021/03/31/ner_posology_small_en.html)           |
| English  | [med_ner.radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html) | [ner_radiology](https://nlp.johnsnowlabs.com/2021/03/31/ner_radiology_en.html)           |
| English  | [med_ner.radiology.wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html) | [ner_radiology_wip_clinical](https://nlp.johnsnowlabs.com/2021/04/01/ner_radiology_wip_clinical_en.html)           |
| English  | [med_ner.risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html) | [ner_risk_factors](https://nlp.johnsnowlabs.com/2021/03/31/ner_risk_factors_en.html)           |
| English  | [med_ner.risk_factors.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html) | [ner_risk_factors_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_risk_factors_biobert_en.html)           |
| English  | med_ner.i2b2 | nerdl_i2b2          |
| English  | [med_ner.tumour](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html) | [nerdl_tumour_demo](https://nlp.johnsnowlabs.com/2021/04/01/nerdl_tumour_demo_en.html)           |
| English  | med_ner.jsl.wip.clinical | jsl_ner_wip_clinical          |
| English  | [med_ner.jsl.wip.clinical.greedy](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html) | [jsl_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/03/31/jsl_ner_wip_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.modifier](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html) | [jsl_ner_wip_modifier_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_ner_wip_modifier_clinical_en.html)           |
| English  | [med_ner.jsl.wip.clinical.rd](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html) | [jsl_rd_ner_wip_greedy_clinical](https://nlp.johnsnowlabs.com/2021/04/01/jsl_rd_ner_wip_greedy_clinical_en.html)           |


## De-Identification Models

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [med_ner.deid.augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html) | [ner_deid_augmented](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_augmented_en.html)           |
| English  | [med_ner.deid.biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html) | [ner_deid_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_biobert_en.html)           |
| English  | [med_ner.deid.enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html) | [ner_deid_enriched](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_enriched_en.html)           |
| English  | [med_ner.deid.enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html) | [ner_deid_enriched_biobert](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_enriched_biobert_en.html)           |
| English  | [med_ner.deid.large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html) | [ner_deid_large](https://nlp.johnsnowlabs.com/2021/03/31/ner_deid_large_en.html)           |
| English  | [med_ner.deid.sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html) | [ner_deid_sd](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_en.html)           |
| English  | [med_ner.deid.sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html) | [ner_deid_sd_large](https://nlp.johnsnowlabs.com/2021/04/01/ner_deid_sd_large_en.html)           |
| English  | med_ner.deid | nerdl_deid          |
| English  | med_ner.deid.synthetic | ner_deid_synthetic          |
| English  | [med_ner.deid.dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html) | [ner_deidentify_dl](https://nlp.johnsnowlabs.com/2021/03/31/ner_deidentify_dl_en.html)           |
| English  | [en.de_identify](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rules | deid_rules          |
| English  | [de_identify.clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html) | [deidentify_enriched_clinical](https://nlp.johnsnowlabs.com/2021/01/29/deidentify_enriched_clinical_en.html)           |
| English  | [de_identify.large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html) | [deidentify_large](https://nlp.johnsnowlabs.com/2020/08/04/deidentify_large_en.html)           |
| English  | [de_identify.rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html) | [deidentify_rb](https://nlp.johnsnowlabs.com/2019/06/04/deidentify_rb_en.html)           |
| English  | de_identify.rb_no_regex | deidentify_rb_no_regex          |



# Chunk resolvers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | [resolve_chunk.athena_conditions](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html) | [chunkresolve_athena_conditions_healthcare](https://nlp.johnsnowlabs.com/2020/09/16/chunkresolve_athena_conditions_healthcare_en.html)           |
| English  | [resolve_chunk.cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html) | [chunkresolve_cpt_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_cpt_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html) | [chunkresolve_icd10cm_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html) | [chunkresolve_icd10cm_diseases_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_diseases_clinical_en.html)           |
| English  | resolve_chunk.icd10cm.hcc_clinical | chunkresolve_icd10cm_hcc_clinical          |
| English  | resolve_chunk.icd10cm.hcc_healthcare | chunkresolve_icd10cm_hcc_healthcare          |
| English  | [resolve_chunk.icd10cm.injuries](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html) | [chunkresolve_icd10cm_injuries_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_injuries_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.musculoskeletal](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html) | [chunkresolve_icd10cm_musculoskeletal_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_musculoskeletal_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.neoplasms](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html) | [chunkresolve_icd10cm_neoplasms_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10cm_neoplasms_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.poison](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html) | [chunkresolve_icd10cm_poison_ext_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_poison_ext_clinical_en.html)           |
| English  | [resolve_chunk.icd10cm.puerile](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html) | [chunkresolve_icd10cm_puerile_clinical](https://nlp.johnsnowlabs.com/2020/04/28/chunkresolve_icd10cm_puerile_clinical_en.html)           |
| English  | resolve_chunk.icd10pcs.clinical | chunkresolve_icd10pcs_clinical          |
| English  | [resolve_chunk.icdo.clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html) | [chunkresolve_icdo_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_icd10pcs_clinical_en.html)           |
| English  | [resolve_chunk.loinc](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html) | [chunkresolve_loinc_clinical](https://nlp.johnsnowlabs.com/2021/04/02/chunkresolve_loinc_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.cd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html) | [chunkresolve_rxnorm_cd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_cd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.in | chunkresolve_rxnorm_in_clinical          |
| English  | resolve_chunk.rxnorm.in_healthcare | chunkresolve_rxnorm_in_healthcare          |
| English  | [resolve_chunk.rxnorm.sbd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html) | [chunkresolve_rxnorm_sbd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_sbd_clinical_en.html)           |
| English  | [resolve_chunk.rxnorm.scd](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html) | [chunkresolve_rxnorm_scd_clinical](https://nlp.johnsnowlabs.com/2020/07/27/chunkresolve_rxnorm_scd_clinical_en.html)           |
| English  | resolve_chunk.rxnorm.scdc | chunkresolve_rxnorm_scdc_clinical          |
| English  | resolve_chunk.rxnorm.scdc_healthcare | chunkresolve_rxnorm_scdc_healthcare          |
| English  | [resolve_chunk.rxnorm.xsmall.clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html) | [chunkresolve_rxnorm_xsmall_clinical](https://nlp.johnsnowlabs.com/2020/06/24/chunkresolve_rxnorm_xsmall_clinical_en.html)           |
| English  | [resolve_chunk.snomed.findings](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html) | [chunkresolve_snomed_findings_clinical](https://nlp.johnsnowlabs.com/2020/06/20/chunkresolve_snomed_findings_clinical_en.html)           |


# New Classifiers

| Language | nlu.load() reference                                         | Spark NLP Model reference          |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| English  | classify.icd10.clinical | classifier_icd10cm_hcc_clinical          |
| English  | classify.icd10.healthcare | classifier_icd10cm_hcc_healthcare          |
| English  | [classify.ade.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html) | [classifierdl_ade_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_biobert_en.html)           |
| English  | [classify.ade.clinical](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html) | [classifierdl_ade_clinicalbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_clinicalbert_en.html)           |
| English  | [classify.ade.conversational](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html) | [classifierdl_ade_conversational_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_ade_conversational_biobert_en.html)           |
| English  | [classify.gender.biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html) | [classifierdl_gender_biobert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_biobert_en.html)           |
| English  | [classify.gender.sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html) | [classifierdl_gender_sbert](https://nlp.johnsnowlabs.com/2021/01/21/classifierdl_gender_sbert_en.html)           |
| English  | classify.pico | classifierdl_pico_biobert          |


# German Medical models

| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed]    | w2v_cc_300d|
| [embed.w2v]    | w2v_cc_300d|
| [resolve_chunk]    | chunkresolve_ICD10GM|
| [resolve_chunk.icd10gm]    | chunkresolve_ICD10GM|
| resolve_chunk.icd10gm.2021    | chunkresolve_ICD10GM_2021|
| med_ner.legal   | ner_legal|
| med_ner    | ner_healthcare|
| med_ner.healthcare    | ner_healthcare|
| med_ner.healthcare_slim    | ner_healthcare_slim|
| med_ner.traffic    | ner_traffic|

# Spanish Medical models
| nlu.load() reference                                         | Spark NLP Model reference          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [embed.scielo.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html) | [embeddings_scielo_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_150d_es.html)| 
| [embed.scielo.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)   | [embeddings_scielo_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_300d_es.html)| 
| [embed.scielo.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)  | [embeddings_scielo_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielo_50d_es.html)| 
| [embed.scielowiki.150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)   | [embeddings_scielowiki_150d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_150d_es.html)| 
| [embed.scielowiki.300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)   | [embeddings_scielowiki_300d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_300d_es.html)| 
| [embed.scielowiki.50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)   | [embeddings_scielowiki_50d](https://nlp.johnsnowlabs.com/2020/05/26/embeddings_scielowiki_50d_es.html)| 
| [embed.sciwiki.150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)   | [embeddings_sciwiki_150d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_150d_es.html)| 
| [embed.sciwiki.300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)   | [embeddings_sciwiki_300d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_300d_es.html)| 
| [embed.sciwiki.50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)   | [embeddings_sciwiki_50d](https://nlp.johnsnowlabs.com/2020/05/27/embeddings_sciwiki_50d_es.html)| 
| [med_ner](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)   |  [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 
| [med_ner.neoplasm](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)  | [ner_neoplasms](https://nlp.johnsnowlabs.com/2021/03/31/ner_neoplasms_es.html)| 
| [med_ner.diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)  | [ner_diag_proc](https://nlp.johnsnowlabs.com/2021/03/31/ner_diag_proc_es.html)| 

# GPU Mode
You can now enable NLU GPU mode by setting `gpu=true` while loading a model. I.e. `nlu.load('train.sentiment' gpu=True)` . If must resart you kernel, if you already loaded a nlu pipeline withouth GPU mode.

# Output Level Relation
This new output level is used for relation extractors and will give you 1 row per relation extracted.


# Bug fixes
- Fixed a bug that caused loading NLU models in offline mode not to work in some occasions


# 1 line Install NLU
```!wget https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh -O - | bash```

# Install via PIP 
```! pip install nlu pyspark==3.0.1```


## Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)"
sci-genie,MachineLearning,1619920075.0,[D] The Topics I Would Choose From If I Ever Did A Ph.D. in AI/ML Within the Next 2 Years. What would be Yours?,
cathie_burry,MachineLearning,1617639889.0,Why are correct AI medical diagnoses seemingly so hard to achieve? [D],"There are a lot of people working on this, a lot of projects invested in this, and a lot of really smart people trying to solve this problem.

The studies I have read have over fitting problems, and people working on them run into a lot of regulatory issues. 

But at the end of the day, it seems like it would be less complicated to deal with some aspects of medicine than some of the other things people have built AI for like AlphaZero.

The roadblocks I hear about don’t add up to the impasse I see, why can’t I go to an AI clinic?"
windy-city-wizard,MachineLearning,1619974865.0,[D] [R] Evaluation set for large number of imbalanced classes?,"I'm working on a classification problem with upwards of 500 classes. I have hundreds of observations for most classes. The top 10% more common classes have tens of thousands of observations, and the bottom 10% have dozens of observations.

My evaluation set currently has 10% of the observations for each class, capped at (arbitrarily) 250. Seems like a sensible setup to me. My metric of interest is top 3 accuracy.

Is there a better way to evaluate my model?"
shreyansh26,MachineLearning,1619962439.0,[P] GPT-1 - Annotated Paper + Paper Summary,"GPT-2 and recently, GPT-3 created a lot of hype when they were launched. However, it all started with the ""Improving Language Understanding by Generative Pre-Training"" paper which introduced the idea of GPT-1. 

As a part of my Paper Notes series, I have gone through the paper and created a brief yet informative summary of the paper. It will take just take a few minutes to understand GPT-1 well. Check out the links below and happy reading!

Paper Summary -   [Improving Language Understanding by Generative Pre-Training](https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf)"
hwbs20,MachineLearning,1619257344.0,An RTX 3070 for protoyping [D],"Is an RTX 3070 (with i9 processor and 16 GB RAM) enough for prototyping deep learning models? My area of work is generative Modeling and the use cases are :

1. to just check if the model/code actually runs and maybe run it on a tiny dataset to make sure things are correct. Once this is done, I will put it up on a cluster to scale up my experiments.
2. Maybe reproduce some models that don’t demand that much compute. (Even in this case i can just check if the code “works” and then push it to the cluster)
3. To train basic GANs, VAEs etc to understand them (main thing is to get results very fast and not wait like an hour)

Or should I wait and go for RTX 3080 with similar other specs? The main problem is that the 3080 is a bit out of my budget and the availability is in august (the ones available right now are way too out of my budget), whereas I want something in this week. Thanks in advance"
thunder_jaxx,MachineLearning,1619812725.0,[D] Unpopular Opinion: Conferences Should Mandate a Limitations Section For Any Paper Introducing some New Model / Method / Variant,"The title says it all and I mean a specific limitations section where researchers convey solid limitations of the new methods they propose. I feel such information is very crucial for disseminating good science in the age of AI and ML.  Limitations can be problem-dependent and help ground the claims of a research paper. The review process can help with making this section better. 

Why? few reasons: 

1. If not explicitly stated, we have to go look it up inside a paper and it may not be found. Many times these are hidden as footnotes or concluding remarks which require the cognitive load to read and process. Stating them reduces the cognitive load to think about such stuff because it's covered as a part of good science. 
2. Makes us stick to doing good science over papers becoming a source of benchmark porn. 
3. Keeps clickbaity titles in check. AI/ML Researchers have brought into the advertizing model of clickbaity titles. I am not against clickbaity titles as many times the title has actually been[ really good and spot on](https://arxiv.org/abs/2104.06644)! But at least for the fairness of science, we should have limitations to ensure that there is a boundary between science and salesmanship. 

I am very aware it puts a lot of load on researchers and seldom people like to do it. But it's healthy cozy it makes us care more about what we learn from the research over what we are selling as research. What are your thoughts?"
Signal-Ad-8598,MachineLearning,1620398804.0,[D] How is tfjs-node performance in comparison with Python version?,"Hi, I've come across some posts showing that tfjs-node is faster than python tensorflow as it's multi-threaded. However the posts were from 3 years ago. Is it still true? Why is tfjs-node not so popular? Should I train models on Node.js, as I'm more familiar with JS syntax than Python?"
__Julia,MachineLearning,1616942750.0,[D] Is it valuable to have a patent in our industry?,"Hello,

I work as a research engineer in R&D, I have heard polarized opinions about ""having/working on patents"" in my small circle, and I would like to hear opinions of other people in the industry. Is it valuable to have ""patents"" in someone's resume?. How is it comparable to having a paper in ML conference?.

The general public's perception of patents is different than reality. In many cases, it is harder to write a paper and make it to top-tier conferences than to write a patent. However, it can show that the author is able to  think out of the box, enhance existing products, and design systems/solutions.

Another argument I have heard is about ""gate-keeping science"", it can be seen as a blocker for others as well. 

&#x200B;

What is the opinion of this community?"
Yuqing7,MachineLearning,1619541221.0,"[R] Microsoft & Peking U Researchers Identify 'Knowledge Neurons' in Pretrained Transformers, Enabling Fact Editing","A research team from Microsoft Research and Peking University peeps into pretrained transformers and investigates how factual knowledge is stored, proposing a method to identify “knowledge neurons,” which can be utilized to explicitly update and erase facts.

Here is a quick read: [Microsoft & Peking U Researchers Identify 'Knowledge Neurons' in Pretrained Transformers, Enabling Fact Editing.](https://syncedreview.com/2021/04/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-6/)

The paper *Knowledge Neurons in Pretrained Transformers* is on [arXiv](https://arxiv.org/pdf/2104.08696.pdf)."
dojoteef,MachineLearning,1617280471.0,[N] John Carmack solves AGI!,"After a few short years of solitary research in AI, legendary programmer John Carmack has just recently unveiled an AGI named Rick, which he created by combining the flexibility of GANs with the expressive power of Predictability Minimization!

I knew he had a penchant for physically building machines, but the resulting quality of the facial expressions and smoothness of motion is uncanny! It helps that he recorded the video in 4K at 60fps. Check it out [here](https://www.youtube.com/watch?v=bxqLsrlakK8)!"
rockwilly,MachineLearning,1619313536.0,[Project] - I made a fun little political leaning predictor for Reddit comments for my dissertation project,
NeedMoreTime4Things,MachineLearning,1617309596.0,[D] Keeping up with research - Poll,"Hi there, 

After talking to some people I realized that most of us have different ways of keeping up with current developments in ML. 

As I am trying to dive into new fields before my PhD, I’d like to know how you manage to stay on top of research.

Thanks in advance!

[View Poll](https://www.reddit.com/poll/mi4hrm)"
KirillTheMunchKing,MachineLearning,1620229627.0,[D] StyleGAN2 Distillation for Feed-forward Image Manipulation. How to gender swap Harry-Potter and edit other images explained!,"# [StyleGAN2 Distillation for Feed-forward Image Manipulation](https://t.me/casual_gan/34)

In this paper from October, 2020 the authors propose a pipeline to discover semantic editing directions in StyleGAN in an unsupervised way, gather a paired synthetic dataset using these directions, and use it to train a light Image2Image model that can perform one specific edit (add a smile, change hair color, etc) on any new image with a single forward pass. If you are not familiar with this paper, check out the [5 minute summary](https://t.me/casual_gan/34).

&#x200B;

[Samples from the model](https://preview.redd.it/3cohkadiobx61.png?width=1280&format=png&auto=webp&s=969a6f2c5e523a4b12d09cc59c5abcc5abaaa6ad)

\[[Arxiv](https://arxiv.org/abs/2003.03581)\]\[[paper explanained in 5 minutes](https://t.me/casual_gan/34)\]"
KirillTheMunchKing,MachineLearning,1619016127.0,[R] Training Generative Adversarial Networks with Limited Data,"# [Training Generative Adversarial Networks with Limited Data](https://t.me/casual_gan/28)

The authors propose а novel method to train a StyleGAN on a small dataset (few thousand images) without overfitting. They achieve high visual quality of generated images by introducing a set of adaptive discriminator augmentations that stabilize training with limited data. More details [here](https://t.me/casual_gan/28).

&#x200B;

[StyleGAN-ada](https://preview.redd.it/02tu3te5gju61.png?width=1280&format=png&auto=webp&s=883df8de3e976f6576bb38d35cc96597f9b0377a)

In case you are not familiar with the paper, read it [here](https://t.me/casual_gan/28)."
ancientmooner,MachineLearning,1618412945.0,[P] Code and pretrained models for Swin Transformer are released (SOTA models on COCO and ADE20K),"Image classification and pretrained models: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)

Object detection on COCO: [https://github.com/SwinTransformer/Swin-Transformer-Object-Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection) 

Semantic segmentation on ADE20K: [https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation)"
Exotic-Photograph-37,MachineLearning,1617994173.0,[P] Low Computation GAN? (Noobie to GAN),"Hello all!

I am a researcher doing a study on GAN content. We wanted to find different ways to manipulate someone's face and map them to doing actions (i.e. dance or sing). It seems like most of the DeepFake libraries we used require a lot of computation time. Are there any libraries that have a low computation time? We were thinking something like [Wombo.ai](https://Wombo.ai), which doesn't take too long (though I don't know if its because they have super powerful servers they connect to). We can't use [Wombo.ai](https://Wombo.ai) directly because it would be a privacy issue as that would require communication with a third party. Does anyone have any tips on libraries that we could use?

Thank you!"
brainggear,MachineLearning,1617276285.0,[R] On the Origin of Species of Self-Supervised Learning,
Realistic_Sea_3634,MachineLearning,1620485299.0,"[D] Why has machine learning become such a toxic field, know-it-all field?","I've worked with many scientists from many different fields and backgrounds, but none come close to the obnoxiousness, pomposity, and outright unpalatable know-it-all vibe from the machine learning community. And I'm sure it's a case of a small rotten bunch smearing the whole field.

This behaviour is most rife in 3 places: the cesspit known as Twitter, Reddit, and somewhat in industry. It's much less rampant, comparatively, in academia in my experience (and just so we're clear, Google Brain/DeepMind/FAIR is not academia). Here is some of what I've observed:

* Think they can dominate a field or little involvement from SMEs
   * Often I see in machine learning a group or groups will swarm on a problem, throw ML at the data, and call the problem ""solved"". Very little (if any) SME involved, and importantly, no follow up. The exceptions to this general are few and far between.
* DL encourages a habit of not learning the basics
   * I have encountered this so often, and it is especially apparent in DL. People will jump straight into e.g. CV or NLP, and not bother learning anything foundational. I've seen and spoken to  numerous people publishing in CV papers in prestigious conferences who don't even know what why colour spaces are useful or even what a pixel is (because it sure as fuck isn't a small square on an image). You may claim that they don't need to know this, but that delusional talk, and they absolutely do. There is a limit to what compute + CNN/transformer can do. After that, you need foundations to know how to improve.
* No real work goes into the vast majority of papers (more of a DL problem)
   * This has been cited by many in the past. However, I must articulate it myself. I also understand that there are many contributing factors to this (and indeed all the other) problem. It's most often slight architecture change or incremental improvement, with no real thought gone into the paper. What this sometimes results in (I have seen numerous times myself in a variety of settings - including my own teams) is a PhD being no good at engineering, or less productive at research than an MSc with experience under his belt. The whole point of a PhD coming into an ML team is to be useful at R&D, and that is not always the case (as much as you'd expect).
* Insolence and arrogance of fairness/ethics crowd
   * This crowd - as it currently operates - simply serves as a cancerous tumour to the ML world. There always point out problems, but never (real) solutions. They act like gatekeepers and god's gift to the world. It brings about massive toxicity to the virtual ML community, and prohibits free speech of the community without fear of repercussions. The this crowd could do with an overhaul, as its leaders are some of the most vitriolic yobos who claim to be academics.
* For such an applied field, very little focus on applications
   * Often the excuse put forward is that fields like math have very few applications straight away. Firstly, ML is not like math - it's more like straight up engineering (especially DL). It's primarily applied, and should thus be much more focused on application. Your slight architecture change or you 0.5% improvement on ImageNet is not a Pythagoreon theorem waiting to happen. It's laziness and just wanting to get your PhD (surely if doing a PhD you actually want to make a real contribution to the field that you can stand behind?). Fields like physics and stats are often applied and make a real world impact with the applications. To be fair, ML also does, but nowhere near as much as it should, especially at your average company in industry.
* Can fix all the worlds problems
   * Over-stating the ability of ML. Not sure if delusion, PR, or a combination of both. Cited lots before by others.

&#x200B;

EDIT 1:I don't know why people are focusing on the definition of a pixel. I reiterate that is the ""small square"" model is helpful, useful, and enough for your work/application, then that's great and you should use that. But don't claim your helpful model to be the definition - because it's not. I don't understand why I'm being downvoted for this :/

Thanks for the discussion. I want to understand the Reddit ML community's views on the original content of this post (minus the pixels).

&#x200B;

EDIT2:  
Realised it would be better to put this here, than in the comment as I did:  


Regarding the ethics crowd - I see many people have commented and require me to clarify my position.

I  feel they intentionally try to make mountains out of molehills, and  often (at least from an online perspective) do not want to engage in  civil discourse or debate when presented to them (instead wanting to  just blame and live in their echo chamber). They act like saviours and  pretentious, as if they are doing god's work. The purpose of  ethics/fairness is to try change things to be more ethics/fair in your  community. It's not to perpetually live in your little echo chamber and  try ""cancel"" those who disagree. Utter toxicity.

Never  mind the fact that many of the popular ""researchers"" in that space  preach against toxic large tech companies/CEOs, unfair practices at top  universities and companies, etc., but they are the first to be working  at places like Google, DeepMind, Microsoft, and attending universities  like Stanford and CMU. If you really want to make a difference to the  plight of POC in tech, then why are you working for DeepMind? Go work  for, or consult for, some little random African start up then to empower  them? Fucking hypocritical, will do whatever is convenient and easy.  Simply a popularity contest.

Again,  I found myself venting as I am quite fervent about this. However, with  them it's so often such an apparent display of self-entitlement and  superiority. Granted this is simply from my observations mostly online,  but also somewhat in-person.  
"
rsree123,MachineLearning,1617678542.0,How to overcome Impostor Syndrome [D],Even  after working in applied ML for close to 15 years I sometimes I get a feeling that I don’t know anything. I would not be able to recall things at the top of my mind but a quick look helps. I would not even remember the details of some fundamental concept. Is this the case with everyone or are you able to retain everything in your mind. People just look at my profile and have high expectations. Any tips to overcome this?
aselsiriwardena,MachineLearning,1619173423.0,[P] Pytorch Load Balance and Scalability,"I need some ideas to execute a test on a PyTorch image generative model for load balancing and scalability?  
Are there any specific tools for that? Do I need to execute a parallel process?"
hardmaru,MachineLearning,1620428651.0,[R] Computer-Aided Design as Language,
__data_science__,MachineLearning,1616513807.0,[D] Advanced Takeaways from fast.ai book,"I recently read the Fast AI deep learning [book](https://www.goodreads.com/book/show/50204643-deep-learning-for-coders-with-fastai-and-pytorch) and wanted to summarise some of the many advanced takeaways & tricks I got from it.  I’m going to leave out the basic things because there’s enough posts about them, i’m just focusing on what I found new or special in the book.

I’ve also put the insights into a [deck](https://saveall.ai/shared/deck/140&4&3K3uXPazkg4) on save all to help you remember them over the long-term. I would **massively recommend using a spaced repetition app (video** [**explanation**](https://youtu.be/AD0aFdRCskQ)**) like anki or** [**save all**](https://saveall.ai/) **for the things you learn** otherwise you’ll just forget so much of what is important. Here’s the takeaways:

# Neural Network Training Fundamentals

* Always **start** an ML project by **producing simple baselines**
   * If is binary classification then could even be as simple as predicting the most common class in the training dataset
   * Other baselines: linear regression, random forest, boosting etc…
* Then you can **use your baseline to clean your data** by looking at the datapoints it gets most incorrect and checking to see if they are actually classified correctly in the data
* In general you can also **leverage your baselines** to **help debug** your models
   * e.g. if you make your neural network 1 layer then it should be able to match the performance of a linear regression baseline, if it doesn’t then you have a bug!
   * e.g. if adding a feature improves the performance of linear regression then it should probably also improve the performance of your neural net unless you have a bug!
* Hyperparameter optimisation can help a bit (especially for the learning rate) but in general there are default hyperparameters that can do quite well and so **closely** **optimising the hyperparameters should be one of the last things you try** rather than the first
* **If you know something** about the problem then try to **inject it as an inductive bias into the training process**
   * e.g. if some of your features are related in a sequential way then incorporate them into training separately using an RNN
   * e.g. if you know the output should only be between -3 and 3 then use sigmoid to design the final layer so that it forces the output of the network to be in this range

# Transfer Learning

* Always use transfer learning if you can by finding a model pre-trained for a similar task and then fine-tune that model for your particular task
   * e.g. see [huggingface](http://huggingface.co/) for help with this in NLP
* **Gradual unfreezing** and **discriminative learning rates** work well when fine-tuning a transfer learned model
   * **Gradual unfreezing** = freeze earlier layers and **train the later layers only**, then **gradually unfreeze** the earlier layers one by one
   * **Discriminative learning rates** = having **different learning rates per layer of your network** (usually **earlier** **layers** have **smaller learning rates** than later layers)

# Tricks to Deal with Overfitting

* **Best way** to deal with **overfitting** is by getting **more data**. **Exhaust this first** before you start regularising with other methods
* **Data augmentation** is really powerful and now possible with text as well as images:
   * **Image** data augmentation -  crop, pad, squish and resize images
   * **Text** data augmentation - negate words, replace words with similes, perturb word embeddings (nice github [repo](https://github.com/QData/TextAttack) for this)
* **Mixup regularisation** = create new data by averaging together training datapoints
* **Backwards training (NLP only):** train an additional separate model that is **fed text backwards** and then **average the outputs** of your two models to get your final prediction

# Other Tricks to Improve Performance

* **Test time augmentation** = at test time, use the **average prediction** from many **augmented versions of the input** as your prediction rather than just the prediction from the true input
* **1 cycle training** = when you increase and reduce the learning rate throughout training in a circular fashion (usually makes a **huge difference)**
* **Learning rate finder algorithm** = algorithm that Fast AI provide to help you automatically discover roughly the best learning rate
* **Never use one-hot encodings,** use **embeddings** instead, even in **tabular data**!
* Using **AdamW** instead of **Adam** can help a little bit
* **Lower precision training** can help and on [pytorch lightning](https://github.com/PyTorchLightning/pytorch-lightning) is just a simple flag you can set
* For **regression problems** if you know the **output should be within a range** then its good to use **sigmoid** to force the neural net output to be within this range
   * I.e. make the network output:  min\_value + sigmoid(output) \* (max\_value - min\_value)
* **Clustering** your features can help you **identify which ones are the most redundant** and then removing the can help performance
* **Label smoothing** = use 0.1 and 0.9 instead of 0 and 1 for label targets (can smoothen training)
* **Don’t dichotomise** your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem
* **Progressive resizing** = train model on smaller resolution images first, then increase resolution gradually (can speed up training a lot)
* Strategically using **bottleneck layers** to force the network to form **more compact representations of the data** at different points can be helpful
* Try using **skip connections** as they can help smooth out the loss surface

&#x200B;

Please let me know if you found this helpful and if there are any other training tricks you use that we should also know about?"
Yuqing7,MachineLearning,1620061055.0,"[R] CMU, UT Austin & Facebook’s CNN Layer Width Optimization Strategies Achieve 320x Overhead Reduction","Researchers from Carnegie Mellon University, the University of Texas at Austin and Facebook AI propose a novel paradigm to optimize widths for each CNN layer. The method is compatible across various width optimization algorithms and networks and achieves up to a 320x reduction in width optimization overhead without compromising top-1 accuracy on ImageNet.

Here is a quick read: [CMU, UT Austin & Facebook’s CNN Layer Width Optimization Strategies Achieve 320x Overhead Reduction](https://syncedreview.com/2021/05/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-10/).

The paper *Width Transfer: On the (In)Variance of Width Optimization* is on [arXiv](https://arxiv.org/pdf/2104.13255.pdf)."
dontreallyknowmuch,MachineLearning,1618540777.0,[R] GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,"Here's some new work from NVIDIA, recently showcased in the GTC 2021 keynote. It can convert user-created Minecraft block worlds into view-consistent realistic-looking worlds! The method learns to perform this translation in the absence of paired Minecraft-real data, by using a GAN pretrained network to generate ""*pseudo-ground truths*"".

tweet: [https://twitter.com/arunmallya/status/1382860338584952840](https://twitter.com/arunmallya/status/1382860338584952840)

arxiv: [https://arxiv.org/abs/2104.07659](https://arxiv.org/abs/2104.07659)

webpage: [https://nvlabs.github.io/GANcraft/](https://nvlabs.github.io/GANcraft/)

Sample outputs:

https://reddit.com/link/mru35h/video/i51arzkyfkt61/player

https://reddit.com/link/mru35h/video/njql7gc2gkt61/player

https://reddit.com/link/mru35h/video/2rqe3ge5gkt61/player

It can even change the style of the output world!

&#x200B;

https://reddit.com/link/mru35h/video/srg38qw7gkt61/player"
mroc_lak,MachineLearning,1616427593.0,[D] How do you test your machine learning models for healthcare to gain confidence before embarking on a clinical trial?,"Hi all, 

I am looking into ML applications in healthcare and am interested in understanding how companies prepare for clinical trials. Which metrics correlate well with clinical trial performance and what are important pitfalls to avoid?"
timscarfe,MachineLearning,1616537282.0,[D] Meta-Gradients in Reinforcement Learning with Tomas Zahavy (DeepMind) and Robert Lange (Video),"Dr. Tom Zahavy, a Research Scientist at DeepMind thinks that reinforcement learning is the most general learning framework that we have today, and in his opinion it could lead to artificial general intelligence. He thinks there are no tasks which could not be solved by simply maximising a reward. 

Back in 2012 when Tom was an undergraduate, before the deep learning revolution he attended an online lecture on how CNNs automatically discover representations. This was an epiphany for Tom. He decided in that very moment that he was going to become an ML researcher.  Tom's view is that the ability to recognise patterns and discover structure is the most important aspect of intelligence. This has been his quest ever since. He is particularly focused on using diversity preservation and metagradients to discover this structure. 

In this discussion we dive deep into meta gradients in reinforcement learning. 

Video: [https://youtu.be/hfaZwgk\_iS0](https://youtu.be/hfaZwgk_iS0)

Pod: [https://anchor.fm/machinelearningstreettalk/episodes/49---Meta-Gradients-in-RL---Dr--Tomas-Zahavy-DeepMind-etbcr9](https://anchor.fm/machinelearningstreettalk/episodes/49---Meta-Gradients-in-RL---Dr--Tomas-Zahavy-DeepMind-etbcr9)"
freshprinceofuk,MachineLearning,1619968135.0,[D] Best CPU real time pose estimation model available?,"Hi,

I've looked into quite a few pose estimation models (both CPU/GPU and 2D and 3D) but I've not seen anything as impressive as Kemtai (kemtai.com - in browser CPU, real time). Can anyone suggest what model this may be?"
Rat-a-ouchie,MachineLearning,1619213594.0,"[D] PhD Applied Maths - Machine Learning, supervisor selection should be one you get along with or something else is more important?","Hey guys,
I'm doing my PhD in Applied Mathematics and love machine learning. 
I want to specialise in a Machine learning based thesis amd I was wondering if anyone of you have any advice for PhD Supervisor selection?
I'm not sure if this is the right place to ask but any advice is appreciated.

In summary,
What do you look for in a supervisor?
How do you find the right research topic for you?"
jj4646,MachineLearning,1619155260.0,"[D] is this the equivalent of the ""what came first, the chicken or the egg"" in machine learning?","https://en.wikipedia.org/wiki/Probably_approximately_correct_learning

I am learning about this concept in statistical learning called ""Probably Approximately Correct"" (PAC) - although the wording of this can seem complicated and technical, (if I understand correctly) the essence of PAC is to show that if a ""target concept"" (e.g. the set of all possible input points corresponding to a certain output), then the error of a machine learning algorithm can be probabilistically bounded with a certain range. I think all this is intended to show, that a machine learning algorithm is useful for making predictions, instead of basing your predictors off which color socks your neighbor is wearing. 

1) PAC framework was developed in 1984 - yet prior to this, many statistical models were being used for making predictions (e.g. regression models). Once PAC was developed, did researchers have to examine all statistical models they were using prior to this, and confirm if these models were compatible with PAC framework?

2) Now, the same question for modern models. When newer machine learning models are developed (e.g. LSTM models, developed long after PAC framework was established) - are these new algorithms tested to make sure they are compatible with PAC framework?

3) Can someone please confirm if my understanding of PAC framework is correct? (source: https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean , https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/ )  A ""hypothesis"" (in the context of PAC) seems to be a general term for a machine learning algorithm, the ""hypothesis space"" is the space of all possible such algorithms (e.g. a linear regression model with specific beta parameters is a individual hypothesis and all possible linear regression models are the hypothesis space), ""D"" is the distribution of the data, and a ""Concept Class"" - but Im still confused about the differences between target concept and concept class. Can someone please clarify this?"
__data_science__,MachineLearning,1617707256.0,[D] Training strategy given Double Descent phenomenon,"How does [double descent](https://openai.com/blog/deep-double-descent/) change the ideal training strategy? 

Before I used to follow this broad training strategy:

1. Make the network big enough until it is overfitting on the training data
2. Then regularise to reduce overfitting

&#x200B;

But the possibility of double descent means this might not be correct anymore, so I was wondering how you guys think about it?"
anianruoss,MachineLearning,1617294131.0,[R] Robustness Certification for Point Cloud Models,"We present the first robustness certifier against semantic transformations (e.g., rotation or shearing) on 3D point cloud models for object classification and part segmentation tasks.

Paper: [https://arxiv.org/abs/2103.16652](https://arxiv.org/abs/2103.16652)

Code: [https://github.com/eth-sri/3dcertify](https://github.com/eth-sri/3dcertify)

**Abstract.** The use of deep 3D point cloud models in safety-critical applications, such as autonomous driving, dictates the need to certify the robustness of these models to semantic transformations. This is technically challenging as it requires a scalable verifier tailored to point cloud models that handles a wide range of semantic 3D transformations. In this work, we address this challenge and introduce 3DCertify, the first verifier able to certify robustness of point cloud models. 3DCertify is based on two key insights: (i) a generic relaxation based on first-order Taylor approximations, applicable to any differentiable transformation, and (ii) a precise relaxation for global feature pooling, which is more complex than pointwise activations (e.g., ReLU or sigmoid) but commonly employed in point cloud models. We demonstrate the effectiveness of 3DCertify by performing an extensive evaluation on a wide range of 3D transformations (e.g., rotation, twisting) for both classification and part segmentation tasks. For example, we can certify robustness against rotations by ±60° for 95.7% of point clouds, and our max pool relaxation increases certification by up to 15.6%."
adammathias,MachineLearning,1619016230.0,"[N] Aim 2.3.0 is out with system resource monitoring, ""reverse grouping"" and more","Highlights with my comments:

### System resource monitoring
An option for automatic tracking of your **GPU**, **CPU**, **memory** and more.  I'm curious how much this covers - disk, network...?

### ""Reverse grouping""
This is what Aim is calling the option to divide by everything but one param (typically seed).  It looks like you pick that variable in the UI for all experiments, it's not clear if you can set a variable as ""default indivisible"" via the API.

### Line chart smoothing
This is self-explanatory.  Maybe be somewhat automatic by default, based on the number of points and scale?

### Standard error and standard deviation
New built-in aggregation modes in addition to min and max

### Support for infinite values and NaN



---

[Full list of release issues and feature requests](https://github.com/aimhubio/aim/milestone/3?closed=1)

[Announcement](https://medium.com/aimstack/aim-v2-3-0-system-resource-usage-and-reverse-grouping-6900dd04a1ff) from Gev"
CanadianTuero,MachineLearning,1617011308.0,[P] Differentiable Optimizers with Perturbations in PyTorch,"After reading [this](https://www.reddit.com/r/MachineLearning/comments/mcdoxs/p_torchsort_fast_differentiable_sorting_and/) post the other day, I learned about using perturbations to create differentiable optimizers ([https://arxiv.org/abs/2002.08676](https://arxiv.org/abs/2002.08676)). There is an official implementation [here](https://github.com/google-research/google-research/tree/master/perturbations), but it is for Tensorflow. 

Since I primarily use PyTorch, and these tools are something which I want to play around with for my own research, I have reimplemented it using native PyTorch. Figured I would share here in case others find it useful as well!

[https://github.com/tuero/perturbations-differential-pytorch](https://github.com/tuero/perturbations-differential-pytorch)"
BotPoetsSociety,MachineLearning,1618053966.0,[P] AI Poetry,"We create poems combining AI models. The poems were generated by a GPT2 model fine-tuned for poetry.

We choose to do no editing at all to the generated poetry. We think there is some fun in reading raw poetry coming from a machine, even with the obvious flaws.

&#x200B;

[Photo: http:\/\/unsplash.com; Voices: Text-to-speech AWS and Azure](https://reddit.com/link/mo2xrk/video/qfmfy3fbzbs61/player)

Youtube channel: [https://www.youtube.com/channel/UChyDkl0l6W4VebsaKSW2sDw](https://www.youtube.com/channel/UChyDkl0l6W4VebsaKSW2sDw)

Twitter: [https://twitter.com/BotPoetsSociety](https://twitter.com/BotPoetsSociety)"
JoshN1986,MachineLearning,1616957813.0,[P] scite: a smart citation index that displays the context of citations and classifies their intent using deep learning,
bendee983,MachineLearning,1617897743.0,[D] Waymo now has a machine learning PhD as its co-CEO,"In 2015, Google hired John Krafcik, a veteran of the automotive industry, to lead its self-driving car efforts, which later spun off as Waymo. Last week, Krafcik stepped down and ceded his role to Dimitry Dolgov, a computer science PhD and a veteran in ML research, and Tekedra Mawakana, a Doctor of Law. 

Why is this important? At the time Krafcik joined Google, the general belief was that deep learning was mature enough for SDCs and reaching production-level SDCs was just a matter of scaling road-testing, gathering enough training data, and training DL models.

But it has become evident that in its current state, DL is not ready to tackle the many challenges of open roads, and many more gaps need to be filled. The legal infrastructure for SDCs is also not ready and many questions remain unanswered.

This is why it makes sense to put an ML engineer and a lawyer at the helm of the company. Deep learning has come a long way in pushing SDCs forward, but a bumpy road still lies ahead.

Read the full analysis here:

[https://bdtechtalks.com/2021/04/08/waymo-ceo-reshuffling-self-driving-car-industry/](https://bdtechtalks.com/2021/04/08/waymo-ceo-reshuffling-self-driving-car-industry/)"
bryant1410,MachineLearning,1619129609.0,[N] Competition on classifying tweets as jokes + more,"Computational Humor task on tweets with 4 subtasks:

* Binary classification of intended humor/non-intended humor
* Funniness scoring.
* Humor Mechanism classification
* Humor Target classification

[https://competitions.codalab.org/competitions/30090](https://competitions.codalab.org/competitions/30090)

It's in Spanish but I think it's not a big blocker if you don't speak it!

The papers from the teams (describing the systems) will be published in the IberLEF Workshop @ SEPLN 2021, Málaga, Spain (or virtual...)."
VinayUPrabhu,MachineLearning,1617308083.0,[P] A small dataset of mis-parsed citations from Google scholar,"TL-DR:  Google scholar's parser is  \*aggressive\* in indexing publications  & \*sometimes\* indexes high-school cafeteria and restaurant menus as papers. The not-so-nice aspect is that there are a lot of agro-journals from the global south that meet the same fate as well.  


1. Dataset: [https://github.com/vinayprabhu/Revenge\_of\_the\_pith\_Sigbovik2021/tree/main/data](https://github.com/vinayprabhu/Revenge_of_the_pith_Sigbovik2021/tree/main/data)
2. Awareness raising paper authored in Sigbovik-humor style: [https://github.com/vinayprabhu/Revenge\_of\_the\_pith\_Sigbovik2021/blob/main/sigbovik\_plants\_2021\_camera\_readyish.pdf](https://github.com/vinayprabhu/Revenge_of_the_pith_Sigbovik2021/blob/main/sigbovik_plants_2021_camera_readyish.pdf)  


  


https://preview.redd.it/vzu35iqpdmq61.jpg?width=741&format=pjpg&auto=webp&s=6abae4633303358e28674615464d08670e671628"
Symbiot10000,MachineLearning,1616967325.0,"[D] Papers on intelligent agents for search (not voice, Alexa, etc.)","I'm looking into writing something on intelligent agents that can be used to search internet resources on regular platforms (i.e. mobile and desktop, not headless platforms like Alexa and other AI voice assistants) on specific topics without the user directly interacting with search engines.

For various reasons, it's incredibly difficult to Google, or to drill down to any papers on the subject, if there are any. The signal-to-noise ratio has beaten me.

I'm not thinking of GPT-3-style generalized oracles that have infinite scope, that abstract a great deal, and that hide their sources; nor some AI project to scrape and replace Google; but perhaps, rather, a machine learning system that has been tasked with a specific domain, such as blastoma research, or compression, which has a specific audience in mind, and which has a greater level of discernment and judgement regarding the 'quality' of sources than the average search engine.

If anyone has a link or two that would point me in the right direction, I'd be grateful."
jj4646,MachineLearning,1619580096.0,[D] do machine learning models handle multicollinearity better than traditional models (e.g. linear regression)?,"When it comes to older and traditional models like linear regression, ensuring that the variables did not have multicollinearity was very important. Multicollinearity greatly harms the prediction ability of a model.

However, older and traditional models were meant to be used on smaller datasets, with fewer rows and fewer colums compared to modern big data. Intuitively, it is easier to identify and correct multicollinearity in smaller datasets (e.g. variable transformations, removing variables through stepwise selection, etc.)

In machine learning models with big data - is multicollinearity as big a problem? 

E.g. are models like randon forest known to sustain a strong performance in the presence of multicollinearity? If so, what makes random forest immune to multicollinearity?

Are neural networks and deep neural networks abke to deal with multicollinearity ? If so, what makes neural networks immune to multicollinearity?

Thanks"
jj4646,MachineLearning,1619072556.0,"[D] Competetive ""Rule Based"" Machine Learning Models","https://en.m.wikipedia.org/wiki/Association_rule_learning

Has anyone ever seen more advanced association rules models that involve machine learning architecture? What is the most advanced machine learning models that can provide a complete set of rules and interpretations when making predictions? Does something like this exist?"
kpang0,MachineLearning,1617823197.0,[P] Vald: a highly scalable distributed fast approximate nearest neighbour dense vector search engine.,"Hi

I've recently released V1 of the Vald, a Cloud-Native distributed fast approximate nearest neighbour dense vector search engine running on Kubernetes as an OSS project under Apache2.0 licence.

It is already running behind Yahoo Japan's image search and some recommendation engine and is also running behind the Japanese National Digital Library Digital Archive retrieval engine.

By using machine learning to convert unstructured data (audio, images, videos, user characteristics, etc.) into vectors and then using Vald to perform vector search on those vectors, it will be possible to operate as a faster and more complex search engine.

&#x200B;

Vald is still a very new project, but we are looking for a lot of feedback from many users.

Please come and visit our site!

&#x200B;

Web: [https://vald.vdaas.org](https://vald.vdaas.org)

GitHub: [https://github.com/vdaas/vald](https://github.com/vdaas/vald)"
SQL_beginner,MachineLearning,1620156717.0,"[D] ""Classifier Technology and the Illusion of Progress"" (2006, Hand)","https://arxiv.org/abs/math/0606441

I found this interesting paper over here, where the author argues that more complex algorithms (e.g. deep neural networks) do not always have significant advantages over simpler algorithms in the real world (hence the illusion). The author brings up many reasons as to why this can happen - some of the reasons are related to mathematics, others are related to experimental design. 

(Note: the author brings up a point here that I am not sure why this is true: ""Conversely, in
the two-class case, although few real data sets have
exactly linear decision surfaces, it is common to find
that the centroids of the predictor variable distributions
of the classes are different, so that a simple linear surface can do surprisingly well as an estimate of the true decision surface"" ...  

Why is it common  to find that the centroids of the predictor variable distributions are different? Why does this allow for a linear surface to estimate the true surface well?)

Here were the thoughts I had after reading this paper: This was paper was published in 2006, before the ""deep learning revolution"" (e.g. in 2012 when Convolution Neural Networks clearly outperformed humans at the imagenet competition). Is it possible that the results from this paper are somewhat irrelevant and outdated? Researchers, universities and companies (e.g. google, Facebook, Microsoft) have probably spent a billion dollars since 2006 on developing more and more complex machine learning models. Using common sense, many of these models have performed well enough so that more research will be done in the future. I agree that for certain problems, perhaps simpler models (e.g. linear regression, decision trees) can perform just as well as deep learning models ... but surely, there are many problems in the real world which require more complex models? Can an argument made as to why complex models are required, using concepts such as the VC Dimension (https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)? Relating to problems such as the initial ""x-or perceptron"" problem - could we not say that big data (data with many columns and many rows) is less likely to be linearly sepperable (i.e. harder to ""shatter"" - shatter =  classify perfectly) compared to smaller datasets? Could we not say that if there are more data points, there exist more configurations that these data points can be arranged in -  making it less probable for them to be analyzed using a simpler model (the VC dimension of a simpler model is lower than the
VC dimension of a complex model) ? Does this fact alone somewhat justify the need to develop complex models?"
l34df4rm3r,MachineLearning,1618580410.0,[D] Graph Convolution and GraphSAGE: why don't people use these together?,"In deep learning for graphs, people graph convolution or graph SAGE, but I have not seen a combination of these two. Intuitively, it should combine both transductive and inductive frameworks. Is there any drawback?

Using DGL, it's quite easy to stack these layers and get an output. So, my question is, why do we not see this used in works that involve graph neural networks (GCNs and its variants)?"
othotr,MachineLearning,1616740968.0,[R] Stanford HAI Spring Conference - Intelligence Augmentation: AI Empowering People to Solve Global Challenges,"Stanford Institute for Human-Centered AI hosted its spring conference today with interesting conversations about how AI can best support humans in healthcare, art, and education to address global challenges. More details and the event recording are available at the [HAI conference site](https://hai.stanford.edu/events/intelligence-augmentation-ai-empowering-people-solve-global-challenges). Here is a quick outline with video sections:

[Welcome & Introductions](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=829): HAI directors Fei-Fei Li, John Etchemendy, Russ Altman, & James Landay 

[Session I: Healthcare](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=1286) 

* Immersive Technologies for Caregiving: Innovation Opportunities and Ecosystem Challenges, Deborah Estrin @ Cornell Tech 
* Student Lightning Talks
* On Complementing and Extending Human Intellect: Principles and Directions, Eric Horvitz @ Microsoft
* Mobilizing AI to Achieve Healthy Child Development Worldwide, Dennis Wall @ Stanford 
* Safer and Proactive Care through AI, Suchi Saria @ Johns Hopkins University 

[Session II: Art](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=8379)

* Other Intelligence: Exoticism and AI, Ken Goldberg @ UC Berkeley
* Student Lightning Talks
* Artful Intelligence: Exoticism and AI, Michele Elam @ Stanford 
* The Digital Griot: A Reimagining of the Archive, Rashaad Newsome @ Stanford 
* Amplifying the Human Artist Through AI, Hilary Hahn & Carol Reiley @ DeepMusic.ai

[Session III: Education](https://crossminds.ai/video/stanford-hai-2021-spring-conference-or-intelligence-augmentation-ai-empowering-people-to-solve-global-challenges-605d2242abf312c051406b12/?timecode=15230)

* Escaping or Automating a Legacy of Bad Instruction, Daniel Schwartz @ Stanford
* Student Lightning Talks
* AI to Super Power Teachers, Chris Piech @ Stanford
* Pushing the Boundaries of Educational Technology, Amy Ogan @ Carnegie Mellon University 
* AI to Accelerate Workplace Learning at Scale, Candace Thille @ Amazon

https://preview.redd.it/p2qg7eutibp61.png?width=1928&format=png&auto=webp&s=1cc8dd6c4458c3da79d00415552ca4424f03d0c2"
dadasenior,MachineLearning,1619096232.0,"[Discussion] I own a 16"" macbook pro with max specs. But how can I make it fast for Keras?","I am training a Convolutional Neural Network with a huge dataset using tensorflow/Keras.

So far, I've been doing it with a notebook in Kaggle...  
it's VERY slow, and the *""GPU acceleration""* seems NOT to be working at all (also for many other users)

So, before I'll make an expensive PC build for ML,  
I am wondering:  
**could I exploit my expensive macbook pro 16"", instead?**

Macbook pro 16""  
2,3 GHz Intel Core i9 8 core  
16 GB 2667 MHz DDR4  
AMD Radeon Pro 5500M 4 GB

What's the best option?

a) plaidML, so that I could exploit the discrete AMD GPU in the macbook  
b) Using the ""accelerated tensorflow"" specifically tailored for macs available [here](https://github.com/apple/tensorflow_macos)  
c) I also own an eGPU powered with a AMD Radeon rx 5700xt 8GB (no CUDA 🙃) is it compatible with something like plaidML?  
d) Your call

Any help from your experience is appreciated, thanks in advance."
bert4QA,MachineLearning,1617439649.0,[R] FeTaQA: Free-form Table Question Answering,
EinsteiniumArmour,MachineLearning,1620342778.0,"[P] Last year, I built a multilingual text simplifier for my undergraduate thesis. This year, I made it work with multilingual BERT!","&#x200B;

[English simplification made by MILES](https://preview.redd.it/kympalub0lx61.png?width=1133&format=png&auto=webp&s=a39d8eaea7cf3178ff5fe9b2424a24800ced531f)

[MILES](https://github.com/Kvasirs/MILES) is a multilingual text simplifier inspired by [LSBert](https://arxiv.org/abs/2006.14939) — A BERT-based lexical simplification approach proposed in 2018. Unlike LSBert, MILES uses the bert-base-multilingual-uncased model, as well as simple language-agnostic approaches to complex word identification (CWI) and candidate ranking. Although not all have been tested, MILES should support at least 22 languages: Arabic, Bulgarian, Catalan, Czech, Danish, Dutch, English, Finnish, French, German, Hungarian, Indonesian, Italian, Norwegian, Polish, Portuguese, Romanian, Russian, Spanish, Swedish, Turkish, and Ukrainian.

Please note: as a result of not using any language-specific resources, MILES does not always offer synonymous substitutions for complex words. Although almost always simpler than the original, selected substitutions may alter the meaning of the text. Please keep this in mind, and feel free to use and tailor MILES to a language of your choosing!

The GitHub repo for MILES can be found [here](https://github.com/Kvasirs/MILES)."
FirstTimeResearcher,MachineLearning,1620101406.0,[D] Petition to the Neurips 2021 conference to extend the deadline,"Original tweet:
> Hi 
> @NeurIPSConf
> , sincere requests from many of my friends, collaborators (and most importantly students) affected by COVID to *please* extend the deadline. Many students I know are working on experiments *while* having COVID and close family members in hospitals!
>A growing number of 
@NeurIPSConf
 papers are being submitted by Indian researchers and students. I personally know many who are scrambling to write papers and experiments in this situation. Please consider extending by at least a week or two. I personally know two students who are in the hospital currently, and who had been working hard to submit a paper this time. While talking to my Indian collaborators, almost every student they have is in a similar situation. An extra week or two would help tremendously!

>Thank you so much! Pl Amplify!

https://twitter.com/rishiyer/status/1389264519885697029"
weifz,MachineLearning,1620391383.0,[D]Why is it impossible to do causal discovery from observational data?,"Hi there,

There is a saying that ""As we shall see, this problem is statistically impossible despite the large number of papers on the topic""(in  [http://www.stat.cmu.edu/\~larry/=sml/Causation.pdf](http://www.stat.cmu.edu/~larry/=sml/Causation.pdf) (sec. 1.1)), could you explain it, since I have read a lot of papers which do causal discovery from observational data?

Thanks!"
SeaworthinessOk834,MachineLearning,1619396009.0,[D] Is Anaconda Worth the Trouble?,"Hi, everyone. I'm getting ready to work with PyTorch and am struggling with whether I should reinstall Anaconda or not. I've had issues with it in the past, where it kind of screwed up the path on my old laptop. I was more careful when I got the new computer, and it was going along fine until it wasn't, so I uninstalled it a few months back. I would really like to use it, but there seems to be no definitive guide as to how to avoid these recurring issues and I was hoping maybe somebody here might have a resource or advice other than ""If you can't make it work, you must just really suck, hur hur."" that you get on other platforms. Thank you in advance.

EDIT: It's strangely reassuring to know that I'm not the only Windows user that has had problems with Anaconda. You've all given me plenty to consider. Thank you for your responses and feedback. "
thisisdhruvagarwal,MachineLearning,1618056732.0,Which Policy Gradient Method was used by Google's Deep Mind to teach AI to walk? [D],"I just saw [this](https://www.youtube.com/watch?v=gn4nRCC9TwQ) video on Youtube.

Which Policy Gradient method was used to train the AI to walk?

Was it DDPG or D4PG or what?"
mimeticaware,MachineLearning,1617515924.0,[D] Hashing techniques to compare large datasets?, Are there implementations or research papers on hashing/fingerprinting techniques for large datasets (greater than 10 GB)? I want to implement a library which generates a hash/fingerprint for large datasets so they can be easily compared. I'm not sure where to start and any existing implementations/research papers would be really helpful!
Rishit-dagli,MachineLearning,1618399521.0,[P] Implementing Perceiver: General perception with Iterative Attention in TensorFlow," Today I am glad to present an implementation of the ""Perceiver: General Perception with Iterative Attention"" Model which builds on top of Transformers but solves the quadratic scaling problem without making any assumptions of the data like the previous approaches in TensorFlow. This means you can use the same model on images, audio, videos, etc! This model also achieves state-of-the-art for some tasks!

PS: This is made ready to use as a Python package so you can get started very easily.

The project: [https://github.com/Rishit-dagli/Perceiver](https://github.com/Rishit-dagli/Perceiver)"
VinayUPrabhu,MachineLearning,1618166994.0,[P] On the extreme compressibility of Dall-E encoding tensors,"Brief abstract:   
I saw a bunch of artsy folks experimenting with interesting downstream applications of the Dall-E encoders and thought I'd share this  observation. As it turns out, one could potentially use much lower \*effective\* vocab-sizes  of \~ 32  instead of 8192 without suffering much visual-quality loss. Or in other words, the intrinsic dimensionality of the vocabulary-space unearthed by tucker-decomposition like tensor-compression techniques is \~ 32.  
So, you can use much lower dimensional encoding vectors (with \~96% compression) rather than the raw 8192 x 32 x32 D vectors in your post-encoding regression/classification/linear-algebra twiddling pipelines.  


PS: Notice the nice space-filling artifact in the .gif when you travel upwards from 3D to 32D.  
Colab: [https://github.com/vinayprabhu/Colabarama/blob/master/Dall\_E\_low\_D.ipynb](https://github.com/vinayprabhu/Colabarama/blob/master/Dall_E_low_D.ipynb)

https://preview.redd.it/w9dq107i9ls61.png?width=277&format=png&auto=webp&s=3b9d5c9d46c2f97b4379ef43663d59735a1a226b

[The obligatory cat gif](https://i.redd.it/khmdle27als61.gif)"
sideonion,MachineLearning,1620432525.0,[D] What DL algorithm to use to track target when I know target coordinates first frame - single target only?,"So,  I have a set of images and I have to define in the first frame which hu  (target) to detect/track in subsequent frames. Most deep learning models pre-train on a data that I don't have. I have images and I want to detect one specific target. I know human detection models exist but how should I use them when I want to detect that particular human in subsequent frames.

There may be multiple humans in the frame but I want to focus only on the one I am trying to detect.

If there are any methods that you can help me with, please let me know. Thanks."
TheCockatoo,MachineLearning,1616836263.0,[D] What's your experience with ML conference rebuttals / letters to area chairs?,"Have you ever written to the area chairs due to reviews by incompetent reviewers (e.g., where the reviewer has very obviously not spent more than 2 minutes on your paper yet rejects with high confidence, and their comments reveal he doesn't understand basic machine learning), and if so, what happened?"
stivi2000,MachineLearning,1617832590.0,[P] Language Independent Sentiment Analysis,"We trained a sentiment model using an english dataset. But we are using it for german sentences and it works surprisingly well.

See here for details and why it actually works:
https://github.com/AOK-PLUS/Sentimentanalysis"
WFHFAWAY,MachineLearning,1617552633.0,[D] Is A Failure Ever Worth Publishing?,"So I did some formal research as part of my MS. I had a architectural idea, couldn't find any example of it in the literature.

I went through the research process, but the application and integration of my idea was not exhaustive due to time constraints. The net result of the research was that integrating the approach into an existing backbone lowered the validation performance slightly on an apples to apples basis.

If anyone had tried this before, maybe it didn't work, but I could find no reference to anyone having tried it.

Are experiments that don't always lead to big improvements never worth publishing? I feel like we can only make progress if we know what has been tried before."
SkyLordOmega,MachineLearning,1618722171.0,[D] Wav2Vec2 training for Hindi language,"I was part of the datasprint with HuggingFace for the wav2vec2 fine-tuning task. I trained it on three datasets for Hindi language.

1. CommonVoice
2. Indic TTS (IITM)
3. IIITH - Indic dataset

While the trained model gives a good performance on the longer audios (2&3 WER 17) its performance on CommonVoice is very bad (WER 56)

CommonVoice has audio of smaller length.

I have attached an image with some example predictions (from the test set) What could be the reason for this drastic degradation in quality? Could I improve the model, by resuming training on only the CommonVoice dataset.

&#x200B;

https://preview.redd.it/8wqzwcho6vt61.png?width=922&format=png&auto=webp&s=b9394a396aec8730fac9630de2ed38b245af5b76"
PaganPasta,MachineLearning,1620454345.0,[D] ICML 2021 Results,"ICML results are due today. Gather around anxious authors, how did you fare?
Relevant content from last week: https://www.reddit.com/r/MachineLearning/comments/n243qw/d_icml_conference_we_plan_to_reduce_the_number_of/


Good luck."
Seankala,MachineLearning,1618462099.0,"[D] Regarding BERT-based models (BERT, RoBERTa, etc.) do we absolutely have to include the [CLS] and [SEP] special tokens in the input data?","The thought just occurred to me while I was processing data. If we're using the `[CLS]` token for classification, then it would obviously make sense to include it, but if we're not using that token do we have to include it?"
Gullible_Dance,MachineLearning,1619642170.0,[D] New paper shows that federated learning is broken?,"Title: See through Gradients: Image Batch Recovery via GradInversion 

([https://arxiv.org/abs/2104.07586](https://arxiv.org/abs/2104.07586))

The authors can recover individual training examples from accumulated gradients. What does this mean for data privacy laws?"
ravode,MachineLearning,1617294699.0,[D] Dask on App Engine or Cloud Run?,"Hello!

I'm wondering if running Dask on App Engine or Cloud Run is a thing?

The  use case is this - there is an ETL job running an sk-learn model (it's  part of an Argo Workflow to be precise) and as of now execution is done  unparallelized on the respective node. We'd like to parallelize the  sk-learn stuff by moving it to Dask. The obvious approach (as Argo is  K8s based) would be to integrate Dask in the K8s cluster.

But  there are a couple of reasons why I wonder whether App Engine or Cloud  Run might also be a viable option. In that case we'd skip on the  horizontal scaling and just instead fire up a strong instance  parallelizing with Dask on all available cores instead."
jj4646,MachineLearning,1620015148.0,[D] stochastic block model vs. standard community detection algorithms,"Has anyone ever come across the ""stochastic block model"" (https://en.m.wikipedia.org/wiki/Stochastic_block_model)? All in all, this seems like a community detection algorithm for graphs (i.e. network clustering). 

Does anyone know in which circumstances it would make more sense to use ""stochastic block models"" compared to community detection algorithms such as ""louvain clustering""?

Thanks"
xiikjuy,MachineLearning,1619972031.0,[D] How to visualize the features of encoder output of an encoder-decoder Transformer model?,"Hello,

I wonder how to visualize the encoder output features of encoder-decoder models like BART, T5.  
For the base Bart model, if max position = 1024, model dimension=768,  
then the feature dimension would be 1024\*768 =786k.  
 I have no experiences using t-sne before, is it still a reasonable choice for features with dimensions at this order?  
 Any suggestions for some good practice?  
 Thanks."
blazejd,MachineLearning,1617211645.0,[Discussion] How was this paper titled?,"This is a little desperate, but a couple of weeks ago I stumbled upon an interesting paper, but I don't remember its title and after a lot of searching I couldn't find it. It explained how using gradients we could identify easy and difficult to classify examples in the data and afterwards it was shown that using just 25% of examples, when chosen properly, we can achieve a similar performance as if we used the whole dataset for training. There was a figure showing images in two dimensions, something like below. If anyone remembered based on this vague description the title, I would really appreciate sharing it.

https://preview.redd.it/c2amg3laeeq61.png?width=555&format=png&auto=webp&s=77551085bdcd24f26843cc795ee014ec039d1a7c"
ccrbltscm,MachineLearning,1618792131.0,[P] Research paper graph for NeRF: foundational work & latest advancements - Link to the interactive graph and paper collection in comments,
meldiwin,MachineLearning,1619994622.0,"[N] Living Robots ""Computationally Designed Organisms""",
innerlee,MachineLearning,1617966854.0,"[N] MMOCR: A Toolbox for Text Detection, Recognition, and Understanding Based on PyTorch","We just released [https://github.com/open-mmlab/mmocr](https://github.com/open-mmlab/mmocr), a new member in OpenMMLab [https://openmmlab.com/](https://openmmlab.com/). This first release supports

&#x200B;

[Text Detection](https://i.redd.it/hoyx87ffg6s61.gif)

**Text Detection**

* PSENet
* PANet
* DBNet
* TextSnake
* MaskRCNN

&#x200B;

[Text Recognition](https://i.redd.it/pp4tpyrjg6s61.gif)

**Text Recognition**

* CRNN
* SAR
* Robustscanner
* SegOCR
* NRTR

&#x200B;

[Key Information Extraction](https://i.redd.it/huzucxylg6s61.gif)

**Key Information Extraction**

* SDMG-R

Longer post see [https://medium.com/@openmmlab/mmocr-a-comprehensive-toolbox-for-text-detection-recognition-and-understanding-795befa726b8](https://medium.com/@openmmlab/mmocr-a-comprehensive-toolbox-for-text-detection-recognition-and-understanding-795befa726b8)"
inigomlap,MachineLearning,1619161977.0,[D] What methodology do you use in data science projects?,"Data scientists out there, what **project methodology** do you or your team use for data science? You can check the methodologies used in the poll in this article: [https://www.sciencedirect.com/science/article/abs/pii/S2214579620300514](https://www.sciencedirect.com/science/article/abs/pii/S2214579620300514)

[View Poll](https://www.reddit.com/poll/mwponm)"
vanstorm9,MachineLearning,1618251549.0,[D] How do you visualize and compare image distributions of datasets?,"I have been in situations where I handle multiple image datasets and where I want to determine how similar two image datasets' distributions are to each other.

This will help me determine things such as whether it is suitable to combine two datasets into one, debug why validation accuracy is high while testing is low, and just straight up discovering more information when comparing image datasets that would aid in important decisions.

The most I have done is generating a pixel intensity histogram for one or multiple images from two datasets and compare them accordingly. Is there anything else I can do to visualize and compare image distributions among datasets? Is there also any other metrics I should also visualize or calculate to compare certain properties of image datasets?"
lsmith1988,MachineLearning,1618030770.0,Search engine used to seek details of videos/images [r],"Other than the caption, are there papers or projects underway where ML can extract information of a given video/image and search the exact location of where this may be on the internet (excluding reverse image search by Google)? For instance search for color wheels, frames of a video, speech, etc - then return its location. I am interested in learning more about this type of technology and whether it has been something that has been done already?"
cgnorthcutt,MachineLearning,1617033396.0,[R] Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"Hi Reddit! We’re excited to share our latest research: label errors are pervasive in 10 of the most-commonly used benchmark test sets used in most machine learning research. We investigate the implication of these label errors, in particular, how they affect the stability of ML model benchmark rankings.

[An example label error from each category for image datasets. The figure shows given labels, CL-guessed alternatives, human-validated corrected labels, and also the second label for multi-class data points. A browser for all label errors across all 10 datasets is available at https:\/\/labelerrors.com. Errors from text and audio datasets are also included in the website.](https://preview.redd.it/07uhmpp8ozp61.png?width=1900&format=png&auto=webp&s=bc3e3c42a4c9a64055bd2a225284f98b8a3dc86d)

**Demo**: [https://labelerrors.com](https://labelerrors.com/)

**Blog Post**: [https://l7.curtisnorthcutt.com/label-errors](https://l7.curtisnorthcutt.com/label-errors)

**Abstract**: We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.

**Paper**: [https://arxiv.org/abs/2103.14749](https://arxiv.org/abs/2103.14749)

**Code**: [https://github.com/cgnorthcutt/cleanlab](https://github.com/cgnorthcutt/cleanlab)

Joint work with Anish Athalye and Jonas Mueller."
,MachineLearning,1616615314.0,[N] Hype GTC 2021,"Get a glimpse at the future of computing during NVIDIA CEO Jensen Huang’s #GTC21 keynote on 4/12 at 8:30 a.m. PDT. Save the date. [https://bit.ly/3rlvfWZ](https://bit.ly/3rlvfWZ)

&#x200B;

I think what we are doing across the AI/ML community is extremely important and pushing boundaries. This keynote is free to listen to (don't even need to register for the conference) and if you haven't heard Nvidia CEO Jensen Huang speak it is a sincerely impressive thing.

I'll have some more useful AI/ML/Data Science information/tips tomorrow. Just thought I'd push this to the masses today."
CauchySchwartzDaddy,MachineLearning,1617076262.0,[D] Advice on getting grants/funding for a college ML research club to cover GPU costs,"Basically I help run a ML club on campus and I want to be able to get some sort of funding to cover GPU costs in the cloud.  One of my friends said ""google just gives out GPUs"" but I couldn't find any info on this.  We really don't need a whole ton to run the whole operation but I was wondering if there were any good ways to go about this."
blissfox-red,MachineLearning,1617171438.0,[D] Nvidia Data Science of the Day posts,"Subsequently to bumping into these posts (whose author disappeared):

[Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506) :  
[https://www.reddit.com/r/MachineLearning/comments/m6b047/p\_embed\_your\_sql\_query\_into\_your\_python\_code\_and/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/?utm_source=share&utm_medium=web2x&context=3)

[With GPUs, K-nearest Neighbor Algorithm Crosses the Finish Line When Others are in the Starting Blocks](https://forums.developer.nvidia.com/t/with-gpus-k-nearest-neighbor-algorithm-crosses-the-finish-line-when-others-are-in-the-starting-blocks/168785) :  
[https://www.reddit.com/r/MachineLearning/comments/m6zve7/p\_with\_gpus\_knearest\_neighbor\_algorithm\_crosses/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/m6zve7/p_with_gpus_knearest_neighbor_algorithm_crosses/?utm_source=share&utm_medium=web2x&context=3)

I arrived at a part I have so far missed on the nvidia webpages (some, if not most of you might already know about it), the Data Science of the Day page, which proposes some posts (about one a day on average it seems) encapsulating usually just a link to a blog post or article about a technical news in data science. Not every post in there might be relevant to a ML practitioner,  but I believe at least some might be. So, for those like me who missed that, or just want to give it a look, here is the link:  
[https://forums.developer.nvidia.com/c/ai-data-science/data-science-of-the-day/323/none](https://forums.developer.nvidia.com/c/ai-data-science/data-science-of-the-day/323/none)"
tranhp129,MachineLearning,1620591990.0,[D] Non Strongly-convex loss is strongly convex in expectation,"I have seen several papers mention that there are some losses that are not strongly convex by themselves but are strongly convex in expectation (eg: logit loss, square loss). Just by taking the derivative of the losses, I can show that these losses are not strongly convex but I don't understand why the expectation of them are strongly convex. For example, the hessian of square loss would be a rank-1 matrix (outer product of vector), thus the square loss is not strongly-convex. But why is taking the expectation of this make any difference? Any help is appreciated!"
gta141,MachineLearning,1620337216.0,[Research] DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic Using Deep Visual Models,"The goal of this paper is to quantify marine plastic using deep learning. Check out our paper at [https://arxiv.org/abs/2105.01882](https://arxiv.org/abs/2105.01882)

&#x200B;

[Predictions using our model](https://reddit.com/link/n6ihm7/video/dzdhwqfikkx61/player)"
RussellEsby,MachineLearning,1618479575.0,[P] Voice Conversion VAE-Cycle-GAN on Melspectrograms,"Hi! I'm following this great work by Ehab A. AlBadawy ([https://ebadawy.github.io/post/speech\_style\_transfer/](https://ebadawy.github.io/post/speech_style_transfer/)). The demonstrated results are incredible. And it really stands out from other approaches both in quality and feasibility.

I've been working on an open source implementation for the past month. The link for it can be found here ([https://github.com/RussellSB/voice-conversion-gan](https://github.com/RussellSB/voice-conversion-gan)). But have since been struggling with a load of mode collapse - or the model outputting ""blurry"" spectrograms not quite capturing the same initial structure as they should.

There were some architectural changes I had to make to be able to execute the model. For one I added a conv layer to the encoder and decoder to get the right dimensions. In the paper there is no description of how they designed their residual blocks either so I used convolutional layers.

I've tried many modifications and experiments to better stabilize training, or more closely replicate the paper, but I feel I've replicated as much as I currently can. Might anyone be able to point out what may be the issue - or keen to lean me to the right direction?"
paulcjh,MachineLearning,1616435921.0,[P] Silero NLP streaming on serverless GPUs (~300ms latency),"Hey everyone,

A couple of weeks ago I put out a post on DeepSpeech running on the serverless setup at Neuro ([https://getneuro.ai](https://getneuro.ai/)), and I've now got Silero running there as well. I've found this model is a lot faster than DS and way more accurate. Seeing around 300ms per request at the moment, hopefully will be closer to 100ms soon but this is a pretty decent speed in this application already. 

The code listens to a mic on your local machine and streams it to the Silero model then returning you the conversion result. You can find the source here: [https://github.com/neuro-ai-dev/npu\_examples/tree/main/silero/python](https://github.com/neuro-ai-dev/npu_examples/tree/main/silero/python). Of course this is all running on serverless so you can hammer it (in theory) as hard as you want.

Silero ([https://github.com/snakers4/silero-models](https://github.com/snakers4/silero-models)) is an audio to text conversion model and can be pretty heavy. If you have any Q's or want to see more of this let me know! I think next I'll be playing around with Spade ([https://github.com/NVlabs/SPADE](https://github.com/NVlabs/SPADE)) and setting up some other bulky vision models.

Cheers"
idg101,MachineLearning,1619555928.0,[D] Unpopular Opinion: I hate the tensorboard Smoothing algorithm and always set the slider to 0.,"Looking at the code for the smoothing slider bar in tensorboard, it implements an exponential moving average which is all useful for the majority of ML tasks I do.  It seems to me that something much more simple like a moving average filter would be much better and make the slider the window length."
Drakshh,MachineLearning,1619710277.0,[Project] Model to evaluate audio clips similarly,"(posted on multiple subreddit but got no response hence posting here... Sorry it this doesn't belong here)

Dear all, I'm relatively new to NLP and ML. Currently I'm working on projects in which I have to compare and score two audio clips. Original clip (single sentence) will be from movie character (animated or real) and in second clip human will try to mimic it. I have to come up with my model to determine similarly and score human clip out of 5.
Factors which I have considered to be used are:
1. Getting text from speech and comparing with original
2. Evaluating similarly from audio spectrogram (spectral centroid and zero crossing rate)
3. Identification of emotion from speech and using emotions embedding (assuming it's available on internet) to measure similarly (will probably use cosine similarly)
I couldn't come up with more factors. Can you please help me come up with new comparison factors or suggest how can I approach this problem in better way?
Thanking you in anticipation..."
cloud_weather,MachineLearning,1619254999.0,[D] StyleGAN2 + CLIP = StyleCLIP: You Describe & AI Photoshops Faces For You,
skeering,MachineLearning,1619970192.0,[R][D] Starting a Post-Doc and Looking for Advice on Research Area,"So I'll be starting a post-doc within the next 6-12 months, and I'm looking around for opportunities. My supervisor advised me not to do a post-doc in the same area as my PhD and instead to ""branch out"" into two main fields, to diversify.

My PhD was/is in XAI, and I'm very happy with how the last 3 years went. Going forward, I was looking for ""hot"" research areas aside from this, and what I personally just like/find interesting. Correct me if I'm wrong, but it seems the big questions in AI going forward are.

1. How to generalise better (e.g., take what you learn from MNIST and apply it to FashionMNIST).
2. How to make systems immune to adversarial attacks.
3. How to get explanations from opaque models (e.g., in medical radiology).
4. How to learn from fewer examples (one shot learning and semi-supervised learning). The ultimate goal being unsupervised learning that works well in the ""real world"".

I'm not sure if I missed anything? Probably what I'm most interested in (aside from XAI) is number 4 above. Would I be correct in assuming any of these four areas are important in the next generation of AI technology? Is there any area I missed? Lastly, would you agree that number 4. is a good area to ""get into"" as a second research interest the next 5-10 years?

Thanks and have a great day."
Jason_s0214,MachineLearning,1619216428.0,[R][D] Our new ICLR'21 work clarifies a misconception regarding distillation and label smoothing in a previous NeurIPS'19 study," [Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://arxiv.org/abs/2104.00676) 

Project page: [http://zhiqiangshen.com/projects/LS\_and\_KD/index.html](http://zhiqiangshen.com/projects/LS_and_KD/index.html)

 Any comments or discussions are welcome!"
Zethsc2,MachineLearning,1618571357.0,[R] mlf-core: a framework for deterministic machine learning,
aspcraft,MachineLearning,1620306117.0,Noise-reduction techniques and evaluation for timeseries data [Discussion],"What are some common practices for dealing with noisy data? What are some common noise-reduction techniques for non-stationary time series?

Also, is there some sort of formula for comparing different methods of noise reduction? For example, if one produces two different timeseries from the original noisy data, is there a way of comparing these two new timeseries to evaluate which is better? For example, in a linear regression the best line is typically chosen by minimising the mean squared error. Is there something similar for this? If not, anyone have any ideas for a criterion that could be adapted into a formula that is then minimised or maximised to get the best noice-reduced timeseries?

I am interested to hear your thoughts and general ideas!"
ottawalanguages,MachineLearning,1618866478.0,[D] Effective Ways of Choosing the Number of Layers/Neurons in a Neural Network,"I have been reading more about the theoretical backgrounds of neural networks (e.g. ""universal approximation theorem"") and have seen several authors demonstrate that even a simple (few layers, many neurons) neural network can (theoretically) approximate the variable of interest (i.e. the response variable) to a ""decent"" level of precision.  However, the implication being that to use simple neural networks in order to achieve good results, this would require a very large number of neurons. Therefore, deeper neural networks have been developed over the years, which attempt to provide good results with more layers but a fewer number of neurons.

This brings me to my situation: I have never been able to successfully fit a neural network to any real-world data that I have used. I have always gotten really bad results with neural networks (after trying all sorts of combinations of number of neurons, number of layers, learning rate, activation function, ""drop out"" regularization, etc.). This seems to be a hyperparameter-grid search problem.

(Ironically, models like CART decision trees have good results on the same data (supervised binary classification) and random forest has produced even better - this data is not ""small by any means"", contains around 30 columns and over 300,000 rows of data).

 Does anyone know if routines have been written (e.g. in tensorflow keras) that can assist in this problem of deciding the number of layers and the number of neurons? Is there a ""ground rule"" for deciding how many layers and how many neurons to begin with? Is there something around that can ""intelligently"" point you in the right direction for how many neurons/layers to choose?"
Unreasonable_Energy,MachineLearning,1618961433.0,Do we already have the ML technology to make eye contact work better in video chat? [D],"**The problem:**

In video chat, if you make ""eye contact"" with the image of your conversation partner's eyes as they appear on your screen, your eyes are not pointed at the camera, and thus you do not appear, to your partner, to be making eye contact with them, but appear to looking in an offset direction -- [relevant xkcd](https://xkcd.com/2430/).

**A solution (in principle?):** 

If you had multiple cameras arrayed at the edges of your screen (say, 3 or 4 webcams around the edges of your screen instead of just 1) it seems like it should be possible, with some machine learning magic, to combine the multiple camera feeds coming from slightly different vantage points into a single feed of a ""virtual vantage point"" within the convex hull of the physical cameras -- that is, a point somewhere in the middle of your screen.  Then all you need is to be able to automatically locate your the image of your conversation partner's eyes within your screen (basically a solved problem, as I understand) and set the virtual vantage point to one of these eye images, or to a point between them -- and voila, when you make eye contact with your screen image if them, they actually see their screen image of you appearing to make eye contact with them.  

**If this would be so easy, why doesn't it already exist?**

Do we not have the machine learning technology already to make this a reality?  Is somebody already doing it?  The potential hurdles that immediately come to mind are:

(1) Learning the virtual-vantage-point transformation would be hard -- maybe?  I'm pretty sure I've already seen more impressive demonstrations of vantage-point-shift than this would require.  I certainly don't have enough understanding of this specialty to know how I'd do it though -- maybe it's hard.

(2) Takes too long to apply the vantage-point transformation in real time, inducing unacceptable lag -- yes, the processing needs to happen in real time, but it can happen on the local machine, and the virtual-vantage-point-feed shouldn't be any more costly to transmit over a network than the standard video feed.  I can imagine applying it locally could be pretty fast, and there already seem to be more superficially-impressive video transformations being applied in real time (face-contouring, etc).

(3) Requires a non-standard hardware setup -- sure, but not like a really expensive or difficult one, just a couple extra webcams and you could do it yourself.  Doesn't work on mobile, but you can't have everything,

(4) Nobody cares about this problem enough to work on it -- I suspect there are surprisingly large gains to be had through making video chat a little less uncanny, but maybe this gain is too small to be worth the effort.

I'd appreciate any thoughts on the feasibility of such a scheme, from a machine-learning perspective."
triplehelix_,MachineLearning,1618407205.0,[D] [R] AI/ML colorisation versus actual color photos from between 1909 and 1915,
mate_classic,MachineLearning,1617894376.0,[D] State of Deep Learning outside CV and NLP,"Computer vision and natural language processing get most of the limelight in the deep learning world, and rightfully so. But, as someone working in another field that fell for the deep learning hype, too, I often see problems that have no direct equivalent in CV or NLP. Nevertheless, people try to adopt successful methods from these fields.

This sometimes leads to results that are not meaningful at all because the researchers don't take the time to adapt the methods properly to the field. Often, datasets are much smaller or have samples that are part of the same group (e.g. steps in a time series). There are enough papers, where one part of a time series is put into the train and the other part into the test set.  I wrote a [blog post](https://krokotsch.eu/research/2021/04/07/One-Eyed-Data-Scientist.html) about one of these problems in my field, predictive maintenance, and how it leads the whole line of research astray.

**So my question would be: how is the state of DL in your field that is not CV or NLP, and does it suffer from blindly copying approaches from them, too?**"
vulnerablebeast,MachineLearning,1618978908.0,[P] Is it possible to use a loss function involving one input and multiple ground truths," Instead of optimizing say,

f(x) = y\_groundtruth\_i - y\_predicted\_i

Can we take an average of the ground truths, say

f(x) = ( (y\_groundtruth\_i+1 - y\_predicted\_i) + (y\_groundtruth\_i - y\_predicted\_i) ) / 2

Has this been done before? Thanks!"
bert4QA,MachineLearning,1618589144.0,[R] Privacy-Adaptive BERT for Natural Language Understanding,
hardmaru,MachineLearning,1617713905.0,[R] MobileDets: Searching for Object Detection Architectures for Mobile Accelerators.,
mihirkarkare,MachineLearning,1617327641.0,[D] Churn prediction using ML,"What would be a good way to define ""churn"" for analysing the behavior of customers who are using Revolving credit? -- 

If a credit card company wants to retain it's revolving credit customers, what would be a good strategy to attack this problem using ML? 

In this situation the term ""churn"" is a little difficult to define as not using Revolving for a month or two is completely natural, and not an indicator of if a customer has made up his/her mind to stop using Revolving credit."
Caffeinated-Scholar,MachineLearning,1619202533.0,[R] FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras,
sl2085,MachineLearning,1616794332.0,[D] What type of machine learning can be used to solve timetable optimisation problems?,I can only really think of more traditional operational research related techniques such as linear programming but are there any other successful techniques people have used to solve timetable optimisation problems?
Competitive-Net-5306,MachineLearning,1617836999.0,[P] About a ML agent for the game Slay the Spire that I've been making,"Hi. I've recently been working on making a setup where I would be able to train and test a machine learning agent for a TCG roguelike game called Slay the Spire. I'd like to share the process and my current status in that project so that I can receive feedback for improvement, and maybe help others who face problems similar to what I went through.

For this project, I had to solve the following problems.

1. Establish connection with the game process, or at least mimic something similar.
2. Find a way to design a model that is capable of making different types of decisions with different format of inputs depending on game state. The model must be able to handle different tasks ranging from selecting card reward to choosing whether throwing a potion at a particular enemy would be beneficial.
3. Find a way to properly reward that model.
4. Run as much games as possible in parallel at any given moment in time

The first problem proved to be a lot easier than I expected. There was already a mod called Communication Mod that practically solved the problem for me the moment I discovered it. It allowed connection and communication between the game process and any arbitrary external python script through stdin and stdout. I just fiddled with it to make it work, and afterwards I was good to go.

Second problem was a bit more tricky, and I'm not quite happy with how I solved it. I just divided the game into distinct situations that requires its own distinct format of input to make its own distinct type of decision. Then, I just made a neural network for every single one of them. I can't shake off the feeling that there might be a more efficient way of doing it, but I'm still a noob.

The way I solved the second problem made solving the third problem a bit more difficult. I wasn't training just one single net, but a collection of nets that together plays the game by trying to predict how beneficial playing some action in some situation would be. I was, however, able to solve it through the following process.

1. Add one extra net to a collection of nets, whose purpose is to predict how much score this AI would get by the end of the game, based on a perfect snapshot of current situation.
2. While the agent is playing a game, create a snapshot at set moments in the game and record every decisions made between each snapshots.
3. When the game is over, assign a label to every snapshots created during that game with a score equal to what it got by the game was over.
4. Train the snapshot net with the label.
5. Label every decision points with snapshot\_score(after) - snapshot\_score(before).
6. Train the nets with the label.

The fourth problem was the most tricky but most enjoyable one to solve. As we all know, more data is usually better when doing machine learning. My data was being generated during training by my agent actually playing the game, so if I could make the agent play more games simultaneously then I would get more data. To do that, however, I had to be able to launch and run multiple games at once.

I was able to achieve that by using virtual machines. Basically, I would create virtual machines as worker nodes that actually runs the game, then make those games communicate with their local python script which would establish socket connection with my main machine that actually runs the agent. But there's one catch. I didn't have enough computing resources to do that in large scale.

My solution to that was google compute engine in their cloud platform. Each instance of it was a virtual machine that can be connected to through the internet, so it was almost perfect for my use. However, putting a GPU to my instances was not practical due to cost, and without it the game run abysmally slow due to rendering issues.

I solved that by just making my own small mod for the game that applies a bytepatch to the code to disable rendering calls in its main loop. It was surprisingly easy due to the game being written in java. However, I still couldn't run the game on cli because it required a window manager. And I wanted to automate the process without having to open up the desktop for all the worker nodes and manually traversing through the gui to launch the game. 

The solution for that problem proved to be deceptively simple. Just attach a command to the gnome-session-properties such that when it starts it automatically fires the game. Then, use ssh connection to open up a new gnome session, which can be done in cli or by a bash script. When things required a reset I could just close the gnome sessions which also can be done via cli and start it back up again.

That's about it!"
cvpy,MachineLearning,1616342349.0,"[P] Face make up powerd by deep learning | change color of lips, eyes and eyeglasses",
StandardDull3128,MachineLearning,1617116497.0,[Discussion] Can I use CNNs to solve this problem?,"What does your intuition/experience/expertise tell you? Would I be able to train a CNN to solve a binary classification problem described below and expect a high accuracy (let's say >90%)?

**The problem:** Predicting whether an image contains one type of texture (class 0) or it contains several textures and clear borders among them (class 1).

**The data:** The data is high resolution (5 centimeters / pixel) agricultural imagery obtained via remote sensing (drone imaging). So pictures depicting different crops, weeds, roads, soil, bushes, etc. I would feed the model square images of 256x256px (or 128x128px).

**Examples of images:** (""neg"" is class 0 and ""pos"" is class 1)[https://imgur.com/a/gnKbncA](https://imgur.com/a/gnKbncA)"
xiikjuy,MachineLearning,1618403267.0,[D] Is captioning a reasonable way towards explainable AI?,"Take video classification for example, someone may visualize spatial(-temporal) attention mask for a way explaining what AI focuses/sees. If I add a video captaining function on top of a video classification model, thus for each test video there is not only a class prediction, but associated sentences. And we can check the sentences generated to have a sense what's in AI's mind to make such prediction .  Does that make sense? or any related published works on this kind of idea?"
mfilion,MachineLearning,1618933040.0,[Project] Continuous 3D Hand Pose Tracking using Machine Learning & Monado OpenXR,"As part of a project backed by INVEST-AI, a program managed by IVADO  Labs, Collabora has developed a multi-stage neural network-based  solution that accurately locates and tracks the hands despite complex  background noise and occlusion between hands. Our system estimates 2D  and 3D joint locations without any depth information. Collabora is also currently working on integrating it into the Monado XR codebase, so it can be  used out-of-the-box with different devices.  
[https://www.collabora.com/news-and-blog/blog/2021/04/20/continuous-3d-hand-pose-tracking-using-machine-learning-and-monado-openxr/](https://www.collabora.com/news-and-blog/blog/2021/04/20/continuous-3d-hand-pose-tracking-using-machine-learning-and-monado-openxr/)"
bjourne-ml,MachineLearning,1617774303.0,[P] Music generation using tracker music in MOD format,"Link: https://modmusicgen.com/

Hi all, I've been working on a project for generating music using neural networks trained on tracker music in MOD format. The tracker music is converted to a simple internal format to make it amendable for training and then converted to MIDI so it sounds more like piano music than tracker music.

I would very much appreciate it if you have some time over to fill out the preference survey. :) Also if you have any questions about the project I can try and answer them in this thread."
techsucker,MachineLearning,1617636798.0,[R] Researchers From the University of Toronto and LG AI Research Develop ‘Explainable’ Artificial Intelligence (AI) Algorithm,"A team of researchers from the University of Toronto and LG AI Research have developed an explainable artificial intelligence (XAI) algorithm. The algorithm can help identify and eliminate defects in display screens.

The algorithm outperformed comparable approaches on industry benchmarks and was developed through an ongoing AI research collaboration between LG and the University of Toronto.

According to the researchers, the XAI algorithm could be applied in other fields, primarily those which require details into how machine learning makes its decisions, including data interpretation from medical scans.

Summary: [https://www.marktechpost.com/2021/04/05/researchers-from-the-university-of-toronto-and-lg-ai-research-develop-explainable-artificial-intelligence-ai-algorithm/](https://www.marktechpost.com/2021/04/05/researchers-from-the-university-of-toronto-and-lg-ai-research-develop-explainable-artificial-intelligence-ai-algorithm/) 

Paper: https://arxiv.org/pdf/2010.00672.pdf"
koolaidman123,MachineLearning,1619518931.0,Visformer: The Vision-friendly Transformer,
NeitherBandicoot,MachineLearning,1616955264.0,[D] Why some major papers in ML aren't peer-reviewed?,"I'm a little bit of an outsider (coming from a life science field) and I noticed that some major publications in the field of ML aren't peer-reviewed, which strikes me as very unusual given that in my field a paper that is not peer-reviewed is as good as a blog post. I understand the need to publish asap because research in the field is moving so fast, but preprints exists for that, so that shouldn't be the reason why.  Could somebody explain to me why peer-review doesn't appear to be prioritized in ML research?  I'm not trying to criticize the way it works, I'm just puzzled and confused because it's not what I'm used to see.


Edit: took off the example because it wasn't representative of my question as the comments suggested."
upulbandara,MachineLearning,1619204466.0,[D] How to extend a text classification ML model to work with more than one language?,"We are using (in production) an ML for text classification. We trained our model using some custom English text corpus. Currently, the model is working acceptable level of accuracy for our purpose. Now we want to extend it to handle French language as well. We are planning to investigate the following two approaches.

1. We have a French-language corpus. Therefore, we would like to train a new model for handling French text.
2. Use the same model trained with English corpus. But use a third-party language translation service (such as Google Translator) to translate French text to English before inputting it into the ML model.

So I would like to know your thoughts regarding these two approaches."
MediocreMinimum,MachineLearning,1617017494.0,[D] What will the major ML research trends be in the 2020s?,"We've entered a new decade -- hurrah!

**What do you think the next 10 years will bring in ML research?** **What conventionally accepted trend do you think will** ***not*** **happen?**

e.g...

Will deep learning continue to *eat everything*? Will multi-task multi-domain learning make few-shot learning available for most domains? (Or is deep learning on the slow end of the sigmoid curve now?)

Will safe, ethical, explainable AI rise, or is that hogwash?

Will advances decouple from compute power?

Will Gary Marcus and Judea Pearl win out in the symbolic/structural/causal war against deep learning?

Are there still major breakthroughs in language? Do we just finetune GPT-3?

Will we make big breakthroughs in theory and fundamental ML? Or is this the decade of *application*? (Healthcare will finally deploy models that beat logistic regression!)"
PhYsIcS-GUY227,MachineLearning,1619445711.0,"[P] Integrating Git, DVC, and MLflow into one","Hey r/MachineLearning. I'm one of the creators of [DAGsHub](https://dagshub.com), and I wanted to share something cool that we've been working on. DVC ([dvc.org](https://dvc.org)) and MLflow ([mlflow.org](https://mlflow.org)) are two open-source projects that are very widely adopted, each for its own specialty. DVC excels at data versioning, and MLflow is used for many things (it's actually multiple tools combined into one), but mainly for its experiment tracking capabilities. Both tools have a built-in tradeoff – since they are open source, setting up storage for DVC and a central tracking server for MLflow can be a pain – requiring you to create cloud accounts, add permissions, and more.

DAGsHub is already integrated with DVC, in the sense that whenever you create a project, it comes with a free, built-in, DVC remote.

Since last week, you also get a free MLflow server, which means you can log experiments directly to DAGsHub and share it with your team or colleagues.

&#x200B;

https://i.redd.it/edvlu237xiv61.gif

Why I think this is awesome:

1. Zero setup – add your MLflow remote server URI and just log experiments
2. Access control built-in – if you have a team and some people need access only to view the experiments but not to log new ones, you can easily control that
3. Better UI for comparison – one of the complaints MLflow users have had was about the inability to compare runs across experiments in MLflow, with DAGsHub that is easily possible as all runs appear in a single list, which you can then filter to fit only a single experiment.
4. Integrating MLflow and DVC – a lot of people are working with both systems and building ad-hoc systems to integrate them, but with this integration, you can create a project that was built for this type of work, integrated with all the tools you need.

Here's a [more detailed blog post](https://dagshub.com/blog/launching-dagshub-integration-with-databricks-mlflow/) from the engineer that built this. I'd love to hear your thoughts about it."
mildlyoverfitted,MachineLearning,1618130003.0,[P] Growing Neural Cellular Automata - Implementation and explanation,"Hey there!

I made a video where I try to explain and implement the article ""Growing neural cellular automata"". It is a niche topic, however, I find it fascinating. Hope some of you could find it useful.

&#x200B;

Original article: [https://distill.pub/2020/growing-ca/](https://distill.pub/2020/growing-ca/)

My video: [https://youtu.be/21ACbWoF2Oo](https://youtu.be/21ACbWoF2Oo)"
temakone,MachineLearning,1617020121.0,[R] Swin Transformer: New SOTA backbone for Computer Vision🔥,"**Swin Transformer: New SOTA backbone for Computer Vision** 🔥*MS Research Asia*

# 👉 What?

New vision Transformer architecture called Swin Transformer that can serve as a backbone in computer vision instead of CNNs.

# ❓Why?

There are two main problems with the usage of Transformers for computer vision.

1. Existing Transformer-based models have tokens of a fixed scale. However, in contrast to the word tokens, visual elements can be different in scale (e.g. objects of varying sizes on the scene)
2. Regular self-attention requires quadratic of the image size number of operations, limiting applications in computer vision where high resolution is necessary (e.g., instance segmentation).

# 🥊 The main ideas of the Swin Transformers:

1. **Hierarchical feature maps** where at each level of hierarchy Self-attention is applied within local non-overlapping windows. The size of the windows is progressively increased with the network depth (inspired by CNNs). This enables building architectures similar to feature pyramid networks (FPN) or U-Net for dense pixel-level tasks.
2. **Window-based Self-attention** reduces the computational overhead.

# ⚙️ Overall Architecture consists of repeating the following blocks:

\- Split RGB image into non-overlapping patches (tokens).

\- Apply MLP to translate raw features into an arbitrary dimension.

\- Apply 2 consecutive Swin Transformer blocks with Window self-attention: **both blocks have the same window size, but the second block uses shifted by \`patch\_size/2\` windows which allows information flow between non-overlapping windows**.

\- Downsampling layer: Reduce the number of tokens by merging neighboring patches in a 2x2 window, and double the feature depth.

&#x200B;

https://preview.redd.it/xtjhyflalyp61.png?width=1920&format=png&auto=webp&s=b0f8539b4e1779ba8263281e4ff2974562858c0d

&#x200B;

https://preview.redd.it/z95z4ycclyp61.png?width=1010&format=png&auto=webp&s=0e27b6b8fb511da3be3b9647da4bacd2b8411dc3

# 🦾 Results

\+ **Outperforms SOTA by a significant margin on COCO segmentation and detection tasks and ADE20K segmentation.**

\+ **Comparable accuracy to the EfficientNet** family on ImageNet-1K classification, while being faster.

&#x200B;

https://preview.redd.it/giw5nz4dlyp61.jpg?width=1920&format=pjpg&auto=webp&s=d8eb93e37dd44ee75e7476c584d41fb14d3b760a

# 👌Conclusion

While Transformers are super flexible, researchers start to **inject in Transformers inductive biases similar to those in CNNs**, e.g., local connectivity, feature hierarchies. And this seems to help tremendously!

&#x200B;

📝 Paper [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)

⚒ Code (promissed soon) [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)

🌐 TL;DR blogpost [https://xzcodes.github.io/posts/paper-review-swin-transformer](https://xzcodes.github.io/posts/paper-review-swin-transformer)

\--

👉  Join my Telegram channel [""Gradient Dude""](https://t.me/gradientdude) not to miss the latest posts like this [https://t.me/gradientdude](https://t.me/gradientdude)"
Kaleidophon,MachineLearning,1616769467.0,[P] deep-significance: Easy and Better Significance Testing for Deep Neural Networks (link below),"Hey!  


Recently, I have become somewhat frustrated at ML / DL papers highlighting scores only stemming from a single run in result tables, and claiming that an approach is superior when it only outperforms others marginally. This is why I re-implemented, tested and packaged a statistical significance test proposed by Dror et al. (2019) that is specifically tailored towards neural networks. I also added information about statistical significance testing and how to apply the mentioned test in the most common scenarios faced by ML practitioners!

[https://github.com/Kaleidophon/deep-significance](https://github.com/Kaleidophon/deep-significance)

I'd be very happy to receive some feedback from the community here and improve this further and help move the field forward :-)"
cloud-native,MachineLearning,1616368252.0,[D] How would you migrate a DS team from HPC cluster to the cloud,"I wanted to ask if you could point me to some resources on how to successfully move a DS team to the cloud, or if you could share experiences from your teams / companies.

I'm working with a team of \~20 old school analysis folks (stats & natural science backgrounds), and we need to move out of our on-prem setup to Microsoft Azure.

We are a research team (not ML research, but domain specific research), and we are all used to our HPC: 

* we can ssh into it and connect VSCode to it.
* we can easily use dask to run distributed processing (via SLURM)
* we have a unix filesystem
* etc.

What is the best cloud setup for such teams? What do people do in research organizations in big tech companies?

* Give the DS team members freedom to run any compute instances they want?
* Run a pool of compute instances that the DS team can ssh into?
* Run a Kubernetes cluster using AKS?
* Use the Azure ML offering, even though we don't do much ML?
* Something else? We rely heavily on CLI tools and VSCode, so Jupyter-only solution is not an option for us.

Any pointers would be greatly appreciated."
yusuf-bengio,MachineLearning,1616767299.0,[D] Dilemma: Mathematically wrong ICML submission got extremely good reviews,"I am tangentially involved in a ICML submission from a research group at a different institute. After the initial reviews, I realized that the main theorem, which the work is based on, is mathematically objectively wrong.

For anyone working in this subfield finding a counterexample that violates the theorem is pretty straightforward. However, none of the reviewers found an error with the theory, even praising the paper with high scores (definitely an accept).

As I was involved in this work only at the very beginning, e.g., for laying out the ideas and roadmap, I never thoroughly read the theoretical parts of the paper before the submission.

The proof exploits subtle differences of two definitions of a concept in older vs newer literature. Moreover, the final paper use of grandiose mathematical notation, making the error harder to spot. In my opinion, it even appears as if the choices of the mathematical writing style and the use of the conflicting definitions were made deliberately.

The problem though is that if I intervene now, the PI (who didn't read beyond the abstract) and the PhD student responsible for the proof (and known for having ""narcissistic"" tendencies) will be pissed. I don't want to risk my career, because the PI is quite a bigshot in the field and the student is extremely well connected to a FANG company (""best friends"" with a top-5 researcher there).

I was thinking that if I am quite about it, the worst thing that can happen is other researchers realizing the error and publishing a ""counter"" paper to disprove ours.

Furthermore, I have the feeling that I can't be the only one having this particular kind of dilemma. Does anyone have experiences with situations like mine?

**Update**:

Thank you for all the feedback on my situation.

As many suggested, I wrote up and discussed the counterexample with the co-authors. During our discussion I realized three things:

1. Coming up the the counterexample and seeing how it violates the main claims is not as straightforward as I original thought. It took even the student who wrote the proof quite a while to realize that there might be some issues.
2. The student and PI are both still convinced about the correctness of the main theoretical results, citing the good reviews as additional evidence that the claims are correct. Their main argument is that the proof uses primarily existing theorems from literature and (correct) reformulation of mathematical expressions. Thus, if there is something wrong with the claims, it must come from the theorems proven in the literature and be already wrong in there.
3. The submission will not be withdrawn from ICML, nor do the co-authors intend to change the paper."
Rokossowsky,MachineLearning,1618739918.0,[D] In what order should one read self-driving related papers?,"Hi,

I've read a few books on DL and self driving. I'd like to delve into papers. I've found this list: [https://paperswithcode.com/task/autonomous-driving](https://paperswithcode.com/task/autonomous-driving) .

Should I just go through the top rated first or would you guy suggest some other, more logical order of understanding these things?

Thanks

Rokossowski"
kakushka123,MachineLearning,1618324760.0,[D] How is Tesla autopilot trained?,"I listened to this podcast, however I wasn't sure that I understood correctly. To my understanding, they have a big NN (does anyone know the architecture?) trained (from scratch before every release?) on a set of milions of image (each handchosen by humans?) that is evolving all the time, e.g images are taken off and taken on based on interaction of the model with the fleet of cars like humans corrections etc. Is that correct?"
Sirisian,MachineLearning,1619820302.0,[R] DINO and PAWS: Advancing the state of the art in computer vision with self-supervised Transformers,"https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training

https://arxiv.org/abs/2104.14294  
https://github.com/facebookresearch/dino  
https://arxiv.org/abs/2104.13963  
https://github.com/facebookresearch/suncet

The DINO research shows their ""model automatically learns class-specific features leading to unsupervised object segmentation.""

> PAWS is a method for semi-supervised learning that builds on the principles of self-supervised distance-metric learning. PAWS pre-trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled image are assigned similar pseudo-labels."
TheHentaiSama,MachineLearning,1620115041.0,[Discussion] Model to predict a class for a signal based on simpler signals," 

Hello everyone,

I'm working on something and I struggle finding a good solution. To make it simple, let's say I have an original signal which is composed of simpler signals. I can make predictions for these simpler signals using models such as Random Forest Classifier and now I need to make predictions for the original signal. The problem is that my signal can be composed of any number of simpler signals and they don't all have the same impact on the final result. I've tried the simple method which consists in a majority vote (making a prediction for each simple signal and then decide the class of the original signal by majority vote) but the results are bad.

So, my question is, does anyone of you know about a model that can handle such a task or do you know about some litterature that could help me ? Thanks in advance !"
schienal,MachineLearning,1618586669.0,[N] Probabilistic ML @ Tübingen is restarting!,"[https://www.youtube.com/watch?v=UbaVGD4Lfis&list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd](https://www.youtube.com/watch?v=UbaVGD4Lfis&list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)

Is anyone thinking of starting a discord server to study along at the course's pace?"
NominalNom,MachineLearning,1619809526.0,[D] A recent history of my pointless stare down with Nvidia which I lost,"Just a quick, comical summary of my experience trying to get a GPU since just prior to the launch of the Ampere cards.

I work in film post production and I have no prior deep learning coding experience, but I wanted a GPU with a lot of memory to process high res imagery with an existing TensorFlow-based open source toolset that I could just about figure out how to use with my small amount of Python experience. I did secure a Titan RTX off Amazon(!) at retail price, but I decided to return it when I found out about the imminent release of the Ampere cards because $1500 sounded a lot better than $2500.

Of course, the 3090 is going for around the same price as the 2.5k rrp I paid for the RTX Titan. I was very naive at that exact moment about the fact that crypto mining was going to go back into full swing, although I was warned by someone on this very sub. I did expect Eth proof of stake to cancel some of that out, but we aren't there yet.

Besides availability issues, there was also the size of the 3090 which Nvidia seem to have intentionaly made huge so it can't fit in a typical deep learning rig or graphics workstation. No problem, I'll get a third party blower card - if I can find one in stock. Whoops, now Nvidia killed them all!

Alright, now I'll look at one of those rinky-dink third party RGB gamer cards that are sufficiently low profile - I only need one card with 24GB VRAM. Okay, they are all sold out forever and if you can get one, it may be as expensive as the previous Titan.

So maybe it's best to get an A5000 or A6000, because tbh it seems they are more likely to go for RRP which makes them not a huge step up from the inflated 3090 prices and they are better supported on Linux IMO.

I have just found out about the LHR cards though, that will be shipping soon. But also that there may not be a 3090 LHR, so it doesn't seem like supply will necessarily improve.

There is now a proprietary GUI-based toolset by the same developers which is now using PyTorch as the back end. But it's still recommended that 24GB is where you really want to be at memory-wise without needing to slice up the input images too much, so this issue is not going anywhere. I had managed to get a 2060 Super with 8GB as an interim card which is really the bare minimum. It's also comical how much those cards are going for now, so I'm glad I snagged one of the last ones at Best Buy for $400 RRP last year."
vishnu_subramaniann,MachineLearning,1616496654.0,"[P] Jarvislabs.ai - An Affordable GPU Cloud with Fast launch, Pause and Resume. Scale GPUs post creation. A100/RTX6K/RTX5K","For the last few years, I have been learning and practicing Deep Learning. Participated in several Kaggle competitions and won few medals. During all these years, I tried several cloud platforms and on-premise systems. Some of them offered simplicity, flexibility, and affordability. But very few to none offered all of these in one platform.

After struggling with different platforms, I know what I would need as a DL researcher. That gave birth to [jarvislabs.ai](https://jarvislabs.ai) with the aim of being simple and affordable. I along with my friends started working on this project a year back. Due to Covid, executing the project became more challenging. As first-time entrepreneurs, we underestimated the complexity of the problem at hand but with persistence, we were able to launch a beta version of the product in December 2020.

With some of the amazing feedback from our early adopters, we have been able to make the product smoother. We would love to invite you all to come and try the platform.

# Features

1. 1 click Jupyter Lab < \[30 seconds\]
2. Pause the instance and Resume from where you left.
3. SSH to the instance.
4. Scale GPUs, storage and change GPU type on resume.
5. Auto-Pause using jarviscloud.pause() in your code, so you can catch up some good night’s sleep while your model trains.
6. Pay per usage – Minute Billing \[After first 15 minutes\]
7. Competitive pricing \[Lowest to our Knowledge\].

&#x200B;

## Pricing

|GPU Type|GPU RAM|Price -$/hr|
|:-|:-|:-|
|RTX 5000|16 GB|0.49|
|RTX 6000|24 GB|0.99|
|A100|40 GB|2.39|

&#x200B;

# Talk to us

We will be happy to assist you in spinning your first instance and many more. You can use one of these platforms to reach us.

1. Chat option on cloud.jarvislabs.ai
2. Email us - [hello@jarvislabs.ai](mailto:hello@jarvislabs.ai)
3. Comment here.

We have come a long way, but we understand that a lot more has to be done. We have listed down all the upcoming product features [here](https://github.com/jarvislabsai/JarvisCloud-ChitChat/discussions/). Deep learning and AI are evolving and how we would use the cloud platforms could evolve in the coming years. Understanding this, we develop in the open by constantly keeping in touch with our users.

Please help us in shaping [Jarvislabs.ai](https://Jarvislabs.ai) with any valuable suggestions/feedback."
olegranmo,MachineLearning,1618557871.0,[Research] Tsetlin Machine Interpretability and Accuracy in NLP Significantly Boosted Using GloVe Synonyms,"&#x200B;

[Tsetlin Machine Boolean Bag-of-Words Boosted by GloVe Synonyms](https://preview.redd.it/6js1wbb5lht61.png?width=1678&format=png&auto=webp&s=35d9f2bfe2abaad7af50bc5f337c8d633f68c053)

By using GloVe to obtain synonyms, we have enhanced the Boolean Bag-of-Words used by Tsetlin Machines in NLP. The added synonyms help the Tsetlin Machine produce fewer and semantically more powerful rules, while boosting accuracy by up to 4%. This makes the transparent and interpretable rules more competitive with deep learning-based NLP models.

[https://arxiv.org/abs/2104.06901](https://arxiv.org/abs/2104.06901)"
cents_less,MachineLearning,1618511530.0,[P] Announcing Feast 0.10: The simplest way to serve features in production,"Hey folks!

Over the last two years we’ve been working on an open source feature store called Feast. The idea behind Feast is that it helps you to operationalize your features. Meaning it helps you build training datasets from your offline features, it helps you load features into an online store in a structured way, and it provides low latency access to your features in production.

The original design of Feast was heavy weight. You needed to run a big stack on Kubernetes with Spark. We’ve been chatting to a bunch of users and the one thing we kept hearing was that they saw value in Feast for productionizing their data, but it was too heavy weight to own.

So we’ve put a lot of energy towards really simplifying Feast. We’ve made it as easy as a pip install. You can now run Feast locally if you’re just getting started with development/testing, but you can also deploy it to cloud providers like GCP if you want something more scalable.

We’d love for you to download it and try it out in your own projects. We’ve got a slack workspace you can join to get involved and ask questions. Also we wouldn’t hate a star on Github either 😉

\---

* Announcement: [https://feast.dev/blog/feast-0-10-announcement/](https://feast.dev/blog/feast-0-10-announcement/) 
* Web site: [https://feast.dev](https://feast.dev)
* Github: [https://github.com/feast-dev/feast](https://github.com/feast-dev/feast)
* Slack: [https://slack.feast.dev](https://slack.feast.dev)"
ottawalanguages,MachineLearning,1618970325.0,"[D] ""no free lunch"" vs neural networks","I read that there is a theorem called ""the no free lunch theorem"" that states : there is no single algorithm that is better than all other algorithms. This is somewhat obvious: complex algorithms should perform better on complex problems, and simpler algorithms should perform better on simpler problems. 

This being said, why have neural network based algorithms been accepted as the main type of algorithms for solving complex real-world problems? Apart from the simple answer, that they ""from observation, they simply perform better on these problems"" - if we were to look at neural networks (e.g. mlp, lstm, cnn), reffering to the mathematical properties of neural networks, how/what could we attribute their success to? Why do we use neural  networks instead of regression models? From a certain prespective, are neural networks defying the ""no free lunch theorem""?"
Andrew_the_giant,MachineLearning,1618067692.0,[D] SARIMAX - Achieving Stationarity first?," 

Newbie at forecasting here, and havn't been able to find the answer to this question as of yet.

This question is in relation to ARIMA type models only.

If I use a hyper-parameter grid search to find the best PDQ, pdq parameters do I need to difference the dataset first? Or by using the parameters the model will achieve stationarity (assuming I use the correct parameters)?"
pcaversaccio,MachineLearning,1617613213.0,[R] Dodrio: Exploring Transformer Models with Interactive Visualization,
DaBeastGeek,MachineLearning,1616594581.0,"[D] What are some non transformer based, NLP models?","Just curious if there’s any well regarded models out there that don’t rely on transformers. I’m looking for something preferably small parameter wise, to test on an edge device. 

Lots of the state of the art things I’m seeing are huge, even the smallest model, Albert has something like 30 million Params. 

Any help is very much appreciated!"
PM_ME_UR_FAV_THINGS,MachineLearning,1619476926.0,"[D] Open Source Nudity/NSFW Classification Review, any suggestions?","Hello!

I am doing a review of Open Source models for Nudity/NSFW content classification and/or labelling. Does anyone have recommendations for projects that are worth checking out? I have Google'd already but I was hoping you all might know of some of those that are more niche."
othotr,MachineLearning,1616651494.0,[R] Minimum-Distortion Embedding,
du_dt,MachineLearning,1618371964.0,[D] Internship at Huawei - your experience?,"Have you had an internship in Huawei (in particular, ML research positions)? What was your experience? Would you recommend it to a friend?

Huawei is actively expanding lately, in particular, in Canada they now are in top-20 by R&D spending ([link](https://www.newswire.ca/news-releases/huawei-canada-ranks-18th-overall-in-corporate-r-amp-d-spending-in-canada-841928676.html)), have seen a lot of intern positions and they are actively hiring.

What's your experience with Huawei?

Cheers!"
ykilcher,MachineLearning,1617114145.0,"[D] Machine Learning PhD Survival Guide 2021 (Video) | Advice on Topic Selection, Papers, Conferences & more! - by Yannic Kilcher","[https://youtu.be/rHQPBqMULXo](https://youtu.be/rHQPBqMULXo)

Hi Everyone. Let me know what you think of this!

This video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you're as clueless as I was when I started. The video is a personal recount of my mistakes and what I've learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that's exactly how I felt.

&#x200B;

Main Takeaways:

\- Select niche topics rather than hype topics

\- Write papers that can't be rejected

\- Don't be discouraged by bad reviews

\- Take reviewing & teaching seriously

\- Keep up your focus

\- Conferences are for networking

\- Internships are great opportunities

\- Team up with complementary skills

\- Don't work too hard

&#x200B;

OUTLINE:

0:00 - Intro & Overview

1:25 - Thesis Topic Selection

4:25 - How To Publish Papers

5:35 - Dealing With Reviewers

6:30 - How To Be A Reviewer

7:40 - Take Teaching Seriously

8:30 - Maintain Focus

10:20 - Navigating Conferences

12:40 - Internships

13:40 - Collaborations

14:55 - Don't Forget To Enjoy

&#x200B;

Transcript: [https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae](https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae)"
sobe86,MachineLearning,1617750723.0,[D] Samy Bengio resigns from Google,"Source: [Bloomberg](https://www.bloomberg.com/news/articles/2021-04-06/google-ai-research-manager-samy-bengio-resigns-in-email-to-staff) ([archive.fo link](https://archive.fo/yy9aI))

(N.B. Samy ≠ Yoshua Bengio, they are brothers). He co-founded Google Brain, and co-authored the original Torch library.

He was Timnit Gebru's manager during the drama at the end of last year. He did not directly reference this in his email today, but at the time [he voiced his support for her](https://www.facebook.com/story.php?story_fbid=3469738016467233&id=100002932057665), and shock at what had happened. In February, [the Ethical AI group was reshuffled, cutting Samy's responsibilities](https://twitter.com/alexhanna/status/1362476196693303297).

[Reuters reports](https://www.reuters.com/article/us-alphabet-google-research-bengio/google-ai-scientist-bengio-resigns-after-colleagues-firings-email-idUSKBN2BT2JT): *Though he did not mention the firings in his farewell note, they influenced his decision to resign, people familiar with the matter said, speaking on condition of anonymity.*"
KirillTheMunchKing,MachineLearning,1619877241.0,[D] An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale - Vision Transformers explained!,"# [An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale](https://t.me/casual_gan/33)

In this paper from late 2020 the authors propose a novel architecture that successfully applies transformers to the image classification task. The model is a transformer encoder that operates on flattened image patches. By pretraining on a very large image dataset the authors are able to show great results on a number of smaller datasets after finetuning the  classifier on top of the transformer model. [More details](https://t.me/casual_gan/33).

[ViT model architecture overview](https://preview.redd.it/45k93yszkiw61.png?width=1280&format=png&auto=webp&s=70e50a28c078fa594d695d3212b509ac774fbe5e)

[\[10 minute paper explanation\]](https://t.me/casual_gan/33) [\[Arxiv\]](https://arxiv.org/abs/2010.11929)"
mippie_moe,MachineLearning,1618432999.0,[D] Lambda GPU Benchmark Center for Deep Learning,"[GPU Benchmarks for Machine Learning](https://lambdalabs.com/gpu-benchmarks)

This is an ongoing project at Lambda. We will continue to add new GPUs and popular models as they're released.

Suggestions for new models or any feedback you have would be much appreciated!"
hotpot_ai,MachineLearning,1617318941.0,[R][D] CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation,
ancientmooner,MachineLearning,1616904933.0,[R] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,"Transformers make the previously saturated COCO and ADE20K benchmarks unblocked again. It creates a new SOTA on COCO and ADE20K:

[COCO det](https://paperswithcode.com/sota/object-detection-on-coco): 58.7 mAP (`+2.7 mAP`)

[COCO inst](https://paperswithcode.com/sota/instance-segmentation-on-coco): 51.1 mAP (`+2.6 mAP`)

[ADE seg](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k-val):  53.5 mIoU (`+3.2 mIoU`)

This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones.

[Arxiv](https://arxiv.org/pdf/2103.14030.pdf)   

[Code](https://github.com/microsoft/Swin-Transformer)"
clint9smith,MachineLearning,1616875481.0,[Discussion] How to create a team recommender when team sizes can change,"I'm trying to use a user-item-rating triple in a SVD recommender model.

User = Phase of a project

Item = Team

Rating = Team rating

The issue I'm trying to solve is how to recommend a team when the team sizes can change from phase to phase. I could recommend each role of a team separately, with the user-item-rating triple being role-person-person\_rating, but the best team isn't necessarily the best person for each role put together."
broutonlab,MachineLearning,1619605595.0,[D] How do you manage Data Science experiments?,"We have prepared an [overview](https://broutonlab.com/blog/data-science-experiments-management-with-weights-and-biases-platform) and a practical example ([google colab](https://colab.research.google.com/drive/1eF4Y14emyJeLxl3Q2rgwFP52RsIaQYKT?usp=sharing)) of how to use W&B platform to manage experiments.

What techniques and tools do you use to manage Data Science experiments in your work?"
petersonsass,MachineLearning,1618921963.0,[D] Why solving the vanishing gradients problem?,"It's often said that recurrent neural nets have the vanishing gradients problem. 

But to my understanding, that's what it should be right? The current hidden state should be less dependent on the distant hidden states.

What's wrong with this argument? Do we really want to have each hidden state's dependency equalized?"
legoonest,MachineLearning,1617351719.0,[P] VinDr Lab - an open-source annotation platform for Medical AI,"Department of Medical Imaging, VinBigdata has decided to release our DICOM annotation tool into the open-source. It's called **VinDr Lab**. It is a web-based tool that allows multiple annotators to work at the same time and remotely. This is the software that we've used to build the dataset for our [Kaggle competition](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/). Following the releasing of the large-scale dataset VinDr-CXR, this is our next contribution in data sharing as well as tools for AI development. We encourage the community to promote the data sharing and tools to drive AI research and development.  
Hope that somebody finds it useful. Enjoy and send your feedback! The tool is publicly available at  
[https://github.com/vinbigdata-medical/vindr-lab](https://github.com/vinbigdata-medical/vindr-lab)

[VinDr Lab DICOM Viewer](https://preview.redd.it/gye835b7vpq61.png?width=3836&format=png&auto=webp&s=2e1e425f996fc8fd747f09b9a3adba8a1948c6ef)"
retro_var,MachineLearning,1619635905.0,[D] Books or Articles in Tree Based Methods with Formal Mathematics?,"Hi. 

I'm searching books or articles explaining Tree Based Methods (Random Forest, Gradient Boost, etc.) with extensive mathematical theory in the background. 

But, the only similar book that i've found was [""Classification and Regression Tree"" by Brieman, Friedman (1984).](https://www.amazon.com/-/es/Leo-Breiman/dp/0412048418)

I will be glad if you could recommend new information.

Thanks."
muzammal-naseer,MachineLearning,1617009271.0,[R] On Generating Transferable Targeted Perturbations,"We study and strengthen target perturbations otherwise features of neural networks in the context of transferability of these patterns from one model to another. We broaden the definition of black-box information available to the attacker by analyzing transferability to unknow target model to unknow training mechanism to unknown decision space or unknown input processing.

[https://arxiv.org/abs/2103.14641](https://arxiv.org/abs/2103.14641)

All pretrained generators are available here:

[https://github.com/Muzammal-Naseer/TTP](https://github.com/Muzammal-Naseer/TTP)

We are also tracking progress of transferable targeted attacks within easy to follow setting. Results will be updated within few days.

[https://github.com/Muzammal-Naseer/TTP#Tracking-SOTA-Targeted-Transferability](https://github.com/Muzammal-Naseer/TTP#Tracking-SOTA-Targeted-Transferability)"
Yuqing7,MachineLearning,1620402702.0,[R] MIT & IBM 'Curiosity' Framework Explores Embodied Environments to Learn Task-Agnostic Visual Representations,"A research team from MIT and MIT-IBM Watson AI Lab proposes Curious Representation Learning (CRL), a framework that learns to understand the surrounding environment by training a reinforcement learning (RL) agent to maximize the error of a representation learner to gain an incentive to explore the environment.

Here is a quick read: [MIT & IBM 'Curiosity' Framework Explores Embodied Environments to Learn Task-Agnostic Visual Representations.](https://syncedreview.com/2021/05/07/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-14/)

 The paper *Curious Representation Learning for Embodied Intelligence* is on [arXiv](https://arxiv.org/pdf/2105.01060.pdf)."
pcaversaccio,MachineLearning,1617532776.0,[R] Labels4Free: Unsupervised Segmentation using StyleGAN,
TheRealMarqupe,MachineLearning,1620562851.0,CNNs Color Invariance [Discussion],"We want to detect a specific object that can have any color, for example, a car. If we train our model with images containing only black and gray cars, will the performance of our model's predictions be worse for images containing cars with different colors than the ones used on training? For example, will our model fail to classify correctly if a car is present or not on an image, for images containing yellow cars?   

If so, what is the best way to achieve color invariance? (explanations with examples and credible sources references would be much appreciated)

Thanks!"
prathameshpck,MachineLearning,1619130146.0,Is fine tuning twice a viable thing to do?? [D],"Don't know if this is the right place to ask this but here goes 

Say I have a dataset with very few samples but i manage to find another dataset of something similar but which is larger 


Would it make sense to finetune twice ? i.e first on the second larger dataset, and then fine tune again on the smaller one?"
dDW5ia5j,MachineLearning,1617178321.0,"[P] Generic template to bootstrap your PyTorch project with PyTorch Lightning, Hydra, W&B, DVC, and Streamlit","Link: https://github.com/lucmos/nn-template

Generic opinionated template to bootstrap your PyTorch project, avoid writing boilerplate code for:  

- **PyTorch Lightning**, lightweight PyTorch wrapper for high-performance AI research. 
- **Hydra**, a framework for elegantly configuring complex applications. 
- **DVC**, track large files, directories, or ML models. Think ""Git for data"". 
- **Weights and Biases**, organize and analyze machine learning experiments. 
- **Streamlit**, turns data scripts into shareable web apps in minutes."
NumiAI,MachineLearning,1616346275.0,[D] Barlow Twins: SSL via Redundancy Reduction,"Paper: [https://arxiv.org/pdf/2103.03230v1.pdf](https://arxiv.org/pdf/2103.03230v1.pdf)

Authors: Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny

This recently published work on self-supervised learning is based on a simple idea yet yield amazing results. I liked reading the paper and got interested to read more about Barlow's redundancy concept from neuroscience.

An interesting fact which I came across is that Barlow himself in a publication back in 2000 revisited his opinion about the redundancy reduction.

&#x200B;

https://preview.redd.it/sueixtaiweo61.jpg?width=1640&format=pjpg&auto=webp&s=632575dfce0c1bfd74ca8797a8c3987f06a9fc2c

&#x200B;

https://preview.redd.it/tc8j3xpvweo61.jpg?width=1640&format=pjpg&auto=webp&s=73fe7ab49e6ab0636968bc326868f4db929d4117

The authors in the paper actually supported the idea of redundancy reduction but as I understand from Barlow's own paper the redundancy increase, not decrease. 

I would like to hear your opinion on this apparent conflict in my understanding. Thanks"
l_atze_l,MachineLearning,1620032568.0,[P] MoViNet in PyTorch,"TL;DR: Implemented MoViNet in PyTorch: [https://github.com/Atze00/MoViNet-pytorch](https://github.com/Atze00/MoViNet-pytorch)

I'm currently working on video recognition, I've found [MoViNets](https://arxiv.org/abs/2103.11511) to be a quite interesting paper, so I decided to implement it on PyTorch, since the code will be released in TF by the authors.

I  will be using this architecture myself, once the code and the weights in TF will be released, so bug reports or comments are very useful to me. Let me know if you find incoherence between paper and implementation as well!

I hope this is useful for someone."
jj4646,MachineLearning,1619062368.0,"[D] decline of traditional ""state space models""","It seems that Recurrent Neural Networks have overtaken traditional ""state space models"" for time series models. Is this because traditional state space models require the analyst to make certain assumptions about how the system transitions between different states - whereas a recurrent neural  network can consider a wide combination of states through hidden layers and deep architecture?"
igorsusmelj,MachineLearning,1616493351.0,[P] Release of lightly 1.1.3 - A python library for self-supervised learning,"We just released a new version of lightly (https://github.com/lightly-ai/lightly) and after the valuable feedback from this subreddit, we thought some of you might be interested in the updates. 

Lightly now supports more models: In addition to SimCLR and MoCo, we have added SimSiam and Barlow Twins (a big thank you to our open-source contributors!). More models, such as BYOL and SwAV are in the pipeline.

We did some benchmarking (https://docs.lightly.ai/getting_started/benchmarks.html) on cifar10 and show the various frameworks in action using different training epochs and batch sizes.
Most models run well on multi-GPU setups using PyTorch Lightning in distributed data-parallel settings. 

We are curious to hear your feedback."
chasep255,MachineLearning,1617563283.0,[D] Why not use momentum based optimizer with WGAN?,I keep reading that I should not use any momentum when optimizing a WGAN.  Nothing I have read offers an explanation as to why.  So why shouldn't I use a momentum based optimizer?  Anyone know the reason?
EscapedLaughter,MachineLearning,1620289418.0,[N] Music Demixing (Audio Source Separation) Competition by Sony | ISMIR 2021,"The [competition](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021?utm_source=reddit&utm_medium=ml&utm_campaign=sony) features 2 baselines:

Open-Unmix: [Code](https://github.com/sigsep/open-unmix-pytorch) | Paper: [https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf](https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf)

CrossNet-UMX: [Code](https://github.com/sony/ai-research-code/tree/master/x-umx) | Paper: [https://arxiv.org/pdf/2010.04228.pdf](https://arxiv.org/pdf/2010.04228.pdf)

The competition can hopefully serve as a benchmark for various source separation models. Participants can get together during the ISMIR Workshop and share their learnings. Sony is also offering 10,000 CHF cash prize for top participants on the leaderboard.

\------------------------------------

[Example](https://d2cowzs755i94n.cloudfront.net/) of music demixing:

https://reddit.com/link/n62hli/video/t8orzh8rmgx61/player

PS: This is my first post on the subreddit, so if I have transgressed any norms, I do apologize."
Hi_I_am_Desmond,MachineLearning,1620404887.0,"[D] In Quantum NNs, are the hybrid optimization methods run on classical hardware?","I have been reading many resources and since I don’t know anyone in those field that I am starting to research, I hope here I can find someone that works in quantum machine learning. While there are many approaches, considering the most common based on “Quantum circuit learning” by Mitarai et al., do they suppose that we use an hybrid hardware where optimization is computed on classical hardware? What are some example with only quantum hardware? Can you link me a paper that makes a clear distinction on the hardware that runs the QNNs?"
PK_thundr,MachineLearning,1619199060.0,[D] How to develop a compelling argument for a paper submission?,"I've been working on a certain loss function for training  a DNN for a while, unfortunately the results have been mediocre (not SoTA, or just matching SoTA).

How do you go about the process of developing a compelling argument from a paper from a simple set of experiments and accuracy results that are incremental progress on what's already been done in the field? 

I realize that this will depend greatly on what you're actually working on, but is there any kind of general guiding principles here?"
CoolThingsOnTop,MachineLearning,1617471222.0,[D][R] Blog Post: Why Are Kronecker Products So Effective?,"This week, the list of ICLR 2021's Outstanding Papers was announced. One of the awarded papers was "" [**Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with** **1/n1/n** **Parameters**](https://openreview.net/forum?id=rcQdycl0zyk)"", which makes use of the Kronecker Product to build a new kind of NN layer.

I have been looking into Kronecker Products for a while now, and it was a pleasant surprise to find this paper on the list of winners. So I decided to write a blog post about it and point out an interesting connection with previous work. Let me know what you think!

Blog post: [https://santiag0m.github.io/blog/2021/04/02/why-are-kronecker-products-so-effective.html](https://santiag0m.github.io/blog/2021/04/02/why-are-kronecker-products-so-effective.html)"
antiquark2,MachineLearning,1618348607.0,"[D] Could this network be used to generate the most attractive image possible? What would it look like... -""ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation Networks""",
No_Effective7572,MachineLearning,1619626129.0,[P] Semi Supervised Segmentation on Graphs using Eikonal Equation with PyOpenCl backend.,"Hey guys,

I would like to share this project with you. [https://github.com/aGIToz/semi-supervised-segmentation-on-graphs](https://github.com/aGIToz/semi-supervised-segmentation-on-graphs)  .  It does segmentation on graphs. Its application on images and pointclouds.

It runs a time-dependent eikonal equation on graphs. Eikonal equation can be used to create generalized distances on manifolds. Classically the equation is solved using fast-marching methods. A time-dependent eikonal equation at steady coincides with fast-marching solution.

Basically I am creating a knn graph with gpu backend using pyopencl and then I run the PDE with gpu backend.

I hope you find it interesting !"
shuvob4,MachineLearning,1618401627.0,[R] Drug Target Interaction research using Machine Learning,"I am a computer science graduate with an aim to pursue my master's in machine learning. As I have no thesis in my undergraduate, I would like to know some information about doing research, I hope to find some guidance from this post.

I am highly interested in healthcare and machine learning. After scouring through the internet, I have found Drug-Target Interaction as a topic of my interest. But after digging a little deeper, I found myself in the middle of the sea with no view of the land. I have read some papers; I found I don't understand most of the things described in the field. I don't have the domain knowledge. Should I seek another topic (If so, can someone guide me with some topics which I can look for) or should I read some more papers so that I can understand the topic better?

Thanks in advance and have a nice day."
hardmaru,MachineLearning,1617802883.0,[R] Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,"Recent paper from FAIR published in [PNAS](https://www.pnas.org/content/118/15/e2016239118). They find that biological structure and function emerge in representations of language models trained on massive databases of protein sequences.

*Summary*

Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.

*Abstract*

In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.

Paper: https://www.pnas.org/content/118/15/e2016239118"
Artistic-Mushroom974,MachineLearning,1617240597.0,[d] Robotics/ AI PhD Salary Expectations for T1.5 School," I'm a junior (first few years) PhD student in Robotics/AI at GTech (CS Rankings for AI/Robotics puts GTech at #6 in the US [http://csrankings.org/#/index?ai&vision&mlmining&nlp&ir&robotics&us](http://csrankings.org/#/index?ai&vision&mlmining&nlp&ir&robotics&us)) . I've seen posts about 250-300k total comp post ML/AI PhD's at big tech or in fintech, but I've also seen some caveats like only for top 4 (MIT/Stanford/CMU/Berkeley) school PhDs. ([https://www.reddit.com/r/MachineLearning/comments/bwwmh9/d\_doing\_a\_phd\_is\_not\_worth\_it\_unless\_exception/](https://www.reddit.com/r/MachineLearning/comments/bwwmh9/d_doing_a_phd_is_not_worth_it_unless_exception/))

So my question is, what is a realistic salary expectations for the next couple schools after the top 4? Are they that much worse than ""top 4?"" Do I still have good prospects of a 250-300k offer (what would be a low, mid, and high range)? Also, what are my prospects if I were to take the Master's rather than finish the full degree?

For context, many labs at my school have a fair (but not outstanding) amount of top conference publications (ICLR,NeurIPS,ICML, CoRL, ICRA).

And no, money is not the only reason I chose this route (I have a lot of curiosity as to the possibilities of AI), but it is demanding and difficult work."
_Arsenie_Boca_,MachineLearning,1620455301.0,[D] Worth learning JAX?,"Recently, more popular papers as ViT start using JAX to implement their models. I mainly use PyTorch but in the past I was playing with the thought of learning tensorflow. I always felt like tensorflow has a lot of legacy code and conventions that make it kinda unneccessarily complicated compared to pytorch.

1. Now that google seems to shift to JAX internally, do you think JAX will replace tensorflow? 
2. And regardless of that, is it worth learning JAX? 
3. If you would learn JAX, which higher-level api  (comparable to torch.nn) would you use like stax, flax, haiku?"
memgamemotron,MachineLearning,1617048073.0,[P] Need help figuring out how to print out the sentence and sentence predictions from my google trax neural network model.,"so to begin with, this is a project I am helping my school with to gain experience.  I have taken plenty of [deeplearning.AI](https://deeplearning.AI) courses which helped me build this codes foundation. 

The data is from a chat bot which provides the chat text, ANGRY labels and HAPPY labels which were going to feed into the model.

https://preview.redd.it/iyo9sgfgo0q61.png?width=1752&format=png&auto=webp&s=53291557217d14836af90988b88bee956ef0f694

Next up is the data preprocessing which includes splitting the data 80/20 for training and evaluation.

&#x200B;

https://preview.redd.it/qk2d4i3wo0q61.png?width=1464&format=png&auto=webp&s=029037b1e9c735cba44b0f291978247f8aac865e

The I preprocess the text by removing special characters, spaces, stop words, and tokenizing text like so:

&#x200B;

https://preview.redd.it/ky1f9nruq0q61.png?width=1436&format=png&auto=webp&s=f1fa6bac026a04acd2e283e47525535f7091374c

Next up I build the vocab list like so:

&#x200B;

https://preview.redd.it/q6fnx8b2r0q61.png?width=1218&format=png&auto=webp&s=9adde66dd739fd5fa02e4b25594849ab81133c44

Then create padded tensors for each sentence:

&#x200B;

https://preview.redd.it/8m66va9vr0q61.png?width=1310&format=png&auto=webp&s=d9b676fe49968da93e081de3dc57fbb54da5d97c

Afterwards create the data generator that we can apply to training and evaluation.

&#x200B;

https://preview.redd.it/1mzl9ytis0q61.png?width=1304&format=png&auto=webp&s=117069b46458f358a315719c556d7e163f58ee64

&#x200B;

https://preview.redd.it/196f3m1ks0q61.png?width=1404&format=png&auto=webp&s=7a387441463aa73e9e60e7eae240fc0de85d73c7

&#x200B;

https://preview.redd.it/xk8u8rrls0q61.png?width=1352&format=png&auto=webp&s=d2b41660c8906f5508201bf503e1f15f968f3a89

&#x200B;

https://preview.redd.it/thqav2its0q61.png?width=1746&format=png&auto=webp&s=443f28186ea9151fafd4555dea564ed16a28bd41

&#x200B;

https://preview.redd.it/w64d88yxs0q61.png?width=1378&format=png&auto=webp&s=73150cdb3582dbfdf51f70adf3a2471768c5ae3f

I then start to create the layers by forward propagation from one layer to the next neighing with a relu activation function

&#x200B;

https://preview.redd.it/hb9czcgft0q61.png?width=1124&format=png&auto=webp&s=5edfb4009a5b6fe9ca36e4ca89db981e0e71ecbf

Defining the Dense Layer:

&#x200B;

https://preview.redd.it/rfysjt6pt0q61.png?width=1336&format=png&auto=webp&s=866f725320c859aefdea53bec957cf82b5897fbd

&#x200B;

https://preview.redd.it/mmksp94qt0q61.png?width=1364&format=png&auto=webp&s=66c70f753403d9954db2b2e876242ae4bae627f3

The next step we will define our model by using Trax and combining all our layers!

&#x200B;

https://preview.redd.it/7g2c91acu0q61.png?width=1256&format=png&auto=webp&s=a496b0432aecef0e486097de82af499b441426ef

&#x200B;

https://preview.redd.it/5swtlc6du0q61.png?width=1584&format=png&auto=webp&s=5f7c79f36b8418dcfe9fac2bd771990bdefeb5ab

I have created training and evaluation loops that will help us understand how our model is performing

&#x200B;

https://preview.redd.it/p1p9ghyiu0q61.png?width=1118&format=png&auto=webp&s=5fe436a51c2d448bf2d7abd03a8cad9d387fe3c2

**Next up we will check out the models accuracy, I know its pretty low but currently a learning student looking to improve and excited on some feedback to help improve this model.** 

&#x200B;

https://preview.redd.it/9xcgv6g6v0q61.png?width=1274&format=png&auto=webp&s=3e6ab61fb41a45e051f589542bf7a7b74b3e8ed3

&#x200B;

https://preview.redd.it/g4hqesz7v0q61.png?width=1882&format=png&auto=webp&s=5ea796575a5b79cf1a384304e63f5c153db5d059

As you can see this model is performing at 67%, The following is the code that I need help with. I am trying to print the observed sentence followed by the prediction. I have tried to do it but am not getting 2 output prediction for ANGRY and HAPPY labels. 

The purpose is to help understand if a customer is either angry or happy with the bot customer experience and also identify if interference is needed from an agent. 

&#x200B;

https://preview.redd.it/kajgcwtwv0q61.png?width=1342&format=png&auto=webp&s=a3b5136cd06daa6c564256918e461c4c74bb3759

Instead of getting 2 predicting labels this is what my output looks like. 

**I would greatly appreciate any help and welcome any ML mentors for this project**.

&#x200B;

Thanks for looking everyone, and can't wait to hear from you all!"
weifz,MachineLearning,1617781633.0,[D] How to define Pearl's causality in time series ?,"Hi all, recently I'm studying the Granger causality and Pearl's causality, and I wonder if there is a way to define Pearl's causality in time series, and what is the relationship between Granger's and Pearl's ?(not differences). Thanks a lot !"
PytonRzeczny,MachineLearning,1616604232.0,[D] Wasserstein GAN math,"Hi.

Could You recommend me some resources to learn about math included in WGAN paper, because when i read it i don't think i understand more than the title of this paper.

I want to implement this, but without knowing what's going on with loss function it's quite useless to me."
AerysSk,MachineLearning,1619271507.0,"[D] ML researchers of Reddit, what qualifications do you seek in a research applicant that you have not met?","I am a third year undergraduate CS student. Recently, I apply to a research training role (similar to Google Brain AI Residency) in a company that they require ""strong math, programming skill and adequate English communication skill"" (my country's major language is not English). To my surprise, my interviewer is a Postdoc researcher at a top US University (which I will not disclose), and it does not seem like he/she is working for the company. The 1 hour interview is scheduled next week, and we have not met in real life.

Dear ML researchers, what qualifications do you seek in these applicants? What questions do you ask and what do you expect the answers to be?"
bendee983,MachineLearning,1620405956.0,[R] AttendSeg: super-compact NN for semantic segmentation for edge devices,"A new neural network developed by researchers at DarwinAI and the University of Waterloo makes it possible to run semantic segmentation on resource-constrained edge devices.

Key highlights:

\- the model has 1.19 million params (in comparison, RefineNet has 85M params). Precision has been reduced to 8 bits without significant compromise in accuracy, which reduces the size of the model to 1.19MB (RefineNet is \~340MB)

\- AttendSeg uses attention condensers to provide compact self-attention

\- The model was generated using ""Generative Synthesis,"" an ML technique that explores the constraint space provided by the engineers (accuracy, model size, etc.) and creates the best model

AttendSeg will be presented at CVPR in June.

Read story with interview with U of Waterloo professor and co-author Alex Wong:

[https://bdtechtalks.com/2021/05/07/attendseg-deep-learning-edge-semantic-segmentation/](https://bdtechtalks.com/2021/05/07/attendseg-deep-learning-edge-semantic-segmentation/)

Full paper here:

[https://arxiv.org/abs/2104.14623](https://arxiv.org/abs/2104.14623)"
AleksanderPet,MachineLearning,1616496675.0,[R] MDMMT: Multidomain Multimodal Transformer for Video Retrieval,"The research team from Lomonosov Moscow State University in cooperation with Intelligent systems and data science lab of Moscow Huawei R&D Center developed a novel multi-modal multi-domain text2video system that outperforms current state-of-the-art results (e.g. from Google, Facebook) on widely used public datasets (MSR-VTT and LSMDC) for the video retrieval task.

Paper: [https://arxiv.org/abs/2103.10699](https://arxiv.org/abs/2103.10699)

Comparison on paperwithcode with others: [MSR-VTT](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt), [MSR-VTT \[1kA split\]](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka), [LSMDC](https://paperswithcode.com/sota/video-retrieval-on-lsmdc).

**Update** on 17th fo April, 2021: we shared the model, the testing scripts (providing the numbers in the paper) and the refined lists (removed train/test intersection) for the scientific community. Please refer to our [github](https://github.com/papermsucode/mdmmt)."
vector_machines,MachineLearning,1617491149.0,[D] Resources for crowdsourcing MCQ based dataset,"I want to crowdsource a dataset which is in MCQ format. Given a sentence, users are required to pick the right option out of 4. I explored mechanical turk and it seems they don't have a template for that, all NLP ones are generic and require hardcoded labels. This means I will have to create a custom one. From last Emnlp, I recall crowdAQ, which is a bit closed form. I wanted to learn more about cost efficient crowdsourcing options if you know any, don't want to spend a lot of time reading docs about APIs. Any suggestions, advice, experience will be appreciated."
argh_usernametaken,MachineLearning,1618393674.0,[Discussion] which NN architecture is best suitable for analysing the structural data of biomolecules,I'm trying to extract meaning out of structural data of biomolecules like protein. Which ML method can be applied on this type of data?
WigglyHypersurface,MachineLearning,1620230052.0,[D] Making sense of this autoencoder archetecture,"I'm looking to implement the architecture from [this paper](https://arxiv.org/pdf/1806.02382.pdf) for imputing some missing data. I'm confused on a couple of points about the choices made. 

Rather than an encoder/decoder setup, this autoencoder adds an extra generator network. I'm having some trouble parsing why this is necessary or desirable though. Also, I'm confused about what the input to the generator is. Is the input just noise and the mask? Also, how the inputs to the encoder are handled when they are missing, i.e. do they just replace the NAs with zero?

If I'm understanding correctly, in the missing data imputation case the mask is just used by the loss function to zero out the log-likelihood values for the data that are missing. Is that correct?

It seems like it would be simpler to have a standard encoder/decoder setup, and just initialize by filling in the missing data with random values, then resample a prediction for each missing value at the end of each epoch and keep training, with the log-likelihood for the missing values masked out. That way you wouldn't need the mask as an additional input to the encoder/generator."
techsucker,MachineLearning,1617727196.0,[N] Facebook AI Introduces A New Self-Supervised Learning Framework For Model Selection And Hyperparameter Tuning For Large-Scale Forecasting,"Researchers at Facebook AI have recently released a new self-supervised learning framework for model selection (SSL-MS) and hyperparameter tuning (SSL-HPT), which provides accurate forecasts with less computational time and resources. The SSL-HPT algorithm estimates hyperparameters 6-20x faster when compared with baseline (search-based) algorithms, producing accurate forecasting results in numerous applications.

At present, Forecasting is one of the significant data science and machine learning tasks performed. Therefore, it is crucial to have fast, reliable, and accurate forecasting results with large amounts of time series data for managing various businesses. 

Time series analysis is used to find trends and forecast future values. A slight difference in hyperparameters in this type of analysis could lead to very different forecast results for a given model and have serious consequences. Therefore, it’s essential to select optimal hyperparameter values. 

Summary: [https://www.marktechpost.com/2021/04/06/facebook-ai-introduces-a-new-self-supervised-learning-framework-for-model-selection-and-hyperparameter-tuning-for-large-scale-forecasting/](https://www.marktechpost.com/2021/04/06/facebook-ai-introduces-a-new-self-supervised-learning-framework-for-model-selection-and-hyperparameter-tuning-for-large-scale-forecasting/) 

Paper: [https://arxiv.org/abs/2102.05740](https://arxiv.org/abs/2102.05740)?

Facebook Source: [https://ai.facebook.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/](https://ai.facebook.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/)"
Lunavahid,MachineLearning,1620639107.0,[Vault] [Discussion] Is Human in the Loop the New Thing?,"  

&#x200B;

Recent discussion around AI and ethics goes all the way to one concept. Do we need humans in the AI pipeline loop or not? Even some CEOs declared they are worried about the fact that AI can push people perception, judgment and eventually make them biased. 

So the ultimate question is doing we need humans in the loop or not. Do we need them to surprise algorithms, analyses outcomes and results, check for hidden biases and find potential pitfalls? (and even to find those patterns we might use another AI system). 

New frontiers in AI need more data to perform better. More data and better algorithms are the recipe of success of several new achievements. 

Let's look at one application that we are familiar with. Annotation and tagging. 

Image Annotation has never been this accessible. There are billions of images, texts, audio and videos. Annotation and tagging provide a solid learning background for AIA systems to learn. 20 year ago the challenge was to recognize an apple, or the number 8 or the letter ""Z"". The new challenge is to identify a Spanish house on a sunny day next to a lake. 

The more sophistication needed form users means more sophisticated annotation and tagging for learning purposes. More sophistication means more labels and annotation and for that reason the need to the human in the loop is increasing."
BrettNMartensen,MachineLearning,1618610104.0,[D] Recency versus Frequency in Prediction,"A neural net can be built to just keep track of the most recent next input symbol in a sequence and use that to predict what the next symbol might be given a familiar sequence. That is, expect it to be what happened the last time.

Or it could be built to keep track of how frequently all of the next symbols that have occurred. That is build up a probability distribution of next stimuli based on a count of them being experienced. Then use the highest probable next stimulus as the prediction.

Does anyone know of any papers that have been published that compare the success rate of these two approaches i.e. Recency versus Frequency for prediction purposes?"
janimezzz,MachineLearning,1617183219.0,[N] Deep learning method for generating proteins will speed up drug development,"Researchers at Chalmers University of Technology, Sweden, present a way to generate synthetic proteins using generative deep learning. The new approach has huge potential for developing efficient industrial enzymes as well as new protein-based medicine, such as antibodies and vaccines.

Read more here: [https://news.cision.com/chalmers/r/unique-ai-method-for-generating-proteins-will-speed-up-drug-development,c3316116](https://news.cision.com/chalmers/r/unique-ai-method-for-generating-proteins-will-speed-up-drug-development,c3316116)

Read the article [**“Expanding functional protein sequence spaces using generative adversarial networks""​**](https://doi.org/10.1038/s42256-021-00310-5) in Nature Machine Intelligence. "
Candid-Wishbone-692,MachineLearning,1619708153.0,[N] Call for Teachable NLP Challenge,"Hi all,

I found this exciting Teachable NLP Challenge and looking for people who want to participate with me!

Is there anyone who wants to participate? All levels in NLP are welcome.

\----------------------------------------------------------------------------------------------------------------------------------

Teachable NLP Challenge is free and open to everyone interested in training their own AI. All you need to be prepared for is good ideas and datasets.

* When: 05/05/2021 - 05/18/2021 11:59 EDT
* How: You just need to submit your AI model link and explanations on your AI (Good example: [https://forum.ainetwork.ai/c/ai-showcase/11](https://forum.ainetwork.ai/c/ai-showcase/11))
* Prizes: Apple Store gift cards, Winners' interviews will be broadcasted through AI Network Youtube Channel(1.48K subscribers)

To participate, submit your info via [https://forms.gle/XfUuNSS2heAn7JtH7](https://forms.gle/XfUuNSS2heAn7JtH7). You will receive an invitation email!

Check how Teachable NLP works: [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65)Or watch a 1-minute tutorial video: [https://youtu.be/hzujZOT1qz8](https://youtu.be/hzujZOT1qz8)"
fernauata,MachineLearning,1620097391.0,[R] House-GAN++: A breakthrough in automated floorplan design via Relational Refinement GAN,"House-GAN++: A breakthrough in automated floorplan design via Relational Refinement GAN w/ convolutional message passing. Joint work with Autodesk Research to appear in  [\#CVPR2021](https://www.facebook.com/hashtag/cvpr2021?__eep__=6&__cft__[0]=AZWYm7n5kJ6SP6qGoFQgTlDL5415dsNn8Z1UngESW8QHjiiKhFKKBnChx5eFUSRxJSmjIBAxsJnLpumzg7kup5su61JcohsFx2GuvAYxJSlkCqdYVNhPiVzpLqitwpcURVXCOop6KRzQusJwdxaedYvR&__tn__=*NK-R).

[The video shows our interactive browser demo. ](https://reddit.com/link/n4ejlo/video/z9e22ww0r0x61/player)

ArXiv: [https://arxiv.org/abs/2103.02574](https://arxiv.org/abs/2103.02574)

You can play around here [http://www.houseganpp.com/](http://www.houseganpp.com/?fbclid=IwAR1YnbsZDA-sSRL_AWSuof1llJH4n9uggg3g-u8S_xbv9tZEQgbq9cwA4Z4)

Project website: [https://ennauata.github.io/houseganpp/page.html](https://ennauata.github.io/houseganpp/page.html?fbclid=IwAR3s6cYsrvWxN5CGGA9T10GRLBsRybTbt6VlI7mbqA-3eXnX2x2TxaUijUA)"
ykilcher,MachineLearning,1618408423.0,[P] Video: I built a Neural Network in Minecraft | Analog Redstone Network w/ Backprop & Optimizer (NO MODS),"[https://youtu.be/7OdhtAiPfWY](https://youtu.be/7OdhtAiPfWY)

I built an analog neural network in vanilla Minecraft without any mods or command blocks. The network uses Redstone wire power strengths to carry the signal through one hidden layer, including nonlinearities, and then do automatic backpropagation and even weight updates.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

1:50 - Redstone Components Explained

5:00 - Analog Multiplication in Redstone

7:00 - Gradient Descent for Square Root Computation

9:35 - Neural Network Demonstration

10:45 - Network Schema Explained

18:35 - The Network Learns a Datapoint

20:20 - Outro & Conclusion

&#x200B;

I built this during a series of live streams and want to thank everyone who helped me and cheered for me in the chat!

&#x200B;

World saves here: [https://github.com/yk/minecraft-neural-network](https://github.com/yk/minecraft-neural-network)

Game here: [https://www.minecraft.net](https://www.minecraft.net)

Multiplier Inspiration: [https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag](https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag)"
human_treadstone,MachineLearning,1616807524.0,[D] Choosing correct baseline model for Metric learning,"I am using Matching and Prototypical network to perform Single shot learning(SSD) on my own dataset. But before this I want to implement the baseline to keep track of improvement. I am thinking about 1-Nearest neighbour as baseline. Is this a good baseline? I could not find any such GitHub implementation of 1-NN for SSD, in case already such baseline implemeted please point me out to such resources."
Yuqing7,MachineLearning,1619711797.0,[R] Toward a New Generation of Neuromorphic Computing: IBM & ETH Zurich's Biologically Inspired Optimizer Boosts FCNN and SNN Training,"IBM and ETH Zurich researchers make progress in reconciling neurophysiological insights with machine intelligence, proposing a novel biologically inspired optimizer for artificial (ANNs) and spiking neural networks (SNNs) that incorporates synaptic integration principles from biology. GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals) leads to improvements in the training time convergence, accuracy and scalability of ANNs and SNNs. 

Here is a quick read: [Toward a New Generation of Neuromorphic Computing: IBM & ETH Zurich's Biologically Inspired Optimizer Boosts FCNN and SNN Training.](https://syncedreview.com/2021/04/29/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-8/)

 The paper *Learning in Deep Neural Networks Using a Biologically Inspired Optimizer* is on [arXiv](https://arxiv.org/pdf/2104.11604.pdf)."
SuperFX,MachineLearning,1618524699.0,[D] A100 vs. A6000 for sequence learning tasks?,"Does anyone have first-hand experience with the performance differences between Nvidia's A100 and A6000 GPUs for large sequence tasks, e.g. using Transformers? I'm aware of the benchmarks that Lambda has posted but was wondering if research groups with hands-on experience using real models have done some benchmarking? The Lambda numbers suggest something on the order of 50% speed gain, for a price differential of 2.5x or so (I think--if people have data on wholesale prices that would be useful too), which is not a great deal, but not terrible. I'm wondering if this holds up in practice as I'd like to assemble a GPU server using one of these GPUs."
_sshin_,MachineLearning,1617440536.0,[D] Short blogpost about EfficientNetV2,[https://mlpaper.com/2021/04/03/review-efficientnetv2-smaller-models-and-faster-training/](https://mlpaper.com/2021/04/03/review-efficientnetv2-smaller-models-and-faster-training/)
Transit-Strike,MachineLearning,1619093209.0,[D] Are we underestimating how important studying theory is?,"I hear a lot of discussions where people essentially say ""Employers and Universities only care if you have good projects that prove you know what you learned. Courses mean nothing.""

and ""Just go to Kaggle and find a cool project to work on.""  


While that is technically true, are we really underselling the importance of theory?  


  
I could open up a Kaggle repo, see that everyone uses LeakyReLU, Batch Norm and ADAM. So I would be tempted to look at youtube videos and kaggle submissions and just recreate the same output. But does it really hold weight? I don't think so.  


I tried something similar and managed to build a decent solution that would later be a github repo. 

&#x200B;

The logic really fails when you try building something bigger and better though. If you have a styleGAN for example, you have to understand what the optimisation process is like. You need to know what people use batch norm for etc etc. It is impossible to understand the code otherwise.

&#x200B;

And even then, with very similar projects, I found myself able to apprach it 100x times better after doing a simple [deeplearning.ai](https://deeplearning.ai) course.

&#x200B;

Sure, you could cheat your way to a project that sounds sexy to a novice that knows nothing about ML. But have you learned anything if you use tf.nn everywhere and find usable results? I am not that sure really."
OverLordGoldDragon,MachineLearning,1620238337.0,[P] Fastest wavelet transforms in Python + synchrosqueezing,"[ssqueezepy 0.6.1](https://github.com/OverLordGoldDragon/ssqueezepy) released w/ benchmarks, CWT up to x75 faster than PyWavelets on CPU and x900 on GPU (and more correct). STFT also CPU- and GPU-accelerated, and both synchrosqueezed.

Also see [Kymatio](https://github.com/kymatio/kymatio) for SOTA on timeseries with limited data, fast and differentiable; [nice lecture](https://youtu.be/4eyUReyIPXg)."
nivter,MachineLearning,1619874458.0,[P] I created a series of YouTube videos on normalizing flows - mostly inspired from UCB's Unsupervised DL code,
dcpyro,MachineLearning,1616358255.0,[D] Is my idea of a Feature Store wrong?,"Should a Feature Store be part of an enterprise data catalog?

To me, a feature store seems to be a highly niche data catalog but  missing a lot of the benefits of having an enterprise data catalog /  data discovery tool. My need is to have generated features discoverable  when searching for data.

For example, if I have dataset A and B used to generate a feature set  AB', I would want to know about that information if I search and ever  come across dataset A or B in my data catalog.

Along with that, it would be beneficial to have the code / git commit that generated the features.

Am I missing something?"
fripperML,MachineLearning,1616589984.0,"[P] New library for performing nested cross validation, optimizing, calibrating and reporting quality of binary classification models"," 

The title says everything. :)

You can check it out here:

[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)

It's my first python package, so I'm sure many things could be improved. But if you find it interesting, please let me know! I will appreciate it a lot!"
gtgski,MachineLearning,1619394734.0,[D] VAE but every neuron models a distribution,"Variational AutoEncoder model has a distinctive middle layer where neuron activations are modeled as a unit gaussian.

Is there any experiments where every neuron is modeled as a unit gaussian? So the rest of the layers are the same as the middle layer."
proximauri,MachineLearning,1617872991.0,[D] Model does not learn with more classes.,"Hi everyone,I have tried to train a VGG16 model with my dataset using transfer learning. The task is face recognition (classification with softmax).

I have first tried to only train with 80 classes. Everything is fine and the model converges and accuracy is over 99%. However, when I train the model  with like 300 classes (including the first 80 identities as well) the network does not learn and the accuracy reaches to 4 percent only.

What could be the reason? Thanks."
ML_WAYR_bot,MachineLearning,1620590405.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 112,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/kadisonsinger: [https://arxiv.org/abs/1802.05296](https://arxiv.org/abs/1802.05296)

/u/Z30G0D: [https://arxiv.org/pdf/2103.02667.pdf](https://arxiv.org/pdf/2103.02667.pdf)

/u/FakespotAnalysisBot: [Link to Fakespot Analysis](https://fakespot.com/product/make-art-with-artificial-intelligence-make-and-sell-your-art-with-ai-blockchain-and-nft-awesome-ai)

Besides that, there are no rules, have fun."
cbsudux,MachineLearning,1617622290.0,[D] Anyone deploy DL models with AWS Lambda? Trying to estimate costs," Has anyone deployed DL models with AWS Lambda with a GPU? Looking to find out the minimum it would cost to deploy it serverless with AWS Lambda with some mid-level GPU - T4/P4 equivalent.

[https://www.fixmyphoto.ai/](https://www.fixmyphoto.ai/) has been deployed ""on a serverless React application with AWS lambda functions handling inference"" according to the [github page](https://github.com/MathiasGruber/PConv-Keras)."
WavyShapes,MachineLearning,1618930996.0,[P] Backprop Model Hub: a curated list of state-of-the-art models,"Hey everyone,

I want to share this [model hub](https://backprop.co/hub) we've been working on. It's a curated list of state-of-the-art text and vision models, including performance benchmarks on some notable datasets.

Our goal is to make this a convenient space to find a production-ready model that is fast and easy to use in real-world scenarios. We're trying to keep the selection focused on quality over quantity.

We've also got an [open-source library](https://github.com/backprop-ai/backprop) that makes using + finetuning these models possible in a few lines of code.

We want to enable more people to take advantage of the latest ML research without needing massive datasets or deep learning expertise.

If this helps you and you'd like to see some more tasks/models, let us know — we'd love to hear people's thoughts."
Seankala,MachineLearning,1616906465.0,[D] Not able to reproduce SoTA baseline results due to (assumed) package mismatch. How should I go about this?,"Hi. I'm a student working on a research project. There's a SoTA baseline model that I have managed to run, but unfortunately the performance is quite a bit lower than that reported in the paper. Searching around the closed issues in GitHub has led me to believe that it may probably be due to a package version mismatch.

Unfortunately the package that the original implementers used is a relatively older version, and in order to install it I believe I'd have to downgrade many of the drivers in the server I'm currently using to run my experiments.

What would your advice be to someone in my shoes? This is my first actual project so I'm not sure how to proceed. Should I just use these results with a disclaimer, or should I do whatever I can to achieve the original performances first?

Thanks."
jnforcer,MachineLearning,1620072239.0,[R] XGBoost works best for intelligent Deep Brain Stimulation,"https://www.biorxiv.org/content/10.1101/2021.04.24.441207v2

Smart brain implants will revolutionize neurotechnology for improving the quality of life in patients with brain disorders. The treatment of Parkinson's disease (PD) with neural implants for deep brain stimulation presents an avenue for developing machine-learning based individualized treatments to refine human motor control. We developed an optimized movement decoding approach to predict grip-force based on sensorimotor electrocorticography and subthalamic local field potentials in PD. We demonstrate that electrocorticography combined with Bayesian optimized extreme gradient boosted decision trees outperform other machine learning approaches. We elucidate a link between dopamine and movement coding capacity in PD, by showing negative correlations between decoding performance and motor symptom severity in the medication OFF state. Finally, we introduce an approach that leverages wholebrain connectomics to predict machine-learning based decoding performance in invasive neurophysiology. Our study provides a framework to aid development of intelligent adaptive deep brain stimulation."
FirstTimeResearcher,MachineLearning,1619818763.0,"[D] ICML Conference: ""we plan to reduce the number of accepted papers. Please work with your SAC to raise the bar. AC/SAC do not have to accept a paper only because there is nothing wrong in it.""","ICML Conference has decided to lower the acceptance rate by 10%: 

https://twitter.com/tomgoldsteincs/status/1388156022112624644

""According to the current Meta-Review statistics, we need to raise the acceptance bar. Please coordinate with ACs on reducing about 10% of accepted submissions."" -- https://twitter.com/ryan_p_adams/status/1388164670410866692"
Yuqing7,MachineLearning,1616607343.0,"[N] CMU, Oxford & Facebook Cross-Lingual Vision-Language Model Achieves New SOTA in Zero-Shot Setting","A research team from CMU, Oxford and Facebook AI proposes a vision-language model that, when trained on a source language, can be applied to different languages without additional annotated training data.

Here is a quick read: [CMU, Oxford & Facebook Cross-Lingual Vision-Language Model Achieves New SOTA in Zero-Shot Setting](https://syncedreview.com/2021/03/24/cmu-oxford-facebook-cross-lingual-vision-language-model-achieves-new-sota-in-zero-shot-setting/)

The paper *Multilingual Multimodal pretraining for Zero-Shot Cross-Lingual Transfer of Vision-Language Models* is on [arXiv](https://arxiv.org/pdf/2103.08849.pdf)."
StellaAthena,MachineLearning,1616370695.0,[P] EleutherAI releases 1.3B and 2.7B GPT-3-style models trained on the Pile,"The [GPT-Neo](https://github.com/EleutherAI/gpt-neo/) project by [EleutherAI](https://www.eleuther.ai/) has released 1.3B and 2.7B parameter GPT-3-style models. The models are trained on [the Pile](https://pile.eleuther.ai), a 800 GB curated dataset EleutherAI released in January.

The release includes:

1. The full modeling code, written in Mesh TensorFlow and designed to be run on TPUs.
2. Trained model weights.
3. Optimizer states, which allow you to continue training the model from where EAI left off.
4. A [Google Colab notebook](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) that shows you how to use the code base to train, fine-tune, and sample from a model.

Before people inevitably get confused, **this is not the same size as GPT-3**. The 1.3B model is a little smaller than GPT-2, and then 2.7B model is about twice as big. GPT-3 comes in a variety of sizes, including 1.3B and 2.7B (which is why we chose those values too). The ""full"" GPT-3 model is 175B parameters.

**Edit:** We've gotten a couple questions about this in Discord, so I wanted to share the following here as well. To the best of my knowledge this is a complete list of all announced autoregressive, non-MoE transformers. A model is considered public if anyone can download the trained model weights for free. No, it's not a typo that I say Facebook has trained a Megatron LM model, you can find the weights [here](https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b).

&amp;#x200B;

|Model|Size|Creator|Public|
|:-|:-|:-|:-|
|**GPT-Neo (small)**|**1.3B**|**EleutherAI**|**Yes**|
|**GPT-2**|**1.5B**|**OpenAI**|**Yes**|
|Meena|2.6B|Google|No|
|GPT-3 2.7B|2.7B|OpenAI|No|
|**GPT-Neo (mid)**|**2.7B**|**EleutherAI**|**Yes**|
|GPT-3 6.7B|6.7B|OpenAI|No|
|Megatron LM|8.3B|NVIDIA|No|
|**Megatron LM**|**11B**|**Facebook**|**Yes**|
|GPT-3 13B|13B|OpenAI|No|
|Turing NLG|17B|Microsoft|No|
|GPT-3 175B|175B|OpenAI|No|"
,MachineLearning,1617032593.0,[P] NeMo Getting Started. Prototyping Conversational AI Application,"Hey all!  Hope everyone is having a great Monday, I know I am. I want to let everyone know about a NVIDIA AI platform piece I use daily, NeMo. 

[NVIDIA NeMo](https://github.com/NVIDIA/NeMo) is a toolkit for building new State-of-the-Art Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new Conversational AI model architectures.

Conversational AI architectures are typically large and require a lot of data and compute for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node mixed-precision training.

[Cool collab notebook for getting started with NeMo](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/NeMo_Getting_Started.ipynb) shows how we can construct a toy demo showing the ability to translate Russian audio files into English. That's what is so neat about NeMo is the pipeline is so diverse; we can translate these audio files into English, then if we wanted to, use BERT for the NLP task for the translated English as well. And so on and so on.   


Anyways, I hope you enjoy this collab book, and to have a poke around NeMo afterwards."
marcos_pereira,MachineLearning,1620577183.0,[D] Machine Learning Collective is hosting OpenClubHouse: a live discussion open to all featuring researchers from Google Brain and Uber AI Labs. Starts in 40 minutes!,
programmerChilli,MachineLearning,1619010169.0,[R] Rotary Positional Embeddings - a new relative positional embedding for Transformers that significantly improves convergence (20-30%) and works for both regular and efficient attention,"This was originally discovered by some Chinese researchers  and had been circulating online for some time before they published their preprint: 
https://arxiv.org/abs/2104.09864

In the meantime, EleutherAI has been playing around with it and found it to be extremely effective across a diverse range of settings and sizes.

https://blog.eleuther.ai/rotary-embeddings/

None of the authors of the Arxiv paper are part of EleutherAI - Eleuther's interest is purely that ""this works really well"".

I wouldn't be surprised to see this positional embedding become the default."
chilled_87,MachineLearning,1620064529.0,[P] Reproducible research: Machine learning for credit card fraud detection,"ML for credit card fraud detection is one of those fields where most of the published research is unfortunately not reproducible. Real-world transaction data cannot be shared for confidentiality reasons, but we also believe authors do not make enough efforts to provide their code and make their results reproducible. 

We just released the five first chapters of a book on the topic, which aims at making a first step towards improving the reproducibility of research in this domain: [https://fraud-detection-handbook.github.io/fraud-detection-handbook](https://fraud-detection-handbook.github.io/fraud-detection-handbook). 

This preliminary release is in a Jupyter book format, making all the experiments and results reproducible, under an open-source license. Published chapters cover the background, motivations, and baseline methodologies. Forthcoming chapters will address more advanced topics, such as class imbalance, feature engineering, deep learning, and interpretability. 

Feedback is welcome!"
ispeakdatruf,MachineLearning,1619311145.0,[D] Looking for interesting classification datasets,"I have some new techniques that I want to try on small cardinality classification problems where the SOTA still has some headroom. So, for example, MNIST is out, since SOTA is already 99.xx%. And so is ImageNet, as the number of classes is too large. The ideal dataset would have <= 10 classes, with SOTA being < 90%. 

Are there any such interesting datasets out there? Thanks in advance."
ilikepancakez,MachineLearning,1618773529.0,[D] Neural Nets (1994),"[https://www.teamten.com/lawrence/writings/plan02.html](https://www.teamten.com/lawrence/writings/plan02.html)

I was in college and interested in neural nets when this was written, and as it happens later graduated and worked with the author of this post. In academia at the time, I believe neural nets were seen as a fading trend, something that was a cute idea but wasn’t destined to work out because they didn’t work well enough and even if they did we didn’t have the compute for anything useful. Personally, I was fascinated, and I know several other people who kept hope as well, one who is now quite a well known AI / neural nets researcher. I didn’t pursue neural nets personally, but I did keep using related genetic algorithms and artificial evolution, which I think was viewed with the same mix of skepticism generally, with pockets of enthusiasts. Gates in the story was probably sharing what he’d been hearing from academics. Lawrence (author of the post) is an incredibly smart person, and I have no doubt was able to see past what many of the researchers were saying.

BTW, I recently happened across a [paper from 1967](http://pageperso.lif.univ-mrs.fr/~edouard.thiel/rech/1967-blum.pdf) (53 years ago) that mentions neural networks in passing as if it was a popular idea at the time - the original paper on the Medial Axis Transform! Scroll to the 2nd page (“364”) near the top: “Consider a continuous isotropic plane (an idealization of an active granular material or a rudimentary neural net) that has the following properties at each point”"
IborkedyourGPU,MachineLearning,1618497148.0,[D] MLFlow vs ClearML vs Gradient Paperspace,"The MLOps tools scene is slightly overcrowded (\s). I want to limit the comparison to the three tools in the subject. Requirements:

 - run on-prem (we've got our own hardware, don't need/want cloud)
 - easily launch experiments. 
 - experiment database: log metrics, create plots, compare different experiments visually, track model performance over time
 - easy to install and use. Setting up some experiment trackers require a level of DevOp savvy that many data science teams don’t have, so that's a no-no.

Nice-to-have:
 - deploy models as API endpoints. We can use Docker for this, so this isn't really crucial, but...it's a nice-to-have.
 - hyperparameter tuning. It's fine if hyperparameter tuning relies on some third-party plugin, as long as it's open source (for example, MLFlow + Hydra + Ax plugin should be able to handle hyperparameter tuning through Bayesian Optimization just fine).
 - open source. AFAIK, MLFlow & ClearML are open source, while Gradient is not.

How do MLFlow, ClearML and Gradient compare in these respects? I'm especially interested in the comparison between MLFlow & ClearML - from my point of view as an outsider, they seem pretty similar, but I could be wrong, ofc.
‍"
darkmabler,MachineLearning,1617811607.0,[D] Models in production - what architecture do you use?,"Hi all! I'm new to the community but have been in the ML space for some time and was curious what everyone around here uses for serving their models in production.

Right now, I have implemented a few projects that utilize AWS EKS and NVIDIA Triton Inference Server. The projects I'm implementing this for need to handle 10's of thousands of concurrent requests so I chose EKS due to how scalable it is.

What are your thoughts/what have you done? Anyone ever use NVIDIA Triton Inference Server? Do you like it more than Tensorflow Serving?"
DeepML42,MachineLearning,1618953886.0,[D] Marginal Likelihood vs. Likelihood for Variational Autoencoder,"In this [discussion](https://www.reddit.com/r/MachineLearning/comments/5qm6ag/d_how_to_calculate_variational_autoencoder_log/) the computation of the likelihood for VAEs is discussed. Here they talk about likelihood, but isn‘t p(x) the marginal likelihood? Is there a difference between marginal likelihood and likelihood for VAEs? If so, how is each computed for VAEs?"
Megixist,MachineLearning,1619071095.0,[P] Implementation of MADGRAD optimization algorithm for Tensorflow,"I am pleased to present a Tensorflow implementation of the MADGRAD optimization algorithm, which was published by Facebook AI in their paper [Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization](https://arxiv.org/pdf/2101.11075v2.pdf) (Aaron Defazio and Samy Jelassi, 2021). When this algorithm was first introduced, several people requested that it be implemented in tf.keras, so I decided to do so. This implementation's main features include:

1. Simple integration into every tf.keras model: Since the MadGrad subclass derives from the      OptimizerV2 superclass, it can be used in the same way as any other tf.keras optimizer.
2. Built-in weight decay support.
3. Full Learning Rate scheduler support.
4. Complete support for sparse vector backpropagation.
5. Available on PyPi so you can install and import it directly.

Any questions or concerns about the implementation or the paper are welcome!

You can check out the repository [here](https://github.com/DarshanDeshpande/tf-madgrad) for examples and test cases. If you like the work then considering giving it a star! :)"
xEdwin23x,MachineLearning,1619117403.0,"[D] In a scale of 1 to 10 how much importance or thought do you guys put into the SWE and quality of the code in general you're writing when doing experiments? I feel its important but at the same time there's lots of pressure to move fast, so how do you strike a balance in this aspect?","I've been reading a lot on how to write better code, not only to become a better SWE but a better ML researcher. I personally feel its REALLY important to write good, clear, modular and reusable code, but at the same time the fast moving nature of ML (in academia at least, dunno in industry) pushes you to get an MVP (minimum viable product, in this case, a model with results) as fast as possible, making it hard to think about what or how should things be done in advance. 

I see even big research groups putting up code that looks ugly or that needs quite a few tweaks to keep changing, so Im curious what are your opinions on this topic and how do you approach this problem."
hardmaru,MachineLearning,1620565245.0,[R] Barlow Twins: Self-Supervised Learning via Redundancy Reduction,
Fun_Huckleberry_8991,MachineLearning,1619411818.0,[P] Reinforcement Learning with multiple simultaneous actions?,"Hi, I am currently working on a project related to using reinforcement learning. The agent has a large set of possible discrete actions. For example, given a graph with N nodes, the possible action can be choosing 1 of N node, then check the stopping criteria.

We actually can solve it with DQN. However, the bottle-neck is that the Agent only picks one node at each step. With a large N, it may make the runtime increase. Does anyone know any possible RL method that the agent can pick multiple nodes (more than one node) at each step? Thanks a lot."
AugustFR,MachineLearning,1619210772.0,College student created AI artists - the future is here - miragegallery.ai [N],
donjuan1337,MachineLearning,1617368020.0,[D] Modeling class errors,"I'm working on a project where I'll be post-processing a predictive system using ML. Both real-valued and ranked variables are predicted by this predictive system.

My strategy is to model the systems errors. For the real valued variable, the difference between the target and prediction is trivial but I have some problem regarding the ranked variable e.g. \[0,1,2,3\].

Each rank can be seen as a class. But I'm unsure how to model errors with class variables. Do you have any suggestion? The reason to model the error is to make the data time invariant which is good since the data set is limited."
Equivalent-Choice-75,MachineLearning,1620009742.0,[D] Does Pytorch/TF/Jax work well with M1 GPU?,"This information is surprisingly difficult to find online. There are several conflicting sources of information. 

Just a simple question - Does any of pytorch/tf/jax work with M1 GPU as well as with Nvidia GPU? Or even comparable?"
SomeParanoidAndroid,MachineLearning,1617702239.0,[D] Choosing ML + numerical computations workstation/server,"I am starting my PhD in machine learning in a newly established lab that is mainly focused on wireless communications and I have been tasked with finding an server/workstation to get for the office. My budget is around $3500-$4500. 

I of course will be utilizing the GPU to train models, but part of my work involves running numerical simulations, optimization algorithms, physical models, this sort of stuff. The rest of the (2-4) people will be dealing with the latter stuff exclusively.

Also, while I write all of my code in python using numba and tensorflow, the others will be using MATLAB. A final constraint is that we are based in Europe and my supervisor is skeptical about ordering from the US or abroad in general due to delivery times and extra complexity. So I would ideally like something that is generally available from suppliers anywhere (no LambdaLabs or System 76 unfortunately :/ ). 

Considering that we will all be probably needing the server concurrently and that our code usually takes several hours, what parts should I be prioritizing? Especially:

* I am thinking of getting an RTX 3080  (seems one of the best VFM GPUs for DL)
* i9/Ryzen or Xeon? supposedly Xeons are able to handle multiple jobs better and stay on without problems longer, but they look WAY slower. I see that they support ECC memory which I don't know if it has any relevance to our field. Also, most of our code could use a good CPU. **\[This is my most important dilemma\]** 
* Would 32GB RAM be enough? I would like to get 64GB, but it may be out of budget. Do you think I should prioritize this?
* Is liquid cooling needed? The server will be placed in our office.
* Does a dual GPU setup make sense? (e.g. 2x 3070) It seems ridiculous to spend such a huge portion of the budget when I have to account for everyone.

Plus some other more general questions/discussion points:

* ""Gaming"" pc as a workstation? If we opt for the i9, some alienware configurations hit the specs spot on. Should I avoid them? **\[I would appreciate opinions on this too\]**
* Are ""workstation solutions for deep learning"" from companies like IBM, Lenovo, HP, e.t.c. in this price range out of touch with reality? They offer some setups with 8GB ram, P1000 with 4GB VRAM (RTX 4000 is the closest to the 30 series I can find) and so on... For the same budget I could have a dream setup from Lambda Labs or something equivalent.
* Linux vs Windows? I feel way more comfortable on the former and I kind of fear that windows will just get in the way unnecessarily.
* MATLAB does support running code remotely, right?"
bendee983,MachineLearning,1617032735.0,[D] Algorithms Are Not Enough,"""Algorithms Are Not Enough"" provides a fresh perspective on the shortcomings of current AI systems.

The main idea discussed in AANE is that current AI systems are heavily dependent on representations. A human engineer must discover a problem and simplify the solution into distinct steps, a set of input data and expected outcomes, or a set of rewards and actions. Only then can an AI algorithm be designed to solve that problem. What we're lacking is not algorithms that can solve complicated problems but algorithms that can seek out and discover new problems and develop their solutions without help from humans.

Review of the book and discussion with the author, Herbert Roitblat, here:

[https://bdtechtalks.com/2021/03/29/ai-algorithms-representations-herbert-roitblat/](https://bdtechtalks.com/2021/03/29/ai-algorithms-representations-herbert-roitblat/)"
jayalammar,MachineLearning,1619708819.0,[P] Explainable AI Cheat Sheet (Image + Video),"Hi r/MachineLearning!

I have created a high-level map to major categories of ML Explainability methods. 

Explainable AI cheat sheet: [https://ex.pegg.io/](https://ex.pegg.io/)

Video going over it: [https://www.youtube.com/watch?v=Yg3q5x7yDeM](https://www.youtube.com/watch?v=Yg3q5x7yDeM)

&#x200B;

It's a large field and this is a non-exhaustive list to help orient people coming into the domain. How do you think I can improve it? All feedback is appreciated!"
Advanced-Hedgehog-95,MachineLearning,1616961935.0,[D] Motivation to use 768 dimensional embeddings from Transformers?,"Is there a scientific reason to have most if not all transformer models have 768- dimensional embeddings?

Even wav2vec2 and mockingjay models for audio have 768 dimensional embeddings."
HashRocketSyntax,MachineLearning,1618613481.0,Why do practitioners still use regular tensorflow? [D],"When I look at the 2.4 \`nn\` class, it has a handful of losses mixed in with the hidden layers, and it doesn't have optimizers. When I look for tensorflow optimizers and tensorflow losses it either points to tf.keras or tf.compat v1.

It's my understanding that a lot of practitioners are using tensorflow (not keras) - why? If this is the case, are they using v1 or v2? Are they able to do more low-level fancy footwork with their layers?

Not trying to be facetious. Truly seeking to understand.

---

*EDIT = here are my takeaways from comments:*
- Custom batch/ epoch operations.
- Performance.
- Legacy code.
- Embedded in devices."
latticeprep,MachineLearning,1617339115.0,[D] How does stripe use GBT's to find edge similarity?,"I'm reading Stripe's [blog post on how it finds similar accounts to flag fraudulent activity](https://stripe.com/blog/similarity-clustering) and it just doesn't make any sense to me. I'm wondering if anyone has any idea on how GBT's can be used on adjacency lists like this.

From the post:

>Over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. By sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. We use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.  
>  
>Because of the wide variety of features we can construct from given pairs of accounts, we decided to use [gradient-boosted decision trees](https://en.wikipedia.org/wiki/Gradient_boosting) (GBDTs) to represent our similarity model. In practice, we’ve found GBDTs strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. When we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. The variant that we use, [XGBoost](https://xgboost.readthedocs.io/en/latest/), is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. You can read more about the infrastructure we use to [train machine learning models](https://stripe.com/en-ca/blog/railyard-training-models) at Stripe in a previous post.  
>  
>Now that we have a trained model, we can use it to predict fraudulent activity. Since this model operates on pairs of Stripe accounts, it’s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. Instead, we first generate a candidate set of edges to be scored. We do this by taking recently created Stripe accounts and creating edges between accounts that share certain attributes. Although this isn’t an exhaustive approach, this heuristic works well in practice to prune the set of candidate edges to a reasonable number.  
>  
>Once the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. We then compute the connected components on the resulting graph. The final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. In particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.  
>  
>This is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster’s operation are created. And the more fraud rings we detect and shutdown at Stripe, the more accurate our clustering model becomes at identifying new clusters in the future.

I found this baffling. I didn't realize GBTs could be used to effectively find cliques in a graph. Has anyone tried this or understand how it works better? I don't really understand how these edge candidates are scored against past known-fraudulent edges. I just don't have a sense of the feature space for this at all."
fool126,MachineLearning,1616478880.0,[D] Recent (2021-03) review papers of different areas in the field,"Motivated by a recent post sharing the review paper [Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models](https://arxiv.org/abs/2103.04922), I wanted to crowdsource compile a list of review papers for other areas. This will be especially helpful for those looking to catch up to specific areas in the ever quickly expanding field."
zeando,MachineLearning,1619902307.0,[R] RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models ( GPT and CTRL Models ),
Lairv,MachineLearning,1618660227.0,[D] How to handle big datasets in computer vision ?,"I'm currently trying to use the [ScanNet](http://www.scan-net.org/) dataset. This dataset contains a lot of things (3D models, depth images, semantic segmentation etc...) and weights a total of 1.3TB. But for my purpose I only need the RGB color images, which should be something like 200Gb. This is still quite huge, and for the moment I can't even download the dataset on my machine.

I have seen several projects (e.g. [Atlas](https://github.com/magicleap/Atlas)) which makes use of this dataset to train their models. How do they do it ? Is it possible to download such datasets on cloud platforms ?

Edit : there seems to be a debate on whether this really is ""big data"", at least at my scale it requires more work than simply training on MNIST, although I'm sure this is nothing compared to what they did to train GPT3"
n31415926535,MachineLearning,1619809245.0,[P] AdapterHub v2: Lightweight Transfer Learning with Transformers and Adapters,"Hi r/MachineLearning!

Yesterday, we released v2 of AdapterHub ([https://adapterhub.ml](https://adapterhub.ml)), a framework that makes adapters accessible for NLP by integrating them with HuggingFace's Transformers library.

**What are adapters?**

* lightweight alternative to full fine-tuning of pre-trained Transformer models
* on-par performance while only updating \~1% of the model's weights
* trained adapter modules (a few MB in size) can be, extracted, shared & plugged into models
* flexible composition of adapters trained on different tasks, e.g. via stacking, fusing, splitting, ...

**What is AdapterHub?**

* *adapter-transformers*: Drop-in replacement of HuggingFace Transformers with adapter support
* *Hub*: 230+ pre-trained, ready-to-use adapters for NLP tasks (similar to HuggingFace model hub)
* Minimal changes (2-3 lines of code) to switch existing Transformers scripts to adapter training
* Easy loading, composing, training, saving & sharing of adapters
* Support for various Transformer models (BERT, RoBERTa, GPT-2, BART, ...)

**GitHub:** [**https://github.com/Adapter-Hub/adapter-transformers**](https://github.com/Adapter-Hub/adapter-transformers)

**Documentation:** [**https://docs.adapterhub.ml/**](https://docs.adapterhub.ml/)

**Pre-trained adapters:** [**https://adapterhub.ml/explore/**](https://adapterhub.ml/explore/)

&#x200B;

https://preview.redd.it/fs4benm4ycw61.png?width=983&format=png&auto=webp&s=99849ba476eb05a47f7286622d92a1e2379eebd0"
eparlan,MachineLearning,1616950375.0,[P] Implementation of DenStream,"I would like to showcase something I worked on a while back. It's a Python implementation of the DenStream algorithm ([https://archive.siam.org/meetings/sdm06/proceedings/030caof.pdf](https://archive.siam.org/meetings/sdm06/proceedings/030caof.pdf)); the algorithm is based on DBScan but for streaming data (i.e. you don't acquire all the data at once during training but iteratively).

The implementation can be found here: [https://github.com/MrParosk/pyDenStream](https://github.com/MrParosk/pyDenStream)

If you have any feedback please let me know!"
pianomano8,MachineLearning,1619197053.0,[R] MLC@Home and MLDS: A Dataset for Weight-Space Analysis of Neural Networks,"# Announcing MLC@Home and MLDS!

***tl;dr:***

Project website: [https://www.mlcathome.org](https://www.mlcathome.org)  
Paper: [https://arxiv.org/abs/2104.10555](https://arxiv.org/abs/2104.10555)

***MLC@Home***

MLC@Home is a volunteer-based distributed computing project dedicated to understanding and explaining machine learning models, specifically neural networks.  It's a collaborative project hosted at [UMBC](https://www.umbc.edu/).  Since July 2020, thousands of volunteers have been training neural networks on their home computers when otherwise idle (using [BOINC](https://boinc.berkeley.edu/),  the same framework as SETI@Home).  The project is less about chasing SotA, and more about understanding how networks encode the data that they do via inspection.  MLC@Home is also the first ML-focussed project to use BOINC framework.

***MLDS: Machine learning Dataset Generator***

Since July 2020, volunteers have been crunching away creating MLDS, a dataset of hundreds of thousands (on our way to ***1.1 million***) neural networks with a small selection of shapes, leaving only the weight space variable. We've [released](https://www.mlcathome.org/mlds.html) a few early versions of this dataset to the public, and commit to making the entire dataset available when complete. This past week, we posted our preliminary findings based on this dataset to [arXiv](https://arxiv.org/abs/2104.10555).  A summary of the preliminary findings in the paper:

* Given enough samples, identically shaped neural networks trained with different training data show clustering behavior in weight space
* We  can classify which networks were trained with which dataset with high accuracy
* We were able to differentiate most networks trained with backdoor/adversarial data versus those trained with clean data

We believe this is the largest publicly-available dataset dedicated to weight space analysis, by at least an order of magnitude, and we're still growing. The dataset leads to all sorts of interesting questions and possibilities to evaluate networks via direct inspection of the weights and structure of a network itself, as opposed to indirect measures of performance such as observed loss on test data.   

MLDS also captures meta information about the training process, such as loss history, what hardware the network was trained on (windows/llinux, amd64/arm64/arm32, cpu/cuda/rocm), and timing information, allowing even more opportunities for comparison. 

***Next Steps***

MLDS  continues to expand the MLDS dataset.  Currently, the dataset consists of small RNNs (200,000 parameters or less), but we're adding support for CNNs currently and have plans to support transformers and arbitrarily-sized networks.  MLDS will continue to grow and be a useful resource for the community. 

MLC@Home has even larger goals.  Leveraging volunteer computing is a capability multiplier for many lines of research.  MLC@Home can support multiple projects at once, and MLDS is just the first.  While MLC@Home will likely never outperform a tightly-coupled cluster for training a single large network, we foresee MLC@Home being useful for:

* Dataset generation (such as MLDS)
* Reproducibility / Robustness studies
* Architecture and Hyperparameter search
* Neuro-evolution

.. just to name a few.  Our community has contributions over 8000 computers, and more joining every day. 

***Interested and/or*** ***Want to help?***

MLC@Home is actively seeing collaborators!

**Individual contributors**

Have a computer? You can help  MLC@Home! Instructions for installing the client and joining are available on our website. 

**Researchers**

MLC@Home is actively looking for research collaborators! If you're interested in weight-space meta analysis and want to talk further about MLDS and how it can be improved; ***or*** if you have a new project idea that could benefit from the generosity of MLC@Home's volunteers,  please each out to us via email/twitter/discord, we'd love to collaborate!  

**Developers / Data Science Engineers**

Running a project that has to support cutting-edge and legacy hardware on multiple platforms is not easy. If you're a software developer or data science engineer and would like to help, please let us know.  Our client is open source, written in C++, and uses the PyTorch C++ API for most computation. We have specific needs for an OSX developer to port the client to Mac platforms, and many enhancements to make overall.  Our website could also use some updating. 

***Summary***

We're very excited about the possibilities of MLC@Home, and these past 9 months have made it a real, useful and growing resource. We'd love community feedback and collaborators to make it grow in new and fun ways.  As always, thanks to our volunteers who make is all possible.. without them, we never would have gotten off the ground. 

\-- MLC@Home Admins  
Website: [https://www.mlcathom.org/](https://www.mlcathom.org/)  
Forums: [https://www.mlcathome.org/mlcathome/forum\_index.php](https://www.mlcathome.org/mlcathome/forum_index.php)  
E-Mail: [mlcathome2020@gmail.com](mailto:mlcathome2020@gmail.com)  
Twitter:   [https://twitter.com/MLCHome2](https://twitter.com/MLCHome2)  
Gitlab: [https://gitlab.com/mlcathome/](https://gitlab.com/mlcathome/)"
giakou4,MachineLearning,1619505394.0,[P] Carotid plaque dataset, Recently i am working on my thesis which is application of some machine learning techniques on carotid plaques to classify them into symptomatic and asymptomatic. However the dateset i was provided consists of 150 images and i would like as more as possible for a better model to be trained. However i am not able to find any free dateset. Has anyone worked with something similar?
kvfrans,MachineLearning,1617836706.0,[R] StampCA: Growing Emoji with Conditional Neural Cellular Automata,"[Visuals](https://i.imgur.com/lOs9nkV.mp4)

Blog post: http://kvfrans.com/stampca-conditional-neural-cellular-automata/

[Twitter thread](https://twitter.com/kvfrans/status/1379925309311442944)

Basic Idea:
Neural CAs define local interactions which together grow into a global design. Instead of one system for one design, we can define a general system which grows *many* designs. This lets us condition our neural CA by giving it different design-specific ""seeds"".

StampCA models encode design-specific information in the cell state, and generic information in the network parameters. This means we can 1. grow many designs without retraining, and 2. grow all these designs in the same world.

[Many Emoji growing in one world](https://i.imgur.com/RGgxhIS.mp4) 

[Stamping emojis in a circle](https://video.twimg.com/ext_tw_video/1379927226125230081/pu/vid/512x512/T6LimIfaKlyHM89I.mp4)

We can also train GAN-based StampCAs. This model can use random values as a seed, and grows into various MNIST-like digits.

[Growing fake MNIST digits](https://video.twimg.com/ext_tw_video/1379930705396830210/pu/vid/482x182/26iemnkd9IY63zHg.mp4?tag=12)

Code for replicating the experiments and/or playing with the models is at [this Colab notebook for Emoji](https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg?usp=sharing#scrollTo=8_qZe_c1uPHf) and [this Colab notebook for MNIST](https://colab.research.google.com/drive/1kgYy6jebUl3bPBVlybWtHOlv635HrcL2)."
bzlister,MachineLearning,1618695659.0,[P] GAN for text generation," I'm looking for a model that can be trained on text in a particular genre, and can produce new text/transform existing text into the style of that genre, using common words and phrases.

I'm aware that GANs have limited success when applied to text generation due to the non-differentiable nature of textual data compared to images, but I'm hoping that there's nonetheless something I can use for this."
Yuqing7,MachineLearning,1620313621.0,[R] Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning,"A research team from Facebook AI conducts a large-scale study on unsupervised spatiotemporal representation learning from videos. The work takes a unified perspective on four recent image-based frameworks (MoCo, SimCLR, BYOL, SwAV) and investigates a simple objective that can easily generalize unsupervised representation learning methodologies to space-time.

Here is a quick read: [Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning.](https://syncedreview.com/2021/05/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-13/)

 The paper *A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning* is on [arXiv](https://arxiv.org/pdf/2104.14558.pdf)."
ZeroHour999,MachineLearning,1620499138.0,[R] Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers,[https://arxiv.org/abs/2105.00173](https://arxiv.org/abs/2105.00173)
MushiML,MachineLearning,1620243275.0,[D] Shuffling Batch Normalization in MoCo - Self Supervised Learning Method,"Hi, 

The authors mentioned in the paper that 

> *Shuffling BN. Our encoders fq and fk both have Batch Normalization (BN) \[37\] as in the standard ResNet \[33\]. In experiments, we found that using BN prevents the model from learning good representations, as similarly reported in \[35\] (which avoids using BN). The model appears to “cheat” the pretext task and easily finds a low-loss solution. This is possibly because the intra-batch communication among samples (caused by BN) leaks information. We resolve this problem by shuffling BN. We train with multiple GPUs and perform BN on the samples independently for each GPU (as done in common practice). For the key encoder fk, we shuffle the sample order in the current mini-batch before distributing it among GPUs (and shuffle back after encoding); the sample order of the mini-batch for the query encoder fq is not altered. This ensures the batch statistics used to compute a query and its positive key come from two different subsets. This effectively tackles the cheating issue and allows training to benefit from BN. We use shuffled BN in both our method and its end-to end ablation counterpart (Figure 2a). It is irrelevant to the memory bank counterpart (Figure 2b), which does not suffer from this issue because the positive keys are from different mini-batches in the past.* 

I was unable to understand that what type of information is leaked due to intra-batch communication. Please can someone help in understanding this point or refer to some source?

&#x200B;

Thanks"
ykilcher,MachineLearning,1619799059.0,[D] Paper Explained - Why AI is Harder Than We Think (Full Video Analysis),"[https://youtu.be/uwfVxckuq50](https://youtu.be/uwfVxckuq50)

The AI community has gone through regular cycles of AI Springs, where rapid progress gave rise to massive overconfidence, high funding, and overpromise, followed by these promises being unfulfilled, subsequently diving into periods of disenfranchisement and underfunding, called AI Winters. This paper examines the reasons for the repeated periods of overconfidence and identifies four fallacies that people make when they see rapid progress in AI.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:10 - AI Springs & AI Winters

5:40 - Is the current AI boom overhyped?

15:35 - Fallacy 1: Narrow Intelligence vs General Intelligence

19:40 - Fallacy 2: Hard for humans doesn't mean hard for computers

21:45 - Fallacy 3: How we call things matters

28:15 - Fallacy 4: Embodied Cognition

35:30 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2104.12871](https://arxiv.org/abs/2104.12871)"
thunder_jaxx,MachineLearning,1619841163.0,[R] Rip van Winkle's Razor: A Simple Estimate of Overfit to Test Data,
Aurora_HF,MachineLearning,1617526327.0,How to handle 2D geo-spatial grid like data samples in ML [D],"I am looking into a problem wherein the whole geographic area is divided into number of bins/pixels so we get nxn matrix covering whole region. Now each bin/pixel has number of parameters/features associated with it e.g., number of buildings in that bin, number of people living in that bin, poverty level of that bin, crime level in that bin, etc. This whole information represents one sample of the training dataset i.e., we have this matrix like data for different geographic regions with corresponding labels. How to best handle this kind of dataset for machine learning task e.g., ML trained on some nxn grids for different areas like this will classify labels for some unseen test nxn grids. I am thinking that in term of CNNs, it may be represented in terms of channels so each channel represents associated features of a bin. Whats your suggestion?

https://i.stack.imgur.com/5IlH8.png

https://i.stack.imgur.com/8GaEJ.png"
VinayUPrabhu,MachineLearning,1617472489.0,[P] What do you mean when you say X-is-all-you-need? The landscape and the dataset of all the 80 odd ML papers that have made these claims either in their title or in the body of the paper,"Github: [https://github.com/vinayprabhu/X-is-all-you-need](https://github.com/vinayprabhu/X-is-all-you-need)  
This is part  of a paper titled: "" **SPICES: SURVEY PAPERS AS INTERACTIVE CHEATSHEET EMBEDDINGS** "" that just got accepted at the upcoming  *Rethinking ML Papers - ICLR 2021 Workshop*  


  


https://preview.redd.it/q0cwxks5yzq61.png?width=1760&format=png&auto=webp&s=90e589d2d881f35bc7d8febab0a7875b789ccd51"
Ingvariuss,MachineLearning,1620406896.0,[D] Data Collection Crowdsourcing - How to animate people?,"Hello,

I started working on an ambitious AI project that would advance a specific field in psychology. The thing with this project is that it needs much data and doing it myself would take me around 4 years if I'd be collecting it on a consistent basis.

The idea was for the community, which is quite alive, to help me out collect some data and I wrote thorough documentation with the project description, rules of data formating and collecting, and have even made a little website that does most of the work for them where they basically need to copy/paste stuff and check some boxes.

I gave the documentation to two of my friends to read and see if everything is sound, after getting a green light I shared the document around in various social media channels. A big chunk of people ran away because the documentation was long while others, out of fear, started attacking the idea of using AI. After some conversation with them, I've concluded that they have no idea about how AI modeling works, what it can and can't do, or what goes under the hood. Which is fine as their backgrounds are colorful.

Teaching them all about AI and how it works and why do I need the data in that format isn't an option as most of them are ignorant. 

My question is: Has anyone ran into something like this and what did you do to overcome it? How do you animate and/or find data collectors? Have in mind that people need to have some psychology knowledge in order to collect this data and be reasonably intelligent.

**TL**;**DR**  I need people to help to collect data but they're too scared of AI. How to effectively crowdsource your projects?"
badge,MachineLearning,1617268644.0,[D] Generalized Additive Models… with trees?,"I’m looking at implementing Generalized Additive Models to work as speedily as possible (the entire end-to-end process), so started looking at using [C#’s ML.NET](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.treeextensions.gam).

I haven’t used C# since ~2014 so reading the code is a bit difficult, but it’s part of the FastTree library and is clearly a tree-based implementation. I tested it with a simple `y ~ sin(x)` model and it was dreadful (the FastForest regressor is much better).

Does anyone have any insight on what’s being used here, or references on the subject? I’ve used GAMs in R and Python and never seen a non-spline-based implementation before."
thunder_jaxx,MachineLearning,1619753760.0,[R] Sharpness-Aware Minimization for Efficiently Improving Generalization,
ottawalanguages,MachineLearning,1620111466.0,[D] Inevitable Manual Work Required in Machine Learning Projects,"I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? 

For example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this ""class 1"") or a non-serious condition (let's call this ""class 0""). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. 

The problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as ""class 1"" or ""class 0"". For example, for ""class 0"" : one of the doctors could clearly write at the end of a report ""all medical tests were conducted and the results and were all negative"", and another doctor could end the report by saying ""the patient should seriously consider changing their lifestyle and eat healthier food. benign."" . 

In this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a ""serious condition"" or a ""non-serious condition""? I was thinking of using something like ""sentiment analysis"" to capture the ""mood"" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is ""dark"" (serious condition) or ""light"" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?

In the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a ""serious"" or a ""non-serious"" condition?

PS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?"
Ingvariuss,MachineLearning,1619513497.0,[P][D] NLP question - Question Answering AI,"Hey people,

I'm working on my personal project which will be quite a challenge. One of its features is that the user can interact with an ""Open-domain question answering"" chatbot which will be trained on the data I provide it.

I want the model to resemble a specific person/group and it will be fed everything that that person/group wrote, said and etc. Have in mind that the model can answer questions in 1-4 sentences and it doesn't need to be based on pure facts. This means that the user won't ask the model questions like ""What is the capital of France?"" but more something along the lines of existential questions (""What is the meaning of Life?"").

Here are the questions I have as I didn't dabble into the NLP world of AI at all:

1. Are there any pre-trained or prebuilt models out there that I could use for this? I've found that the open-source Pavlov AI library has some interesting ones.
2. Which models would suit this task the best?
3. Are there any features I should watch out for or provide more information on?

The biggest part of the job will be to collect relevant data on the group I want the model to resemble. What would be some of the best practices when making the data as informative as it can be? Also, if I want there to be 4 groups that the model can resemble - Do I need to train 4 models or can I filter what a model learned into 4 categories?

Thanks for all replies and questions in advance. If some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great."
SQL_beginner,MachineLearning,1620420157.0,"[D] has anyone ever worked on a machine learning model for ""queues""?","Has anyone ever worked on a machine learning model for ""queues""? Suppose there is a bakery: the bakery has has ""n"" people working, ""m"" people in line""  and ""q"" orders that they are currently working on. The bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer's order is ready and how long will the next customer have to wait before they can place an order. 

Has anyone ever come across a machine learning model which can predict waiting and processing times? I have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit, as well as trying different m/m/k combinations... but has anyone ever come across an instance where machine learning algorithms (e.g. random forest, neural networks) are used to predict waiting times? 

I saw something like this: https://arxiv.org/abs/2002.10788 

But there was no python or R code for this paper. Can anyone recommend some source (blog, github, website, book, YouTube lectures etc) which show and provide computer code for analyzing queues using machine learning models?

Thanks"
aseigo,MachineLearning,1618740258.0,[P] Livebook: Jupyter-style environment for machine learning and scientific computing with Elixir,
_Arsenie_Boca_,MachineLearning,1620480658.0,[D] Multiple fine-tuning steps order,"When trying to maximize performance on a target task, transfer learning can not only be applied by fine-tuning a pretrained model on this task, but one might also want to incorporate several fine-tuning steps, that specialize the model to the target task. This is called (domain)-adaptive and behavorial fine-tuning (terminology see [here](https://ruder.io/recent-advances-lm-fine-tuning/)) depending on whether the data shares a domain or task setting with the target task. 

This is probably especially helpful in settings where the amount of labelled data for the specific target task is limited and the domain is significantly different from the pretraining data (e.g. [here](https://www.aclweb.org/anthology/2020.acl-main.740.pdf) related to adapting via language modeling).

The fine-tuning steps might contain

* domain adaptation via language modeling
* task adaptation via the same task setting as the target task from a different domain
* task adaptation via the same task setting as the target task from a similar domain
* the target task itself

Now, when applying several of those fine-tuning / adaptation steps, how would one choose their order? Empirically testing all possible orders might not be feasible and a ""wrong"" order might make the model forget valuable skills/knowledge. 

My intuition would roughly be the order listed above with the reasoning that ordering ascendingly with the similarity to the target task will lead to the best model.

Is there a rule of thumb? Any literature investigating this or at least applying and briefly discussing it?"
savoga,MachineLearning,1618414525.0,[D] Advantages of ML approaches for anomaly detection?,"What are the advantages of ML approaches for anomaly detection (isolation forests, autoencoders, DBSCAN etc.) over more traditional approaches such as finding points that are situated in the extreme part of a distribution?"
blkpingu,MachineLearning,1616784241.0,[D] Looking for Deep Learning Workstation / Server vendor in Europe / Germany,"I want to buy a workstation and so far I only found Lambdaworks and BIZON, and both are located in the US.

Does anyone know a company that sells workstations or servers in Germany or at least the EU?"
KaleidoscopeBest1569,MachineLearning,1620233329.0,[R] Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation,"**Project Webpage:** [https://zubair-irshad.github.io/projects/robo-vln.html](https://zubair-irshad.github.io/projects/robo-vln.html)  
**Pytorch Code and Dataset:** [https://github.com/GT-RIPL/robo-vln](https://github.com/GT-RIPL/robo-vln)  
**ArXiv paper:** [https://arxiv.org/abs/2104.1067](https://arxiv.org/abs/2104.10674) 

**Abstract:**

Deep Learning has revolutionized our ability to solve complex problems such as Vision-and-Language Navigation (VLN). This task requires the agent to navigate to a goal purely based on visual sensory inputs given natural language instructions. However, prior works formulate the problem as a navigation graph with a discrete action space. In this work, we lift the agent off the navigation graph and propose a more complex VLN setting in continuous 3D reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics the challenges of real world navigation. Robo-VLN tasks have longer trajectory lengths, continuous action spaces, and challenges such as obstacles. We provide a suite of baselines inspired by state-of-the-art works in discrete VLN and show that they are less effective at this task. We further propose that decomposing the task into specialized high- and low-level policies can more effectively tackle this task. With extensive experiments, we show that by using layered decision making, modularized training, and decoupling reasoning and imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms existing baselines in all key metrics and sets a new benchmark for Robo-VLN."
whyhateverything,MachineLearning,1617030324.0,[P] Passing embeddings to faiss for clustering,"Hi everyone,

I am having a hard time with faiss documentation for clustering. I have embeddings created with Sentence Transformers and now I want to use faiss to do the clustering part and get the final list that shows which document belongs to which cluster. I have no idea how to do this with faiss or more precisely how to pass my embeddings to it.

I would be endlessly grateful for your help..

Thanks in advance"
lifeonahilltop,MachineLearning,1620348179.0,[D] How do companies typically develop ML models for different customers?,"I work in the fraud protection industry, where my company ships fraud protection solutions (basically a binary classifier wrapped in some UI) to other companies in e-commerce and healthcare sectors. As we are getting more customers these days, we have to decide whether we want to build a single ML model or custom ML models for them. From an engineering and MLOps point of view, having a single model is more scalable. However, how to combine data from multiple customers in training and how to take performance tradeoffs between different customers doesn't seem straightforward.

Can anyone inform me how this is typically done in industry? Do people use techniques like domain adaptation or are the assumptions too brittle in reality? I imagine this is a common challenge in industry, and any pointers to relevant materials, case studies, etc. would be greatly appreciated."
lukasus,MachineLearning,1617900076.0,[P] Mask2Face: How We Built AI That Shows the Face Beneath the Mask,"Can you virtually remove a face mask to see what a person looks like underneath? STRV Machine Learning team proves it’s possible via an image inpainting-based ML solution. Here’s exactly how we approached the problem — from the preconditions to the implementation, results, and future improvements:   

[Mask2Face: How We Built AI That Shows the Face Beneath the Mask](https://www.strv.com/blog/mask2face-how-we-built-ai-that-shows-face-beneath-mask-engineering)

And here is the link to the project on GitHub: [https://github.com/strvcom/strv-ml-mask2face](https://github.com/strvcom/strv-ml-mask2face)"
atif_hassan,MachineLearning,1618641825.0,[P] PyImpetus - A Markov Blanket based new SOTA feature selection algorithm for python,"PyImpetus is a Markov Blanket based **feature selection algorithm** that selects a subset of features by considering their performance both individually as well as a group. This allows the algorithm to not only select the best set of features but also select the **best set of features that play well with each other**. For example, the best performing feature might not play well with others while the remaining features, when taken together could out-perform the best feature. PyImpetus takes this into account and produces the best possible combination. Thus, the algorithm provides a minimal feature subset. So, **you do not have to decide on how many features to take. PyImpetus selects the optimal set for you.**

PyImpetus has been completely revamped and now supports **binary classification, multi-class classification and regression** tasks. It has been tested on 14 datasets and outperformed state-of-the-art Markov Blanket learning algorithms on all of them along with traditional feature selection algorithms such as Forward Feature Selection, Backward Feature Elimination and Recursive Feature Elimination.

&#x200B;

**Performance Comparison:**

For classification tasks, Accuracy as a metric (higher score is better) has been used while for Regression tasks, Mean Squared Error as a metric (lower score is better) has been used. The final model used for comparison on all tasks is a decision tree with scores being reported on 5-fold cross validation.

|**Dataset**|**# of samples**|**# of features**|**Task Type**|**Score using all features**|**Score using PyImpetus**|**# of features selected**|**% of features selected**|
|:-|:-|:-|:-|:-|:-|:-|:-|
|Ionosphere|351|34|Classification|88.01%|92.86%|14|42.42%|
|Arcene|100|10000|Classification|82%|84.72%|304|3.04%|
|AlonDS2000|62|2000|Classification|80.55%|88.49%|75|3.75%|
|slice\_localization\_data|53500|384|Regression|6.54|5.69|259|67.45%|

&#x200B;

[Link to the GitHub repo](https://github.com/atif-hassan/PyImpetus)

[Link to pypi](https://pypi.org/project/PyImpetus/)

&#x200B;

PyImpetus already has over 15K+ downloads so do check it out and please let me know how it worked out for you in your Machine Learning project. Don't forget to pen down your feedback and doubts in the comment section.

And of course, don't forget to star the GitHub repo!!

Thank you and have an amazing day!"
Quantum_Stat,MachineLearning,1619531012.0,"The NLP Index: 3,000+ code repos for hackers and researchers. [Project]","Want to introduce “The NLP Index”, a new asset in NLP code discovery. It's free and open to the public.

It houses over 3,000 code repositories that one can search including a side bar with some of the most important topics in NLP today. The engine is search as you type and typo tolerant (it’s crazy fast). The index includes the arxiv research paper PDF, ConnectedPapers link, and its GitHub repo.

https://index.quantumstat.com/"
RyanAI100,MachineLearning,1618772283.0,[D] A Rigorous Study on Pretrained Model for NER | Research Papers Summary 014,
pinter69,MachineLearning,1619975673.0,[R] Few-Shot Patch-Based Training (Siggraph 2020) - Dr. Ondřej Texler - Link to free zoom lecture by the author in comments,
NeoDio_02,MachineLearning,1619551730.0,[D] Question on ROUGE scores for evaluating summaries,"I am doing a project on text summarization, and I want to run a ROUGE  benchmark test on my summarization model. One thing I am confused about  is the large differences in scores I am seeing in different sources. For  example, here:[https://github.com/andersjo/pyrouge](https://github.com/andersjo/pyrouge) They get a ROUGE-1 f-score of \~0.7, while in this paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9321308](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9321308) they show ROUGE-1 f-scores like 41. Can someone explain the discrepancy and how ROUGE scores work? Thanks!"
bert4QA,MachineLearning,1618242156.0,[R] KILT: a Benchmark for Knowledge Intensive Language Tasks,
Yuqing7,MachineLearning,1617673931.0,[N] Yann LeCun Team Uses Dictionary Learning To Peek Into Transformers' Black Boxes,"A Yann LeCun team proposes dictionary learning to provide detailed visualizations of transformer representations and insights into semantic structures such as word-level disambiguation, sentence-level pattern formation, and long-range dependency captured by transformers.

Here is a quick read: [Yann LeCun Team Uses Dictionary Learning To Peek Into Transformers' Black Boxes](https://syncedreview.com/2021/04/05/yann-lecun-team-uses-dictionary-learning-to-peek-into-transformers-black-boxes/)

The paper *Transformer Visualization via Dictionary Learning: Contextualized Embedding as a Linear Superposition of Transformer Factors* is on [arXiv](https://arxiv.org/pdf/2103.15949.pdf)."
SZenne_,MachineLearning,1617887636.0,What cloud computing setup should I get for my Deepfakes [R]," Hi!

For my graduation project I am doing research on the creation of Deepfakes. Now I have a small budget of $200 and I would want to spend this on 2 weeks op cloud computing because my own computer doesn't have the right requirements to run Deepfake software. I just want to do some 10 second basic deepfakes to test out the softwares.

So does anybody have any recommendations on which online cloud computing servers I could use and what I should look out for?"
chasep255,MachineLearning,1620298405.0,[D] Learning a discrete encoding for raw audio.,"I have been working on various ways to generate music using neural nets.  I started with a WGAN using 1D convs which produced not the best results. I would really like to use an RNN to generate audio just like how you would generate text by sampling the output and feeding that sample back to the input on the next time step.  Since the audio I am using is sampled at 22050hz this method is not practical.  So what I decided to do is first use an auto encoder with a gumbel softmax in-between.   This way I could compress the audio data and learn a discrete encoding for it.  Once I have this encoder I plan to train an RNN on the encoding just like you would on text, then I can use the decoder to convert this into sound.  Currently the best architecture I have is as follows...

The encoder compresses the audio down 256x to a discrete representation with 4096 different values.  
Hard gumbel softmax between encoder and expander.  
The expander  expands the discrete representation back out to the length of the original raw audio.  
The decoder uses an RNN to predict the next sample based on the prior sample and input from the expander. (I tried a wavenet instead of RNN and so far the RNN produces a better result, I care more about quality than speed.)

\--Encoder--

\#256 Mu-Law Encoding  
input\_audio = keras.Input((None,), dtype = 'int32')

x = layers.Embedding(256, 8)(input\_audio)    

x = layers.Conv1D(128, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(256, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(4096, 1, padding = 'same')(x)

\--Expander--

 input\_data = keras.Input((None, 4096))

e = layers.Conv1D(32, 1, padding = 'same', kernel\_initializer = 'random\_normal', use\_bias = False)(input\_data)

x = layers.Conv1D(512, 5, padding = 'same')(e)  
x = layers.LeakyReLU()(x)

x = layers.Conv1D(512, 5, padding = 'same')(e)  
x = layers.LeakyReLU()(x)

x = layers.Conv1DTranspose(256, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

x = layers.Conv1DTranspose(128, 32, strides = 16, padding = 'same')(x)  
x = layers.LeakyReLU()(x)

\--Decoder--  
prior\_audio\_input = keras.Input((None,), batch\_size = 1 if stateful else None, dtype = 'int32')  
expander\_input = keras.Input((None, 128), batch\_size = 1 if stateful else None)  
x = layers.Embedding(256, 8)(prior\_audio\_input)  
x = layers.Concatenate(axis = -1)((expander\_input, x))  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.GRU(1024, return\_sequences = True, stateful = stateful)(x)  
x = layers.GRU(1024, return\_sequences = True, stateful = stateful)(x)  
\#Add expander\_input in again to help with gradients  
x = layers.Concatenate(axis = -1)((expander\_input, x))  
x = layers.Dropout(0.5)(x)  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.TimeDistributed(layers.Dense(512))(x)  
x = layers.LeakyReLU()(x)  
x = layers.TimeDistributed(layers.Dense(256))(x)

\--Training--

\#r is mu-law encoded audio samples  
r\_prior = r\[..., :-1\]  
r = r\[..., 1:\]

e = encoder(r, training = training)  
e\_sm = tf.nn.softmax(e)  
enc\_loss = tf.reduce\_mean(e\_sm \* (tf.math.log(e\_sm + 1e-20) - tf.math.log(1.0 / tf.cast(tf.shape(e\_sm)\[-1\], tf.float32))))

e = model.gumbel\_softmax(e, 1.0)  
\#Convert e to a one hot representation  
e = tf.stop\_gradient(model.argmax2onehot(e) - e) + e

e = expander(e, training = training)  
...  
\#take a random chunk of the expander output if the sequence is too long  
\#also add noise to r\_prior  
g = decoder((r\_prior, e), training = training)  
sound\_loss = tf.reduce\_mean(tf.keras.losses.sparse\_categorical\_crossentropy(r, g, from\_logits = True))  
total\_loss = sound\_loss + enc\_loss  


So I would like to ask a few things...

1) Do you think the way I am using the gumbel softmax makes sense?

2) Since I am using the ""hard"" version of the gumbel softmax with the stop\_graidents do I still need to use the temperature parameter and anneal it?

3) Does my loss function make sense?  I added the enc\_loss term to the output of the encoder which adds a penalty for it deviating from a flat distribution.  I figure this is like the second term in the ELBO loss function used in a VAE.  I found it helps the model to learn to use more of the encoding.

4) Should I be using the gumbel softmax at all?  Maybe it makes more sense to just use a normal softmax and always take the argmax.  I figure the randomness adds helps it explore the possible encodings.

5) Any other thoughts?"
glampiggy,MachineLearning,1620164503.0,Is it common for Transfer Learning to decrease the accuracy of a model? [Project],"I trained PSPNet and DeepLab from scratch and also using pre-trained backbones on a very specific urban scene dataset. The pre-trained backbone I used was ResNet, with weights downloaded from the ImageNet dataset. I then trained both models without freezing any layers. My accuracy for the models from scratch proved to be higher than the models with pre-trained backbones. My dataset is relatively small; only 1000 images.

Could this have happened because the ImageNet dataset is too general when compared to the specific dataset I am working on, and has thus limited its learning ability? Or have I most likely done something wrong?"
deadmanscurve,MachineLearning,1617728458.0,[Research] How do you train on and evaluate GLUE STS-B?,"Hello!


I'm experimenting with multi-task learning, and I have read some of the literature around it. I'm trying to benchmark all of the GLUE tasks, but I can't figure out how people train and evaluate [STS-B](https://huggingface.co/datasets/viewer/?dataset=glue). The labels are similarity scores between two sentences, ranging from 0 to 5.


Spearman's correlation is used for evaluation. If the labels were discrete, I could train a classification model and compute this, but they are not- they are floats. I have read that some train a regression model with sigmoid activation, after normalizing the labels to range \[0, 1\]. If this is done, is an L2 loss used? How is the Spearman's correlation then computed exactly?


This information has been surprisingly difficult to find for a standard benchmark dataset!


Any info is gladly appreciated :D"
miladink,MachineLearning,1618988935.0,[D]Is im2latex considered solved?,I hope you know the im2latex dataset by OpenAI in which you need to infer the equation that generated a latex image from the image. I found it a little surprising that actually little work is done there. Is reading equations from the images considered a solved problem?
minimaxir,MachineLearning,1620230683.0,[N] Wired: It Began As an AI-Fueled Dungeon Game. It Got Much Darker (AI Dungeon + GPT-3),"https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/

If you haven't been following the drama around AI Dungeon, this is a good summary and a good discussion on filter/algo difficulty."
IglooAustralia88,MachineLearning,1617224051.0,[P] How Bad is a Bad Classifier: Is there any signal here?,"I have a text classification problem where I am classifying song lyrics by the performing artist.  Right now I have a small dataset of 3000 lyrics (two lines each) each from 9 different artists.  All of my train, dev, and test sets are balanced by artist.

As a baseline I encoded each lyrics observation using Bert and performed a Logistic Regression.  I got a 0.35 top-1 and a 0.65 top-3 on a held out test set.

I'm not sure how to phrase this, but is this classifier doing anything, and is it worth trying to improve on (my original idea was comparing word-level vs. subword-level NN classifiers for the task)?  Like obviously with 9 artists a random classifier would be doing \~0.1111, and this is 3x that, so seemingly it has found some signal.  However, it's hard to look at a 0.35 score and think there's anything there.

I guess my question is, can I hope to find some signal in this small amount of data for this text classification text, or is this baseline implementation trying to tell me that the data is very noisy and thus the experiment will be hard to get significant results from?

Thanks!"
Separate_Run2806,MachineLearning,1620376282.0,[D] Tool to help ML Engineering follow code best practices.,"Hello, my data science team was involved in an increasing amount of ML engineering tasks but had small knowledge on code quality best practices so we made this VS Code Extension to help python developers build unit tests quickly and efficiently. Our AI makes suggestions of input and then generate the test file for you. Would love to hear your feedback if you are willing to give a try to it. It's available here [www.ponicode.com](https://www.ponicode.com)

Are you doing unit tests at all when implementing your models? Are you ever testing your code at the pre processing or evaluation stage?"
idg101,MachineLearning,1617314266.0,[D] Hyper-parameter tuning takes the majority of my time!," I  am a 4th year PhD student with a few papers published already.  As I  come to the end of my PhD, I find I spend the majority of my time  hyper-parameter tuning to get the best performance possible.  This takes  \*forever\* to do on my dual GPU machine.  Does anyone have any shortcuts  on how to do this?  I have tried just tuning over the best 1-5 epochs  but I find that this isn't a good proxy for long-term validation loss  convergence.

The lifecycle of  research seems to be come up with an idea, and then tune the network to  show that your performance is better than some benchmark.  After doing  this a few times, it seems like a complete waste of research cycles  spent.  There has to be a better way."
Superb-Drawer5214,MachineLearning,1617108593.0,"[D] If the number of machine learning PhD graduate is increasing rapidly, wouldn't it get exponentially harder to be hired at machine learning related jobs without PhD?",It seems everyone wants to do machine learning these days and those who did PhD in machine learning is increasing rapidly. Wouldn't it get harder and harder to be employed in machine learning related jobs without PhD?
Yuqing7,MachineLearning,1620141876.0,[R] Huawei & Tsinghua U Method Boosts Task-Agnostic BERT Distillation Efficiency by Reusing Teacher Model Parameters,"A research team from Huawei Noah’s Ark Lab and Tsinghua University proposes Extract Then Distill (ETD), a generic and flexible strategy for reusing teacher model parameters for efficient and effective task-agnostic distillation that can be applied to student models of any size. 

Here is a quick read: [Huawei & Tsinghua U Method Boosts Task-Agnostic BERT Distillation Efficiency by Reusing Teacher Model Parameters.](https://syncedreview.com/2021/05/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-11/)

 The paper *Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation* is on [arXiv](https://arxiv.org/pdf/2104.11928.pdf)."
opensourcecolumbus,MachineLearning,1617938253.0,[P] BERT-based Financial Question Answering System,"Built this production-ready project using Jina, PyTorch, and Hugging Face transformers. Adapted a passage reranking approach by first retrieving the top-50 candidate answers, then reranking the candidate answers using FinBERT-QA, a BERT-based model fine-tuned on the FiQA dataset that achieved the state-of-the-art results.

### [GitHub Repo](https://github.com/yuanbit/jina-financial-qa-search)

Looking for your feedback and questions. What would you need me to do to make it more useful and easy to use for your own use case?"
mridal,MachineLearning,1620289195.0,[D] ACL-IJCNLP 2021 Acceptances are out,"How did everyone do? Mine was accepted in ACL findings with scores of 3, 4 and 3."
minidiable,MachineLearning,1619767355.0,[D] Object detection - useful resources,"Hi all, I am working for a new company. I am a drone expert with some knowledge in Computer Vision but my main expertise is about control, state estimation, and robotics. One of the main macro tasks for the new job will revolve around the topic of object detection from onboard an aircraft (small, medium, large size).

I know that we could divide the field of object detection into two main streams:

\- the data-driven approaches (e.g., machine learning, deep learning-based)

\- the classical approaches (using classic computer vision)

However, I have never implemented an object detector and I would really like to know more about the history of it and the main tools I can use to achieve this. Could you point out useful resources like books, online courses (best if on Coursera), links, milestone papers, youtube videos, and channels, etc.?

Thanks!"
bendee983,MachineLearning,1618849898.0,[D] The challenges of applied machine learning,"From setting up the data infrastructure (breaking down the silos, creating data lakes, etc.) to collecting and cleaning training data, to setting up a scalable networking and compute clusters, dealing with business objectives and ethical issues, applying machine learning to real-world applications poses challenges that are absent in academic and research settings. 

Four key challenges in applied machine learning include the following:

* Defining the problem: Are you solving the right problem? Does it align with your business objectives? What is the desired accuracy from the model? Are you addressing a special problem or has anyone else solved it before? Can it be solved with a pre-trained model or do you need to develop your own models?
* Collecting training data: Are public datasets (ImageNet, MSCOCO, etc.) enough to train your models (most probably not)? Can you buy the data, do you already have it, or do you have to collect it? If you already have the data, is it in a consolidated store or is it siloed? Do you need to clean and label the data?
* Maintaining models: How much will your model be affected by decay? How frequently do you need to retrain your models? Do you have a pipeline for continuously collecting and maintaining your data in the future?
* Gathering the right team: Do you have the talent in-house? Do you have subject matter experts who can weigh in on the model's performance? Do you have a product manager who can define and assess business objectives for your machine learning strategy? Do you have a process to get input and feedback from the end-users of your system? Do you have software engineers who can integrate your models into existing products and IT staff who can provide reliable and scalable infrastructure for training and inference?

These are some of the challenges discussed in my latest article on the challenges of applied machine learning:

[https://bdtechtalks.com/2021/04/19/applied-machine-learning-challenges/](https://bdtechtalks.com/2021/04/19/applied-machine-learning-challenges/)

You'll find a lot more in  [*Real World AI: A Practical Guide for Responsible Machine learning*](https://appen.com/real-world-ai/) by Alyssa Simpson Rochwerger and Wilson Pang"
Bitmore11,MachineLearning,1617281029.0,[D][P] Raven Protocol,"I am looking for any input on the Raven Protocol project. Raven is a decentralized network of compute nodes that utilize idle compute power for AI training where speed is the key. Is this a protocol you could benefit from? Besides adding to their library, is there anything that this protocol is missing for the solution to the problem they are trying to solve? I am not experienced in machine learning and would like input from those of you that stand to benefit the most from Raven Protocol.

[Website](https://www.ravenprotocol.com/)

[Github](https://github.com/raven protocol)"
Yuqing7,MachineLearning,1617249392.0,[N] Google Research’s Novel High Efficient Neural Volumetric Representation Enables Real-Time View Synthesis,"A Google Research team accelerates Neural Radiance Fields’ rendering procedure for view-synthesis tasks, enabling it to work in real-time while retaining its ability to represent fine geometric details and convincing view-dependent effects.

Here is a quick read: [Google Research’s Novel High Efficient Neural Volumetric Representation Enables Real-Time View Synthesis](https://syncedreview.com/2021/03/31/google-researchs-novel-high-efficient-neural-volumetric-representation-enables-real-time-view-synthesis/)

The paper *Baking Neural Radiance Fields for Real-Time View Synthesis* is on [arXiv](https://arxiv.org/pdf/2103.14645.pdf)."
montebicyclelo,MachineLearning,1620460319.0,[P] Patchless MLP-Mixer,"https://github.com/sradc/patchless_mlp_mixer

This is a preliminary exploration of an even simpler MLP-Mixer style architecture."
Tolstoyevskiy,MachineLearning,1617215508.0,[D] Comparison of experiment tracking tools,"Hey r/MachineLearning! 

A while back I published a post [comparing data versioning tools](https://dagshub.com/blog/data-version-control-tools/), and people seemed to find it useful. So I wrote another one comparing  experiment tracking tools – there are so many options to choose, and it make sense to consider the pros and cons of each option. 

[https://dagshub.com/blog/how-to-compare-ml-experiment-tracking-tools-to-fit-your-data-science-workflow/](https://dagshub.com/blog/how-to-compare-ml-experiment-tracking-tools-to-fit-your-data-science-workflow/)

My criteria were mainly around: 

1. What do you want to track
2.  Where is the data saved
3. What visualization capabilities are there
4. How easy is it to set up
5. How stable it is
6. Does it support large scale experimentation

As usual, here is a summary in table form, though the article of course contains more details:

[Tabular comparison of ML experiment tracking tools](https://preview.redd.it/b3rlshkcqdq61.png?width=1600&format=png&auto=webp&s=c55613adee339bee7f606b81f2a4427cb670ab32)"
haipinglu,MachineLearning,1619300697.0,"[P] An introduction to PyKale https://github.com/pykale/pykale​, a PyTorch library that provides a unified pipeline-based API for knowledge-aware multimodal learning and transfer learning on graphs, images, texts, and videos to accelerate interdisciplinary research. Welcome feedback/contribution!",
stupidMZ,MachineLearning,1617940296.0,"[R]Group-Free 3D Object Detector: New SOTA on ScanNet V2(69.1 mAP@0.25, 52.8@mAP@0.5) and SUN RGB-D(62.8 mAP@0.25 and 42.3 mAP@0.5)🔥","**Group-Free 3D Object Detection via Transformers(**another work from MSRA Visual Computing Group)

Paper link:   [\[2104.00678\] Group-Free 3D Object Detection via Transformers (arxiv.org)](https://arxiv.org/abs/2104.00678)

Code link:  [zeliu98/Group-Free-3D: Group-Free 3D Object Detection via Transformers (github.com)](https://github.com/zeliu98/Group-Free-3D)

👉 What?

A simple yet effective method for directly detecting 3D objects from the 3D point cloud without the handcraft and heuristic point grouping mechanism.

❓Why?

The mainstream point-based object detectors based on the heuristic point grouping step, but the complexity and diversity of objects in the real scene may lead to wrong point assignments and degrade the 3D object detection performance, shown in Fig. 1

&#x200B;

[Fig 1.  With the heuristic point grouping step, all points in the blue box of RoI-Pooling or blue ball of Voting are assigned and aggregated to derive the object features, resulting in wrong assignments. Our group-free based approach automatically learns the contribution of all points to each object, which has the ability to alleviate the drawbacks of the hand-crafted grouping. ](https://preview.redd.it/m6rbt1f6l2s61.png?width=524&format=png&auto=webp&s=37c77b021b7a7f14d44c52210ad4e2fc3607a7b1)

&#x200B;

🥊 The main ideas

1. Using the transformer decoder to model the relationship between the points and objects. All the points are used to form the feature of objects and the weighting of each point is automatically learned during training.
2. A multi-stage iterative box prediction framework is adopted, and the spatial encoding is refined iteratively in each stage.
3. A free-lunch multi-stage ensemble mechanism is used for improving the performance.

⚙️ Overall Architecture

PointNet is used as the backbone, the detection head is the multi-stage transformer decoder, shown in Fig. 2.

&#x200B;

[Fig 2.  This figure illustrates the simple architecture of our approach, including three major components: a backbone network to extract feature representations for each point in the point cloud, a sampling method to generate initial object candidates, and stacked attention modules to extract and refine object representations from all points. ](https://preview.redd.it/85qadvidl2s61.png?width=1107&format=png&auto=webp&s=42a4a31b10ccc824304d233df236b2d75fbdc94c)

🦾 Results

Very high performance was achieved on ScanNet V2 and SUN RGB-D

[Table. 1 Results on ScanNet V2](https://preview.redd.it/gahnurngl2s61.png?width=708&format=png&auto=webp&s=7080af89923d1933097dfef85c2a6664558f2d09)

&#x200B;

[Table. 2 Results on SUN RGB-D](https://preview.redd.it/ls35kckil2s61.png?width=827&format=png&auto=webp&s=6957ab3066b81a4bfaf93efdd67579ef657402e2)"
giangblackk,MachineLearning,1618393121.0,[R] Time Series Forecasting Survey,"I can't find any thorough survey about time series forecasting methods with benchmarks?

Does anyone have a suggestion?"
sgevorg,MachineLearning,1616600557.0,[N] Aim 2.2.0 is out! Hugging Face integration and advanced params table management ...,"Hi r/MachineLearning community!

Excited to launch [Aim](https://aimstack.io) v2.2.0  🎉🎉🎉

We are building an open-source self-hosted tool for AI training run comparison. It can handle 1000s of experiments at once and has a simple, straightforward API - super-easy to get started with.

Thanks for the incredible support - helping us democratize AI dev tools.

Check out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZSwiaW5kaWNhdG9yIjpmYWxzZSwieEFsaWdubWVudCI6InN0ZXAiLCJwb2ludHNDb3VudCI6NTB9fSwiZm9jdXNlZCI6eyJjaXJjbGUiOnsiYWN0aXZlIjpmYWxzZSwicnVuSGFzaCI6bnVsbCwibWV0cmljTmFtZSI6bnVsbCwidHJhY2VDb250ZXh0IjpudWxsLCJzdGVwIjpudWxsfX19LCJzZWFyY2giOnsicXVlcnkiOiJsb3NzLCBibGV1IGlmIGhwYXJhbXMubGVhcm5pbmdfcmF0ZSAhPSAwLjAwMDAxIGFuZCBjb250ZXh0LnN1YnNldCBpbiAodGVzdCwgdmFsKSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5tYXhfayJdLCJncm91cEJ5U3R5bGUiOltdLCJncm91cEJ5Q2hhcnQiOlsibWV0cmljIl0sImFnZ3JlZ2F0ZWQiOnRydWUsImFnZ3JlZ2F0ZWRBcmVhIjoibWluX21heCIsImFnZ3JlZ2F0ZWRMaW5lIjoibWF4Iiwic2VlZCI6eyJjb2xvciI6MTAsInN0eWxlIjoxMH0sInBlcnNpc3QiOnsiY29sb3IiOmZhbHNlLCJzdHlsZSI6ZmFsc2V9fX0=)

Below are a few highlights of this release. Check out the [full release post here](https://medium.com/aimstack/aim-v2-2-0-hugging-face-integration-57efa2eec104).

**1. HuggingFace Integration**

[Aim and Hugging Face](https://preview.redd.it/wna6uq6svzo61.png?width=2240&format=png&auto=webp&s=b06c286cde98511ddba8ecb9caa29e05208a1312)

**2. Metric Visibility Control: hide metrics while they are still in search**

[Hide individual metrics as well as collectively](https://i.redd.it/2nxdnewiwzo61.gif)

**3. Column Resize: control your screen real estate for super-long params**

[Drag column edges back-and-forth to resize](https://i.redd.it/xwnkk9ikwzo61.gif)

If you haven't yet, drop us a star for support! 🙌

Come say hi at the [Aim Slack community](https://slack.aimstack.io/).

Check out this version and let us know how we can improve it further 🙏"
doyougitme,MachineLearning,1618477849.0,"""[Discussion]"" Should I be using DVC (Data Version Control) in my day-to-day work?","I've been following [dvc.org](https://dvc.org) for a while now but am yet to be fully sold on when/if I should be using it in my everyday work. Dev work in ML does seem unduly clumsy (because of the experimental nature of the work I guess) but I'm not sure whether data versioning is the problem and whether DVC is the solution. I've been freelancing for a while now, so I'm not sure what the stack for ML devs looks like these days.

I'm curious to know whether DVC forms a popular part of the data and ML engineering stack both for working individually and in a team. If so, what are the unique advantages it provides and how has it made your life better? If you don't use it, why not? Or what are the shortcomings that made you give it up?"
questions2067,MachineLearning,1619731841.0,[D] Does anyone here have a career in machine learning that they applied to the medical field?,"I’m really looking to just ask what you career is like/to learn more about it. I’m currently in undergrad and not sure what I want to do, but this topic interests me."
htahir1,MachineLearning,1617206594.0,[D] Why ML should be written as pipelines from the get-go,"Thought id share my ideas on why a multi-step fragmented approach to productionalization of ML is flawed. Rather, we should be creating abstractions geared towards data scientists to incentivize them to write more production-ready code from day 0. Happy  to hear thoughts from the community.  


[https://towardsdatascience.com/why-ml-should-be-written-as-pipelines-from-the-get-go-b2d95003f998](https://towardsdatascience.com/why-ml-should-be-written-as-pipelines-from-the-get-go-b2d95003f998)"
DNA1987,MachineLearning,1619196786.0,[D] 3D CNN how to deal with empty space,"Hello, 

I am working on a 3D CNN, I am using voxel to represent protein with one channel for each amino acid, so total 20 channel, 

There is lot of empty space in my voxel ~90% and as each channel just encode one type of amino acid, so each channel is even more empty. It is not like an RGB image where each channel has a integer gradient on all positions of x * y.   

My current network only work for a couple of samples, after that it needs too much training. Somehow I am thinking it could be like an unbalance class problem, but the 3D space convolution thing is confusing me."
Inferrd_F,MachineLearning,1618153944.0,[P] How to make your Models available ?,"I've created a tool to make your models available for other people: [Inferrd.com](https://Inferrd.com)   


**What is it?** Create an API for your models in seconds.

**What can you do with it?**   
  •  Put your models to use right when they're trained so you don't waste time creating/monitoring an infrastructure.   
  •  Create engaging demos to show users what your models can do in real conditions.

**Why is it interesting ?**

* You can drag-and-drop to deploy all your models, it's that simple ! Afterwards you can call them from your jupyter notebook or any IDE. The whole process from registration to deployment won't take you more than 2 minutes.
* You know what you pay for, no hidden fees. We charge a little bit more than the price of the RAM. We want to have a 256MB RAM box for 1$ per month.

I am an Infrastructure Engineer. I built this because I wanted everyone to be able to make the most out of their models. What I usually see is models sleeping on the shelf somewhere, forever waiting to be deployed by someone else, so this is their time to shine.  


Lastly why use [inferrd.com](https://inferrd.com) when you can use AWS/GCP? I think AWS/GCP/Azure become too expensive once you're captive. So I wanted to build a tool that allows you to migrate easily and won't make you dependant of it.   


AMA!"
Grid_AI,MachineLearning,1617751199.0,[N] Latest Innovations with Grid.ai and PyTorch Lightning,"Join us on April 13th at 11 am ET to hear about Grid.ai and PyTorch Lightning's latest innovations with our CEO and Founder William Falcon and Thomas Chaton, Research Engineering Manager. This discussion is excellent for AI researchers, machine learning engineers, and data scientists looking for new ways to accelerate and improve their current AI model training process. Leave with tangible strategies, new tools, and great ideas.

Register now and submit any questions you have for William and Thomas!

[https://zoom.us/webinar/register/1016176774118/WN\_yW66h71HSz-MXWNGAu6OOg](https://zoom.us/webinar/register/1016176774118/WN_yW66h71HSz-MXWNGAu6OOg)"
Evening-Use-7142,MachineLearning,1617874115.0,[D] Pytorch (geometric) over neo4j,"Hi \*,

&#x200B;

Are there any good guidelines/best practices/ cookbooks/ example code for running pytorch GNN over large cloud-based database such as neo4j?

&#x200B;

My data is going to be stored at AWS using neo4j. Currently its a small db but we expect it to grow. Are there any good guidelines to working with data that way?

&#x200B;

I know neo4j has a DS library which allows for running algorithms directly over the data but we wish to design our own.

&#x200B;

&#x200B;

The goals are currently standard ones such as link-prediction and node classifications.

&#x200B;

&#x200B;

Thank you all"
soulslicer0,MachineLearning,1616883606.0,[D] PSA: IEEE PDF Express stores passwords in Plaintext,"For CVPR 2021, I had to use the IEEE PDF Express PDF Checking service. It asks that we create an account and forces password rules (Capital letter etc.). So I foolishly decided to use my personal bank/email password.

I then get an email saying that ""You have created your account etc."" and it prints the password right there in my email in plain text.

Absolute bullshit"
radjeep,MachineLearning,1620015489.0,[Discussion] A very rudimentary solution to the XOR problem with a single layer (excluding output) neural network.,"With two inputs x^(1) and x^(2) that can have binary values (0 or 1).We have two weights for each of those w^(1) and w^(2), and a bias unit b.

Our output neuron z hence will have the equation z = w^(1)x^(1) \+ w^(2)x^(2) \+ b.

Let's say we set our bias unit to 0, and w^(1) to 1 and w^(2) to -1.In this way we will have output 0 for input combinations (0, 0) and (1, 1) - which is required.

And we will have output 1 when input is (1, 0) and the only exceptional case would be that of output being -1 when input is (0, -1). But we can mitigate this by using and additional condition that the result is 0 only when the output is 0 and it is 1 when the output is any other value than 0.

Thoughts?"
keepthepace,MachineLearning,1619525246.0,[P] I am writing a copyleft license for machine learning models. I'd love some comments and criticism,"Hello,

I am a software developer in robotics with a lot of love for open source. Recent events have made me ponder how one could release a trained model in order to keep it really open/free and I think (but would love to be proved otherwise) that there are no licenses applicable right now, as trained weights are a very different beast from source code or executable binaries. So I tried to write one by modifying the Affero GPL license (something the FSF explicitly allows in their FAQ): [I posted it here](https://github.com/yquemener/MLMPL) 

This is actually an old subject. I saw a discussion on the [debian-legal mailing list from 2009](https://lists.debian.org/debian-legal/2009/05/msg00028.html) about it. It also [crops up](https://lists.debian.org/debian-devel/2018/07/msg00175.html) occasionally on [debian-devel](https://lists.debian.org/debian-devel/2019/05/msg00380.html), was discussed at the [2012 Debconf](https://saimei.ftp.acc.umu.se/pub/debian-meetings/2012/debconf12/high/888_Machine_learning_threats_and_opportunities_for_Debian_and_Free_Software.ogv) and was also discussed by the [ffmpeg team](https://ffmpeg.org/pipermail/ffmpeg-devel/2018-July/231834.html).

Having read that and many other cases and opinions, I think it is necessary to have a new license to ""open source"" trained weights but it is not necessarily a difficult task. I think (thanks to the previous free software efforts) it is already 95% done, we just need to adapt an existing license but be a bit more explicit about what a model is and not try to shoehorn a license designed for compiled software into the machine learning world.

I am not formally trained in copyright law, so I'd really appreciate someone with such a background to poke holes at my proposition, but everybody's constructive criticism is welcomed!"
P4TR10T_TR41T0R,MachineLearning,1619775478.0,[P] A review of recent research on transformers in vision,"Hey folks,

I recently ~~published~~ released a review on transformers in vision, focused on the last months of research. I remember a few discussions around Vision Transformers in particular, but I would like to highlight a fact: while they're often seen as an intellectual curiosity that only Google can train, recent derivative models enjoy significant efficiency, outperforming all but EfficientNetV2 (if you allow for DeiT-like hard-label distillation) or all but EfficientNetV2 and Normalizer Free Networks (if you do not allow for hard-label distillation) on ImageNet.

Transformers-based models are also advantaged in large data regimes (e.g. ImageNet-21k or Google's internal JFT-300M datasets) due to their reduced inductive bias.

The post discusses discusses this, includes interactive performance visualizations and more, so consider checking it out: [https://iaml-it.github.io/posts/2021-04-28-transformers-in-vision/](https://iaml-it.github.io/posts/2021-04-28-transformers-in-vision/)

One last thing: it's the first time I write a blog post, so any feedback (be it about the format or the content itself) is deeply appreciated."
DAL59,MachineLearning,1619555922.0,[D] Whats the point of CLIP opposed to Dalle?,"Clip generates very weird images with artifacts and massive distortions, and often tiles the screen with requested objects.  Dalle on the other hand creates completely normal looking pictures most of the time.  So why would anyone use CLIP?"
mlconvergence,MachineLearning,1619949450.0,[R] Generative Minimization Networks: Training GANs Without Competition,
cosapocha,MachineLearning,1619473464.0,[P] How would you measure uncertainty in a classification task?,"Hello everybody, I've been using dropout, deep ensembles, evidence, and bayes by backprop to generate samples and then measure uncertainty.

With the samples I can compute the mean and variance per class and see if the bars overlap with each other, or sum all the variances and see if it is a big number, or see if there is two or more means too high and then say the model is not reliable there. I mean, I can think of a lot of methods, but I was wondering if there is a standard, or if any of you guys have a good idea of how to measure the uncertainty of a model in classification. 

Thank you very much!"
hardmaru,MachineLearning,1620619558.0,[R] Learning Controllable Content Generators,
KirillTheMunchKing,MachineLearning,1617126474.0,[D] Are there any other GAN based image editing projects with an encoder-generator architecture that actually work in real time?,"I mostly see GAN image editing projects rely on Pix2Pix distillation to work in realtime, but the authors of ""Using latent space regression to analyze and leverage compositionality in GANS"" claim their encoder -> generator setup works in realtime. I tried the demo from github, and it does work pretty fast for small edits, kinda strange that it hangs for larger edits. 

In case you are not familiar with the paper, and want to learn about it, I explained the main ideas in my  [telegram channel](https://t.me/casual_gan/17)"
thedeepreader,MachineLearning,1618683213.0,[P] Demo of Swin Transformer for Object Detection,
Zweiter,MachineLearning,1619199699.0,[R] Sim-to-Real Learning of All Common Bipedal Gaits,
bjourne-ml,MachineLearning,1616320233.0,[D] Thoughts on unlikelihood training/antitraining?,"What is the sub's thoughts on unlikelihood training/antitraining?

The idea is dirt-simple; if the model does what you want you
reward it (likelihood) and if it doesn't you slap it
(unlikelihood). Say you have a language model that predicts the next
word. Such models tend to, should we say, ""cheat"" when confused by
guessing on words it has already seen.

For example, suppose the sentence is:

    in particular standard likelihood training and decoding

The model is given all words except for the last and is asked to
predict the next word. Likely, it will assign a high probability to
some or all of the preceding words (a well-known problem), leading to
continuations like:

    in particular standard likelihood training and in
    in particular standard likelihood training and particular
    in particular standard likelihood training and standard
    in particular standard likelihood training and training
    ...

This leads to repetitive text. Unlikelihood training/antitraining
attempts to solve this by penalizing the model for assigning
probability mass to preceding incorrect words. Suppose part of the
probability distribution produced by the model looks like:

    p(in|in particular standard likelihood training and) = 0.2
    p(particular|in particular standard likelihood training and) = 0.02
    p(standard|in particular standard likelihood training and) = 0.05
    p(standard|in particular standard likelihood training and) = 0.02

Then we'd calculate the unlikelihood (penalty) as

    -alpha*(log(1-0.2) + log(1-0.02) + log(1-0.05)+(1-0.02)),

where alpha is a weighting hyper-parameter. Backprop and the next time
the model makes the same prediction, it will assign less probability
mass to the preceding words.

Increasing diversity/reducing repetition is just one use case for
unlikelihood. Suppose you have the sentence

    recent large-scale language models ...

The target word is ""are"" but if the model predicts ""is"" you penalize
it because ""is"" is grammatically incorrect! Same for ""Perplexity and
accuracy ..."" - reward the model for ""were"" but penalize it for ""was"".

Here are papers that explain the concept much better than I can. The authors claim to have achieved
some very impressive results:

* [Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)
* [Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training](https://arxiv.org/abs/1911.03860)"
Present-Percentage88,MachineLearning,1619541186.0,[R] Online study: predicting COVID-19 misinformation in Twitter data using AI,"Hi everyone,

I am conducting a user study for my master's thesis regarding explainable AI (XAI) solutions that can predict COVID-19 misinformation on Twitter. Misinformation has been an important topic during the pandemic. Explainable AI solutions can help us detect misinformation and simultaneously explain their prediction which is key to understanding how they work.

Your participation will be beneficial towards the battle against misinformation and the pursuit towards transparent and responsible AI. During the online study, you will label COVID-19 related tweets and answer a set of questionnaires.

Requirements for participation:

* At least 18 years old
* Run the study on a laptop/desktop computer

You can participate autonomously using the following link: 

[https://userstudy-thesis.herokuapp.com/en/group1](https://userstudy-thesis.herokuapp.com/en/group1)

Thank you in advance and stay safe."
fedetask,MachineLearning,1618071318.0,[R] Applications of Graph Neural Networks to Reinforcement Learning,Could you point me to some papers applying Graph Neural Networks to Reinforcement Learning tasks?
luisgasco,MachineLearning,1619167986.0,"[R] - Call For Participants MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents","**\*\*\* CFP2  MESINESP2 track: Medical Semantic Indexing (BioASQ – CLEF 2021) \*\*\***

[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) 

**MESINESP2 Awards by BSC-Plan TL \[2,700€\]**

**Test sets and additional data are now available**

There is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.

Following the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):

**MESINESP-L – Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).

**MESINESP-T – Clinical trials**: for automatic labelling of clinical trials summaries.

**MESINESP-P – Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.

**Key information**

**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) 

**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)

**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)

MESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.

A large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (> 1.5 million entity mentions) with  medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*. 

Participating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages. 

**Important dates**

* April 19: Updated Train, Validation and Test sets release
* April 19: Additional datasets release (Medical entities present in documents)
* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline
* May, 7: Start of the evaluation period
* May, 17: End of the evaluation period
* May,28 :Submission of Participant Papers at CLEF2021
* July, 2: Camera ready paper submission.
* Sep 21-24: CLEF 2021 Conference

**Publications and BioASQ/CLEF2021 workshop**

Teams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.

**Main Track organizers**

* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.
* **Luis Gascó**, Barcelona Supercomputing Center (BSC), Spain.
* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.
* **Elena Primo-Peña,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.
* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.
* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.
* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.
* **Renato Murasaki,** BIREME – Organización Panamericana de la Salud (WHO), Brasil.

**Scientific Committee**

* **Tristan Naumann,** Microsoft Research (USA)
* **Prof. Xavier Tannier,** Sorbonne Université and LIMICS (France)
* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)
* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Parminder Batia,** Amazon Health AI (USA)
* **Prof. Irena Spasic,** School of Computer Science & Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)
* **Jose Luis Redondo García,** Amazon Alexa, Amazon (UK)
* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Politécnica de Madrid (Spain)
* **Prof. Allan Hanbury,**  E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)
* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)
* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)
* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)
* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)
* **Prof. Henning Müller,** University of Applied Sciences Western Switzerland – Valais (Switzerland)
* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)
* **Georg Rehm,** Deutsches Forschungszentrum für Künstliche Intelligenz (Germany)
* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)
* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)
* **Prof. Jesús Tramullas,** Departamento de Ciencias de la Documentación e Historia de la Ciencia, Universidad de Zaragoza (Spain)"
Intelligent-Fun-5311,MachineLearning,1619703113.0,[P] Real-time Object Detection on Jetson Nano,Hi everyone. I need help in real-time object detection on Jetson Nano. I trained a yolov3 model a while back and it is pretty accurate but gives a very low FPS of 0.223 on nano. I tried converting the model to tflite but that gives an even lower fps of 0.07. Please tell me how I can achieve real-time inference without any loss in accuracy?? Thanks in advance.
lkncy,MachineLearning,1619314378.0,[P] ESL Solution,"I found a really good website containing solutions to most exercises in ESL (The Elements of Statistical Learning)

https://yuhangzhou88.github.io/ESL_Solution/"
ML_WAYR_bot,MachineLearning,1619380804.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 111,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/evanatyourservice: [ASAM](https://arxiv.org/abs/2102.11600)

/u/awesomeai: [MAKE ART with Artificial Intelligence](https://www.amazon.com/dp/B091J3T4HM)

Besides that, there are no rules, have fun."
ML_WAYR_bot,MachineLearning,1616961605.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 109,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)||

Most upvoted papers two weeks ago:

/u/boy_named_su: https://arxiv.org/pdf/1609.02943.pdf

/u/Vinay_Kumar20: [https://acuvate.com/blog/machine-learning-in-supply-chain/](https://acuvate.com/blog/machine-learning-in-supply-chain/)

Besides that, there are no rules, have fun."
Programmierer,MachineLearning,1616598438.0,[R] Mastering Real-Time Strategy Games with Deep Reinforcement Learning: Mere Mortal Edition,"By employing an array of techniques that includes a novel form of automatic domain randomization, curricula, canonicalization of spatial features, an omniscient value function, and a network architecture designed to encode task-specific invariants, we can train deep reinforcement learning agents for the CodeCraft real-time strategy game within hours on a single GPU.

&#x200B;

Blog post: [https://clemenswinter.com/2021/03/24/mastering-real-time-strategy-games-with-deep-reinforcement-learning-mere-mortal-edition/](https://clemenswinter.com/2021/03/24/mastering-real-time-strategy-games-with-deep-reinforcement-learning-mere-mortal-edition/)

Code: [https://github.com/cswinter/DeepCodeCraft](https://github.com/cswinter/DeepCodeCraft)"
kul_xjia,MachineLearning,1616341170.0,[R] Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images,[https://arxiv.org/pdf/2101.02824.pdf](https://arxiv.org/pdf/2101.02824.pdf)
KirillTheMunchKing,MachineLearning,1617985631.0,[R] ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement - Explained,"[ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://t.me/casual_gan/24) 

A great idea to improve StyleGAN inversion for complex real images that builds on top of the recent e4e and pSp papers.

The authors propose a fast iterative method of image inversion into the latent space of a pretrained StyleGAN generator that acheives SOTA quality at a lower inference time. The core idea is to start from the average latent vector in W+ and predict an offset that would make the generated image look more like the target, then repeat this step with the new image and latent vector as the starting point. With the proposed approach a good inversion can be obtained in about 10 steps. More details [here](https://t.me/casual_gan/24)

[The inversions are awesome!](https://preview.redd.it/sa63gu0dc6s61.png?width=1106&format=png&auto=webp&s=56a647abbb5b1bb0a44f6c75e5fd78bb81ec5858)

P.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/24):"
fripperML,MachineLearning,1617172367.0,"[D] What’s the simplest, most lightweight but complete and 100% open source MLOps toolkit? -> MY OWN CONCLUSIONS","Although I have posted this summary in the [thread](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/), most people won't find it, so to make it more visible I post it as another thread.

First of all, I have to thank the reddit ML community in general and each of you in particular for the detailed, insightful and interesting answers I have received in the past few days. I have learnt a lot and the picture in my head is now clearer. Now, I am posting a summary with the things that, for me, make more sense (it's my opinion and will serve as our guideline for making the decision, so it's not just a bare summary).

**General advice**

We should start with a reduced set of tools, the most useful ones, in order to have the flexibility to change or adapt our projects to a new infrastructure a provider could offer us. This is something that could happen.

**End-to-end solutions**

There are mainly two solutions that are 100% open source and free to install and use, and that may solve most of the requirements of ML practitioners: [Hopsworks](https://hopsworks.readthedocs.io/en/stable/) and [ClearML](https://allegro.ai/clearml/docs/). Among this two, if I had to chose one right now, it will be ClearML. Hopsworks might be much more complete, but ClearML seems to have a bigger community behind it and to be easier to install and use. So ClearML will be something to take a look at in case we go for an all-in-one package. I also like the idea of having a platform with an UI with all our projects.

**Python Programming**

[Flake8](https://flake8.pycqa.org/en/latest/) (including flake8-docstrings), [MyPy](http://mypy-lang.org/) and [Black](https://black.readthedocs.io/en/stable/) are hugely recommended. [Google style guide](https://google.github.io/styleguide/pyguide.html) is something to take a look at too.

This morning I have found this [guide](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/) that might be worth it, as it covers many good practices. Also this [article](https://martinheinz.dev/blog/14).

Regarding the IDE, VSCode is not the same as Visual Studio, the most recommended one is VSCode.

[Poetry](https://python-poetry.org/) is also something to consider. But also one should be careful with it: its current development state is not very promising and maybe pip is more secure, as it is the official way.

**CI and Deployment**

Jenkins is a good tool, although maybe not the easiest one (Gitlab, Drone, and Circle are all easier to use). Docker might not be totally needed, but is hugely recommended as it is becoming a standard, and even many of the libraries rely on it (for example, ClearML does). In addition, it works very well with Jenkins.

We should switch from SVN to git (strongly recommended). [Gitlab](https://about.gitlab.com/) is a good option.

**Project Scaffolding**

[CookieCutter](https://cookiecutter.readthedocs.io/en/1.7.2/) or [Kedro](https://kedro.readthedocs.io/en/stable/) are the winners. I still think we will stick to Kedro template, because it offers extra functionality, and I like to think of each project as a set of pipelines to be run. Anyway, some cookiecutter templates are very good, like this [one](https://github.com/TezRomacH/python-package-template). In case we use both Kedro and ClearML, we'll have to figure out how to integrate its pipelines with ClearML tasks. But in the slack channel of ClearML there are other teams doing the same, so at least it's possible.

**Documentation**

[Sphinx](https://www.sphinx-doc.org/en/master/index.html) for the documentation is totally recommended (Google style docstrings). [Napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html) can be very useful for helping with that. This covers documentation of the actual code. For documenting the business objective and other project related stuff, we could use jupyter notebooks in order to have everything inside the repo.

**Project registry**

ClearML if we finally chose it. Otherwise, we migth use an internal wiki or just the repository with a clear documentation.

**Data Exploration and Preparation**

We should use PySpark when things go ""big"", and Pandas when things fit in memory.

**Tests**

I expected Great Expectations library to be recommended, but nobody told anything. Instead, unit testing and/or smoke tests using [pytest](https://docs.pytest.org/en/stable/). And checking them with Jenkins. Anyway, if Kedro ends up being our project template, I'll keep an eye on the [plugin](https://github.com/tamsanh/kedro-great) with [Great Expectations](https://github.com/great-expectations/great_expectations).

**Feature Store, Data Versioning**

Maybe not so important in the beginning. [DVC](https://dvc.org/doc) looks good, but it's not easy to use.

**Workflow engine or orchestrator**

In our case, we have one, but otherwise it is an important piece. Prefect is maybe the option I like the most for its simplicity, but Luigi is also a tool that I like.

Kedro, also related with this, because it is a tool for defining pipelines, does not care about how to run the pipelines and you can deploy them in several engines like Luigi, Prefect, Airflow or Kubeflow.

**Model registry**

Its importance depends on several considerations:

* If you have too many models in production.
* If models are frecuently retrained.
* If lots of models are trained and or tested in parallel.
* If some models make real-time predictions, and their performance is critical.

If any of the previous point happens to be true, a model registry can be a very important piece of the MLOps solution. Otherwise, you can consider it not essential.

**Experimenting**

It's an important piece. If we use ClearML, this will be solved. Otherwise, we might try [MLFlow](https://www.mlflow.org/docs/latest/index.html) using Kedro-MLFlow or [PipelineX](https://pipelinex.readthedocs.io/en/latest/).

[Hydra](https://hydra.cc/docs/intro/) can be an interesting addition to define configurations, although Kedro does have a nice way too.

**Training**

Apart from the ""classical"" libraries, in case of DL for simplicity [PyTorch Lighting](https://www.pytorchlightning.ai/) will be our first option. Anyway, hardware limitations could be an issue (when models don't fit into memory, when training must be distributed... so that problems should be at least foreseen... both TensorFlow and PyTorch have ways of dealing with it).

**Model serving**

[FastAPI](https://fastapi.tiangolo.com/). Or even simpler: [DL4J](https://deeplearning4j.org/), to be used in Java when we need to communicate with the rest of the applications in real time.

Other interesting solutions are [BentoML](https://github.com/bentoml/BentoML) and [Cortex](https://www.cortex.dev/), we should take a look at it too.

When high availability is important, we should take into account having redundant nodes and a resilient infraestructure (Kubernetes could be a solution).

**Visualization**

We should take a look at [voila](https://voila.readthedocs.io/en/stable/using.html) and [streamlit](https://streamlit.io/).

**Model monitoring**

We could use Jenkins pipelines or ad-hoc scheduled processed. We don't need a tool for that."
KirillTheMunchKing,MachineLearning,1617381315.0,[R] StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery - SOTA StyleGAN image editing,"[StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://t.me/casual_gan/18)

This idea is so elegant, yet powerful:  
The authors use the recent CLIP model in a loss function to train a mapping network that takes text descriptions of image edits (e.g. ""a man with long hair"", ""Beyonce"", ""A woman without makeup"") and an image encoded in the latent space of a pretrained StyleGAN generator and predicts an offset vector that transforms the input image according to the text description of the edit.

&#x200B;

https://preview.redd.it/kemg73bcfsq61.png?width=1438&format=png&auto=webp&s=0c3a259abe37ef32bafa022195ca4ce3f3ab320b

P.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/18):"
meldiwin,MachineLearning,1620241072.0,"[N] Joscha Bach ""cognitive Architectures""","&#x200B;

Hello Everyone,

We (IEEE Soft Robotics Podcast) are going to have Joscha Bach on the podcast, if you have any questions or arguments to Joscha, please send them here:

[https://docs.google.com/forms/d/e/1FAIpQLSegi1wwNNrYaxvkfVvRe3pB5fk6HUuSbIsL1N8b9r41EB2NEg/viewform?vc=0&c=0&w=1&flr=0&gxids=7628](https://docs.google.com/forms/d/e/1FAIpQLSegi1wwNNrYaxvkfVvRe3pB5fk6HUuSbIsL1N8b9r41EB2NEg/viewform?vc=0&c=0&w=1&flr=0&gxids=7628)

Thanks,

https://preview.redd.it/uxjjln65mcx61.png?width=2586&format=png&auto=webp&s=d093db04455bf0b4909ee1818e67055d2ce0aa6b"
HashRocketSyntax,MachineLearning,1616324158.0,[P] AIQC (deep learning framework) is now seeking collaborators.,"AIQC is now open to contributors! [Link to low hanging fruit GitHub issues](https://github.com/aiqc/aiqc/issues).

AIQC is a framework for rapid and reproducible deep learning that aims to drive adoption of deep learning in scientific research.

\> It does so by providing an object oriented Python API (similar to ORM) that serves as bumper rails for both advanced and entry-level deep learning workflows. The high level API allows you to perform best practice machine learning by simply calling [\`aiqc.Pipeline\` and \`aiqc.Experiment\`](https://aiqc.readthedocs.io/en/latest/notebooks/keras_multi-label_classification.html)

https://preview.redd.it/8bxqk6g03do61.png?width=1784&format=png&auto=webp&s=3e6addbfb5bc309d01b186ce4c077067e6d6b044"
aminnikanjam,MachineLearning,1617047762.0,"[R] A survey on ""Design Smells in Deep Learning Programs"""," Our research group (SWAT Lab., Polytechnique Montréal under supervision of Prof. Foutse Khomh) is conducting a survey on “Design Smells in Deep Learning Programs”. We have prepared an online survey that takes around 5-10 minutes to complete asking about relevance and severity of observed design issues in DL programs. 

We are looking for participants who have a strong background and experience in research/ development of Deep Learning programs (specially convolutional networks-CNNs). Please feel free to participate if you find yourself eligible. Moreover, you could kindly share this survey with colleagues/friends who you consider eligible to participate.

The results of this survey will be publicly accessible through arXiv.org in anonymized form. At no point in the survey will we ask you for your name, and we will not be logging your IP address to allow anonymity. If you would like to know more about this study, feel free to contact us with your questions.

Link: [https://forms.gle/Yedpq3Dx8tAoxYkL8](https://forms.gle/Yedpq3Dx8tAoxYkL8)

We really appreciate your time and support!

Best regards,

Amin Nikanjam (amin.nikanjam@polymtl.ca),

Foutse Khomh

SWAT Lab., Polytechnique Montréal, Montréal, Canada

[http://swat.polymtl.ca/](http://swat.polymtl.ca/)"
skwaaaaat,MachineLearning,1617210746.0,[Discussion] Methods for interpreting the meta knowledge learned by meta-learning methods?,"Hi, I've been trying to draw some insights from the meta-learned model, e.g., the task relationship, summarization of the shared task structure, etc.

To be more specific, for example, when learning from certain chemistry datasets, a good meta-learning method should implicitly learn about the shared chemistry principles. And my question is, how could we visualize/interpret such learned meta knowledge? 

I couldn't find any relevant literature on this topic. Could someone point me in the right direction? Thanks a lot!"
Seankala,MachineLearning,1619520030.0,[D] Is there any work highlighting the effectiveness of using bilinear transformations for certain tasks?,"I recently read a paper in computer vision titled [_Learning Deep Bilinear Transformation for Fine-grained Image Representation (Zheng et al., 2019)_](https://arxiv.org/abs/1911.03621) about a particular type of bilinear transformation (coined the ""group bilinear"") and they claim that bilinear transformations work well for fine-grained image recognition. Some works in natural language processing (in particular relation extraction) also claim that bilinear classifier layers work well.

I'm curious if there's any work out there that details _why_ this may be the case? Most of the material I read claim that the transformation ""learn semantic grouping"" or ""learn pairwise factorization,"" but it usually ends at about that.

Any recommendations or opinions are appreciated. Thanks."
ydennisy,MachineLearning,1620044509.0,[D] Many Logistic Regression heads.,"I am interested in building / finding a NN architecture which would consist of a network, which can be viewed as an embedding layer, with many LogReg heads on top each one predicting a certain class. The purpose is to learn a dense representation of features, which would be good at predicting the various classes, hence the loss from the LR heads needs to be propagated down into the network.

The interest I have here is would this architecture work well for new unseen classes from a similar domain / problem space.

Has anyone come across such a model architecture implementation?"
mLalush,MachineLearning,1618474344.0,[D] Why have the standard data formats in object detection remained as COCO/PASCAL VOC/YOLO as opposed to switching over to a nested columnar format?,"In image classification, early standards generally had users divide image files into different training and validation folders, as well as (sometimes) requiring separate folders for each class label. While researchers seem to have held on to these standards, the APIs of deep learning libraries eventually evolved to allow more flexible usage which didn't necessarily require users to shuffle around image files in folders should they want to adjust their training and validation setups.

Keeping training and validation files in different folders probably has certain benefits for the reproducibility of research, namely in making it *abundantly* clear how the train/val split was performed. However, this convention -- when enforced -- does introduce something of a ""barrier"" for beginners just looking to train a model on their existing labeled data with minimal data wrangling.

Thus, existing library APIs for image classification seem to have converged to loading references to filepaths, either as a list or in a tabular/columnar format, while encouraging users to perform their own train/val splits on these references to the data. This allows for maximum flexibility, as users can have their data organized however they wish, and easily perform whatever augmentations they wish to perform on the splits.

Most approachable tutorials for adapting a model to custom datasets now follow the standard of having users define their own dataloaders. This new standard, however, does not seem to have proliferated to *user facing* object detection and semantic/instance segmentation libraries.

Here, some of you might interject and point out that

1. The data is hierarchical in nature. Several bounding boxes, polygon coordinates or RLEs may exist for each image. COCO/PASCAL VOC/YOLO are the natural way to store such data.
2. Storing hierarchical data in a (nested) columnar format will introduce memory overhead.
3. Data formats such as .csv, .tsv don't work well with nested data.

None of these, in my opinion, are convincing arguments for why so many object detection libraries need to break with the conventions that have evolved for image classification. Hierarchical data can easily be stored as lists nested in cells of a dataframe. We don't necessarily have to repeat a row for however many objects exist in a given image.  The arrow ([https://arrow.apache.org/install/](https://arrow.apache.org/install/)) library exists for storing and loading nested columnar data.

Am I insane in thinking that object detection library APIs will eventually -- but inevitably -- converge to the standard of image classification APIs? Why haven't they already? Are the established standards too entrenched? Torchvision already seems to be heading there: [https://pytorch.org/tutorials/intermediate/torchvision\_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). I can't say the same for most other libraries, where the standard remains ""Please organize your data after these very specific instructions and execute this script with 20 poorly documented (optional) args""."
hardmaru,MachineLearning,1619877683.0,[R] Emerging Properties in Self-Supervised Vision Transformers,
Science_Squid,MachineLearning,1618565067.0,[D] AutoML MOOC,"In a collaboration of groups working on AutoML we developed a ***free*** ***MOOC*** on this topic. If you'd like to learn more about AutoML the MOOC is live at [https://learn.ki-campus.org/courses/automl-luh2021](https://learn.ki-campus.org/courses/automl-luh2021).

In 65 videos with overall \~19h, we cover Hyperparameter Optimization (HPO), Neural Architecture Search (NAS), Bayesian Optimization (BO), Evolutionary Algorithms (EAs) and meta-learning for AutoML. The course includes quizzes and coding exercises (in python and R) to allow you to deepen your expertise.

If you're interested check out the trailer for the course [https://youtu.be/9wzS7tGwI9g](https://youtu.be/9wzS7tGwI9g)

Edit: Tried to make more apparent that the course is free."
akirp001,MachineLearning,1617202841.0,[D] Are there any practical reasons for learning about the Boltzmann machines and how they work?,"About three years ago, I used a replicated softmax for some text data that required its own special embedding. I was finally able to apply something to a topic I had spent a ton of time trying to understand.

And yet today, even in that use case, I would probably go to hugging faces. 

It seems to me, there just isn't any notable papers or breakthroughs using it.

Part of my machine learning journey was going through the last chapter on generative modeling in the MIT book. It's certainly a slog and you get a lot out of it but now I wonder if it's even worth spending anytime on the boltzman sections."
vonum,MachineLearning,1619692257.0,[D] Audio processing on mobile devices,"Hello, has anyone been doing audio processing on mobile devices? I am having trouble finding tooling for audio processing. What tools did you use and which technologies?

  


I am using tensorflow js, so loading and making predictions is not a problem, however transforming the audio input from the microphone is a problem due to not being able to find libraries."
yusuf-bengio,MachineLearning,1617269822.0,[D] Keras: Killed by Google,"First of all, this is not a rant about Tensorflow (it actually is but more on that later). Disclaimer: I have been working on research projects with Teano, JAX, PT, TF 1 &2, and of course the original Keras.

The **original Keras** was just a high-level API specification for machine learning, which was really nice when collaborating with people who have less engineering background. The API was framework agnostic and the main implementation supported multiple backends (Teano, Tensorflow, and MS-CNTK)

Essentially, the API design resembled the abstractions of modern high-level frameworks such as PyTorch-Lightning and fast.ai, with slightly different *design* *flavors* (e.g., a Keras model combines the network with the metrics and training code in a single object, whereas other frameworks usually separate the network from the learner object).

The huge advantage of keras was that it was available and the API stable **back in 2016, 2017.** I think this is something remarkable in a field that moves so fast.

But then, you know the story, Google announced its plans to incorporated it into Tensorflow 2. This wouldn't have been a problem on its own, but it slowly killed keras for 3 reasons:

1. During the time-span of this merge, the keras API was effectively ""frozen"", making it lag behind alternatives in terms of features
2. The release of TF2 came too late. On top of that, the first versions were buggy and even now are lacking some basic features.
3. Instead of making a hard cut between TF 1 and 2, Google decided that it's better to carried over a lot of baggage and crap from TF1, making the framework extremely bloated. When something does not work, you get overwhelmed by long cryptic error messages and stacktraces longer than your screen can visualize.

So, this post is really intended as a **funeral for the keras API**.

Looking forward to know your thoughts.

EDIT: I have nothing personal against Google. Far from it, I really like their impressive contributions to ML (Colab, TPU, JAX, ...), but the story with keras and TF2 is really frustrating for me who liked working with it in the past."
hyunwoongko,MachineLearning,1616882340.0,[P] Openchat 1.1 is released! (support 30+ conversational model),"&#x200B;

https://preview.redd.it/laki77rl6np61.png?width=2412&format=png&auto=webp&s=bdb626be79abfa4c4e3720e6203655d00a71548b

Hello, I'm hyunwoongko who made openchat, an artificial intelligence conversation framework. openchat is an open-source framework that allows you to communicate with artificial intelligence with **just one line of code.** 

&#x200B;

Today, openchat has been updated to version 1.1 and I am writing this article. Unlike before, most engines have been changed to parlai and we can support 30+ conversational models. If you want to talk with artificial intelligence, try installing an openchat!

&#x200B;

For more information, plz visit [https://github.com/hyunwoongko/openchat](https://github.com/hyunwoongko/openchat) . Thank you!"
Pestocalypse,MachineLearning,1620144671.0,[N] Transformer and Capsule co-inventors launch new API-based NLP startup,"Announcement here: [https://twitter.com/AidanNGomez/status/1389574000796479489](https://twitter.com/AidanNGomez/status/1389574000796479489)

Article: [https://www.theglobeandmail.com/business/article-toronto-startup-backed-by-ai-experts-aims-to-bring-google-quality/](https://www.theglobeandmail.com/business/article-toronto-startup-backed-by-ai-experts-aims-to-bring-google-quality/)

>Some of the world’s leading artificial intelligence experts are backing a Toronto startup co-founded by protégés of AI luminaries Geoffrey Hinton and Jeff Dean that is attempting to make it easier for humans to talk to machines.  
>  
>Cohere Inc. is officially launching Tuesday, offering to plug companies into its machine-learning software over the internet by uploading three lines of code into their systems that provide access to their [technology](https://archive.is/o/dFsXl/https://www.theglobeandmail.com/topics/technology/). Cohere claims its software will provide a richer understanding of human language, including semantics, sentiments and tone.  
>  
>Early investors include Prof. Hinton, the University of Toronto professor and Google engineering fellow known as the “godfather” of deep learning; Ian Goodfellow, Apple’s head of AI; Raquel Urtasun, chief scientist and head of Uber’s ATG research and development division; Nvidia AI director Sanja Fidler and AI pioneers Fei-Fei Li and Pieter Abbeel. Toronto AI financier Radical Ventures wrote the first cheque to Cohere."
HybridRxN,MachineLearning,1618034652.0,Machine learning is getting easier software engineering still hard [D],"Hi, this is my first discussion post here. I'm currently an ML graduate student at university.

What do you think of [this](https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046) article?
Will those seeking or currently in machine learning roles be replaced by sophisticated frameworks in a few years driving down jobs? Or will jobs remain stable with more complex demands and advanced questions enabled by these tools ? Should Machine Learning enthusiasts and students interested in industry jobs focus more on things that are more difficult to automate like data engineering, data augmentation, devops and reliable interpretability (or something else?) than Kaggle-style architecture stacking?  

There may be some evidence for this at least in NLP.  With Huggingface [AutoNLP product](https://huggingface.co/autonlp) and [GPT-3 enabled applications ](https://openai.com/blog/gpt-3-apps/) .
I am really curious to hear the thoughts of the community.

Edit: article may be behind a paywall because of Medium so you can use incognito or outline site

Edit2: This is what GPT3 thinks (from one sample): ""I think the job market will continue to grow. The tools are getting easier, but there is still a lot of work to do. I think the ML community is going to have to figure out how to make the tools easier to use, but also how to make them more powerful. I think we will see a lot of ML engineers who are experts at using these tools, but not necessarily experts in the underlying algorithms.

I think there will always be a need for people who can design and implement new algorithms. I think there will also always be a need for people who can understand the limitations of the tools, and can design experiments to test out new ways of using the tools"""
Combination-Fun,MachineLearning,1620065927.0,[R] Video explaining Swin Transformers,"In the paper series of videos we present the latest ""Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"" paper this week. Hope its useful for understanding how transformers are increasingly getting used for vision tasks: [https://youtu.be/tFYxJZBAbE8](https://youtu.be/tFYxJZBAbE8)"
_conquistador,MachineLearning,1620076306.0,[P] Hi r/machinelearning! We created an AI-assisted video annotation tool that speeds up labelling time by 17x - looking for BETA testers to help us refine web application,"As part of our ML postgrad, we developed a tool for our lab that speeds up video labelling times by 15-20x using few-shot classification / human-in-the-loop input (e.g., a person labels a few frames and the algorithm handles the rest).

We’re looking for beta testers to help refine the platform to additional real-world use-cases. Besides early access to the platform, we’ll give any early testers free lifetime access once we launch.

Please fill out this form to get early access: [https://forms.gle/CGWd29xNv24Kwm1Y6](https://forms.gle/CGWd29xNv24Kwm1Y6)"
SherdyRavers,MachineLearning,1616770387.0,[R] I'm currently doing research about Gaussain Processes and I'm trying to develop a deeper understanding. My main concern is GP prior sampling,"I'm having a hard time understanding Gaussian Processes prior sampling. Here is the article from medium I'm using to learn about Gaussian Processes \[Article link\]\[[https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804](https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804)\].

&#x200B;

I'm going to start off by explaining my understanding of the partconfuses me. So we get the probability of 2 functions using this PDF equation. I'll call it equation\_1

&#x200B;

https://preview.redd.it/4ctmv5soydp61.png?width=671&format=png&auto=webp&s=2b154b7e075a8307051321bb486cd2fa112fec81

https://preview.redd.it/xmhxv4soydp61.png?width=790&format=png&auto=webp&s=4eb78b6195a0df8b384234ac2261afa23e4dcba2

&#x200B;

Then we plot each function and get the probability of each function using this pdf graph. I'll call it graph\_1 because I'm going to ask questions about it later on

&#x200B;

&#x200B;

https://preview.redd.it/6h5o4edqydp61.png?width=914&format=png&auto=webp&s=48b0143ba36230692fdace4529ed45f28400c9b9

After it is assumed that the prior is sampled using 600 data points and 50 functions are obtained. I've put the graph obtained below. I'll call it graph\_2

https://preview.redd.it/b4bwlfdqydp61.png?width=754&format=png&auto=webp&s=7d571dfc66db12c0ab4fce660ed6c21a8ee9e019

P.S: If you don't understand my questions because I missed out a few details, you can refer to thsi Article for missing details. \[Article link\]\[1\]

\*\*My Questions\*\*

1. When training the GP prior, is X and X\* two data points from the raining data?
2. For graph\_1, where does the Gaussian distribution come from?, also where do the positions of the functions on the x-axis come from? are they the standard deviation postions of the functions results?
3. For graph\_2, whats on the y axis? Is it the results axis standardised i.e instead of showing the actual result, the standardised result is shown?"
King-Little,MachineLearning,1619775635.0,[D] M1 MacBooks versus Google Colab for deep learning,"I am just starting getting into deep learning with `tf.keras`. I am at the point where I have to decide where I want to develop. The thesis project will be timeseries prediction. My options are PyCharm on Macbook Air M1 2020 or a 2013 4th Gen Intel i5 Linux desktop and Google Colab (please let me know if there are others).

So the question I have now is which one is faster/better suited for my puropses. M1 got [hyped](https://machinelearning.apple.com/updates/ml-compute-training-on-mac) a lot so I thought the M1 would savage my desktop (and acutally the hype biased my purchase decision), but well its only slightly better (like 1.2-1.5x faster in my cifar10 benchmark) and I wonder if its worth the effective 1-2 GB of RAM left on MacOS vs the \~14 GB on my Linux machine. Further there is Colab and I can't really tell which one will win the race, since Colab limits resources by demand but also allows distributed fit on cloud TPUs, which would introduce some extra coding efforts. Then again I have to say: so does ML on Apple Silicon, which comes with [a handful of limitations](https://github.com/apple/tensorflow_macos#additional-information), a [peculiar MiniConda setup](https://github.com/apple/tensorflow_macos/issues/153), a [lot of issues](https://github.com/apple/tensorflow_macos/issues) (also severe ones, like training errors etc., problems which I would not even recognize) which are actually not really being worked on.

Is from the perspective of professional data scientists, which I hope to find here, a clear indication on which I should choose?"
LynnHoHZL,MachineLearning,1619491098.0,[R] EigenGAN: Layer-Wise Eigen-Learning for GANs,"We post the paper and code of our new work EigenGAN which unsupervisedly learns hierarchical interpretable dimensions for GANs. Welcome to discuss.

Paper: [https://arxiv.org/pdf/2104.12476.pdf](https://arxiv.org/pdf/2104.12476.pdf)

Code: [https://github.com/LynnHo/EigenGAN-Tensorflow](https://github.com/LynnHo/EigenGAN-Tensorflow)

&#x200B;

[Gender](https://i.redd.it/81csinpunmv61.gif)

[Pose \(Yaw\)](https://i.redd.it/kf84ko6comv61.gif)

&#x200B;

[Painting Style](https://i.redd.it/zncv9a3lomv61.gif)

[Hue](https://i.redd.it/dl13laanomv61.gif)"
jj4646,MachineLearning,1619378764.0,[D] why are neural networks better than polynomial approximation?,Has anyone ever come across a formal mathematical explanation as to why neural networks  are more powerful than polynomial approximation?  Have some results (e.g. papers) been proven that conclusively show neural networks have certain advantages over polynomial approximation?
Yuqing7,MachineLearning,1616720149.0,[N] Tsinghua & MIT’s P-Tuning Boosts Performance on NLU Benchmarks,"Tsinghua & MIT researchers break the stereotype that GPTs can generate but not understand language, showing that GPTs can compete with BERT models on natural language understanding tasks using a novel P-tuning method that can also improve BERT performance in both few-shot and supervised settings.

Here is a quick read: [GPT Understands, Too! Tsinghua & MIT’s P-Tuning Boosts Performance on NLU Benchmarks](https://syncedreview.com/2021/03/25/gpt-understands-too-tsinghua-mits-p-tuning-boosts-performance-on-nlu-benchmarks/)

The paper *GPT Understands, Too* is on [arXiv](https://arxiv.org/pdf/2103.10385.pdf)."
austingwalters,MachineLearning,1619503940.0,[P] Data Profiler | What's in your data?,"Hello /r/MachineLearning

I thought the community might be interested in a project I have been apart of.

Our team has been working on a python library called the [DataProfiler](https://github.com/capitalone/DataProfiler). 

The project had two objectives:

1. Quickly and accurate (cheaply) identify sensitive data (PII/NPI) in datasets.
2. Generate data profiles which can be utilized in downstream (ML) applications

Regarding sensitive data detection, we published a workshop paper on the model within the library: [Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions](https://aaai-kdf.github.io/kdf2021/assets/pdfs/KDF_21_paper_10.pdf)

In addition to sensitive data detection, the library also calculates statistical features and general characteristics of a dataset. This has helped our team quickly evaluate datasets, but also enabled the profiles use in downstream applications.

Some nifty features the community may be interested in:  


* [Can load most files into a DataFrame with a single command](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/data_readers.html) (identifies headers, formats, etc)
* [Extending the current entity detection model with transfer learning](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/data_labeling.html#extending-a-data-labeler-with-transfer-learning) is easy and takes only a few lines of code (or retrain from scratch).
* The model works on both structured (CSV, TSV, JSON, etc) and unstructured data (text)
* It's possible (though a tad rough) to [add a new custom model for entity detection](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/add_new_model_to_data_labeler.html)
* [Profiles can be saved, loaded and merged](https://capitalone.github.io/DataProfiler/docs/0.4.4/html/profiler.html)

Generally, we are looking for feedback and curious what the community thinks of the project?"
regalalgorithm,MachineLearning,1618964072.0,[D] New Tag for Self Promotion Content?,"As this sub has grown, it has become normal for podcast / YouTube / blog creators to post links to their episodes /  videos / posts as [D] text posts on here, which are little more than links to the content. There was a [discussion ](https://www.reddit.com/r/MachineLearning/comments/j1s3yw/d_recent_increase_in_self_promotion_content/) specifically about this hald a year ago. I've done this myself quite a bit, as co-runner of two publications and a podcast about AI. I've stopped a while ago since it felt potentially obnoxious / spammy (actually I got called out for my newsletter posts, lol) , but still see it a lot. 

Maybe I'm just salty, but I do wonder if these are in the spirit of this sub. So, wanted to share an idea: perhaps there should a tag [SP] specifically for self promotion? This would differentiate it from [D] posts that are actually for discussion. I am not sure how much this community cares about this, and gramted it is pretty easy to differentiate self promotion posts from normal posts based on the title. But still, felt like floating the idea as someone who was not sure what is cool to do and what is obnoxious."
throwaway_secondtime,MachineLearning,1618380124.0,"[D] Sam Altman, Founder of OpenAI, proposes a ""Wealth For All"" plan for dealing with AI disruption",https://moores.samaltman.com/
ScienTecht,MachineLearning,1619845553.0,[P] I created a way to learn machine learning through Jupyter,"Hey all,

I’ve been working on a new way to help people practice machine learning concepts. 

Since most professionals in data science use Jupyter notebooks, I thought it’d be really cool for people to learn through interactive Jupyter notebooks as well. Here I’ve written an exercise that guides you through building a K-Nearest Neighbors classifier from scratch. As far as I know, I haven’t seen this done elsewhere. 

Please [**check it out**](https://www.confetti.ai/questions/1-3?utm_source=reddit&utm_medium=web&utm_campaign=jupyterhub-knn) and let me know what you think!"
a_computer_pun,MachineLearning,1617902342.0,[Research] Companies for compiling training data,"I was wondering if anyone had any useful companies that they've used for data retrieval. I need to retrieve data for Machine Learning training using sample data from event sites for training of a web scraper. So far I've looked into using Fiverr for some of the data but the results are pretty hit or miss. Here's a list of some I've looked into so far but have no idea about whether they are useful or not:

Lionbridge AI

Amazon Mechanical Turk

Clickworker

Appen

Globalme

Google Labeling Service

BasicAI"
this_username_is_tkn,MachineLearning,1620337867.0,[Research] Seeing use of ML for ordinary work puts a smile on my face.,"The objective of this paper is to find an alternative to conventional method of concrete mix design. For finding the alternative, 4 machine learning algorithms viz. multi-variable linear regression, Support Vector Regression, Decision Tree Regression and Artificial Neural Network for designing concrete mix of desired properties. 

[original Article ](https://dx.doi.org/10.22115/scce.2021.248779.1257)"
Caffeinated-Scholar,MachineLearning,1619117310.0,[R] Outcome-Driven Reinforcement Learning via Variational Inference,
aledinuso,MachineLearning,1618959406.0,[D] When do you start optimizing hyperparameters when trying out a new idea?,"When implementing a new idea, I find it hard to decide how much time I should spend on getting it to work before moving on to the next one. So I would like to know how other people do it, do you optimize hyperparameters always before abandoning a new approach?  Or is it more like the last thing to do if you already had some level of success?"
tdls_to,MachineLearning,1618660921.0,[N] Spotify Confidence - open source for analyzing a/b test data,"what do I all think about this library Spotify open sourced? sounds pretty useful but wondering if anyone has tried this or similar ones and if you have any recommendations?

https://github.com/spotify/confidence"
chasep255,MachineLearning,1617278214.0,[D] Using activity regularization instead of batch norm.,"Is there any reason I can't accomplish the same goal of batch normalization using an activity regularizer? Basically I would add a penalty to the loss function for a layer who's activation does not have an mean of zero and variance of one. There are two reasons I might prefer this method...

I am trying to train GANs and when using batch norm it behaves differently at inference then during training. This causes the discriminator to be able to have something like 90% accuracy while the generator also thinks it has 90% accuracy. I can solve this by using a really fast momentum of 0.5 but does not seem like the best solution. Ideally I want something that behaves the same during training as inference.

Secondly, I only have 8GB of vram on my RTX 3070 (really wish I went for the 3090 now). This poses a tight constraint on my batch size depending on the size of the model. As I understand batch norm works best with large batches of at least size 32. I usually am training with smaller batches.

I can't really think of a reason not to try this. Any thoughts? Also I am unsure what would be the best term to add to the loss function. I was thinking something like the following but there may be a better way.

`def normal_reg(x):`  
`return tf.square(tf.reduce_mean(x)) + tf.square(tf.math.reduce_variance(x) - 1)`"
Purple-Ad-3492,MachineLearning,1619042230.0,[D] Thoughts on using VADER for sentiment analysis on texts other than social media?,"Given that the package is designed for sentiment analysis tuned to social media (Twitter, NYTo, Amazon Movie Reviews). I like this tool for its sentiment rating on a -4 to 4 scale, but it’s limitations seem constrained as far as its classification system, i.e. words and phrases are pre-set and unlike other SA tools, words can’t be added to lists that would simply classify positive/neutral/negative on a -1 to 1 scale. 

(VADER tool was constructed and scores were given in these ranges based on an evaluation of 9,000 token features by 10 independent humans. )

I’ve seen the Harry Potter Book project subsected by chapter using VADER. Therefore my first question pertains to thoughts on reliability of these results? 

https://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-sentiment-analysis-1b474b13651d


I’m also looking into non-English text translations. i.e. texts that are originally written in another language and using the VADER sentiment analysis on them. I’d rather find some sort of work-around other than translating the texts themselves, perhaps translating the lexicons in the .txt file and in the code example negate, booster and special_case words lists.

...Although, I couldn’t say the word translation would have the same effective score as its English version. So maybe the preventive hiccup method would be to translate first .... 
or use another SA tool like Dostoyevsky (for Russian)."
rahulkumar1210,MachineLearning,1618165092.0,[P] Footprint recognition using feature extraction algorithm,"If someone can help me with this project and implementation. Please reply.

I am implementing the flow chart (attached image) in python. Got stuck in some stages (coding error).

Feature Extraction algorithm using is Principal Component Analysis (PCA), Independent Component Analysis (ICA), Linear Discriminant Analysis (LDA), VGG16, VGG19, InceptionV3, and ResNet50.

I want to use the Combination classifier (KNN or SVM) or one at a time. I want to conclude and compare the result of each of these algorithms. and declare the best.

[Process](https://preview.redd.it/tlqwjkz4fls61.png?width=561&format=png&auto=webp&s=f1e16cdbd5f32472071a0a6d91cf1eadc25d42c8)"
sensetime,MachineLearning,1616860563.0,[D] Jürgen Schmidhuber's work on fast weights from 1991 is similar to linearized variants of Transformers,"I saw that Schmidhuber [tweeted](https://twitter.com/SchmidhuberAI/status/1375345693758521345) a new blog post:

https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html

and in the post he discussed (in the Schmidhuber style) some of the works he did from the 1990's, in particular the use of ""fast weights"" which in principle would allow neural nets to learn to ""program"" other neural nets. He mentions that the methods proposed enabled ""fast weight changes through additive outer products of self-invented activation patterns"" which are similar to today's self-attention mechanism used in Transformers. Recently there has been several variants of Transformers that uses linear approximation for efficiency purposes, and such works demonstrate similar performance as the version with softmax, which he claims to be similar to fast-weights.

Apart from this blog post, Schmidhuber's lab also published an article recently on this topic, “Linear Transformers Are Secretly Fast Weight Memory Systems” (https://arxiv.org/abs/2102.11174). In this paper, they also propose better ways to linearize transformers inspired by some techniques from the fast-weight days, and show improvements compared to other linear variants of transformers, so I think this topic / discussion would be of interest to this forum."
VDevAGI,MachineLearning,1616668296.0,[D] Few-shot learning in practice.,"Although tons of few shot learning approaches have come up in recent years, they're only tested on Omniglot, miniImageNet and the likes.

I was wondering if these methods work for practical use cases. For instance can the latest CVPR'21 (say) SOTA few-shot paper be used to industrial problems with some engineering?

Or well-known baseline approaches like RFs, SVMs, kNN beat the SOTA in practice and are more robust."
begooboi,MachineLearning,1620577957.0,[D] How do we define a discriminative model?,Generative models are those which learns the distribution of the data. If generative models approximates the distribution of data then what does a discriminative model learns?
Equivalent-Choice-75,MachineLearning,1618590893.0,[D] ML PhD at top 5-10 ranked school vs RE at FAANG (Applied teams),"Hi,

I'm trying to decide between ML PhD at 5-10 CS school (in USA) and MLE at FAANG (It is one of the applied teams and not pure research teams like FAIR/Google Brain)

If going to industry is eventual goal, which of these two options is preferable?

ML PhD - Highly reputed but takes 5 years to finish. Not sure if the effort + wait is worth the reward.MLE - Might not be cutting edge work. Focus is still on the product, but get to do decent amount of engineering + research for the product. Title of the role is Research Engineer though!

Eventually, I'd love to work in places like DeepMind, AI for drug discovery, AI for Climate Change, etc.

Towards this, is it beneficial to do a 5 year PhD or start from MLE/RE and graduate my way there? Really confused as to how to make this decision.

Thanks!"
hellohihello__,MachineLearning,1616586780.0,[D] [P] Evaluation Metrics for Pre-Trained Faster R-CNN,"Hi everyone

Recently I've been working on creating a social distancing detection model for which I used detectron2 for Faster R-CNN pre-trained weights.

I am using video datasets.

But the issue now is that I am unable to perform an evaluation for the model.  I don't know how to evaluate precision, recall, mAP etc since it is pre-trained.

Is there any way to perform the evaluation for a pre-trained faster R-CNN?

I do not have ground truth values for the datasets."
ProbablyCloseEnough,MachineLearning,1620535166.0,[R] Slurm Interface Investigation Report,"This is a followup on my previous threads [\[R\] Slurm interface survey (2 minutes)](https://www.reddit.com/r/MachineLearning/comments/lfn7d9/r_slurm_interface_survey_2_minutes/?utm_source=share&utm_medium=web2x&context=3) and [\[R\] Slurm Interface Prototype Evaluation Survey (2 minutes)](https://www.reddit.com/r/MachineLearning/comments/mf847y/r_slurm_interface_prototype_evaluation_survey_2/?utm_source=share&utm_medium=web2x&context=3). Your responses to these surveys helped me complete coursework in the human-computer interaction course I was taking. Thank you.

The first survey was an instance of **needfinding**, which is where I investigate the users of an interface with an aim to find their demographics, the things they are trying to do with the interface, the context of these tasks, and their impressions of the interface(s) they use to do a certain **target task**, which in this case is scheduling a computing job on shared computing resources using Slurm. The following is copied from my paper.

>There are a few takeaways that I can get from the survey results. Most respondents use a command line interface, presumably the one that Slurm provides, and they are not satisfied and not unsatisfied with it. Most respondents use Slurm fewer than 10 times per week, which is expected because we expect computing jobs to have high enough complexity to be worth scheduling with Slurm. Other insights might be compromised by bias, as explained below.  
>  
>I had intended to take certain steps to control for biases, but I had not anticipated the limited time and responses that I would have to gather responses. Therefore, some of the mechanisms that I had intended to use to reduce bias are not implemented. One such mechanism was providing the survey in Mandarin in addition to English. I only provided it in English because I did not have time to verify a Mandarin translation. Thus, English-literate users are overrepresented. Another mechanism was distributing the survey in different communities. Because I was limited to 25 responses, I had to stop gathering responses before I could send it to communities other than students taking CS 6750 at Georgia Tech and [r/machinelearning](https://www.reddit.com/r/machinelearning) on Reddit, which is frequented by machine learning researchers. Thus, academic users and researchers are overrepresented. I was able to implement mechanisms that were inherent to the survey design, such as not showing questions before participants start the survey and asking questions so that they do not encourage certain responses over others.

I executed one other needfinding activity, in which I observed 3 participants doing the target task, giving them the option to explain their actions. I also performed a heuristic evaluation of the Slurm interface for doing the target task. Based on these activities, I defined performance goals for proposed prototype interfaces.

>A new interface should maintain functionality for the most common tasks that the users do, which includes the target task, as well as determining available partitions, charge IDs, and hardware configurations that are available on the computing clusters. Criteria for evaluation is whether a user can perform those tasks. At the same time, it should be at least as accurate and efficient compared to the command line interface. Evaluation criteria are time and keystrokes or clicks. It should require the same or less time or the same or fewer keystrokes or clicks to perform the same tasks and doing them predictably and reliably.  
>  
>At the same time, it should be more learnable such that a new user should be able to learn to use it from within the interface, and perform at least the target task without script errors caused by forgetting key words or syntax. The criterion for evaluation is whether a new user can do so.  
>  
>In terms of accessibility, if appropriate files are in place, performing at least the target task must require only mouse clicks or finger taps, with a keyboard required only for first-time authentication. The criterion for evaluation is whether a user can do so.  
>  
>The interface must be compatible with client devices running Linux, and preferably also Windows and macOS. The interface must be compatible with Slurm servers running Linux and comply with their terms of use. The criterion for evaluation is whether it can be installed and used on Linux, connecting to a Slurm server running Linux.  
>  
>The development of the interface must cost no more than 20 hours of development time for a single researcher who has limited experience in graphical user interface development.

At the time, I was not sure whether I had to implement the interface. Upon learning that I did not, I decreased the importance of cost.

I then proceeded to brainstorm 20 prototype interfaces. I used my gut feeling to evaluate the anticipated performance of each idea in terms of functionality, accuracy, efficiency, learnability, accessibility, compatibility, compliance, and cost. Weights were 1, 1, 0.8, 1, 0.8, 1, 1, -0.2 respectively. (The weight of cost is negative because lower is better.) I found the top three ideas were

1. A simple executable with a form
2. A DAG for organizing jobs
3. A run configuration available in an IDE

The third one was presented in my second survey. [This figure](https://imgur.com/0m9onoh) was presented with the following text.

>Consider an interface that is an extension that can be installed on an integrated development environment (IDE), that enables scheduling jobs in the same way that a user would run them locally through a run configuration. The configuration settings panel (shown in the figure above) enables changing common settings and enforces a valid configuration. Upon clicking the Run button, the interface attempts to schedule the job.

[This figure](https://imgur.com/7AH5zCX) summarizes the responses I got from the survey. Because respondents need not perceive the intervals between response options to be constant, I can't do any statistical tests, but I can still eyeball the results to inform a subsequent round of needfinding.

>The main takeaways are that the prototype might not meet efficiency requirements. As expected, the greatest advantages the proposed interface has over the command line interface (which I determined users typically use) seem to be that it is more accurate, more learnable, more memorable, and that it fits better into users' workflows. I was surprised that many respondents expected to perform the target task somewhat slower. None expected to do it much more quickly, and 2 expected to do it much more slowly. An interpretation would be that many respondents are so experienced at using the command line interface that it would be difficult to outperform them.  
>  
>Changes that are suggested from the feedback are that the next iteration of this prototype, if it will be subject to further development, should try to increase efficiency. However, the demographic may find that the sacrifice in efficiency is worth the advantages in other areas. I did not collect information about the relative importance of these aspects.

At this point, the course assignments changed such that I had to investigate a different interface. I've become very busy with work, and since I won't be paid by anyone for continuing this investigation, and because my grade is no longer being held hostage, I'm going to just leave this as is."
sarmientoj24,MachineLearning,1619179833.0,"[P] Is it possible to create a benchmark OSes performance in terms of ML training, prediction?","I am thinking of a possible topic for Advanced OS (for a small research paper):  


My goal is to try and check the performance evaluation of different Operating Systems such as Ubuntu, Debian, Fedora, Mint, Windows, CentOS, etc. in terms of training and prediction from simple ML models to CNN and RNNs.

Is it possible to do some benchmarks **focusing on Operating Systems** for this? If so, what kind of tools can I use to do some benchmarks? For example, check CPU usage, RAM usage, etc. 

I am planning to do the following:  
\- Create a Docker container of the specified OS  
\- Start tool benchmark/measurement   
\- Start Training #1  
\- Start Prediction #1  
\- Start Training #2  
...  
and so on...  


My questions would be:  
\- is it possible? I haven't seen papers tackling OS effect on Machine Learning training  
\- Is there a measurement tool?  
\- Can I do these given the outline of my preferred methodology"
meldiwin,MachineLearning,1617817881.0,"[N] Dieter Fox "" The Next Generation Of Robotics"" New Episode","Hello Guys,

\*\* please feel free to remove if it is not relevant\*\*

We   (IEEE Soft Robotics Podcast) recently Interviewed Prof.Dieter Fox, and we would like  to have your feedback about the episode, any comments would be helpful for future guests :)

You can find the episode here

Audio: [https://soundcloud.com/ieeeras-softrobotics/dieter-fox-the-next-generation-of-robotics](https://soundcloud.com/ieeeras-softrobotics/dieter-fox-the-next-generation-of-robotics)

Video: [https://youtu.be/ssTFlcoAdsc](https://youtu.be/ssTFlcoAdsc)"
ilovemouchou,MachineLearning,1619365371.0,[D] Need help figuring out job offers,"I just got an offer from Amazon for a Research Scientist position in one of the AWS customer facing teams. I really like the ideas of working as a ML consultant on super diverse projects but I'm a bit scared of the potential pressure and hard deadlines that might come with it. On the other hand, I got an offer from Google for a Data Scientist position in one of their Trust and Safety team. In terms of work life balance and overall employee well being Google is an amazing company (I interned there and loved it) but I'm afraid being a Data Scientist in Trust and Safety could be more of a niche. I was wondering how easy it would be to move to another team within Google after a couple of years.

I don't care too much about the comp, I come from academia so any tech job offer feels like I've won the lottery compared to my postdoc salary :) but more about growth and opportunities within these companies, or how these positions would be perceived by recruiters in the future. I also care about flexibility with remote work : my family is in Europe and my partner in the US so it would be great to be able to spend extended periods of time on both continents.

Another thing : if I decline the Google offer to go to Amazon, will it be harder for me to get into Google later?"
MohamedRashad,MachineLearning,1620348992.0,[D] Is there an idea similar to SPP but for Convolutions ?,"I need a way to keep the output of the last convolution layer fixed (32x24 for example) whatever the input size is.

Variable Input Size -> Fixed feature map size, Do anyone have an idea on how to do something like this?"
svantana,MachineLearning,1620286725.0,"[D] Is the concept of an 'epoch' being phased out, or even harmful?","I've noticed a trend that more papers are reporting the number of training ""steps"" rather than epochs.It kinda makes sense since datasets now vary in size from the dozens to the billions. This got me thinking: is the concept of an epoch potentially harmful, as it enforces the idea of the data as something finite? Ideally, minibatch training approximates IID sampling from an infinite dataset (the true distribution). What is the argument against IID sampling of the training data?

I recently saw someone who was scatterplotting batch losses during training. This is more informative than the standard ""loss per epoch"" plot. It could be combined with a moving average of the latest K batches, which is \~gaussian, so confidence intervals are easily added. And with IID sampling there won't be any leftover batches with odd sizes.

EDIT: The reason this came up for me is because I have a model trained on two datasets with different losses. I was struggling with how to define an epoch when it dawned on me, why even use epochs at all? Just sample the data for each batch. Works fine. And one less nested loop is good for your health :)"
fasttosmile,MachineLearning,1618593591.0,[R] Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little,"
https://arxiv.org/abs/2104.06644"
adcamuto,MachineLearning,1620562568.0,[R] Explicit Regularisation in Gaussian Noise Injections,"This work studies the regularisation induced in neural networks by Gaussian noise injections (GNIs). Though such injections have been extensively studied when applied to data, there have been few studies on understanding the regularising effect they induce when applied to network activations.

Key findings:

\-  The work derives the explicit regulariser of such injections, which is a positive term added to the loss function obtained marginalising out the injected noise. 

\- This regulariser penalises networks that learn functions with high-frequency content in the Fourier domain and most heavily regularises neural network layers that are closer to the output (see Figure below for an illustration of this). 

arxiv: [https://arxiv.org/abs/2007.07368](https://arxiv.org/abs/2007.07368)

code: [https://github.com/alexander-camuto/exp\_reg\_GNIs](https://github.com/alexander-camuto/exp_reg_GNIs)

&#x200B;

[Figure:   We illustrate the effect of GNIs injected throughout a network’s activations.  Each coloured dot represents a neuron’s activations. We add GNIs, represented as circles, to each layer’s activations bar the output layer. GNIs induce a network for which each layer learns a progressively lower frequency function, represented as a sinusoid matching in colour to its corresponding layer](https://preview.redd.it/yk00zwag63y61.png?width=1308&format=png&auto=webp&s=f66628ab9cc84ca7d801c0d34781a898eba2993a)"
SQL_beginner,MachineLearning,1617811685.0,[D] Feature Selection for Large Datasets,"To begin my question, I would like to quote a paper (by Ishawaran et al) on ""random forests for survival analysis data"", in which the authors (very concisely) outline the difficulties of feature selection (i.e. which variables to include in a statistical model) in classical regression models and how this problem is somewhat alleviated with more advanced models :

""Further, because these methods (i.e. classical regression models, e.g. cox ph regression - even though it's semi-parametric) are often parametric, nonlinear effects of variables must be modeled by transformations or expanding the design matrix to include specialized basis functions. Often ad hoc approaches, such as stepwise regression, are used to determine if nonlinear effects exist. Identifying interactions, especially those involving multiple variables, is also problematic. This must be done by brute force (examining all two-way and threeway interactions, e.g.), or must rely on subjective knowledge to narrow the search.

In contrast, these difficulties are handled automatically using forests. We illustrate the ease with which RSF can uncover complex data structures through an in-depth case study of the prognostic implications of being underweight, overweight, or obese and having severe, but stable coronary artery disease.

Investigators have noted complex patterns surrounding possible reverse causation in underweight individuals, interactions with smoking, and an unclear inflection point at which point increasing body mass confers increased risk Some have identified a possible obesity paradox among patients with established heart disease in which increased body mass predicts better survival. To clarify these issues, we analyzed a large cohort of patients with coronary artery disease undergoing isolated coronary artery bypass surgery. Using RSF, (random survival forest) we identified a complex relationship between long-term survival, body mass, renal (kidney) function, smoking, and number of internal coronary artery bypass grafts. We believe our novel findings help explain some of the apparent contradictions previously reported.""

Source: https://arxiv.org/pdf/0811.1645.pdf

Essentially, the authors claim that traditional regression models struggle with feature selection and the newer models (e.g. bagging, random forest) are able to better deal with feature selection. I do remember from an intro stats class, the somewhat tedious process of determining which variables to include in a multiple linear regression model. As the authors described, I remember there was something called ""CP Mallow's Criteria"" in which potential variables were repeatedly included and excluded in the regression model and the value of CP Mallow's Criteria was monitored - a final selection of variables for the model was decided on the basis of this criteria. However, this selection process becomes inefficient for large datasets (if I understand correctly, this means you would have to refit the model for many different combinations of variables, resulting in a ""combinatorics explosion"" for a large number of variables). Like the authors mention, you can also ""manually hard code"" interaction terms in the model (e.g. log(var1), var1var2, var1/(var2var3), var1/(var2+var3), etc.) - and there an infinite such number of potential interactions. Improper feature selection can also result in unwanted effects such as multicollinearity. The last point I would like to bring up - although my knowledge of mathematics is not strong enough to fully substantiate it - is that classical regression models are said to have a tendency to overfit (I don't know why - I have seen visual demonstrations of this, but I don't know if there is a mathematical explanation behind this, or if it's just an empirical observation) and poorly generalize to new data (again, I don't know why); and that classical regression models are only able to ""recognize linearly separable patterns in the data"" (intuitively I can understand this, e.g. draw a circle of red points and a smaller circle of blue points that fits in the red circle, a single line can not separate the two colors - but I don't know if there is a mathematical explanation behind this).

This brings me to my question about feature selection for large datasets. With the advent of technology, data is becoming bigger and bigger everyday - convolution neural networks are the ""go to method"" for analyzing pictures (a standard black and white picture is said to have 786 variables), whereas DNA is said to have even more. In such instances, it surely must be impossible to address feature selection as done in conventional statistical modelling. Please excuse my poor understanding of math - but my understanding is that newer statistical models have ""built in"" methods of handling the feature selection problem. For instance, random forest ""randomly"" chooses different combinations of variables and sees which combinations result in better model performance, the exact randomizing mechanism (uncorrelated trees) is said to also prevent against multicollinearity (I ahve heard that the creator of the random forest algorithm Leo Breiman claims through theoretical statistics that random forest by definition can not ""over fit"" and has some desirable error bounds and convergence properties - is this true?). Meanwhile, I have read on data science blogs (I'm not going to lie) that deep neural networks are able to ""automatically"" learn and consider ""useful"" combinations of features for approximating the target function (am I correct?).

All in all, what I want to ask here : for large datasets, where sometimes the features don't have any immediate meanings (e.g. a patient's blood pressure vs the information contained in the 231st pixel of a photograph) - is there any ""real"" way to handle feature selection? Or is this usually taken care of by the statistical model itself (e.g. random forest and neural networks)? I have seen examples online where people attempted to write a massive FOR LOOP in which they train the same model with thousands of variable combinations ... but I am not sure how feasible this is.

Can someone please provide a comment on this?

Thanks"
mikegartrell,MachineLearning,1619447583.0,[N] Deadline extended: Call for papers: KDD 2021 Workshop on Bayesian Causal Inference for Real-World Interactive Systems,"[https://bcirwis2021.github.io](https://bcirwis2021.github.io/)

August 14 - 18, 2021 (final workshop date TBD)

&#x200B;

\---

Submission deadline (**extended**): May 20, 2021, anywhere on Earth

Format: 3 page extended abstract + references + appendices, ACM Proceeding Template

Submission website: [https://cmt3.research.microsoft.com/BCIRWIS2021](https://cmt3.research.microsoft.com/BCIRWIS2021)

\---

&#x200B;

Increasingly we use machine learning to build interactive systems that learn from past actions and the reward obtained. Theory suggests several possible approaches, such as contextual bandits, reinforcement learning, the do-calculus, or plain old Bayesian decision theory. What are the most theoretically appropriate and practical approaches to doing causal inference for interactive systems?

We are particularly interested in case studies of applying machine learning methods to interactive systems that *did* or *did* *not* use Bayesian or *likelihood* based methods, with a discussion about why this choice was made in terms of practical or theoretical arguments. We also welcome submissions in the following areas:

* Offline evaluation of recommender and interactive systems.
* Comparison of Bayesian, off-policy and other heuristic approaches for offline metrics.
* Probabilistic approaches applied to contextual bandits and reinforcement learning approaches.
* Probabilistic approaches to incrementality and attribution.
* Non-Bayesian approaches and trade-offs with Bayesian/Likelihood approaches.
* Bayesian methods in a production environment.

&#x200B;

Organizers

* Nicholas Chopin (ENSAE)
* Mike Gartrell (Criteo AI Lab)
* Dawen Liang (Netflix)
* Alberto Lumbreras (Criteo AI Lab)
* David Rohde (Criteo AI Lab)
* Yixin Wang (UC Berkeley)"
xdtolm,MachineLearning,1618772636.0,[P] VkFFT now supports OpenCL,"Hello, I am the creator of the VkFFT - GPU Fast Fourier Transform library. In the latest update, I have added OpenCL as a backend option (in addition to Vulkan, CUDA and HIP) so if some of you are interested in OpenCL FFT - feel free to check it out and ask any questions! The performance of it is on the same level as other backends.

GitHub link: [https://github.com/DTolm/VkFFT](https://github.com/DTolm/VkFFT)"
zy415,MachineLearning,1616513517.0,[D] IJCAI 2021 Paper Reviews,IJCAI 2021 paper reviews are supposed to be released soon (tomorrow). Creating a discussion thread for this year's reviews.
gokuresearch,MachineLearning,1617167679.0,[D] Is it worth buying a GPU workstation as a graduate student?,"I'm graduate student pursuing masters with thesis. I'm also hoping to pursue PhD in the future. Right now, I've a laptop with GTX 1070 and it helped me a lot for learning various algorithms in ML/DL, which also indirectly helped me publish few research papers in well known conferences during my undergraduate.

But with the growing computation requirements for DL, I was wondering that is it worth investing in a proper workstation having 2x RTX 3080 or other similar high configs for a graduate student (who is already a part of the well funded research lab and have access to GPU clusters for lab's research work)??

I love to do personal experiments and exploring latest research whenever possible. And because of the current trend, it requires more and more computations to just explore most of the things. Apart from that, global chip shortage is also a concern as it's leading to increase in GPU prices overall.

Therefore, I would like to receive any kind of suggestions and thoughts.



Note to admin: Please let me know if this post is out of scope here."
thisisdhruvagarwal,MachineLearning,1619340081.0,Bullet Physics vs Pandas3D [D],"Since you know that Mujoco is not free.. Hence, I was looking for some alternatives and got Bullet Physics and Pandas3d... But I am not sure which one to use... Which do you think is more easy to learn and which one would you prefer?"
techsucker,MachineLearning,1616957686.0,[R] Researchers at Lawrence Livermore National Laboratory (LLNL) Developed a Novel Deep Learning Framework for Symbolic Regression,"At the Lawrence Livermore National Laboratory (LLNL), scientists have developed a novel framework and an accompanying visualization tool that utilizes deep reinforcement learning for symbolic regression problems, outperforming baseline methods on benchmark problems.

Their paper was recently accepted as an oral presentation at the International Conference on Learning Representations (ICLR 2021). In their paper, the researchers describe applying deep reinforcement learning to discrete optimization. Discrete optimization focuses on problems that deal with discrete “building blocks” that must be combined in a particular order or configuration to optimize the desired property. They focused on a type of discrete optimization called symbolic regression. Symbolic regression finds short mathematical expressions that fit data gathered from an experiment. It aims to discover the underlying equations or dynamics of a physical process.

Summary: [https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/](https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/) 

Paper: [https://openreview.net/forum?id=m5Qsh0kBQG](https://openreview.net/forum?id=m5Qsh0kBQG)"
user692646,MachineLearning,1620387919.0,[D] Element-wise multiplication instead of Convolution,"I'm considering using element-wise multiplication as the fundamental operation in a net, and I wanted to know if this has been done before in the literature.

The operation for a single layer would work as follows:

1. Given an M x N input image, multiply it element-wise by an M x N weight matrix.
2. Add an M x N matrix of biases to the output of step 1.
3. Compute a non-linearity (e.g. ReLU) element-wise on the output of step 2.
4. Use a K x K average pooling kernel with stride K to down-sample the output of step 3.

The reason I am considering using this operation instead of a conv layer is because I think the conv layer is a bit too regularized for my problem. By that, I mean if the convolution operation were treated as a matrix-vector multiplication, such that the input to the conv layer is an image re-shaped to be a column vector, then the matrix would be too sparse.

If needed, I could enforce sparsity later with L1 regularization, for example.

Has this sort of operation been considered before in the literature?"
regularized,MachineLearning,1620406026.0,[D] Have you heard of MBZUAI (Mohamed bin Zayed University of Artificial Intelligence)?,"When I look at their faculty, I see very prominent ML researchers such as Eric Xing and Le Song.

[https://mbzuai.ac.ae/study#faculty-sec](https://mbzuai.ac.ae/study#faculty-sec)

Have you heard of this university? What do you think? 

Are these professors really based in Abu Dhabi? Maybe someone who is from CMU (Eric Xing's former university) and from GaTech (Le Song's former university) can comment."
mistermysterioyster,MachineLearning,1618110169.0,[D] Paper Reading Group #017 - Adversarial vulnerabilities of human decision-making. (Link to full slides in comments!),
davex32,MachineLearning,1620599656.0,[P] Latest TensorFlow 2.5.0rc3 optimized wheels with CUDA 11 and Python3.9,"I  built some wheels for the new Tensorflow 2.5.0rc3 with CUDA 11 and cuDNN 8  in case anyone finds them useful. This includes SSE4.X,AVX2,FMA  instructions: I usually build these for skylake march  (and in this case with glibc 2.33)  or other  architectures on request, depending on my availability.

Why is this useful? For when you install the official binaries and see a warning like this:  
`Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2`

[https://github.com/davidenunes/tensorflow-wheels](https://github.com/davidenunes/tensorflow-wheels)

in case anyone finding these useful, contribute to my coffee addiction 🤣☕ and support these builds and related projects here: [https://github.com/sponsors/davidenunes](https://github.com/sponsors/davidenunes) or [https://ko-fi.com/davidenunes](https://ko-fi.com/davidenunes)

or just say hi [@davidelnunes](https://twitter.com/davidelnunes) on Twitter."
post_hazanko,MachineLearning,1618592038.0,[D] Filling in missing data for bad video on client end,"I think that would be an interesting application, I've started to see it where you can make a blurry image become super clear.

So I'm wondering if there would be a thing at some point where they integrate a ""model"" on the client side think Web/JS for video transmission... usually what you see is the local/source is crisp and the recipient is a little worse, makes sense.

Concern is how accurate the fill is."
thunder_jaxx,MachineLearning,1617344522.0,"[D] For Anyone Who Has Clocked More Than 50+ Days Of DL Model Training Time, Do You Use Anything Other Than Adam or AdamW?","For almost All ML projects which had DL, I used AdamW and it just worked. So fucking well. So a few questions to fellow Redditors who might be training models frequently :

Do you use a different optimizer? Why?  

Do you tune the Beta values? 

Have you consciously ever chosen not to use Adam? Why? 

I have seen some recent fancy optimizers like [PCGrad](https://arxiv.org/pdf/2001.06782.pdfhttps://arxiv.org/pdf/2001.06782.pdf) but never found the need to use it. When did you use them if you had to?"
brandonrussell757,MachineLearning,1619633167.0,[D] TP/FP Object Detection Question," Hey everyone, so I am in the middle of implementing the mAP metric from scratch so I can get details on all the data associated with the calculation of the mAP itself (ie. AP, Precision, Recall, TP, FP, FN).

I understand the logic behind calculating TP/FP in such that:

* TP = class detections with IOU >= IOU\_THRESH
* FP = class detections with IOU < IOU\_THRESH

My question is how would you calculate a situation where a **SINGLE** ***predicted*** bounding box overlaps **TWO** or **MORE** ***ground truth*** bounding boxes with an **IOU** that meets the **IOU\_THRESH,** for each of the ground truth boxes? My initial thought would be you would just rule the one with the highest **IOU** as the TP and all others as FP. Am I right with this assumption?"
ilikepancakez,MachineLearning,1618923355.0,Generative Adversarial Transformers [R],
SubstantialRange,MachineLearning,1616789302.0,[D] What are the CASP competition equivalents for scientific fields other than protein folding?,"The Critical Assessment of protein Structure Prediction is a bi-annual contest that measures the progress of computational methods in the domain of protein folding. It was famously won last year by DeepMind's Alphafold.

What are some similar benchmarks in other scientific fields that most people may not know about?"
Firehead1971,MachineLearning,1617264663.0,[D] Collecting ideas and hot topics for possible PhD thesis,"Hi,  I am an Argentinian ML researcher who is at the beginning of his  dissertation (i.e. choosing an suitable and interesting topic). I think  it would not be bad if we could collect the hot topics here and thus  have a simple overview of them. For example, what are the edging topics  where the world is right now? Which topics are still in the shadows and  might come up soon? So I will just start and write what comes to my  mind:

\- Transformer Models (still a hot topic)

\- Neural Network Optimization Techniques (more on a mathematical and comparison basis)

\- ML Integration in different areas of life (health care, road traffic,...)

\- MLops for enterprises or production environments (how does it look in reality?)

\- Make ML more understandable (can you use ml to teach ml?)

\- Computer Vision (certainly there are lot of possibilities there but also already lot of things have been done here)

\-  Doing some fancy audio ML classification stuff to recognize approaching  killer insects/animals (cross field between ml and biology)

\- You might continue this list with your ideas..."
Puzzleheaded-Drop297,MachineLearning,1617041579.0,"[D] EMNIST dataset down, network unreachable","I'm using torchvision to download the EMNIST dataset. But it is down at this moment. Can someone send me a copy? Thanks!

[http://www.itl.nist.gov/iaui/vip/cs\_links/EMNIST/gzip.zip](http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip)

Perhaps we need better ways to host datasets like this.

\------------

The dataset just went back online."
mistermysterioyster,MachineLearning,1616330212.0,[D] Paper Reading Group #014 - Accurate uncertainties for deep learning using calibrated regression.,
dsmlthrowaway,MachineLearning,1617121603.0,[D] Looking for advice: hiring data practitioners,"I work for a large company focused on supply chain. Medium cost of living city. We are starting to incorporate machine learning techniques to improve a lot of our processes. We've had good success and the goal of ""cushioning the blow of investment"" has been met.

So now we're looking to invest in scaling out this team (hiring maybe 3-5 additional people). The rub of the matter is: I'm not really sure how to attract the right talent for the job. We do not need cutting edge stuff. For most business problems, the value is immense if we can get something ""good enough"" out of the door. Pragmatic, but in between 'hacker' and 'enterprise.'

I come from a comp sci/programming background, moved into data, and now am basically using the tools without much in depth knowledge, but have the business domain knowledge to know where to successfully apply them. Ideally, I would love to hire ""problem solvers"" who care more about the business and generating value than how they accomplish it. I think these types of positions will start popping up a lot more as the barrier to entry decreases, but I'm having the damndest time on attracting the right people. It's not really a data science role, or a data engineering role, or a ML role. It's kind of a 'jack of all trades."" A data practitioner is what I call it, but I don't see any similar postings on indeed (if anyone knows of any I could cheat off of, that'd be great).

So now the questions:

1. Do you guys have any advice on what job title we should be posting?
2. What would you expect a salary range to be like for this type of position? Midlevel data folks vs mid level programming seems to pay less, but I don't want to drive away anyone.
3. What types of phrases can I use to make it clear that this isn't necessarily a ""data science"" role or a ""ml"" role. It's a problem solving role around data. I'd hate for anyone to be disappointed in the work."
Yuqing7,MachineLearning,1618591151.0,[N] ETH Zurich Leverages Spiking Neural Networks To Build Ultra-Low-Power Neuromorphic Processors,"A research team from ETH Zurich leverages existing spike-based learning circuits to propose a biologically plausible architecture that is highly successful in classifying distinct and complex spatio-temporal spike patterns. The work contributes to the design of ultra-low-power mixed-signal neuromorphic processing systems capable of distinguishing spatio-temporal patterns in spiking activity.

Here is a quick read: [An Error-Propagation Spiking Neural Network Compatible With Neuromorphic Processors](https://syncedreview.com/2021/04/16/eth-zurich-leverages-spiking-neural-networks-to-build-ultra-low-power-neuromorphic-processors/).

The paper *An Error-Propagation Spiking Neural Network Compatible With Neuromorphic Processors* is on [arXiv](https://arxiv.org/pdf/2104.05241.pdf)."
ArulVendhan,MachineLearning,1617187489.0,[P] Hawking Date Time Parser is Open-Source Now,"It's a great pleasure to announce our Natural Language Date Time Parser using Stanford CoreNLP in the backend is open-source now. Do Check and Let us Know the Feedback.  
Github: [https://github.com/zoho/hawking](https://github.com/zoho/hawking)

Blog : [https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html](https://www.zoho.com/blog/general/zias-nlp-based-hawking-date-time-parser-is-now-open-source.html)  
Tweet from [Stanford University](https://twitter.com/stanfordnlp) : [https://twitter.com/stanfordnlp/status/1376914683127492614?s=20](https://twitter.com/stanfordnlp/status/1376914683127492614?s=20)

\#nlp #stanfordnlp #datetimeparser"
TheElementsOf,MachineLearning,1620638064.0,[D] Trustworthiness in current AI applications,"I am currently doing a research on trustworthiness in current applications of AI and would like to discuss this issue, as I believe that it is very important aspect if the AI should be use on daily basis by everyone. My main interest is in automotive industry, i.e., trust in self-driving cars and / or weaker decision support systems for driving. I believe that this discussion will be beneficial for everyone, especially if many people from different fields will contribute, as a subject such as trust or ethics should never be done by one person / organisation. Please include your ideas of current and future problems and maybe also some references.

&#x200B;

I see the main problem of trustworthiness of AI applications, which are not interpretable (black-box models). However, even if researchers would somehow understand the decision process of a model and why it arrived to the decision it did, how can we ensure consumer that it is safe to use the machine with such setting, if we can not process all possible situations and outcomes anyway? On the other hand, if the model is interpretable, how can we ensure consumer that their data are still private? Is an interpretability of a model problem in privacy preserving? And how to explain pros and cons to the wide population? "
strngelet,MachineLearning,1620054250.0,[P] Python library to boost T5 models speed up to 5x & reduce the model size by 3x,"Edit :

>**T5** \- (Text to Text Transfer Transformer) is a large seq2seq **transformer** model ( it has both encoder and decoder). it is pre-trained on the C4 (Colossal Clean Crawled Corpus) dataset and is flexible for fine-tuning on a variety of downstream tasks. It achieves state-of-the-art results on many NLP benchmarks. T5 models can be used for several **NLP** tasks such as Summarization, translation, Q&A, text generation etc... for more info on the model refer to [this](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) article by Google.

I wanted to share this new library I've been working on and that I open-sourced!.

here are some links to the library:

💻 [GitHub Repository](https://github.com/Ki6an/fastT5)

🐍 [PyPi project](https://pypi.org/project/fastt5/)

&#x200B;

[logo](https://preview.redd.it/ez5ghvoc6xw61.png?width=1280&format=png&auto=webp&s=72a11a53b71e1954d386247b7339e2d46e8d7610)

as the title suggests, you can increase the inference speed of any pretrained T5 model and also decrease the models' size, in a single line of code.

The library can be installed with `pip install fastt5`. This code snippet from the repository's README gives a concise overview:

&#x200B;

[usage](https://preview.redd.it/85wkze0l6xw61.png?width=1496&format=png&auto=webp&s=f4acb68e3e0bb10b6ce57d79f1aa6704bf020ec5)

The fastT5 library exports the T5 model to onnx with `past_key_values`, then quantizes it and runs it on onnxruntime.

The exported onnx models support the `generate()` method of huggingface transformers for inferencing.

for more information on the project refer to the repository [here](https://github.com/Ki6an/fastT5#reduce-t5-model-size-by-3x-and-increase-the-inference-speed-up-to-5x)."
OnlyProggingForFun,MachineLearning,1620476125.0,[R] Learning to Relight Portraits based on the Background,"A novel per-pixel lighting representation in a deep learning framework, which explicitly models the diffuse and the specular components of appearance, producing relit portraits with convincingly rendered effects like specular highlights. This might be a great extension for more realistic online (Zoom) calls with a background!

[Read the article](https://www.louisbouchard.ai/backgrounds-with-lighting/) or [watch the video](https://youtu.be/rVP2tcF_yRI), whatever you prefer!

**References**  
Pandey et al., 2021, Total Relighting: Learning to Relight Portraits for Background Replacement, doi: 10.1145/3450626.3459872

https://preview.redd.it/eno436ppwvx61.png?width=1280&format=png&auto=webp&s=751d751548bba92dba7139d497524347d4a7ef8d"
pcaversaccio,MachineLearning,1617109338.0,[R] Can Vision Transformers Learn without Natural Images?,
mamrollahi,MachineLearning,1616615847.0,"[D] Compare my word embedding models (Count based, PMI, SPPMI) #","I am going to build some models over wiki-dump dataset and then try to compare the results to WS353 (for word similarity). So, I need to check whether my understanding is correct or not. Firstly, I need to read the text from wiki file and tokenize it, so that I am able to build the co-occurrence matrix. I am going to build the co-occurrence matrix in 3 ways : based on count, based on PMI and based on SPPMI. Then, I am going to build the embedding matrix using SVD. So, after that, I can have the word embedding matrix and I can compare the results to WS353.

So, the way is correct ? Thanks"
New-Psychology-1148,MachineLearning,1620287525.0,[D] Why git is not enough for data science,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

I wrote [a blog post](https://dagshub.com/blog/how-to-use-git-for-data-science/) about why Git is good but not enough for our day-to-day job as data scientists.

I think there is a lot of potential in using Git, combined with other tools, to track our code and take advantage of its features to also help us track our data and models.

What do you think about this Git+ML flow? Does anyone know any other workflow?

[https://dagshub.com/blog/how-to-use-git-for-data-science/](https://dagshub.com/blog/how-to-use-git-for-data-science/)

**TL;DR** Git is used in almost every software development project to track code and file changes. Based on this ability to track every change, there has also been a tremendous increase in Gits adoption for Data science projects. In this post we discuss;

1. Benefits of Git for data science
2. The gaps and limitations of Git
3. Best practices for using Git for data science projects"
PaganPasta,MachineLearning,1616769689.0,[D]Doubt in Bayes by Backprop,"I am trying to go through this work: [https://arxiv.org/pdf/1505.05424.pdf](https://arxiv.org/pdf/1505.05424.pdf) by Blundell et al. 

I am not very familiar with Bayesian side of things for DNNs and will try to summarize some of the stuff which I understood below for others much like me. Also, I have highlighted the things which I failed to understand. It'll be helpful if someone can clarify these.

1.   Point estimates based on MLE or MAP give you a (sort of) solution for P(w|D). 
2. P(w|D) written in bayesian form is intractable to compute. The alternative is to find a proxy for P(w|D) called as variational posterior q(w|θ) which minimizes L(θ, w) = KL(q(w|θ)||P(w|D))
3. Eq-(1) breaks it further into the known ELBO form.
4. Proposition 1. is introduced as the generalisation for the gaussian re-parameterisation trick. I can understand the intention behind it somewhat but didn't get the proof and result part. Where is the first term in the right-hand expression coming from?\[**HELP**\]
5.  L(θ, w) is estimated empirically through monte-carlo sampling. eq(2)
6. The gaussian variational posterior estimate part now applies the Proposition 1 to practice . However eq(3) and (4) stem from proposition 1. Hence, not clear to me. 
7. For the prior on w, P(w), authors suggest sampling from mixture of 2 gaussians. Also it appears that params for the priors remain constant in training. 
8. Lastly a weighting scheme for the contribution of each of the Loss params(Likelihood vs complexity). 

I have gone through couple of blog posts but proposition 1 is still unclear to me :/ Any help in appreciated. Sorry in advance if its very basic and/or common knowledge."
PsychologicalDemand0,MachineLearning,1617212778.0,[R] Explainability Guided Multi-Site COVID-19 CT Classification,"Happy to share with you our recent paper on COVID-19 detection from CT images.

Our method achieves state-of-the art results over multiple datasets with a sizable margin.

[Explainability Guided Multi-Site COVID-19 CT Classification](https://arxiv.org/abs/2103.13677)

Abstract :

Radiologist examination of chest CT is an effective way for screening COVID-19 cases. In this work, we overcome three challenges in the automation of this process: (i) the limited number of supervised positive cases, (ii) the lack of region-based supervision, and (iii) the variability across acquisition sites. These challenges are met by incorporating a recent augmentation solution called SnapMix, by a new patch embedding technique, and by performing a test-time stability analysis. The three techniques are complementary and are all based on utilizing the heatmaps produced by the Class Activation Mapping (CAM) explainability method. Compared to the current state of the art, we obtain an increase of five percent in the F1 score on a site with a relatively high number of cases, and a gap twice as large for a site with much fewer training images.

joint work with : Tal Shaharabany , Lior Wolf"
ai_researcherr,MachineLearning,1617050577.0,[Discussion] AI and Memory Wall,"A brief blogpost analyzing the overhead of training recent SOTA models, especially Transformers, arguing that \*memory\* will soon become the main bottleneck, and not training \*FLOPs\*. It would be great to have the community's feedback about this, and whether you agree/disagree with this conclusion:

[https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8](https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8)

TLDR:

\- The computational cost of training recent SOTA Transformer based models in NLP has been scaling at a rate of 750x/2yrs, and the model parameter count is scaling at 240x/2yrs.

\- In contrast, the GPU/TPU DRAM capacity has been only scaling at a rate of 2x/2yrs. In the meantime, the peak hardware FLOPS has been scaling at a rate of 3.1x/2yrs. 

\- To put these numbers into perspective, peak hardware FLOPS has increased by 90,000x, while DRAM/Interconnect bandwidth has only scaled by a factor of 30x over the past 20 years.

\- ""No exponential can continue forever"", and delaying an exponential with the rate of 240x/2yrs will not be feasible for long. We need to rethink the training, deployment, and design of AI models and hardware to deal with this increasingly challenging memory wall."
kaleb7589,MachineLearning,1617994609.0,[N] GTC 2021 Free Registration,"[FREE GTC 2021 Registration ](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH)


Sign up folks, it’s FREE, amazing talks and a key note you won’t want to miss!"
bendee983,MachineLearning,1619457034.0,[R] ThreeDWorld Transport Challenge -- Embodied AI,"New challenge by researchers at IBM, MIT, and Stanford aims to provide a realistic simulated environment to train and test RL models on task and motion planning (TAMP) problems. 

The challenge takes place in the ThreeDWorld environment, which is a visually, audibly, and physically realistic simulation. The agent is placed inside a multi-roomed house and must locate and carry objects to the specified destination within a specified number of steps. The agent is a two-armed robot. It can only carry two items simultaneously. But the environment also has containers that the agent can use to carry multiple items at once. The agent must find the right balance between exploration, carrying tasks, using containers, etc.

According to the researchers, pure end-to-end RL approaches perform poorly on the challenge. On the other hand, a hybrid approach where the RL agent is controlled by a rule-based high-level planner improves the performance (though the problem is still far from solved).

The challenge makes use of some simplifications in terms of computer vision and action/state complexity:

\- the agent uses magnetic hands and doesn't need to handle objects with fingers

\- the environment is viewed in first person, though the agent is provided with RGB, depth, and segmentation maps

\- movements and rotation are limited to specific increments (0.25m movements, 15-degree rotations)

The challenge is still open for submissions and will be presented at CVPR embedded AI workshop in June. It will be interesting to see what new innovations the challenge will usher.

Full story with comments from lead researcher:

[https://bdtechtalks.com/2021/04/26/reinforcement-learning-embodied-ai/](https://bdtechtalks.com/2021/04/26/reinforcement-learning-embodied-ai/)

Challenge website:

[http://tdw-transport.csail.mit.edu/](http://tdw-transport.csail.mit.edu/)

Gym code for TDW environment:

[https://github.com/chuangg/tdw-transport-challenge-starter-code](https://github.com/chuangg/tdw-transport-challenge-starter-code)

Arxiv paper:

[https://arxiv.org/abs/2103.14025](https://arxiv.org/abs/2103.14025)"
SomeParanoidAndroid,MachineLearning,1617579186.0,[D] Practical tips for Active Learning (my approach does not outperform random sampling),"Hello fellow practitioners,

I have a dataset for classification where the labeling process is the output of a very computationally expensive physical simulation (that I have to run for every datapoint). So I proposed to incorporate the active learning framework to limit the amount of data needed.

I am implementing the approach(es) from Yarin Gal's et al. [Deep Bayesian Active Learning with Image Data](https://arxiv.org/abs/1703.02910). In that they use their *MC Dropout* Bayesian neural network and they take advantage of its ability to quantify uncertainties in the predictions.  This allows them to apply the acquisition functions on the pool/unlabelled data and include the `argmax` datapoint into the training set at every iteration.

My problem is that all those functions are no better than randomly choosing training instances every time in my dataset.

The parameter settings that correspond to the figure I am showing are the following (should be self explanatory I believe):

    # Specific parameters for the MC Dropout CNN 
    dropout_p                     = 0.3   # Higher values make the network less certain
    reg                           = 10e-5 # Higher values make the network less certain
    MC_samples                    = 30    # The higher the number, the more accurate the predictions will be
    
    # Parameters for the Active Learning iterations
    initial_dataset_size          = 0.05 # 5% of the training dataset (that is, 220 datapoints)
    training_set_increments       = 2    # How many new datapoints to add from the pool at every iteration
    training_epochs_per_iteration = 1    # For reference, training on the full dataset converges after about 35 epochs

I have been experimenting with different values but running a comparison takes about half a day, so I would be very grateful to anyone that has dealt with such a situation before and has any pratical tips so as to choose appropriate values. Guestimates from anyone are welcome as well.

Briefly, I found out that increasing the `initial_dataset_size` to 15% or `training_epochs_per_iteration` to 5 does not provide any advantage to random sampling. Meddling with the regularization value however does help, but the results I am showing are the best I can get so far.

[Comparison of performance of different acquisition functions as a function of the size of the dataset](https://preview.redd.it/5bzraj6pr8r61.png?width=958&format=png&auto=webp&s=66a7cff96ac48cbd7cccc7cec40ee41b9b3b7cb2)"
Competitive-Rub-1958,MachineLearning,1620563385.0,[D] Are ViT's good enough for moderate/low data conditions?,"The OG `ViT` was pretty data heavy, and there were some improvements with Data efficient flavours.

But has there been any significant advances in places like CIFAR-100 without huge pre-training dataset? I would hardly classify IMAGENET21K as something that is used in the real-world.

I have about 200k images over 50 classes (Million Songs Dataset) for pre-training, and a 20K subset (MagNaTune) for fine-tuning. Should I use any of the latest Visual Transformers or standard CNN's?

Thanks for taking the time out to address my query!"
